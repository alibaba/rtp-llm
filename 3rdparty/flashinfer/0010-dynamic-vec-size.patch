diff --git a/flashinfer/jit/activation.py b/flashinfer/jit/activation.py
index e82a7df9..4c3044b4 100644
--- flashinfer/jit/activation.py
+++ flashinfer/jit/activation.py
@@ -39,7 +39,12 @@ void {{ func_name }}(at::Tensor& out, at::Tensor& input, int64_t cuda_stream) {
 
   cudaStream_t stream = reinterpret_cast<cudaStream_t>(cuda_stream);
   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
-    uint32_t vec_size = 16 / sizeof(c_type);
+    // Dynamically choose vec_size to ensure d is divisible by it
+    uint32_t max_vec_size = 16 / sizeof(c_type);
+    uint32_t vec_size = max_vec_size;
+    while (vec_size > 1 && d % vec_size != 0) {
+      vec_size /= 2;
+    }
     dim3 block(std::min(d / vec_size, 1024U));
     flashinfer::activation::act_and_mul_kernel<c_type, {{ act_func_name }}>
         <<<grid, block, 0, stream>>>(static_cast<c_type*>(out.data_ptr()),
