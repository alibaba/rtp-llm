diff --git a/flashinfer/jit/activation.py b/flashinfer/jit/activation.py
index d29166ad..33702051 100644
--- a/flashinfer/jit/activation.py
+++ b/flashinfer/jit/activation.py
@@ -33,6 +33,45 @@ using namespace flashinfer;
 
 {{ act_func_def }}
 
+// Kernel template with runtime vec_size parameter
+template <typename T, float (*Activation)(const float&), uint32_t VEC_SIZE>
+__global__ void act_and_mul_kernel_vec(T* __restrict__ out, const T* __restrict__ input, const int d) {
+  constexpr uint32_t vec_size = VEC_SIZE;
+  const int64_t token_idx = blockIdx.x;
+  const int64_t thread_idx = threadIdx.x;
+  const int64_t stride = blockDim.x;
+  const int64_t offset = token_idx * 2 * d;
+
+#if (__CUDACC_VER_MAJOR__ >= 12 && defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))
+  asm volatile("griddepcontrol.wait;");
+#endif
+
+#pragma unroll 1
+  for (uint32_t idx = thread_idx; idx < d / vec_size; idx += stride) {
+    vec_t<float, vec_size> x_vec, y_vec, out_vec;
+    x_vec.cast_load(input + offset + idx * vec_size);
+    y_vec.cast_load(input + offset + d + idx * vec_size);
+#pragma unroll
+    for (uint32_t i = 0; i < vec_size; ++i) {
+      out_vec[i] = Activation(x_vec[i]) * y_vec[i];
+    }
+    out_vec.cast_store(out + token_idx * d + idx * vec_size);
+  }
+
+  const int64_t remaining_offset = d - d % (stride * vec_size);
+  // process the remaining elements
+#pragma unroll 1
+  for (int64_t idx = thread_idx; idx < d % (stride * vec_size); idx += stride) {
+    float x = input[offset + remaining_offset + idx],
+          y = input[offset + remaining_offset + d + idx];
+    out[token_idx * d + remaining_offset + idx] = Activation(x) * y;
+  }
+
+#if (__CUDACC_VER_MAJOR__ >= 12 && defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))
+  asm volatile("griddepcontrol.launch_dependents;");
+#endif
+}
+
 void {{ func_name }}(TensorView out, TensorView input, bool enable_pdl) {
   int d = input.size(input.ndim() -1) / 2;
   int64_t num_tokens = input.numel() / input.size(input.ndim() -1);
@@ -41,7 +80,12 @@ void {{ func_name }}(TensorView out, TensorView input, bool enable_pdl) {
   cudaSetDevice(out.device().device_id);
   const cudaStream_t stream = get_stream(out.device());
   DISPATCH_DLPACK_DTYPE_TO_CTYPE_FP16(input.dtype(), c_type, [&] {
-    uint32_t vec_size = 16 / sizeof(c_type);
+    // Dynamically choose vec_size to ensure d is divisible by it
+    uint32_t max_vec_size = 16 / sizeof(c_type);
+    uint32_t vec_size = max_vec_size;
+    while (vec_size > 1 && d % vec_size != 0) {
+      vec_size /= 2;
+    }
     cudaLaunchConfig_t config;
     config.gridDim = num_tokens;
     config.blockDim = std::min(d / vec_size, 1024U);
@@ -53,10 +97,23 @@ void {{ func_name }}(TensorView out, TensorView input, bool enable_pdl) {
     config.numAttrs = 1;
     config.attrs = attrs;
 
-    auto kernel = flashinfer::activation::act_and_mul_kernel<c_type, {{ act_func_name }}>;
-
-    cudaLaunchKernelEx(&config, kernel, static_cast<c_type*>(out.data_ptr()),
-                       static_cast<c_type*>(input.data_ptr()), d);
+    // Select and launch kernel based on vec_size
+    c_type* out_ptr = static_cast<c_type*>(out.data_ptr());
+    c_type* input_ptr = static_cast<c_type*>(input.data_ptr());
+    
+    if (vec_size == 8) {
+      cudaLaunchKernelEx(&config, act_and_mul_kernel_vec<c_type, {{ act_func_name }}, 8>, 
+                         out_ptr, input_ptr, d);
+    } else if (vec_size == 4) {
+      cudaLaunchKernelEx(&config, act_and_mul_kernel_vec<c_type, {{ act_func_name }}, 4>, 
+                         out_ptr, input_ptr, d);
+    } else if (vec_size == 2) {
+      cudaLaunchKernelEx(&config, act_and_mul_kernel_vec<c_type, {{ act_func_name }}, 2>, 
+                         out_ptr, input_ptr, d);
+    } else {
+      cudaLaunchKernelEx(&config, act_and_mul_kernel_vec<c_type, {{ act_func_name }}, 1>, 
+                         out_ptr, input_ptr, d);
+    }
 
     cudaError_t err = cudaGetLastError();
     TVM_FFI_ICHECK(err == cudaSuccess) << "Failed to launch kernel: " << cudaGetErrorString(err);
