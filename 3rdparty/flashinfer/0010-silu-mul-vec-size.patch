diff --git a/csrc/activation.cu b/csrc/activation.cu
index c997708d..8b9f20f8 100644
--- csrc/activation.cu
+++ csrc/activation.cu
@@ -40,10 +40,33 @@ void silu_and_mul(at::Tensor& out, at::Tensor& input, int64_t cuda_stream) {
   cudaStream_t stream = reinterpret_cast<cudaStream_t>(cuda_stream);
   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
     uint32_t vec_size = 16 / sizeof(c_type);
+    while (vec_size > 1 && d % vec_size != 0) {
+      vec_size /= 2;
+    }
     dim3 block(std::min(d / vec_size, 1024U));
-    flashinfer::activation::act_and_mul_kernel<c_type, silu><<<grid, block, 0, stream>>>(
-        static_cast<c_type*>(out.data_ptr()), static_cast<c_type*>(input.data_ptr()), d);
-
+    switch (vec_size) {
+      case 16:
+        flashinfer::activation::act_and_mul_kernel<c_type, silu, 16><<<grid, block, 0, stream>>>(
+            static_cast<c_type*>(out.data_ptr()), static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      case 8:
+        flashinfer::activation::act_and_mul_kernel<c_type, silu, 8><<<grid, block, 0, stream>>>(
+            static_cast<c_type*>(out.data_ptr()), static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      case 4:
+        flashinfer::activation::act_and_mul_kernel<c_type, silu, 4><<<grid, block, 0, stream>>>(
+            static_cast<c_type*>(out.data_ptr()), static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      case 2:
+        flashinfer::activation::act_and_mul_kernel<c_type, silu, 2><<<grid, block, 0, stream>>>(
+            static_cast<c_type*>(out.data_ptr()), static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      case 1:
+        flashinfer::activation::act_and_mul_kernel<c_type, silu, 1><<<grid, block, 0, stream>>>(
+            static_cast<c_type*>(out.data_ptr()), static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      default: TORCH_CHECK(false, "check act_and_mul vec_size"); return false;
+    }
     return true;
   });
 }
@@ -57,7 +80,7 @@ void gelu_tanh_and_mul(at::Tensor& out, at::Tensor& input, int64_t cuda_stream)
   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
     uint32_t vec_size = 16 / sizeof(c_type);
     dim3 block(std::min(d / vec_size, 1024U));
-    flashinfer::activation::act_and_mul_kernel<c_type, gelu_tanh><<<grid, block, 0, stream>>>(
+    flashinfer::activation::act_and_mul_kernel<c_type, gelu_tanh, 16><<<grid, block, 0, stream>>>(
         static_cast<c_type*>(out.data_ptr()), static_cast<c_type*>(input.data_ptr()), d);
 
     return true;
@@ -73,7 +96,7 @@ void gelu_and_mul(at::Tensor& out, at::Tensor& input, int64_t cuda_stream) {
   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
     uint32_t vec_size = 16 / sizeof(c_type);
     dim3 block(std::min(d / vec_size, 1024U));
-    flashinfer::activation::act_and_mul_kernel<c_type, gelu><<<grid, block, 0, stream>>>(
+    flashinfer::activation::act_and_mul_kernel<c_type, gelu, 16><<<grid, block, 0, stream>>>(
         static_cast<c_type*>(out.data_ptr()), static_cast<c_type*>(input.data_ptr()), d);
 
     return true;
diff --git a/flashinfer/jit/activation.py b/flashinfer/jit/activation.py
index e82a7df9..11aac5a0 100644
--- flashinfer/jit/activation.py
+++ flashinfer/jit/activation.py
@@ -40,11 +40,38 @@ void {{ func_name }}(at::Tensor& out, at::Tensor& input, int64_t cuda_stream) {
   cudaStream_t stream = reinterpret_cast<cudaStream_t>(cuda_stream);
   DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(input.scalar_type(), c_type, [&] {
     uint32_t vec_size = 16 / sizeof(c_type);
+    while (vec_size > 1 && d % vec_size != 0) {
+      vec_size /= 2;
+    }
     dim3 block(std::min(d / vec_size, 1024U));
-    flashinfer::activation::act_and_mul_kernel<c_type, {{ act_func_name }}>
-        <<<grid, block, 0, stream>>>(static_cast<c_type*>(out.data_ptr()),
-                                     static_cast<c_type*>(input.data_ptr()), d);
-
+    switch (vec_size) {
+      case 16:
+        flashinfer::activation::act_and_mul_kernel<c_type, {{ act_func_name }}, 16>
+            <<<grid, block, 0, stream>>>(static_cast<c_type*>(out.data_ptr()),
+                                         static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      case 8:
+        flashinfer::activation::act_and_mul_kernel<c_type, {{ act_func_name }}, 8>
+            <<<grid, block, 0, stream>>>(static_cast<c_type*>(out.data_ptr()),
+                                         static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      case 4:
+        flashinfer::activation::act_and_mul_kernel<c_type, {{ act_func_name }}, 4>
+            <<<grid, block, 0, stream>>>(static_cast<c_type*>(out.data_ptr()),
+                                         static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      case 2:
+        flashinfer::activation::act_and_mul_kernel<c_type, {{ act_func_name }}, 2>
+            <<<grid, block, 0, stream>>>(static_cast<c_type*>(out.data_ptr()),
+                                         static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      case 1:
+        flashinfer::activation::act_and_mul_kernel<c_type, {{ act_func_name }}, 1>
+            <<<grid, block, 0, stream>>>(static_cast<c_type*>(out.data_ptr()),
+                                         static_cast<c_type*>(input.data_ptr()), d);
+        break;
+      default: TORCH_CHECK(false, "check act_and_mul vec_size"); return false;
+    }
     return true;
   });
 }
diff --git a/include/flashinfer/activation.cuh b/include/flashinfer/activation.cuh
index 0fc6715c..69b5d262 100644
--- include/flashinfer/activation.cuh
+++ include/flashinfer/activation.cuh
@@ -25,9 +25,8 @@ namespace flashinfer {
 
 namespace activation {
 
-template <typename T, float (*Activation)(const float&)>
+template <typename T, float (*Activation)(const float&), uint32_t vec_size>
 __global__ void act_and_mul_kernel(T* __restrict__ out, const T* __restrict__ input, const int d) {
-  constexpr uint32_t vec_size = 16 / sizeof(T);
   const int64_t token_idx = blockIdx.x;
   const int64_t thread_idx = threadIdx.x;
   const int64_t stride = blockDim.x;
