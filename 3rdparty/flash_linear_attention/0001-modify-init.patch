diff --git fla/__init__.py fla/__init__.py
index d74295e0..e69de29b 100644
--- fla/__init__.py
+++ fla/__init__.py
@@ -1,111 +0,0 @@
-# -*- coding: utf-8 -*-
-
-from fla.layers import (
-    ABCAttention,
-    Attention,
-    BasedLinearAttention,
-    BitAttention,
-    Comba,
-    DeltaFormerAttention,
-    DeltaNet,
-    GatedDeltaNet,
-    GatedDeltaProduct,
-    GatedLinearAttention,
-    GatedSlotAttention,
-    HGRN2Attention,
-    HGRNAttention,
-    LightNetAttention,
-    LinearAttention,
-    LogLinearMamba2,
-    MesaNet,
-    MomAttention,
-    MultiheadLatentAttention,
-    MultiScaleRetention,
-    NativeSparseAttention,
-    PaTHAttention,
-    ReBasedLinearAttention,
-    RodimusAttention,
-    RWKV6Attention,
-    RWKV7Attention
-)
-from fla.models import (
-    ABCForCausalLM,
-    ABCModel,
-    BitNetForCausalLM,
-    BitNetModel,
-    CombaForCausalLM,
-    CombaModel,
-    DeltaFormerForCausalLM,
-    DeltaFormerModel,
-    DeltaNetForCausalLM,
-    DeltaNetModel,
-    GatedDeltaNetForCausalLM,
-    GatedDeltaNetModel,
-    GatedDeltaProductForCausalLM,
-    GatedDeltaProductModel,
-    GLAForCausalLM,
-    GLAModel,
-    GSAForCausalLM,
-    GSAModel,
-    HGRN2ForCausalLM,
-    HGRN2Model,
-    HGRNForCausalLM,
-    HGRNModel,
-    LightNetForCausalLM,
-    LightNetModel,
-    LinearAttentionForCausalLM,
-    LinearAttentionModel,
-    LogLinearMamba2ForCausalLM,
-    LogLinearMamba2Model,
-    MesaNetForCausalLM,
-    MesaNetModel,
-    MLAForCausalLM,
-    MLAModel,
-    MomForCausalLM,
-    MomModel,
-    NSAForCausalLM,
-    NSAModel,
-    PaTHAttentionForCausalLM,
-    PaTHAttentionModel,
-    RetNetForCausalLM,
-    RetNetModel,
-    RodimusForCausalLM,
-    RodimusModel,
-    RWKV6ForCausalLM,
-    RWKV6Model,
-    RWKV7ForCausalLM,
-    RWKV7Model,
-    TransformerForCausalLM,
-    TransformerModel
-)
-
-__all__ = [
-    'ABCAttention', 'ABCForCausalLM', 'ABCModel',
-    'Attention', 'TransformerForCausalLM', 'TransformerModel',
-    'BasedLinearAttention',
-    'BitAttention', 'BitNetForCausalLM', 'BitNetModel',
-    'Comba', 'CombaForCausalLM', 'CombaModel',
-    'DeltaNet', 'DeltaNetForCausalLM', 'DeltaNetModel',
-    'DeltaFormerAttention', 'DeltaFormerForCausalLM', 'DeltaFormerModel',
-    'GatedDeltaNet', 'GatedDeltaNetForCausalLM', 'GatedDeltaNetModel',
-    'GatedDeltaProduct', 'GatedDeltaProductForCausalLM', 'GatedDeltaProductModel',
-    'GatedLinearAttention', 'GLAForCausalLM', 'GLAModel',
-    'GatedSlotAttention', 'GSAForCausalLM', 'GSAModel',
-    'HGRNAttention', 'HGRNForCausalLM', 'HGRNModel',
-    'HGRN2Attention', 'HGRN2ForCausalLM', 'HGRN2Model',
-    'LightNetAttention', 'LightNetForCausalLM', 'LightNetModel',
-    'LinearAttention', 'LinearAttentionForCausalLM', 'LinearAttentionModel',
-    'LogLinearMamba2', 'LogLinearMamba2ForCausalLM', 'LogLinearMamba2Model',
-    'MesaNet', 'MesaNetForCausalLM', 'MesaNetModel',
-    'MomAttention', 'MomForCausalLM', 'MomModel',
-    'MultiheadLatentAttention', 'MLAForCausalLM', 'MLAModel',
-    'MultiScaleRetention', 'RetNetForCausalLM', 'RetNetModel',
-    'NativeSparseAttention', 'NSAForCausalLM', 'NSAModel',
-    'PaTHAttention', 'PaTHAttentionForCausalLM', 'PaTHAttentionModel',
-    'ReBasedLinearAttention',
-    'RodimusAttention', 'RodimusForCausalLM', 'RodimusModel',
-    'RWKV6Attention', 'RWKV6ForCausalLM', 'RWKV6Model',
-    'RWKV7Attention', 'RWKV7ForCausalLM', 'RWKV7Model',
-]
-
-__version__ = '0.4.0'
diff --git fla/ops/__init__.py fla/ops/__init__.py
index e45884b6..539d191e 100644
--- fla/ops/__init__.py
+++ fla/ops/__init__.py
@@ -1,53 +1,2 @@
 # -*- coding: utf-8 -*-
-
-from .abc import chunk_abc
-from .attn import parallel_attn
-from .based import fused_chunk_based, parallel_based
-from .comba import chunk_comba, fused_recurrent_comba
-from .delta_rule import chunk_delta_rule, fused_chunk_delta_rule, fused_recurrent_delta_rule
-from .forgetting_attn import parallel_forgetting_attn
-from .gated_delta_rule import chunk_gated_delta_rule, fused_recurrent_gated_delta_rule
-from .generalized_delta_rule import (
-    chunk_dplr_delta_rule,
-    chunk_iplr_delta_rule,
-    fused_recurrent_dplr_delta_rule,
-    fused_recurrent_iplr_delta_rule
-)
-from .gla import chunk_gla, fused_chunk_gla, fused_recurrent_gla
-from .gsa import chunk_gsa, fused_recurrent_gsa
-from .hgrn import fused_recurrent_hgrn
-from .lightning_attn import chunk_lightning_attn, fused_recurrent_lightning_attn
-from .linear_attn import chunk_linear_attn, fused_chunk_linear_attn, fused_recurrent_linear_attn
-from .log_linear_attn import chunk_log_linear_attn
-from .mesa_net import chunk_mesa_net
-from .nsa import parallel_nsa
-from .path_attn import parallel_path_attn
-from .retention import chunk_retention, fused_chunk_retention, fused_recurrent_retention, parallel_retention
-from .rwkv6 import chunk_rwkv6, fused_recurrent_rwkv6
-from .rwkv7 import chunk_rwkv7, fused_recurrent_rwkv7
-from .simple_gla import chunk_simple_gla, fused_chunk_simple_gla, fused_recurrent_simple_gla, parallel_simple_gla
-
-__all__ = [
-    'chunk_abc',
-    'parallel_attn',
-    'fused_chunk_based', 'parallel_based',
-    'chunk_delta_rule', 'fused_chunk_delta_rule', 'fused_recurrent_delta_rule',
-    'parallel_forgetting_attn',
-    'chunk_gated_delta_rule', 'fused_recurrent_gated_delta_rule',
-    'chunk_comba', 'fused_recurrent_comba',
-    'chunk_dplr_delta_rule', 'chunk_iplr_delta_rule',
-    'fused_recurrent_dplr_delta_rule', 'fused_recurrent_iplr_delta_rule',
-    'chunk_gla', 'fused_chunk_gla', 'fused_recurrent_gla',
-    'chunk_gsa', 'fused_recurrent_gsa',
-    'fused_recurrent_hgrn',
-    'chunk_lightning_attn', 'fused_recurrent_lightning_attn',
-    'chunk_linear_attn', 'fused_chunk_linear_attn', 'fused_recurrent_linear_attn',
-    'chunk_log_linear_attn',
-    'chunk_mesa_net',
-    'parallel_nsa',
-    'parallel_path_attn',
-    'chunk_retention', 'fused_chunk_retention', 'fused_recurrent_retention', 'parallel_retention',
-    'chunk_rwkv6', 'fused_recurrent_rwkv6',
-    'chunk_rwkv7', 'fused_recurrent_rwkv7',
-    'chunk_simple_gla', 'fused_chunk_simple_gla', 'fused_recurrent_simple_gla', 'parallel_simple_gla',
-]
+from .gated_delta_rule import chunk_gated_delta_rule, fused_recurrent_gated_delta_rule
\ No newline at end of file
diff --git tests/ops/test_gated_delta.py tests/ops/test_gated_delta.py
index 44e1ae15..ca4e6647 100644
--- tests/ops/test_gated_delta.py
+++ tests/ops/test_gated_delta.py
@@ -8,6 +8,7 @@ import torch
 import torch.nn.functional as F
 from einops import rearrange, repeat
 
+import sys; sys.path.append("/home/baowending.bwd/flash-linear-attention")
 from fla.ops.gated_delta_rule import chunk_gated_delta_rule, fused_recurrent_gated_delta_rule
 from fla.utils import assert_close, device, is_intel_alchemist
 
