--- csrc/kernels/activation_kernels.cu
+++ csrc/kernels/activation_kernels.cu
@@ -18,6 +18,7 @@
 #include <torch/extension.h>
 #include <c10/cuda/CUDAGuard.h>

+#include <cassert>
 #include <cmath>

 #include "hip_compat.h"
@@ -27,6 +28,88 @@

 using fp8_type = ck_tile::fp8_t;

+namespace {
+#define FLASHINFER_INLINE inline __attribute__((always_inline)) __device__
+
+template <typename float_t, size_t vec_size>
+struct vec_t {
+  FLASHINFER_INLINE float_t& operator[](size_t i);
+  FLASHINFER_INLINE const float_t& operator[](size_t i) const;
+  FLASHINFER_INLINE void load(const float_t* ptr);
+  FLASHINFER_INLINE void store(float_t* ptr) const;
+  FLASHINFER_INLINE float_t* ptr();
+};
+
+template <size_t vec_size>
+struct vec_t<c10::BFloat16, vec_size> {
+  static_assert(vec_size % 8 == 0, "Invalid vector size");
+  int4 data[vec_size / 8];
+
+  FLASHINFER_INLINE c10::BFloat16& operator[](size_t i) { return ((c10::BFloat16*)data)[i]; }
+  FLASHINFER_INLINE const c10::BFloat16& operator[](size_t i) const {
+    return ((const c10::BFloat16*)data)[i];
+  }
+  FLASHINFER_INLINE c10::BFloat16* ptr() { return reinterpret_cast<c10::BFloat16*>(&data); }
+  FLASHINFER_INLINE void load(const c10::BFloat16* ptr) {
+#pragma unoll
+    for (size_t i = 0; i < vec_size / 8; ++i) {
+      data[i] = ((int4*)ptr)[i];
+    }
+  }
+  FLASHINFER_INLINE void store(c10::BFloat16* ptr) const {
+#pragma unoll
+    for (size_t i = 0; i < vec_size / 8; ++i) {
+      ((int4*)ptr)[i] = data[i];
+    }
+  }
+};
+
+
+template <size_t vec_size>
+struct vec_t<c10::Half, vec_size> {
+  static_assert(vec_size % 8 == 0, "Invalid vector size");
+  int4 data[vec_size / 8];
+  FLASHINFER_INLINE c10::Half& operator[](size_t i) { return ((c10::Half*)data)[i]; }
+  FLASHINFER_INLINE const c10::Half& operator[](size_t i) const { return ((const c10::Half*)data)[i]; }
+  FLASHINFER_INLINE c10::Half* ptr() { return reinterpret_cast<c10::Half*>(&data); }
+  FLASHINFER_INLINE void load(const c10::Half* ptr) {
+#pragma unroll
+    for (size_t i = 0; i < vec_size / 8; ++i) {
+      data[i] = ((int4*)ptr)[i];
+    }
+  }
+  FLASHINFER_INLINE void store(c10::Half* ptr) const {
+#pragma unroll
+    for (size_t i = 0; i < vec_size / 8; ++i) {
+      ((int4*)ptr)[i] = data[i];
+    }
+  }
+};
+
+template <size_t vec_size>
+struct vec_t<float, vec_size> {
+  static_assert(vec_size % 4 == 0, "Invalid vector size");
+  float4 data[vec_size / 4];
+
+  FLASHINFER_INLINE float& operator[](size_t i) { return ((float*)(data))[i]; }
+  FLASHINFER_INLINE const float& operator[](size_t i) const { return ((const float*)(data))[i]; }
+  FLASHINFER_INLINE float* ptr() { return reinterpret_cast<float*>(&data); }
+  FLASHINFER_INLINE void load(const float* ptr) {
+#pragma unroll
+    for (size_t i = 0; i < vec_size / 4; ++i) {
+      data[i] = ((float4*)ptr)[i];
+    }
+  }
+  FLASHINFER_INLINE void store(float* ptr) const {
+#pragma unroll
+    for (size_t i = 0; i < vec_size / 4; ++i) {
+      ((float4*)ptr)[i] = data[i];
+    }
+  }
+};
+
+}
+
 namespace vllm
 {

@@ -37,12 +120,25 @@
       const scalar_t *__restrict__ input, // [..., 2, d]
       const int d)
   {
+    constexpr uint32_t vec_size = 16 / sizeof(scalar_t);
     const int64_t token_idx = blockIdx.x;
-    for (int64_t idx = threadIdx.x; idx < d; idx += blockDim.x)
-    {
-      const scalar_t x = VLLM_LDG(&input[token_idx * 2 * d + idx]);
-      const scalar_t y = VLLM_LDG(&input[token_idx * 2 * d + d + idx]);
-      out[token_idx * d + idx] = ACT_FN(x) * y;
+    const int64_t thread_idx = threadIdx.x;
+    const int64_t stride = blockDim.x;
+    const int64_t offset = token_idx * 2 * d;
+    const scalar_t* x_ptr = input + offset;
+    const scalar_t* y_ptr = x_ptr + d;
+    const int64_t iters = d / vec_size;
+    out += token_idx * d;
+
+    for (uint32_t idx = thread_idx; idx < iters; idx += stride) {
+      vec_t<scalar_t, vec_size> x_vec, y_vec, out_vec;
+      x_vec.load(x_ptr + idx * vec_size);
+      y_vec.load(y_ptr + idx * vec_size);
+      #pragma unroll
+      for (uint32_t i = 0; i < vec_size; ++i) {
+        out_vec[i] = ACT_FN(x_vec[i]) * y_vec[i];
+      }
+     out_vec.store(out + idx * vec_size);
     }
   }

@@ -105,6 +201,7 @@
   dim3 block(std::min(d, 1024));                                                                                  \
   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));                                               \
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();                                                   \
+  assert(d % 8 == 0);                                                                                             \
   VLLM_DISPATCH_FLOATING_TYPES(                                                                                   \
       input.scalar_type(), "act_and_mul_kernel", [&] { vllm::act_and_mul_kernel<scalar_t, KERNEL<scalar_t>>       \
                                                            <<<grid, block, 0, stream>>>(out.data_ptr<scalar_t>(), \

--- aiter/jit/optCompilerConfig.json
+++ aiter/jit/optCompilerConfig.json
@@ -17,7 +17,7 @@
             "f'{AITER_CSRC_DIR}/kernels/activation_kernels.cu'"
         ],
         "flags_extra_cc": [],
-        "flags_extra_hip": [],
+        "flags_extra_hip": ["'-ffast-math'"],
         "extra_ldflags": "None",
         "extra_include": [
             "f'{AITER_CSRC_DIR}/include/ck_tile'"

