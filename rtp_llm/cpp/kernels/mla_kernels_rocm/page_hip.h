// !!! This is a file automatically generated by hipify!!!
#include "hip/hip_runtime.h"
#include "rtp_llm/cpp/kernels/mla_kernels_rocm/utils.h"
#include "rtp_llm/cpp/kernels/mla_kernels_rocm/exception.h"
#include "rtp_llm/cpp/kernels/mla_kernels_rocm/fastdiv.h"
#include "rtp_llm/cpp/kernels/mla_kernels_rocm/pytorch_extension_utils_hip.h"
#include "rtp_llm/cpp/kernels/rocm_utils/vec_dtypes_hip.h"
#include <hip/hip_fp16.h>

using namespace rtp_llm;
namespace rtp_llm {

#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x " must be a CUDA tensor")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")

#define CHECK_LAST_DIM_CONTIGUOUS(x)                                                                                   \
    TORCH_CHECK(x.strides()[x.strides().size() - 1] == 1, #x "must be contiguous at last dimension")

#define CHECK_INPUT(x)                                                                                                 \
    CHECK_CUDA(x);                                                                                                     \
    CHECK_CONTIGUOUS(x)
#define CHECK_LAST_DIM_CONTIGUOUS_INPUT(x)                                                                             \
    CHECK_CUDA(x);                                                                                                     \
    CHECK_LAST_DIM_CONTIGUOUS(x)

#define CHECK_DIM(d, x) TORCH_CHECK(x.dim() == d, #x " must be a " #d "D tensor")
#define CHECK_EQ(a, b) TORCH_CHECK((a) == (b), "CHECK_EQ(" #a ", " #b ") failed. ", a, " vs ", b)

template<typename T1, typename T2>
__forceinline__ __device__ __host__ T1 ceil_div(const T1 x, const T2 y) {
    return (x + y - 1) / y;
}

template<typename DType, typename IdType>
struct paged_kv_mla_t {
    uint_fastdiv page_size;
    uint32_t     head_dim_ckv;
    uint32_t     head_dim_kpe;
    uint32_t     batch_size;
    uint32_t     stride_page_ckv;
    uint32_t     stride_page_kpe;
    uint32_t     stride_n_ckv;
    uint32_t     stride_n_kpe;

    // Internal layout:
    // [max_num_pages, page_size, head_dim]
    DType*  ckv_data;
    DType*  kpe_data;
    IdType* indices;

    // [batch_size + 1] The page indptr array, with the first element 0, the last element nnz_pages
    IdType* indptr;
    // [batch_size] The offset of the last page for each request in the batch
    IdType* last_page_len;
    // [batch_size] The start position of each request in the batch.
    IdType* rope_pos_offset;

    /*!
     * \brief Construct an empty paged key-value cache
     */
    __host__ __device__ __forceinline__ paged_kv_mla_t():
        head_dim_ckv(0),
        head_dim_kpe(0),
        batch_size(0),
        stride_page_ckv(0),
        stride_page_kpe(0),
        stride_n_ckv(0),
        stride_n_kpe(0),
        ckv_data(nullptr),
        kpe_data(nullptr),
        indices(nullptr),
        indptr(nullptr),
        last_page_len(nullptr),
        rope_pos_offset(nullptr) {}

    /*!
     * \brief Construct a paged mla kv cache
     * \param page_size The size of each page
     * \param head_dim_compressed_kv The dimension of compressed-kv
     * \param head_dim_kpe The dimension of k-pe
     * \param batch_size The batch size
     * \param compressed_kv_data The start pointer of compressed-kv cache, cache should be contiguous
     * \param kpe_data The start pointer of k-pe cache, cache should be contiguous
     * \param indices The page indices array
     * \param indptr The page indptr array
     * \param last_page_len The offset of the last page for each request in the batch
     * \param rope_pos_offset The start position of each request in the batch.
     */
    __host__ __forceinline__ paged_kv_mla_t(uint32_t page_size,
                                            uint32_t head_dim_compressed_kv,
                                            uint32_t head_dim_kpe,
                                            uint32_t batch_size,
                                            DType*   compressed_kv_data,
                                            DType*   kpe_data,
                                            IdType*  indices,
                                            IdType*  indptr,
                                            IdType*  last_page_len,
                                            IdType*  rope_pos_offset = nullptr):
        page_size(page_size),
        head_dim_ckv(head_dim_compressed_kv),
        head_dim_kpe(head_dim_kpe),
        batch_size(batch_size),
        ckv_data(compressed_kv_data),
        kpe_data(kpe_data),
        indices(indices),
        indptr(indptr),
        last_page_len(last_page_len),
        rope_pos_offset(rope_pos_offset) {
        stride_page_ckv = page_size * head_dim_ckv;
        stride_n_ckv    = head_dim_ckv;
        stride_page_kpe = page_size * head_dim_kpe;
        stride_n_kpe    = head_dim_kpe;
    }

    /*!
     * \brief Construct a paged key-value cache with custom kv-cache strides
     * \param page_size The size of each page
     * \param head_dim_compressed_kv The dimension of compressed-kv
     * \param head_dim_kpe The dimension of k-pe
     * \param batch_size The batch size
     * \param compressed_kv_data The start pointer of compressed-kv cache, cache should be contiguous
     * \param compressed_kv_strides custom strides of each dimensions of compressed-kv cache
     * \param kpe_data The start pointer of k-pe cache, cache should be contiguous
     * \param kpe_strides custom strides of each dimensions of k-pe cache
     * \param indices The page indices array
     * \param indptr The page indptr array
     * \param last_page_len The offset of the last page for each request in the batch
     * \param rope_pos_offset The start position of each request in the batch.
     */
    __host__ __forceinline__ paged_kv_mla_t(uint32_t       page_size,
                                            uint32_t       head_dim_compressed_kv,
                                            uint32_t       head_dim_kpe,
                                            uint32_t       batch_size,
                                            DType*         compressed_kv_data,
                                            const int64_t* compressed_kv_strides,
                                            DType*         kpe_data,
                                            const int64_t* kpe_strides,
                                            IdType*        indices,
                                            IdType*        indptr,
                                            IdType*        last_page_len,
                                            IdType*        rope_pos_offset = nullptr):
        page_size(page_size),
        head_dim_ckv(head_dim_compressed_kv),
        head_dim_kpe(head_dim_kpe),
        batch_size(batch_size),
        ckv_data(compressed_kv_data),
        kpe_data(kpe_data),
        indices(indices),
        indptr(indptr),
        last_page_len(last_page_len),
        rope_pos_offset(rope_pos_offset) {
        stride_page_ckv = compressed_kv_strides[0];
        stride_n_ckv    = compressed_kv_strides[1];
        stride_page_kpe = kpe_strides[0];
        stride_n_kpe    = kpe_strides[1];
    }

    __host__ __device__ __forceinline__ size_t get_elem_offset_ckv(size_t page_idx,
                                                                   size_t entry_idx,
                                                                   size_t feat_idx) const {
        return page_idx * stride_page_ckv + entry_idx * stride_n_ckv + feat_idx;
    }

    __device__ __forceinline__ DType* get_ckv_ptr(size_t page_idx, size_t entry_idx, size_t feat_idx) const {
        return ckv_data + get_elem_offset_ckv(__ldg(indices + page_idx), entry_idx, feat_idx);
    }

    __host__ __device__ __forceinline__ size_t get_elem_offset_kpe(size_t page_idx,
                                                                   size_t entry_idx,
                                                                   size_t feat_idx) const {
        return page_idx * stride_page_kpe + entry_idx * stride_n_kpe + feat_idx;
    }

    __device__ __forceinline__ DType* get_kpe_ptr(size_t page_idx, size_t entry_idx, size_t feat_idx) const {
        return kpe_data + get_elem_offset_kpe(__ldg(indices + page_idx), entry_idx, feat_idx);
    }
};

template<uint32_t head_dim_ckv, uint32_t head_dim_kpe, uint32_t vec_size, typename DType, typename IdType>
__global__ void AppendPagedKVMlaCacheKernel(paged_kv_mla_t<DType, IdType> paged_kv_mla,
                                            DType* __restrict__ append_ckv,
                                            DType* __restrict__ append_kpe,
                                            IdType* __restrict__ batch_indices,
                                            IdType* __restrict__ positions,
                                            uint32_t nnz,
                                            size_t   append_ckv_stride_n,
                                            size_t   append_kpe_stride_n) {
    uint32_t tx       = threadIdx.x;
    uint32_t cta_id   = blockIdx.x;
    uint32_t num_ctas = gridDim.x;

#pragma unroll 4
    for (uint32_t i = cta_id; i < nnz; i += num_ctas) {
        uint32_t page_iter, entry_idx;
        paged_kv_mla.page_size.divmod(
            paged_kv_mla.indptr[batch_indices[i]] * paged_kv_mla.page_size + positions[i], page_iter, entry_idx);
        DType* ckv_ptr = paged_kv_mla.get_ckv_ptr(page_iter, entry_idx, tx * vec_size);
        vec_t<DType, vec_size>::memcpy(ckv_ptr, append_ckv + i * append_ckv_stride_n + tx * vec_size);

        if (tx * vec_size < head_dim_kpe) {
            DType* kpe_ptr = paged_kv_mla.get_kpe_ptr(page_iter, entry_idx, tx * vec_size);
            vec_t<DType, vec_size>::memcpy(kpe_ptr, append_kpe + i * append_kpe_stride_n + tx * vec_size);
        }
    }
}

template<typename DType, typename IdType>
hipError_t AppendPagedKVMlaCache(paged_kv_mla_t<DType, IdType> paged_kv,
                                 DType*                        append_ckv,
                                 DType*                        append_kpe,
                                 IdType*                       batch_indices,
                                 IdType*                       positions,
                                 uint32_t                      nnz,
                                 size_t                        append_ckv_stride_n,
                                 size_t                        append_kpe_stride_n,
                                 hipStream_t                   stream = nullptr) {
    int dev_id            = 0;
    int num_sms           = 0;
    int num_blocks_per_sm = 0;
    FLASHINFER_CUDA_CALL(hipGetDevice(&dev_id));
    FLASHINFER_CUDA_CALL(hipDeviceGetAttribute(&num_sms, hipDeviceAttributeMultiprocessorCount, dev_id));

    uint32_t           head_dim_ckv = paged_kv.head_dim_ckv;
    uint32_t           head_dim_kpe = paged_kv.head_dim_kpe;
    constexpr uint32_t HEAD_CKV_DIM = 512;
    constexpr uint32_t HEAD_KPE_DIM = 64;
    FLASHINFER_CHECK(head_dim_ckv == HEAD_CKV_DIM, "head_dim_ckv must be equal to 512");
    FLASHINFER_CHECK(head_dim_kpe == HEAD_KPE_DIM, "head_dim_kpe must be equal to 64");
    constexpr uint32_t vec_size = 2;

    uint32_t bdx         = HEAD_CKV_DIM / vec_size;
    uint32_t num_threads = bdx;
    uint32_t smem_size   = 0;
    auto     kernel      = AppendPagedKVMlaCacheKernel<HEAD_CKV_DIM, HEAD_KPE_DIM, vec_size, DType, IdType>;
    FLASHINFER_CUDA_CALL(
        hipOccupancyMaxActiveBlocksPerMultiprocessor(&num_blocks_per_sm, kernel, num_threads, smem_size));
    num_blocks_per_sm = min(num_blocks_per_sm, ceil_div(int(nnz), num_sms));
    dim3  nblks(num_blocks_per_sm * num_sms);
    dim3  nthrs(bdx);
    void* args[] = {(void*)&paged_kv,
                    (void*)&append_ckv,
                    (void*)&append_kpe,
                    (void*)&batch_indices,
                    (void*)&positions,
                    (void*)&nnz,
                    (void*)&append_ckv_stride_n,
                    (void*)&append_kpe_stride_n};
    FLASHINFER_CUDA_CALL(cudaLaunchKernel((void*)kernel, nblks, nthrs, args, 0, stream));
    return hipSuccess;
}

void append_paged_mla_kv_cache(at::Tensor append_ckv,
                               at::Tensor append_kpe,
                               at::Tensor batch_indices,
                               at::Tensor positions,
                               at::Tensor ckv_cache,
                               at::Tensor kpe_cache,
                               at::Tensor kv_indices,
                               at::Tensor kv_indptr,
                               at::Tensor kv_last_page_len,
                               int64_t    cuda_stream) {

    CHECK_LAST_DIM_CONTIGUOUS(append_ckv);
    CHECK_LAST_DIM_CONTIGUOUS(append_kpe);
    CHECK_INPUT(batch_indices);
    CHECK_INPUT(positions);
    // NOTE(Zihao): doesn't have to be contiguous
    CHECK_LAST_DIM_CONTIGUOUS_INPUT(ckv_cache);
    CHECK_LAST_DIM_CONTIGUOUS_INPUT(kpe_cache);
    CHECK_INPUT(kv_indices);
    CHECK_INPUT(kv_indptr);
    CHECK_INPUT(kv_last_page_len);
    CHECK_DIM(2, append_ckv);
    CHECK_DIM(2, append_kpe);
    CHECK_DIM(1, batch_indices);
    CHECK_DIM(1, positions);
    CHECK_DIM(3, ckv_cache);
    CHECK_DIM(3, kpe_cache);
    CHECK_DIM(1, kv_indices);
    CHECK_DIM(1, kv_indptr);
    CHECK_DIM(1, kv_last_page_len);
    unsigned int nnz        = append_ckv.size(0);
    unsigned int batch_size = kv_last_page_len.size(0);
    CHECK_EQ(kv_indptr.size(0), batch_size + 1);
    CHECK_EQ(batch_indices.size(0), nnz);
    CHECK_EQ(positions.size(0), nnz);
    auto device = append_ckv.device();
    CHECK_EQ(append_ckv.device(), device);
    CHECK_EQ(append_kpe.device(), device);
    CHECK_EQ(ckv_cache.device(), device);

    CHECK_EQ(kv_indices.device(), device);
    CHECK_EQ(kv_indptr.device(), device);
    CHECK_EQ(kv_last_page_len.device(), device);

    unsigned int page_size, ckv_dim, kpe_dim;
    page_size = ckv_cache.size(1);
    ckv_dim   = ckv_cache.size(2);
    kpe_dim   = kpe_cache.size(2);

    // get kv_cache_strides
    const int64_t* ckv_strides = ckv_cache.strides().data();
    const int64_t* kpe_strides = kpe_cache.strides().data();

    auto append_ckv_strides  = append_ckv.strides();
    auto append_ckv_stride_n = append_ckv_strides[0];
    auto append_kpe_strides  = append_kpe.strides();
    auto append_kpe_stride_n = append_kpe_strides[0];

    CHECK_EQ(append_ckv.size(1), ckv_dim);
    CHECK_EQ(append_kpe.size(1), kpe_dim);

    auto kv_scalar_dtype = ckv_cache.scalar_type();

    // const c10::hip::OptionalHIPGuardMasqueradingAsCUDA device_guard(device);
    // auto stream = at::hip::getCurrentHIPStreamMasqueradingAsCUDA();

    cudaStream_t stream = reinterpret_cast<cudaStream_t>(cuda_stream);

    // using c_type = __half;
    bool success = DISPATCH_PYTORCH_DTYPE_TO_CTYPE(kv_scalar_dtype, c_type, [&] {
        paged_kv_mla_t<c_type, int32_t> paged_mla_kv(page_size,
                                                     ckv_dim,
                                                     kpe_dim,
                                                     batch_size,
                                                     static_cast<c_type*>(ckv_cache.data_ptr()),
                                                     ckv_strides,
                                                     static_cast<c_type*>(kpe_cache.data_ptr()),
                                                     kpe_strides,
                                                     static_cast<int32_t*>(kv_indices.data_ptr()),
                                                     static_cast<int32_t*>(kv_indptr.data_ptr()),
                                                     static_cast<int32_t*>(kv_last_page_len.data_ptr()));

        hipError_t status = AppendPagedKVMlaCache(paged_mla_kv,
                                                  static_cast<c_type*>(append_ckv.data_ptr()),
                                                  static_cast<c_type*>(append_kpe.data_ptr()),
                                                  static_cast<int32_t*>(batch_indices.data_ptr()),
                                                  static_cast<int32_t*>(positions.data_ptr()),
                                                  nnz,
                                                  append_ckv_stride_n,
                                                  append_kpe_stride_n,
                                                  stream);

        TORCH_CHECK(status == hipSuccess, "AppendPagedKVMlaCache failed with error: ", hipGetErrorString(status));
        return true;
    });

    TORCH_CHECK(success, "AppendPagedKVMlaCache failed to dispatch with dtype ", kv_scalar_dtype);
}

};  // namespace rtp_llm
