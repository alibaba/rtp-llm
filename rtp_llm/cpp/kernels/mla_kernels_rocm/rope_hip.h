// !!! This is a file automatically generated by hipify!!!
#include "hip/hip_runtime.h"
#include "rtp_llm/cpp/kernels/mla_kernels_rocm/layout.h"
#include "rtp_llm/cpp/kernels/mla_kernels_rocm/utils.h"
#include "rtp_llm/cpp/kernels/mla_kernels_rocm/exception.h"
#include "rtp_llm/cpp/kernels/mla_kernels_rocm/pytorch_extension_utils_hip.h"
#include "rtp_llm/cpp/kernels/rocm_utils/vec_dtypes_hip.h"

using namespace std;
using namespace rtp_llm;

namespace rtp_llm {

template<uint32_t vec_size, uint32_t bdx, typename T>
__device__ __forceinline__ vec_t<float, vec_size>
                           vec_apply_llama_rope_cos_sin_interleave_reuse_half(const T*                      x,
                                                                              const vec_t<float, vec_size>& cos,
                                                                              const vec_t<float, vec_size>& sin,
                                                                              const uint32_t                rotary_dim = vec_size * bdx) {
    vec_t<float, vec_size> vec, vec_before;
    vec.cast_load(x + threadIdx.x * vec_size);

    if (threadIdx.x * vec_size < rotary_dim) {
        vec_before = vec;
#pragma unroll
        for (uint32_t i = 0; i < vec_size; ++i) {
            // i / 2 is to get the index of the first half of cos and sin
            vec[i] = vec[i] * cos[i / 2] + ((i % 2 == 0) ? -vec_before[i ^ 1] : vec_before[i ^ 1]) * sin[i / 2];
        }
    }
    return vec;
}

template<uint32_t vec_size, uint32_t bdx, typename T>
__device__ __forceinline__ vec_t<float, vec_size> vec_apply_llama_rope_cos_sin(const T*                      x,
                                                                               const vec_t<float, vec_size>& cos,
                                                                               const vec_t<float, vec_size>& sin,
                                                                               const uint32_t rotary_dim = vec_size
                                                                                                           * bdx) {
    vec_t<float, vec_size> permuted_vec, vec;
    vec.cast_load(x + threadIdx.x * vec_size);

    if (threadIdx.x * vec_size < rotary_dim) {
        permuted_vec.cast_load(x
                               + ((threadIdx.x * vec_size < rotary_dim / 2) ? threadIdx.x * vec_size + rotary_dim / 2 :
                                                                              threadIdx.x * vec_size - rotary_dim / 2));
#pragma unroll
        for (uint32_t i = 0; i < vec_size; ++i) {
            vec[i] = vec[i] * cos[i]
                     + ((threadIdx.x * vec_size < rotary_dim / 2) ? -permuted_vec[i] : permuted_vec[i]) * sin[i];
        }
    }
    return vec;
}

template<bool interleave, uint32_t head_dim, uint32_t vec_size, uint32_t bdx, typename DType, typename IdType>
__global__ void BatchQKApplyRotaryPosIdsCosSinCacheKernel(DType* q,
                                                          DType* k,
                                                          DType* q_rope,
                                                          DType* k_rope,
                                                          float* __restrict__ cos_sin_cache,
                                                          IdType* __restrict__ pos_ids,
                                                          uint32_t nnz,
                                                          uint32_t num_qo_heads,
                                                          uint32_t num_kv_heads,
                                                          uint32_t rotary_dim,
                                                          size_t   q_stride_n,
                                                          size_t   q_stride_h,
                                                          size_t   k_stride_n,
                                                          size_t   k_stride_h,
                                                          size_t   q_rope_stride_n,
                                                          size_t   q_rope_stride_h,
                                                          size_t   k_rope_stride_n,
                                                          size_t   k_rope_stride_h) {
    uint32_t       bx = blockIdx.x, tx = threadIdx.x, ty = threadIdx.y;
    const uint32_t bdy = blockDim.y;

    vec_t<float, vec_size> cos, sin;
    if (bx * bdy + ty < nnz) {
        const uint32_t idx             = bx * bdy + ty;
        const IdType   pos             = pos_ids[idx];
        const int      half_rotary_dim = rotary_dim / 2;

        // 1. if interleave:
        //  - cos = cos_sin_cache[pos_id][tx * vec_size // 2]
        //  - sin = cos_sin_cache[pos_id][(rot_dim // 2) + tx * vec_size // 2]
        // 2. if not interleave
        //  - cos = cos_cache[pos_id][(tx * vec_size) % (rot_dim // 2)]
        //  - sin = sin_cache[pos_id][(rot_dim // 2) + (tx * vec_size) % (rot_dim // 2)]
        if (tx * vec_size < rotary_dim) {
            int sin_offset = rotary_dim / 2;
            int vec_idx;
            if constexpr (interleave) {
                vec_idx = (tx * vec_size) / 2;  // Force integer division
            } else {
                vec_idx = (tx * vec_size) % half_rotary_dim;  // Use half_rotary_dim
            }
            cos.load(cos_sin_cache + (pos * rotary_dim) + vec_idx);
            sin.load(cos_sin_cache + (pos * rotary_dim) + (sin_offset + vec_idx));
        }

        // not to unroll the loop, because num head might be large and might lead to worse performance
#pragma unroll 1
        for (uint32_t qo_head_idx = 0; qo_head_idx < num_qo_heads; ++qo_head_idx) {
            DType* q_ptr      = q + get_elem_offset_impl(idx, qo_head_idx, 0, q_stride_n, q_stride_h);
            DType* q_rope_ptr = q_rope + get_elem_offset_impl(idx, qo_head_idx, 0, q_rope_stride_n, q_rope_stride_h);
            vec_t<float, vec_size> q_vec;
            if constexpr (interleave) {
                q_vec = vec_apply_llama_rope_cos_sin_interleave_reuse_half<vec_size, bdx>(q_ptr, cos, sin, rotary_dim);
            } else {
                q_vec = vec_apply_llama_rope_cos_sin<vec_size, bdx>(q_ptr, cos, sin, rotary_dim);
            }
            q_vec.cast_store(q_rope_ptr + tx * vec_size);
        }

#pragma unroll 1
        for (uint32_t kv_head_idx = 0; kv_head_idx < num_kv_heads; ++kv_head_idx) {
            DType* k_ptr      = k + get_elem_offset_impl(idx, kv_head_idx, 0, k_stride_n, k_stride_h);
            DType* k_rope_ptr = k_rope + get_elem_offset_impl(idx, kv_head_idx, 0, k_rope_stride_n, k_rope_stride_h);
            vec_t<float, vec_size> k_vec;
            if constexpr (interleave) {
                k_vec = vec_apply_llama_rope_cos_sin_interleave_reuse_half<vec_size, bdx>(k_ptr, cos, sin, rotary_dim);
            } else {
                k_vec = vec_apply_llama_rope_cos_sin<vec_size, bdx>(k_ptr, cos, sin, rotary_dim);
            }
            k_vec.cast_store(k_rope_ptr + tx * vec_size);
        }
    }
}

template<bool interleave, uint32_t head_dim, uint32_t vec_size, uint32_t bdx, typename DType, typename IdType>
__global__ void BatchQKApplyRotaryPosIdsCosSinCacheHeadParallelismKernel(DType* q,
                                                                         DType* k,
                                                                         DType* q_rope,
                                                                         DType* k_rope,
                                                                         float* __restrict__ cos_sin_cache,
                                                                         IdType* __restrict__ pos_ids,
                                                                         uint32_t nnz,
                                                                         uint32_t num_qo_heads,
                                                                         uint32_t num_kv_heads,
                                                                         uint32_t rotary_dim,
                                                                         size_t   q_stride_n,
                                                                         size_t   q_stride_h,
                                                                         size_t   k_stride_n,
                                                                         size_t   k_stride_h,
                                                                         size_t   q_rope_stride_n,
                                                                         size_t   q_rope_stride_h,
                                                                         size_t   k_rope_stride_n,
                                                                         size_t   k_rope_stride_h) {
    uint32_t       bx = blockIdx.x, tx = threadIdx.x, ty = threadIdx.y;
    uint32_t       by  = blockIdx.y;
    const uint32_t bdy = blockDim.y;

    vec_t<float, vec_size> cos, sin;
    if (bx * bdy + ty < nnz) {
        const uint32_t idx = bx * bdy + ty;
        const IdType   pos = pos_ids[idx];

        const int half_rotary_dim = rotary_dim / 2;

        // 1. if interleave:
        //  - cos = cos_sin_cache[pos_id][tx * vec_size // 2]
        //  - sin = cos_sin_cache[pos_id][(rot_dim // 2) + tx * vec_size // 2]
        // 2. if not interleave
        //  - cos = cos_cache[pos_id][(tx * vec_size) % (rot_dim // 2)]
        //  - sin = sin_cache[pos_id][(rot_dim // 2) + (tx * vec_size) % (rot_dim // 2)]
        if (tx * vec_size < rotary_dim) {
            int sin_offset = rotary_dim / 2;
            int vec_idx;
            if constexpr (interleave) {
                vec_idx = (tx * vec_size) / 2;  // Force integer division
            } else {
                vec_idx = (tx * vec_size) % half_rotary_dim;  // Use half_rotary_dim
            }
            cos.load(cos_sin_cache + (pos * rotary_dim) + vec_idx);
            sin.load(cos_sin_cache + (pos * rotary_dim) + (sin_offset + vec_idx));
        }

        if (by < num_qo_heads) {
            uint32_t qo_head_idx = by;
            DType*   q_ptr       = q + get_elem_offset_impl(idx, qo_head_idx, 0, q_stride_n, q_stride_h);
            DType*   q_rope_ptr  = q_rope + get_elem_offset_impl(idx, qo_head_idx, 0, q_rope_stride_n, q_rope_stride_h);
            vec_t<float, vec_size> q_vec;
            if constexpr (interleave) {
                q_vec = vec_apply_llama_rope_cos_sin_interleave_reuse_half<vec_size, bdx>(q_ptr, cos, sin, rotary_dim);
            } else {
                q_vec = vec_apply_llama_rope_cos_sin<vec_size, bdx>(q_ptr, cos, sin, rotary_dim);
            }
            q_vec.cast_store(q_rope_ptr + tx * vec_size);
        } else {
            uint32_t kv_head_idx = by - num_qo_heads;
            DType*   k_ptr       = k + get_elem_offset_impl(idx, kv_head_idx, 0, k_stride_n, k_stride_h);
            DType*   k_rope_ptr  = k_rope + get_elem_offset_impl(idx, kv_head_idx, 0, k_rope_stride_n, k_rope_stride_h);
            vec_t<float, vec_size> k_vec;
            if constexpr (interleave) {
                k_vec = vec_apply_llama_rope_cos_sin_interleave_reuse_half<vec_size, bdx>(k_ptr, cos, sin, rotary_dim);
            } else {
                k_vec = vec_apply_llama_rope_cos_sin<vec_size, bdx>(k_ptr, cos, sin, rotary_dim);
            }
            k_vec.cast_store(k_rope_ptr + tx * vec_size);
        }
    }
}

constexpr uint32_t constexpr_max(uint32_t a, uint32_t b) {
    return (a > b) ? a : b;
}

#define DISPATCH_INTERLEAVE(interleave, INTERLEAVE, ...)                                                               \
    if (interleave) {                                                                                                  \
        const bool INTERLEAVE = true;                                                                                  \
        __VA_ARGS__                                                                                                    \
    } else {                                                                                                           \
        const bool INTERLEAVE = false;                                                                                 \
        __VA_ARGS__                                                                                                    \
    }

template<typename DType, typename IdType>
hipError_t BatchQKApplyRotaryPosIdsCosSinCache(DType*      q,
                                               DType*      k,
                                               DType*      q_rope,
                                               DType*      k_rope,
                                               float*      cos_sin_cache,
                                               IdType*     pos_ids,
                                               uint32_t    nnz,
                                               uint32_t    num_qo_heads,
                                               uint32_t    num_kv_heads,
                                               uint32_t    rotary_dim,
                                               uint32_t    head_dim,
                                               size_t      q_stride_n,
                                               size_t      q_stride_h,
                                               size_t      k_stride_n,
                                               size_t      k_stride_h,
                                               size_t      q_rope_stride_n,
                                               size_t      q_rope_stride_h,
                                               size_t      k_rope_stride_n,
                                               size_t      k_rope_stride_h,
                                               bool        interleave,
                                               hipStream_t stream = nullptr) {
    int dev_id  = 0;
    int num_sms = 0;
    FLASHINFER_CUDA_CALL(hipGetDevice(&dev_id));
    FLASHINFER_CUDA_CALL(hipDeviceGetAttribute(&num_sms, hipDeviceAttributeMultiprocessorCount, dev_id));

    DISPATCH_INTERLEAVE(interleave, INTERLEAVE, {
        DISPATCH_HEAD_DIM(head_dim, HEAD_DIM, {
            // operate on 16 Bytes at a time
            constexpr uint32_t vec_size = constexpr_max(16 / sizeof(DType), HEAD_DIM / 32);
            // how many threads needed per head_dim
            constexpr uint32_t bdx = HEAD_DIM / vec_size;
            // how many threads needed per block
            uint32_t num_threads = ::max(128U, bdx);
            // how many tokens can we process in a block
            uint32_t bdy = num_threads / bdx;
            // how many blocks needed to process all tokens
            uint32_t nblks_x = (nnz + bdy - 1) / bdy;
            void*    args[]  = {(void*)&q,
                                (void*)&k,
                                (void*)&q_rope,
                                (void*)&k_rope,
                                (void*)&cos_sin_cache,
                                (void*)&pos_ids,
                                (void*)&nnz,
                                (void*)&num_qo_heads,
                                (void*)&num_kv_heads,
                                (void*)&rotary_dim,
                                (void*)&q_stride_n,
                                (void*)&q_stride_h,
                                (void*)&k_stride_n,
                                (void*)&k_stride_h,
                                (void*)&q_rope_stride_n,
                                (void*)&q_rope_stride_h,
                                (void*)&k_rope_stride_n,
                                (void*)&k_rope_stride_h};
            auto     kernel_0 =
                BatchQKApplyRotaryPosIdsCosSinCacheKernel<INTERLEAVE, HEAD_DIM, vec_size, bdx, DType, IdType>;

            int num_blocks_per_sm_0 = 0;
            FLASHINFER_CUDA_CALL(hipOccupancyMaxActiveBlocksPerMultiprocessor(
                &num_blocks_per_sm_0, kernel_0, num_threads, /*smem_size=*/0));
            uint32_t num_ctas_0 = num_blocks_per_sm_0 * num_sms;

            if ((nnz + bdy - 1) / bdy >= num_ctas_0) {
                dim3 nblks(nblks_x);
                dim3 nthrs(bdx, bdy);
                FLASHINFER_CUDA_CALL(cudaLaunchKernel((void*)kernel_0, nblks, nthrs, args, 0, stream));
            } else {
                dim3 nblks(nblks_x, num_qo_heads + num_kv_heads);
                dim3 nthrs(bdx, bdy);
                auto kernel_1 = BatchQKApplyRotaryPosIdsCosSinCacheHeadParallelismKernel<INTERLEAVE,
                                                                                         HEAD_DIM,
                                                                                         vec_size,
                                                                                         bdx,
                                                                                         DType,
                                                                                         IdType>;
                FLASHINFER_CUDA_CALL(cudaLaunchKernel((void*)kernel_1, nblks, nthrs, args, 0, stream));
            }
        });
    });

    return hipSuccess;
}

void apply_rope_pos_ids_cos_sin_cache(at::Tensor q,
                                      at::Tensor k,
                                      at::Tensor q_rope,
                                      at::Tensor k_rope,
                                      at::Tensor cos_sin_cache,
                                      at::Tensor pos_ids,
                                      bool       interleave,
                                      int64_t    cuda_stream) {
    CHECK_LAST_DIM_CONTIGUOUS(q);
    CHECK_LAST_DIM_CONTIGUOUS(k);
    CHECK_INPUT(cos_sin_cache);
    CHECK_INPUT(pos_ids);
    auto device = q.device();
    CHECK_EQ(k.device(), device);
    CHECK_EQ(cos_sin_cache.device(), device);
    CHECK_EQ(pos_ids.device(), device);
    CHECK_DIM(3, q);  // q: (nnz, H_Q, D)
    CHECK_DIM(3, k);  // k: (nnz, H_K, D)
    // cos_sin_cache: (max_seq_len, R)
    // First half of R is cos, second half is sin
    CHECK_DIM(2, cos_sin_cache);
    CHECK_EQ(q.size(0), k.size(0));
    CHECK_EQ(q.size(2), k.size(2));
    unsigned int rotary_dim      = cos_sin_cache.size(1);
    unsigned int num_qo_heads    = q.size(1);
    unsigned int num_kv_heads    = k.size(1);
    unsigned int head_dim        = q.size(2);
    unsigned int nnz             = q.size(0);
    size_t       q_stride_n      = q.stride(0);
    size_t       q_stride_h      = q.stride(1);
    size_t       k_stride_n      = k.stride(0);
    size_t       k_stride_h      = k.stride(1);
    size_t       q_rope_stride_n = q_rope.stride(0);
    size_t       q_rope_stride_h = q_rope.stride(1);
    size_t       k_rope_stride_n = k_rope.stride(0);
    size_t       k_rope_stride_h = k_rope.stride(1);

    const c10::hip::OptionalHIPGuardMasqueradingAsCUDA device_guard(q.device());
    // auto stream = at::hip::getCurrentHIPStreamMasqueradingAsCUDA();

    hipStream_t stream = reinterpret_cast<hipStream_t>(cuda_stream);
    DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(q.scalar_type(), c_type, [&] {
        hipError_t status = BatchQKApplyRotaryPosIdsCosSinCache(static_cast<c_type*>(q.data_ptr()),
                                                                static_cast<c_type*>(k.data_ptr()),
                                                                static_cast<c_type*>(q_rope.data_ptr()),
                                                                static_cast<c_type*>(k_rope.data_ptr()),
                                                                static_cast<float*>(cos_sin_cache.data_ptr()),
                                                                static_cast<int32_t*>(pos_ids.data_ptr()),
                                                                nnz,
                                                                num_qo_heads,
                                                                num_kv_heads,
                                                                rotary_dim,
                                                                head_dim,
                                                                q_stride_n,
                                                                q_stride_h,
                                                                k_stride_n,
                                                                k_stride_h,
                                                                q_rope_stride_n,
                                                                q_rope_stride_h,
                                                                k_rope_stride_n,
                                                                k_rope_stride_h,
                                                                interleave,
                                                                stream);

        TORCH_CHECK(status == hipSuccess,
                    "BatchQKApplyRotaryPosIdsCosSinCache failed with error code "
                        + std::string(hipGetErrorString(status)));

        return true;
    });
}

};  // namespace rtp_llm
