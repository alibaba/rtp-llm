from __future__ import annotations
import libth_transformer_config
import torch
import typing
__all__: list[str] = ['', '', '', 'MultimodalInput', 'RtpEmbeddingOp', 'RtpLLMOp', 'EmbeddingCppOutput', 'TypedOutput']
class MultimodalInput:
    mm_type: int
    tensor: torch.Tensor
    url: str
    def __init__(self, url: str, tensor: torch.Tensor, mm_type: int) -> None:
        ...
class RtpEmbeddingOp:
    def __init__(self) -> None:
        ...
    def decode(self, token_ids: torch.Tensor, token_type_ids: torch.Tensor, input_lengths: torch.Tensor, request_id: int, multimodal_inputs: list[MultimodalInput]) -> typing.Any:
        ...
    def init(self, model: typing.Any, mm_process_engine: typing.Any) -> None:
        ...
    def stop(self) -> None:
        ...

class TypedOutput:
    isTensor: bool
    t: torch.Tensor
    map: list[dict[str, torch.Tensor]]
    def __init__(self) -> None:
        ...
    def setTensorOutput(self, tensor: torch.Tensor) -> None:
        ...
    def setMapOutput(self, tensor_map: list[dict[str, torch.Tensor]]) -> None:
        ...
        
class EmbeddingQueryOutput:
    output: TypedOutput
    error_info: typing.Any
    def __init__(self) -> None:
        ...
    def setMapOutput(self, output: typing.Any) -> None:
        ...
    def setTensorOutput(self, output: typing.Any) -> None:
        ...
    def setErrorInfo(self, error_info: typing.Any) -> None:
        ...
    
class RtpLLMOp:
    def __init__(self) -> None:
        ...
    def init(self, model: typing.Any, mm_process_engine: typing.Any, propose_model: typing.Any, token_processor: typing.Any) -> None:
        ...
    def pause(self) -> None:
        ...
    def restart(self) -> None:
        ...
    def start_http_server(self, model_weights_loader: typing.Any, lora_infos: typing.Any, gang_info: typing.Any, tokenizer: typing.Any, render: typing.Any) -> None:
        ...
    def stop(self) -> None:
        ...
