diff --git a/docs/references/Environment.md b/docs/references/Environment.md
index 925fddb86..5c5627187 100644
--- a/docs/references/Environment.md
+++ b/docs/references/Environment.md
@@ -19,7 +19,7 @@ When adding new parameter information, you need to specify the parameter type, v
 In `rtp_llm/cpp/th_op/ConfigModules.h`, all related parameter information classifications are placed here. Let's illustrate with the following example:
 
 ```c++
-struct ParallelismDistributedConfig {
+struct ParallelismConfig {
     int tp_size = 1;
     int ep_size = 1;
     int dp_size = 1;
@@ -28,7 +28,6 @@ struct ParallelismDistributedConfig {
     int world_rank = 0;
     int local_world_size = 1;
     std::string to_string() const;
-    void update_from_env_for_test();
 };
 ```
 
diff --git a/example/test.py b/example/test.py
deleted file mode 100644
index fd0ffd1d2..000000000
--- a/example/test.py
+++ /dev/null
@@ -1,60 +0,0 @@
-import asyncio
-import os
-
-import rtp_llm.models
-from rtp_llm.config.py_config_modules import StaticConfig
-from rtp_llm.distribute.worker_info import g_worker_info, update_master_info
-from rtp_llm.model_factory import ModelFactory
-from rtp_llm.openai.api_datatype import ChatCompletionRequest, ChatMessage, RoleEnum
-from rtp_llm.openai.openai_endpoint import OpenaiEndpoint
-from rtp_llm.pipeline import Pipeline
-from rtp_llm.test.utils.port_util import PortsContext
-
-
-async def main():
-    with PortsContext(None, 1) as ports:
-        start_port = ports[0]
-        StaticConfig.server_config.start_port = start_port
-        update_master_info("127.0.0.1", start_port)
-        g_worker_info.reload()
-        StaticConfig.model_config.model_type = "qwen_2"
-        StaticConfig.model_config.checkpoint_path = "Qwen/Qwen2-0.5B-Instruct"
-        os.environ["DEVICE_RESERVE_MEMORY_BYTES"] = str(3 * 1024 * 1024 * 1024)
-        model_config = ModelFactory.create_normal_model_config()
-        model = ModelFactory.from_huggingface(
-            model_config.ckpt_path, model_config=model_config
-        )
-        pipeline = Pipeline(model.config, model.tokenizer)
-
-        # usual request
-        for res in pipeline(
-            "<|im_start|>user\nhello, what's your name<|im_end|>\n<|im_start|>assistant\n",
-            max_new_tokens=100,
-        ):
-            print(res.generate_texts)
-
-        # openai request
-        openai_endpoint = OpenaiEndpoint(
-            model.config, model.tokenizer, pipeline.backend_rpc_server_visitor
-        )
-        messages = [
-            ChatMessage(
-                **{
-                    "role": RoleEnum.user,
-                    "content": "你是谁？",
-                }
-            ),
-        ]
-        request = ChatCompletionRequest(messages=messages, stream=False)
-        response = openai_endpoint.chat_completion(
-            request_id=0, chat_request=request, raw_request=None
-        )
-        async for res in response:
-            pass
-        print((await response.gen_complete_response_once()).model_dump_json(indent=4))
-
-        model.stop()
-
-
-if __name__ == "__main__":
-    asyncio.run(main())
diff --git a/rtp_llm/__init__.py b/rtp_llm/__init__.py
index 060d85370..b5e47c842 100644
--- a/rtp_llm/__init__.py
+++ b/rtp_llm/__init__.py
@@ -16,11 +16,9 @@ logging.basicConfig(
 from rtp_llm.config.log_config import LOGGING_CONFIG
 from rtp_llm.utils.torch_patch import *
 
-## for `__init__.py`, we reserve the envs, don't use StaticConfig.
 LOG_PATH = os.environ.get("LOG_PATH", "logs")
 os.makedirs(LOG_PATH, exist_ok=True)
 
-
 file_logger_init_success = False
 if os.environ.get("FT_SERVER_TEST") is None:
     LOGGING_CONFIG["loggers"][""]["level"] = LOGLEVEL
diff --git a/rtp_llm/access_logger/access_logger.py b/rtp_llm/access_logger/access_logger.py
index 875e3372f..4b0e317f5 100644
--- a/rtp_llm/access_logger/access_logger.py
+++ b/rtp_llm/access_logger/access_logger.py
@@ -1,4 +1,5 @@
 import logging
+import os
 from typing import Any, Dict
 
 from rtp_llm.access_logger.json_util import dump_json
@@ -10,9 +11,9 @@ ACCESS_LOGGER_NAME = "access_logger"
 QUERY_ACCESS_LOGGER_NAME = "query_access_logger"
 
 
-def init_access_logger() -> None:
+def init_access_logger(log_path: str, backup_count: int) -> None:
     access_logger = logging.getLogger(ACCESS_LOGGER_NAME)
-    handler = get_handler("access.log")
+    handler = get_handler("access.log", log_path, backup_count)
     formatter = logging.Formatter("%(message)s")
     access_logger.handlers.clear()
     access_logger.parent = None
@@ -21,9 +22,9 @@ def init_access_logger() -> None:
         access_logger.addHandler(handler)
 
 
-def init_query_access_logger() -> None:
+def init_query_access_logger(log_path: str, backup_count: int) -> None:
     access_logger = logging.getLogger(QUERY_ACCESS_LOGGER_NAME)
-    handler = get_handler("query_access.log")
+    handler = get_handler("query_access.log", log_path, backup_count)
     formatter = logging.Formatter("%(message)s")
     access_logger.handlers.clear()
     access_logger.parent = None
@@ -33,9 +34,9 @@ def init_query_access_logger() -> None:
 
 
 class AccessLogger:
-    def __init__(self) -> None:
-        init_access_logger()
-        init_query_access_logger()
+    def __init__(self, log_path: str, backup_count: int) -> None:
+        init_access_logger(log_path, backup_count)
+        init_query_access_logger(log_path, backup_count)
         self.logger = logging.getLogger(ACCESS_LOGGER_NAME)
         self.query_logger = logging.getLogger(QUERY_ACCESS_LOGGER_NAME)
 
diff --git a/rtp_llm/access_logger/log_utils.py b/rtp_llm/access_logger/log_utils.py
index c5ee22406..2a5b05d7a 100644
--- a/rtp_llm/access_logger/log_utils.py
+++ b/rtp_llm/access_logger/log_utils.py
@@ -3,13 +3,7 @@ from typing import Optional
 
 from concurrent_log_handler import ConcurrentRotatingFileHandler
 
-from rtp_llm.config.py_config_modules import StaticConfig
-
-LOG_PATH_KEY = "LOG_PATH"
-
-
-def get_handler(file_name: str) -> Optional[logging.Handler]:
-    log_path = StaticConfig.profiling_debug_config.log_path
+def get_handler(file_name: str, log_path: str, backup_count: int) -> Optional[logging.Handler]:
     if log_path == "":
         return None
     else:
@@ -17,6 +11,6 @@ def get_handler(file_name: str) -> Optional[logging.Handler]:
             filename=f"{log_path}/{file_name}",
             mode="a",
             maxBytes=100 * 1024 * 1024,
-            backupCount=StaticConfig.profiling_debug_config.log_file_backup_count,
+            backupCount=backup_count,
             use_gzip=True,
         )
diff --git a/rtp_llm/async_decoder_engine/async_model.py b/rtp_llm/async_decoder_engine/async_model.py
index b3be443aa..4c109ee6d 100644
--- a/rtp_llm/async_decoder_engine/async_model.py
+++ b/rtp_llm/async_decoder_engine/async_model.py
@@ -6,17 +6,17 @@ import torch
 from rtp_llm.async_decoder_engine.engine_creator import create_engine
 from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
 from rtp_llm.config.generate_config import GenerateConfig
-from rtp_llm.config.task_type import TaskType
 from rtp_llm.distribute.worker_info import g_parallel_info
-from rtp_llm.models.base_model import BaseModel, GenerateInput
+from rtp_llm.models.base_model import BaseModel
 from rtp_llm.models.propose_model.propose_model import ProposeModel
-from rtp_llm.ops import EngineScheduleInfo, KVCacheInfo, WorkerStatusInfo
+from rtp_llm.ops import EngineScheduleInfo, KVCacheInfo, TaskType, WorkerStatusInfo
+from rtp_llm.utils.base_model_datatypes import GenerateInput
 from rtp_llm.utils.gemm_utils.device_map import get_device
 
 
 class AsyncModel:
     def __init__(
-        self, model: BaseModel, propose_model: Optional[ProposeModel] = None
+        self, model: BaseModel, gang_info, propose_model: Optional[ProposeModel] = None
     ) -> None:
         self.model = model
         self.propose_model = propose_model
@@ -26,7 +26,7 @@ class AsyncModel:
 
         assert self.config.max_seq_len > 0
         self.tokenizer = model.tokenizer
-        self.decoder_engine_ = create_engine(self.model, self.propose_model)
+        self.decoder_engine_ = create_engine(self.model, self.config, gang_info, self.propose_model)
         self.decoder_engine_.start()
 
     def is_multimodal(self) -> bool:
@@ -40,7 +40,8 @@ class AsyncModel:
             logging.info(f"error get device name with error: {e}")
             manchine_name = "unknown"
         parallel_info = f"TP{g_parallel_info.tp_size}_PP{g_parallel_info.pp_size}_EP{g_parallel_info.ep_size}"
-        weight_info = f"W{self.config.gpt_init_params.quant_algo.getWeightBits()}A{self.config.gpt_init_params.quant_algo.getActivationBits()}"
+        quant_algo = self.model.py_model_config.quant_algo
+        weight_info = f"W{quant_algo.getWeightBits()}A{quant_algo.getActivationBits()}"
         return "_".join([manchine_name, parallel_info, weight_info])
 
     @property
@@ -49,7 +50,7 @@ class AsyncModel:
 
     @property
     def task_type(self) -> TaskType:
-        return self.model.task_type
+        return self.model.py_model_config.task_type
 
     def stop(self):
         self.decoder_engine_.stop()
diff --git a/rtp_llm/async_decoder_engine/embedding/embedding_engine.py b/rtp_llm/async_decoder_engine/embedding/embedding_engine.py
index ba1ccb87e..0675c56e8 100644
--- a/rtp_llm/async_decoder_engine/embedding/embedding_engine.py
+++ b/rtp_llm/async_decoder_engine/embedding/embedding_engine.py
@@ -27,7 +27,7 @@ class EmbeddingCppEngine(BaseEngine):
     @override
     def start(self):
         if self.model.is_multimodal():
-            self.mm_engine = MMProcessEngine(self.model)
+            self.mm_engine = MMProcessEngine(self.model, self.model.vit_config)
         else:
             self.mm_engine = None
         self.cpp_engine.init(self.model, self.mm_engine)
diff --git a/rtp_llm/async_decoder_engine/engine_creator.py b/rtp_llm/async_decoder_engine/engine_creator.py
index a7787955f..97a61e844 100644
--- a/rtp_llm/async_decoder_engine/engine_creator.py
+++ b/rtp_llm/async_decoder_engine/engine_creator.py
@@ -23,15 +23,29 @@ def check_exeutor_type(model: BaseModel):
 
 
 def create_engine(
-    model: BaseModel, propose_model: Optional[ProposeModel] = None
+    model: BaseModel, 
+    config: object,
+    gang_info,
+    propose_model: Optional[ProposeModel] = None
 ) -> BaseEngine:
-    torch.ops.rtp_llm.init_engine(
-        model.config.gpt_init_params.profiling_debug_logging_config.ft_alog_conf_path
-    )
+    """
+    Create an engine for the given model and config.
+    
+    Args:
+        model: The BaseModel instance
+        config: Configuration object containing profiling_debug_logging_config and other configs
+        gang_info: GangInfo instance from GangServer
+        propose_model: Optional propose model for speculative decoding
+    
+    Returns:
+        BaseEngine instance
+    """
+    torch.ops.rtp_llm.init_engine(config.profiling_debug_logging_config.ft_alog_conf_path)
+    
     executor_type = check_exeutor_type(model)
     logging.info(f"executor_type: {executor_type}")
     if executor_type == ExecutorType.Normal:
-        return RPCEngine(model, propose_model)
+        return RPCEngine(model, gang_info, propose_model)
     elif executor_type == ExecutorType.Embedding:
         return EmbeddingCppEngine(model)
     else:
diff --git a/rtp_llm/async_decoder_engine/rpc_engine.py b/rtp_llm/async_decoder_engine/rpc_engine.py
index 8a9b2875c..76f7e821b 100644
--- a/rtp_llm/async_decoder_engine/rpc_engine.py
+++ b/rtp_llm/async_decoder_engine/rpc_engine.py
@@ -5,7 +5,8 @@ from typing_extensions import override
 from rtp_llm.async_decoder_engine.base_engine import BaseEngine
 from rtp_llm.cpp.model_rpc.model_rpc_client import ModelRpcClient
 from rtp_llm.frontend.token_processor import TokenProcessor
-from rtp_llm.models.base_model import BaseModel, GenerateInput, GenerateOutputs
+from rtp_llm.models.base_model import BaseModel
+from rtp_llm.utils.base_model_datatypes import GenerateInput, GenerateOutputs
 from rtp_llm.models.propose_model.propose_model import ProposeModel
 from rtp_llm.ops import EngineScheduleInfo, KVCacheInfo, WorkerStatusInfo
 from rtp_llm.ops.rtp_llm.rtp_llm_op import RtpLLMOp
@@ -14,7 +15,7 @@ from rtp_llm.utils.mm_process_engine import MMProcessEngine
 
 class RPCEngine(BaseEngine):
     def __init__(
-        self, model: BaseModel, propose_model: Optional[ProposeModel] = None
+        self, model: BaseModel, gang_info, propose_model: Optional[ProposeModel] = None
     ) -> None:
         self.model = model
         self.propose_model = propose_model
@@ -24,13 +25,18 @@ class RPCEngine(BaseEngine):
             self.tokenizer, self.model.config.special_tokens
         )
         if self.model.is_multimodal():
-            self.mm_engine = MMProcessEngine(self.model)
+            self.mm_engine = MMProcessEngine(self.model, self.model.vit_config)
         else:
             self.mm_engine = None
         self.rtp_llm_op_ = RtpLLMOp(
             model, self.mm_engine, propose_model, self.token_processor
         )
-        self.model_rpc_client = ModelRpcClient(self.config)
+        self.model_rpc_client = ModelRpcClient(
+            self.model.engine_config.parallelism_config.ffn_disaggregate_config,
+            self.model.engine_config.pd_sep_config.max_rpc_timeout_ms,
+            self.model.engine_config.pd_sep_config.decode_entrance,
+            gang_info=gang_info,
+        )
 
     @override
     def start(self) -> None:
diff --git a/rtp_llm/config/engine_config.py b/rtp_llm/config/engine_config.py
new file mode 100644
index 000000000..141f72254
--- /dev/null
+++ b/rtp_llm/config/engine_config.py
@@ -0,0 +1,284 @@
+from dataclasses import dataclass
+from typing import Any, Dict, Optional
+import logging
+
+from rtp_llm.config.kv_cache_config import KVCacheConfig
+from rtp_llm.config.py_config_modules import PyEnvConfigs, WORKER_INFO_PORT_NUM
+from rtp_llm.distribute.worker_info import (
+    g_master_info,
+    g_parallel_info,
+    g_worker_info,
+    ParallelInfo,
+)
+from rtp_llm.ops import (
+    ArpcConfig,
+    CacheStoreConfig,
+    ConcurrencyConfig,
+    DeviceResourceConfig,
+    FMHAConfig,
+    FfnDisAggregateConfig,
+    HWKernelConfig,
+    MiscellaneousConfig,
+    ModelSpecificConfig,
+    MoeConfig,
+    ParallelismConfig,
+    PDSepConfig,
+    ProfilingDebugLoggingConfig,
+    RoleType,
+    RuntimeConfig,
+    SpeculativeExecutionConfig,
+)
+
+@dataclass
+class EngineConfig:
+    """Engine configuration collection created from py_env_configs.
+    
+    It contains only configuration objects that are related to py_env_configs,
+    not model-specific configs like PyModelConfig or MMModelConfig.
+    """
+    # Parallelism and runtime configs
+    parallelism_config: ParallelismConfig
+    runtime_config: RuntimeConfig
+    
+    # Specialized configs from py_env_configs
+    pd_sep_config: PDSepConfig
+    concurrency_config: ConcurrencyConfig
+    fmha_config: FMHAConfig
+    kv_cache_config: KVCacheConfig
+    profiling_debug_logging_config: ProfilingDebugLoggingConfig
+    hw_kernel_config: HWKernelConfig
+    device_resource_config: DeviceResourceConfig
+    moe_config: MoeConfig
+    model_specific_config: ModelSpecificConfig
+    sp_config: SpeculativeExecutionConfig
+    cache_store_config: CacheStoreConfig
+    misc_config: MiscellaneousConfig
+    arpc_config: ArpcConfig
+    
+    @staticmethod
+    def create(py_env_configs: PyEnvConfigs, gang_info) -> 'EngineConfig':
+        """Create and fully initialize EngineConfig from py_env_configs.
+        
+        This method creates the EngineConfig dataclass and performs all necessary
+        initialization including parallelism setup, runtime config setup, worker
+        address updates, and PD separation config setup.
+        
+        Args:
+            py_env_configs: PyEnvConfigs instance containing all configuration
+            gang_info: GangInfo instance from GangServer.
+        
+        Returns:
+            Fully initialized EngineConfig instance
+        """
+        
+        # Create ParallelismConfig and setup from parallel_info
+        parallelism_config = ParallelismConfig()
+        setup_parallelism_config(
+            parallelism_config,
+            g_parallel_info,
+            py_env_configs.ffn_disaggregate_config,
+        )
+        
+        runtime_config = py_env_configs.runtime_config
+        
+        # Directly use C++ binding objects from py_env_configs
+        pd_sep_config = py_env_configs.pd_separation_config
+        concurrency_config = py_env_configs.concurrency_config
+        fmha_config = py_env_configs.fmha_config
+        kv_cache_config = py_env_configs.kv_cache_config
+        profiling_debug_logging_config = py_env_configs.profiling_debug_config
+        hw_kernel_config = py_env_configs.py_hw_kernel_config
+        device_resource_config = py_env_configs.device_resource_config
+        model_specific_config = py_env_configs.model_specific_config
+        misc_config = py_env_configs.misc_config.misc_config
+        moe_config = py_env_configs.moe_config
+        sp_config = py_env_configs.sp_config
+        cache_store_config = py_env_configs.cache_store_config
+        arpc_config = py_env_configs.arpc_config
+        
+        # Setup pd_sep_config role_type based on vit_separation
+        from rtp_llm.ops import VitSeparation
+        if py_env_configs.vit_config.vit_separation == VitSeparation.VIT_SEPARATION_ROLE:
+            pd_sep_config.role_type = RoleType.VIT
+        else:
+            # role_config.role_type property automatically converts string to RoleType enum
+            pd_sep_config.role_type = py_env_configs.role_config.role_type
+        
+        # Create EngineConfig instance
+        engine_config = EngineConfig(
+            parallelism_config=parallelism_config,
+            runtime_config=runtime_config,
+            pd_sep_config=pd_sep_config,
+            concurrency_config=concurrency_config,
+            fmha_config=fmha_config,
+            kv_cache_config=kv_cache_config,
+            profiling_debug_logging_config=profiling_debug_logging_config,
+            hw_kernel_config=hw_kernel_config,
+            device_resource_config=device_resource_config,
+            moe_config=moe_config,
+            model_specific_config=model_specific_config,
+            sp_config=sp_config,
+            cache_store_config=cache_store_config,
+            misc_config=misc_config,
+            arpc_config=arpc_config,
+        )
+        
+
+        runtime_config.max_generate_batch_size = concurrency_config.concurrency_limit
+        
+        # Update worker addresses (needs parallelism_config and gang_info)
+        if gang_info is not None:
+            update_worker_addrs(
+                engine_config.runtime_config,
+                engine_config.parallelism_config,
+                gang_info,
+            )
+        
+        # Setup PD separation config
+        setup_pd_sep_config(
+            engine_config.pd_sep_config,
+            cache_store_config,
+        )
+ 
+        return engine_config
+
+# ============================================================================
+# EngineConfig setup and initialization functions
+# ============================================================================
+
+def setup_parallelism_config(
+    parallelism_config: ParallelismConfig,
+    parallel_info: ParallelInfo = g_parallel_info,
+    py_ffn_disaggregate_config: Optional[FfnDisAggregateConfig] = None,
+) -> None:
+    """Setup ParallelismConfig from parallel_info and master/worker info.
+    
+    Also sets up FfnDisAggregateConfig if it's a member of ParallelismConfig.
+    
+    Args:
+        parallelism_config: ParallelismConfig instance to setup
+        parallel_info: ParallelInfo for parallelism setup
+        py_ffn_disaggregate_config: Optional FfnDisAggregateConfig from py_env_configs
+    """
+    parallelism_config.tp_size = parallel_info.tp_size
+    parallelism_config.tp_rank = parallel_info.tp_rank
+    parallelism_config.ep_size = parallel_info.ep_size
+    parallelism_config.ep_rank = parallel_info.ep_rank
+    parallelism_config.dp_size = parallel_info.dp_size
+    parallelism_config.dp_rank = parallel_info.dp_rank
+    parallelism_config.ffn_tp_rank = parallel_info.ffn_tp_rank
+    parallelism_config.ffn_tp_size = parallel_info.ffn_tp_size
+    parallelism_config.enable_sp = parallel_info.ffn_sp_size > 1
+    # Note: local_rank is a computed property in ParallelInfo, not a field in ParallelismConfig
+    parallelism_config.world_size = parallel_info.world_size
+    parallelism_config.world_rank = parallel_info.world_rank
+    parallelism_config.local_world_size = parallel_info.local_world_size
+    parallelism_config.pp_size = parallel_info.pp_size
+    parallelism_config.ffn_sp_size = parallel_info.ffn_sp_size
+    
+    # Set port and IP related fields
+    parallelism_config.nccl_ip = g_master_info.ip
+    parallelism_config.tp_nccl_port = g_master_info.tp_nccl_port
+    parallelism_config.dp_tp_nccl_port = g_master_info.dp_tp_nccl_port
+    parallelism_config.ffn_tp_nccl_port = g_master_info.ffn_tp_nccl_port
+    parallelism_config.model_rpc_port = g_worker_info.rpc_server_port
+    parallelism_config.http_port = g_worker_info.http_port
+    parallelism_config.th_nccl_port = g_master_info.th_nccl_port
+    
+    # Setup FfnDisAggregateConfig if it's a member of ParallelismConfig
+    # Note: This assumes ParallelismConfig has ffn_disaggregate_config as a member
+    # If not, the C++ code needs to be updated first
+    if py_ffn_disaggregate_config and py_ffn_disaggregate_config.enable_ffn_disaggregate:
+            # 暂时先限制tp=1, 更多支持在python版本实现
+            assert (
+                parallel_info.tp_size == 1 and parallel_info.world_size > 1
+            ), "enable_ffn_disaggregate must be used in dp = 1 world_size > 1"
+            attention_dp_size = parallel_info.world_size - 1
+            attention_tp_size = 1
+            ffn_tp_size = 1
+            assert (
+                attention_tp_size == ffn_tp_size
+            ), "attention_tp_size must be equal to ffn_tp_size"
+            parallelism_config.ffn_disaggregate_config.enable_ffn_disaggregate = True
+            parallelism_config.ffn_disaggregate_config.attention_tp_size = attention_tp_size
+            parallelism_config.ffn_disaggregate_config.attention_dp_size = attention_dp_size
+            parallelism_config.ffn_disaggregate_config.ffn_tp_size = ffn_tp_size
+            # TODO: remove it, ffn dp is stupid
+            parallelism_config.ffn_disaggregate_config.ffn_dp_size = 1
+            parallelism_config.ffn_disaggregate_config.is_ffn_rank = (
+                parallel_info.world_rank >= attention_tp_size * attention_dp_size
+            )
+    
+    logging.info(f"th_nccl_port: {parallelism_config.th_nccl_port}")
+    
+
+def update_worker_addrs(
+    runtime_config: RuntimeConfig,
+    parallelism_config: ParallelismConfig,
+    gang_info) -> None:
+    """Update worker addresses in runtime_config based on gang info."""
+    worker_addrs = []
+    worker_grpc_addrs = []
+    local_rank = parallelism_config.local_rank
+    for member in gang_info.members:
+        logging.info(
+            f"member world rank: {member.world_rank}, member local rank: {member.local_rank}, local rank: {local_rank}, "
+            f"tp_size: {parallelism_config.tp_size}, dp_size: {parallelism_config.dp_size}, dp_rank: {parallelism_config.dp_rank}"
+        )
+        if int((member.world_rank / parallelism_config.tp_size) % parallelism_config.dp_size) == parallelism_config.dp_rank:
+            worker_addrs.append(
+                f"{member.ip}:{member.cache_store_listen_port}:{member.cache_store_rdma_listen_port}"
+            )
+            worker_grpc_addrs.append(f"{member.ip}:{member.rpc_server_port}")
+            logging.info(
+                f"append member for pd sep "
+                f"{member.ip}:{member.rpc_server_port}, {member.cache_store_listen_port}, "
+                f"{member.cache_store_rdma_listen_port} to local rank {local_rank}, world rank {member.world_rank}"
+            )
+    runtime_config.worker_grpc_addrs = worker_grpc_addrs
+
+
+def setup_pd_sep_config(
+    pd_sep_config: PDSepConfig,
+    cache_store_config,
+) -> None:
+    """Setup PDSepConfig from worker info and cache_store_config."""
+    # Update pd_sep_config fields
+    pd_sep_config.cache_store_listen_port = g_worker_info.cache_store_listen_port
+    pd_sep_config.cache_store_connect_port = g_worker_info.cache_store_connect_port
+    pd_sep_config.cache_store_rdma_listen_port = g_worker_info.cache_store_rdma_listen_port
+    pd_sep_config.cache_store_rdma_connect_port = g_worker_info.cache_store_rdma_connect_port
+    pd_sep_config.remote_rpc_server_port = g_worker_info.remote_rpc_server_port
+    pd_sep_config.worker_port_offset = WORKER_INFO_PORT_NUM
+    
+    # Override with values from other sources
+    if pd_sep_config.role_type in [RoleType.PREFILL, RoleType.DECODE]:
+        pd_sep_config.cache_store_rdma_mode = (
+            cache_store_config.cache_store_rdma_mode
+        )
+
+
+def finalize_scheduler_config(
+    fifo_scheduler_config: Any,  # FIFOSchedulerConfig
+    max_seq_len: int,
+) -> None:
+    """Finalize fifo_scheduler_config with computed values.
+    
+    Args:
+        fifo_scheduler_config: FIFOSchedulerConfig instance to finalize
+        max_seq_len: Maximum sequence length from model config
+    """
+    # fast_gen_max_context_len uses fast_gen_context_budget from fifo_scheduler_config
+    if fifo_scheduler_config.fast_gen_context_budget == -1:
+        fifo_scheduler_config.fast_gen_max_context_len = 1024
+    else:
+        fifo_scheduler_config.fast_gen_max_context_len = fifo_scheduler_config.fast_gen_context_budget
+    logging.info(f"fast_gen_max_context_len: {fifo_scheduler_config.fast_gen_max_context_len}")
+
+    # Set max_batch_tokens_size if not set from py_runtime_config
+    if fifo_scheduler_config.max_batch_tokens_size == 0:
+        fifo_scheduler_config.max_batch_tokens_size = (
+            fifo_scheduler_config.max_context_batch_size * max_seq_len
+        )
+    logging.info(f"max_batch_tokens_size: {fifo_scheduler_config.max_batch_tokens_size}")
+
diff --git a/rtp_llm/config/generate_config.py b/rtp_llm/config/generate_config.py
index cb52a2951..3ba1a4e02 100644
--- a/rtp_llm/config/generate_config.py
+++ b/rtp_llm/config/generate_config.py
@@ -7,7 +7,6 @@ from pydantic import BaseModel
 from transformers.tokenization_utils_base import PreTrainedTokenizerBase
 
 from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
-from rtp_llm.config.py_config_modules import StaticConfig
 from rtp_llm.utils.check_util import *
 from rtp_llm.utils.util import check_with_info
 
@@ -201,17 +200,24 @@ class GenerateConfig(BaseModel):
         self.stop_words_list += special_tokens.stop_words_id_list
         self.stop_words_str += special_tokens.stop_words_str_list
 
-    def add_thinking_params(self, tokenizer):
-        end_think_token_id = StaticConfig.generate_env_config.think_end_token_id
+    def add_thinking_params(self, tokenizer, generate_env_config):
+        """Add thinking parameters from generate_env_config.
+        
+        Args:
+            tokenizer: Tokenizer instance.
+            generate_env_config: GenerateEnvConfig object.
+        """
+        
+        end_think_token_id = generate_env_config.think_end_token_id
         self.end_think_token_ids = (
             [end_think_token_id] if end_think_token_id != -1 else []
         )
         if (
-            bool(StaticConfig.generate_env_config.think_mode)
+            bool(generate_env_config.think_mode)
             and tokenizer
             and end_think_token_id == -1
         ):
-            think_end_tag: str = StaticConfig.generate_env_config.think_end_tag.encode(
+            think_end_tag: str = generate_env_config.think_end_tag.encode(
                 "utf-8"
             ).decode("unicode_escape")
             tokenized_result: List[int] = tokenizer.encode(
@@ -219,7 +225,7 @@ class GenerateConfig(BaseModel):
             )
             self.end_think_token_ids = tokenized_result
         self.in_think_mode = (
-            bool(StaticConfig.generate_env_config.think_mode)
+            bool(generate_env_config.think_mode)
             and len(self.end_think_token_ids) >= 0
         )
 
diff --git a/rtp_llm/config/gpt_init_model_parameters.py b/rtp_llm/config/gpt_init_model_parameters.py
deleted file mode 100644
index 3d0b3375f..000000000
--- a/rtp_llm/config/gpt_init_model_parameters.py
+++ /dev/null
@@ -1,1558 +0,0 @@
-import json
-import logging
-import math
-import os
-import typing
-
-# make sure so init
-from dataclasses import dataclass, field, fields
-from enum import Enum
-from typing import Any, Dict, List, Optional, Set
-
-import torch
-
-from rtp_llm.config.py_config_modules import (
-    PyEnvConfigs,
-    StaticConfig,
-    get_env_bool,
-    get_env_int,
-    get_env_str,
-)
-from rtp_llm.config.quant_config import (
-    Fp8BlockWiseQuantConfig,
-    Fp8PerChannelCompressedQuantConfig,
-    Fp8PerTensorCompressedQuantConfig,
-    QuantizationConfig,
-    init_quant_config,
-)
-from rtp_llm.config.task_type import TaskType, check_task_type
-from rtp_llm.distribute.gang_info import GangInfo, get_gang_info
-from rtp_llm.distribute.worker_info import (
-    WORKER_INFO_PORT_NUM,
-    ParallelInfo,
-    g_master_info,
-    g_parallel_info,
-    g_worker_info,
-)
-from rtp_llm.ops import (
-    ArpcConfig,
-    BatchDecodeSchedulerConfig,
-    CacheStoreConfig,
-    ConcurrencyConfig,
-    DeviceResourceConfig,
-    EplbMode,
-    FIFOSchedulerConfig,
-    FMHAConfig,
-    GptInitParameter,
-    HWKernelConfig,
-    KVCacheConfig,
-    MiscellaneousConfig,
-    MlaOpsType,
-    ModelSpecificConfig,
-    MoeConfig,
-    ParallelismDistributedConfig,
-    ProfilingDebugLoggingConfig,
-    QuantAlgo,
-    RoleType,
-    SchedulerConfig,
-    ServiceDiscoveryConfig,
-    SpecialTokens,
-    SpeculativeExecutionConfig,
-)
-from rtp_llm.utils.gemm_utils.cutlass_config import load_cutlass_gemm_config
-from rtp_llm.utils.util import closest_power_of_2
-from rtp_llm.utils.weight_type import WEIGHT_TYPE
-
-updated_params: Set[str] = set()
-
-
-def get_pad_size(size: int, align_size: int):
-    return (align_size - (size % align_size)) % align_size
-
-
-class DataClassBase:
-    @classmethod
-    def from_dict(cls, kvs: Dict[str, Any]):
-        n_kvs = {k: v for k, v in kvs.items() if k in {f.name for f in fields(cls)}}
-
-        # 兼容老的sparse config使用的key 没有加layer
-        for k, v in kvs.items():
-            if k in ["head_num", "inter_size"] and isinstance(v, list):
-                n_kvs.update({"layer_" + k: v})
-
-        data_class = cls(**n_kvs)
-        return data_class
-
-
-mc_sim_7b_63 = [
-    [0],
-    [0, 0],
-    [1],
-    [0, 1],
-    [2],
-    [0, 0, 0],
-    [1, 0],
-    [0, 2],
-    [3],
-    [0, 3],
-    [4],
-    [0, 4],
-    [2, 0],
-    [0, 5],
-    [0, 0, 1],
-    [5],
-    [0, 6],
-    [6],
-    [0, 7],
-    [0, 1, 0],
-    [1, 1],
-    [7],
-    [0, 8],
-    [0, 0, 2],
-    [3, 0],
-    [0, 9],
-    [8],
-    [9],
-    [1, 0, 0],
-    [0, 2, 0],
-    [1, 2],
-    [0, 0, 3],
-    [4, 0],
-    [2, 1],
-    [0, 0, 4],
-    [0, 0, 5],
-    [0, 0, 0, 0],
-    [0, 1, 1],
-    [0, 0, 6],
-    [0, 3, 0],
-    [5, 0],
-    [1, 3],
-    [0, 0, 7],
-    [0, 0, 8],
-    [0, 0, 9],
-    [6, 0],
-    [0, 4, 0],
-    [1, 4],
-    [7, 0],
-    [0, 1, 2],
-    [2, 0, 0],
-    [3, 1],
-    [2, 2],
-    [8, 0],
-    [0, 5, 0],
-    [1, 5],
-    [1, 0, 1],
-    [0, 2, 1],
-    [9, 0],
-    [0, 6, 0],
-    [0, 0, 0, 1],
-    [1, 6],
-    [0, 7, 0],
-]
-
-
-@dataclass
-class SparseConfig(DataClassBase):
-    layer_num: int = 0
-    layer_head_num: List[int] = field(default_factory=lambda: [])
-    layer_inter_size: List[int] = field(default_factory=lambda: [])
-
-    def check(self) -> bool:
-        if self.layer_num == 0:
-            logging.info("sparse config layer_num must not be empty")
-            return False
-        if len(self.layer_head_num) != self.layer_num:
-            logging.info(
-                f"sparse config layer_num and head_num must match, layer_num: {self.layer_num}, head_num: {self.layer_head_num}"
-            )
-            return False
-        if len(self.layer_inter_size) != self.layer_num:
-            logging.info(
-                f"sparse config layer_num and inter_size must match, layer_num: {self.layer_num}, inter_size: {self.layer_inter_size}"
-            )
-            return False
-        return True
-
-
-class VitParameters:
-    # config includes origin vit config in ckpt/config.json
-    config: Dict[str, Any] = {}
-    special_token_ids: Dict[str, Any] = {}
-    special_tokens: Dict[str, Any] = {}
-    vit_weights: Any = None
-
-
-class TemplateType(Enum):
-    chat = "chat"
-    vqa = "vqa"
-    base = "image"
-
-
-class ConfigMode(Enum):
-    SimpleMode = 1
-    ComplexMode = 2
-
-
-class GptInitModelParameters:
-    __slots__ = {
-        "gpt_init_params",
-        "_model_related_types",
-        "has_lm_head_bias",
-        "src_quantization_bit",
-        "ptuning_path",
-        "tp_split_emb_and_lm_head",
-        "mm_related_params",
-        "lora_infos",
-        "multi_task_prompt",
-        "normalize_lm_head_weight",
-        "ref_module",
-        "ref_dict",
-        "tie_word_embeddings",
-        "task_type",
-        "add_special_tokens",
-        "template_type",
-        "build_position_ids",
-        "vit_run_batch",
-        "phy2log",
-        "is_mtp",
-        "num_nodes",
-        "quant_config",
-        "py_env_configs",
-        "config_dtype",
-        "th_nccl_port",
-    }
-
-    # copy from rtp_llm/ops/libth_transformer.pyi for python intelligence
-    activation_type: str
-    add_bias_linear: bool
-    block_nums: int
-    cache_store_connect_port: int
-    cache_store_listen_port: int
-    cache_store_rdma_connect_port: int
-    cache_store_rdma_listen_port: int
-    cache_store_rdma_mode: bool
-    ckpt_path: str
-    cross_attn_input_len: int
-    data_type: str
-    decode_entrance: bool
-    config_dtype: str
-    decode_polling_kv_cache_step_ms: int
-    decode_retry_timeout_ms: int
-    decode_retry_times: int
-    deepseek_mscale_all_dim: float
-    deepseek_rope_mscale: float
-    dp_rank: int
-    dp_size: int
-    dp_tp_nccl_port: int
-    th_nccl_port: int
-    embedding_size: int
-    enable_eplb: bool
-    enable_fast_gen: bool
-    enable_partial_fallback: bool
-    enable_sp: bool
-    enable_speculative_decoding: bool
-    ep_rank: int
-    ep_size: int
-    eplb_mode: EplbMode
-    eplb_update_time: int
-    expert_num: int
-    fast_gen_max_context_len: int
-    ffn_tp_nccl_port: int
-    ffn_tp_rank: int
-    ffn_tp_size: int
-    gen_num_per_circle: int
-    has_lm_head: bool
-    has_moe_norm: bool
-    has_positional_encoding: bool
-    has_post_decoder_layernorm: bool
-    has_pre_decoder_layernorm: bool
-    head_num: int
-    head_num_kv: int
-    hidden_size: int
-    http_port: int
-    include_sep_tokens: bool
-    input_embedding_scalar: float
-    input_vocab_size: int
-    inter_padding_size: int
-    inter_size: int
-    is_causal: bool
-    is_multimodal: bool
-    is_sparse_head: bool
-    kv_cache_data_type: str
-    kv_cache_mem_mb: int
-    kv_lora_rank: int
-    layer_head_num: list[int]
-    layer_head_num_kv: list[int]
-    layer_inter_padding_size: list[int]
-    layer_inter_size: list[int]
-    layer_num: int
-    layernorm_eps: float
-    layernorm_type: str
-    load_cache_timeout_ms: int
-    local_rank: int
-    logit_scale: float
-    max_context_batch_size: int
-    max_generate_batch_size: int
-    max_rpc_timeout_ms: int
-    max_seq_len: int
-    mla_ops_type: MlaOpsType
-    mm_position_ids_style: int
-    mm_sep_tokens: list[list[int]]
-    model_name: str
-    model_rpc_port: int
-    moe_inter_padding_size: int
-    moe_k: int
-    moe_layer_index: list[int]
-    moe_n_group: int
-    moe_normalize_expert_scale: bool
-    moe_style: int
-    moe_topk_group: int
-    routed_scaling_factor: float
-    mrope_section: list[int]
-    nccl_ip: str
-    nope_head_dim: int
-    norm_type: str
-    num_layers: int
-    num_valid_layer: int
-    org_embedding_max_pos: int
-    phy_exp_num: int
-    position_id_len_factor: int
-    position_ids_style: int
-    pre_allocate_op_mem: bool
-    pre_seq_len: int
-    prefill_max_wait_timeout_ms: int
-    prefill_retry_timeout_ms: int
-    prefill_retry_times: int
-    prefix_projection: bool
-    py_eplb: typing.Any
-    q_lora_rank: int
-    q_scaling: float
-    qk_norm: bool
-    quant_algo: QuantAlgo
-    rdma_connect_retry_times: int
-    remote_rpc_server_port: int
-    reserve_runtime_mem_mb: int
-    residual_scalar: float
-    reuse_cache: bool
-    reverse_e_h_norm: bool
-    rope_head_dim: int
-    rotary_embedding_base: float
-    rotary_embedding_dim: int
-    rotary_embedding_mscale: float
-    rotary_embedding_offset: int
-    rotary_embedding_scale: float
-    rotary_embedding_style: int
-    rotary_factor1: float
-    rotary_factor2: float
-    partial_rotary_factor: float
-    rotary_embedding_extrapolation_factor: float
-    scheduler_reserve_resource_ratio: int
-    scoring_func: int
-    seq_size_per_block: int
-    size_per_head: int
-    softmax_extra_scale: float
-    special_tokens: SpecialTokens
-    tokenizer_path: str
-    tp_nccl_port: int
-    tp_rank: int
-    tp_size: int
-    type_vocab_size: int
-    use_all_gather: bool
-    use_attention_linear_bias: bool
-
-    use_cross_attn: bool
-    use_fp32_to_compute_logit: bool
-    use_kvcache: bool
-    use_logn_attn: bool
-    use_mla: bool
-    use_norm_attn_out_residual: bool
-    use_norm_input_residual: bool
-    using_hf_sampling: bool
-    v_head_dim: int
-    vit_separation: int
-    vocab_size: int
-    warm_up: bool
-    warm_up_with_loss: bool
-    worker_addrs: list[str]
-    worker_grpc_addrs: list[str]
-    worker_port_offset: int
-    world_size: int
-    role_type: RoleType
-    quant_config: Optional[QuantizationConfig]
-
-    batch_decode_scheduler_config: BatchDecodeSchedulerConfig
-    cache_store_config: CacheStoreConfig
-    concurrency_config: ConcurrencyConfig
-    device_resource_config: DeviceResourceConfig
-    fifo_scheduler_config: FIFOSchedulerConfig
-    fmha_config: FMHAConfig
-    hw_kernel_config: HWKernelConfig
-    kv_cache_config: KVCacheConfig
-    misc_config: MiscellaneousConfig
-    arpc_config: ArpcConfig
-    model_specific_config: ModelSpecificConfig
-    moe_config: MoeConfig
-    parallelism_distributed_config: ParallelismDistributedConfig
-    profiling_debug_logging_config: ProfilingDebugLoggingConfig
-    scheduler_config: SchedulerConfig
-    service_discovery_config: ServiceDiscoveryConfig
-    speculative_decoding_config: SpeculativeExecutionConfig
-    py_env_configs: PyEnvConfigs
-
-    def __init__(
-        self,
-        head_num: int,
-        size_per_head: int,
-        layer_num: int,
-        max_seq_len: int,
-        vocab_size: int,
-        **kwargs: Any,
-    ):
-        hidden_size = head_num * size_per_head
-        self.gpt_init_params = GptInitParameter(
-            head_num, size_per_head, layer_num, max_seq_len, vocab_size, hidden_size
-        )
-        self._model_related_types: Dict[str, str] = {
-            "layernorm_type": "setLayerNormType",
-            "norm_type": "setNormType",
-            "activation_type": "setActivationType",
-            "data_type": "setDataType",
-            "kv_cache_data_type": "setKvCacheDataType",
-        }
-        self.has_lm_head_bias = False
-        self.normalize_lm_head_weight = False
-        self.src_quantization_bit = 0
-        self.tp_split_emb_and_lm_head = True
-
-        self.ptuning_path = None
-        self.multi_task_prompt = None
-        self.pre_seq_len = 0
-        self.prefix_projection = False
-        self.mm_related_params: VitParameters = VitParameters()
-        self.ref_module: Optional[torch.nn.Module] = None
-        self.ref_dict: Dict[str, torch.Tensor] = {}
-        self.task_type = TaskType.LANGUAGE_MODEL
-
-        self.tie_word_embeddings = False
-        self.nccl_ip = g_master_info.ip
-        self.tp_nccl_port = g_master_info.tp_nccl_port
-        self.dp_tp_nccl_port = g_master_info.dp_tp_nccl_port
-        self.th_nccl_port = g_master_info.th_nccl_port
-        self.ffn_tp_nccl_port = g_master_info.ffn_tp_nccl_port
-        self.model_rpc_port = g_worker_info.rpc_server_port
-        self.http_port = g_worker_info.http_port
-        self.cache_store_listen_port = g_worker_info.cache_store_listen_port
-        self.cache_store_connect_port = g_worker_info.cache_store_connect_port
-        self.cache_store_rdma_listen_port = g_worker_info.cache_store_rdma_listen_port
-        self.cache_store_rdma_connect_port = g_worker_info.cache_store_rdma_connect_port
-        self.remote_rpc_server_port = g_worker_info.remote_rpc_server_port
-        self.worker_port_offset = WORKER_INFO_PORT_NUM
-
-        self.add_special_tokens = True
-        self.template_type = TemplateType.chat
-        self.build_position_ids = False
-        self.routed_scaling_factor = 1.0
-        self.vit_run_batch = False
-
-        self.is_multimodal = False
-        self.model_name = ""
-
-        self.world_size = g_parallel_info.world_size
-        self.phy2log: List[List[int]] = []
-
-        self.is_mtp = False
-        self.qk_norm = False
-        self.quant_config = None
-        self.config_dtype = None
-
-        # For cpp, we use `gpt_init_params`, `py_env_configs` for python.
-        # There are some common envs in cpp and python, so they will
-        # share some configs together.
-        self.update_gpt_init_params_from_env()
-        self.py_env_configs = PyEnvConfigs()
-        self.py_env_configs.update_from_env()
-        self.py_env_configs.parallelism_distributed_config = (
-            self.gpt_init_params.parallelism_distributed_config
-        )
-        StaticConfig.parallelism_distributed_config = (
-            self.gpt_init_params.parallelism_distributed_config
-        )
-
-        self.vit_separation = self.py_env_configs.vit_config.vit_separation
-        logging.info(f"vit_separation: {self.vit_separation}")
-        self.role_type = (
-            RoleType.VIT
-            if self.vit_separation == 1
-            else self.py_env_configs.role_config.role_type
-        )
-
-        for k, v in kwargs.items():
-            setattr(self, k, v)
-
-    # read and write directly through GptInitModelParameters.k
-    def __getattr__(self, k: str):
-        return getattr(self.gpt_init_params, k)
-
-    def __setattr__(self, k: str, v: Any):
-        updated_params.add(k)
-        if k in self.__slots__:
-            object.__setattr__(self, k, v)
-        elif v is not None:
-            self.gpt_init_params.__setattr__(k, v)
-            if k in self._model_related_types:
-                getattr(self.gpt_init_params, self._model_related_types[k])()
-
-    def update(self, update_params: Dict[str, Any]):
-        for k, v in update_params.items():
-            setattr(self, k, v)
-        return self
-
-    def update_worker_addrs(self):
-        worker_addrs = []
-        worker_grpc_addrs = []
-        for member in get_gang_info().members:
-            logging.info(
-                f"member world rank: {member.world_rank}, member local rank: {member.local_rank}, local rank: {self.local_rank}, "
-                f"tp_size: {self.tp_size}, dp_size: {self.dp_size}, dp_rank: {self.dp_rank}, use_all_gather: {self.use_all_gather}"
-            )
-            if int((member.world_rank / self.tp_size) % self.dp_size) == self.dp_rank:
-                worker_addrs.append(
-                    f"{member.ip}:{member.cache_store_listen_port}:{member.cache_store_rdma_listen_port}"
-                )
-                worker_grpc_addrs.append(f"{member.ip}:{member.rpc_server_port}")
-                logging.info(
-                    f"append member for pd sep "
-                    f"{member.ip}:{member.rpc_server_port}, {member.cache_store_listen_port}, "
-                    f"{member.cache_store_rdma_listen_port} to local rank {self.local_rank}, world rank {member.world_rank}"
-                )
-        self.worker_grpc_addrs = worker_grpc_addrs
-        self.worker_addrs = worker_addrs
-
-    def update_gpt_init_params_from_env(
-        self, parallel_info: ParallelInfo = g_parallel_info
-    ):
-
-        # ParallelismDistributedConfig
-        self.gpt_init_params.parallelism_distributed_config = (
-            ParallelismDistributedConfig(
-                tp_size=parallel_info.tp_size,
-                ep_size=parallel_info.ep_size,
-                dp_size=parallel_info.dp_size,
-                world_size=parallel_info.world_size,
-                world_rank=parallel_info.world_rank,
-                local_world_size=parallel_info.local_world_size,
-                pp_size=parallel_info.pp_size,
-                ffn_sp_size=parallel_info.ffn_sp_size,
-            )
-        )
-
-        # CacheStoreConfig
-        self.gpt_init_params.cache_store_config = CacheStoreConfig(
-            cache_store_rdma_mode=get_env_bool("CACHE_STORE_RDMA_MODE", False),
-            wrr_available_ratio=get_env_int("WRR_AVAILABLE_RATIO", 80),
-            rank_factor=get_env_int("RANK_FACTOR", 0),
-            thread_count=get_env_int("CACHE_STORE_THREAD_COUNT", 16),
-            rdma_connect_timeout_ms=get_env_int(
-                "CACHE_STORE_RDMA_CONNECT_TIMEOUT_MS", 250
-            ),
-            rdma_qp_count_per_connection=get_env_int(
-                "CACHE_STORE_RDMA_QP_COUNT_PER_CONNECTION", 2
-            ),
-            messager_worker_thread_count=get_env_int(
-                "MESSAGER_WORKER_THREAD_COUNT", 32
-            ),
-            messager_io_thread_count=get_env_int("MESSAGER_IO_THREAD_COUNT", 4),
-        )
-
-        # ConcurrencyConfig
-        self.gpt_init_params.concurrency_config = ConcurrencyConfig(
-            concurrency_with_block=get_env_bool("CONCURRENCY_WITH_BLOCK", False),
-            concurrency_limit=get_env_int("CONCURRENCY_LIMIT", 32),
-        )
-
-        # FMHAConfig
-        self.gpt_init_params.fmha_config = FMHAConfig(
-            enable_fmha=get_env_bool("ENABLE_FMHA", True),
-            enable_trt_fmha=get_env_bool("ENABLE_TRT_FMHA", True),
-            enable_paged_trt_fmha=get_env_bool("ENABLE_PAGED_TRT_FMHA", True),
-            enable_open_source_fmha=get_env_bool("ENABLE_OPENSOURCE_FMHA", True),
-            enable_paged_open_source_fmha=get_env_bool(
-                "ENABLE_PAGED_OPEN_SOURCE_FMHA", True
-            ),
-            enable_trtv1_fmha=get_env_bool("ENABLE_TRTV1_FMHA", True),
-            fmha_perf_instrument=get_env_bool("FMHA_PERF_INSTRUMENT", False),
-            fmha_show_params=get_env_bool("FMHA_SHOW_PARAMS", False),
-            disable_flash_infer=get_env_bool("DISABLE_FLASH_INFER", False),
-            enable_xqa=get_env_bool("ENABLE_XQA", True),
-        )
-
-        # KVCacheConfig
-        self.gpt_init_params.kv_cache_config = KVCacheConfig(
-            reuse_cache=get_env_bool("REUSE_CACHE", False),
-            multi_task_prompt=get_env_str("MULTI_TASK_PROMPT"),
-            multi_task_prompt_str=get_env_str("MULTI_TASK_PROMPT_STR"),
-            enable_3fs=get_env_bool("ENABLE_3FS", False),
-            match_timeout_ms=get_env_int("THREEFS_MATCH_TIMEOUT_MS", 1000),
-            rpc_get_cache_timeout_ms=get_env_int(
-                "THREEFS_RPC_GET_CACHE_TIMEOUT_MS", 3000
-            ),
-            rpc_put_cache_timeout_ms=get_env_int(
-                "THREEFS_RPC_PUT_CACHE_TIMEOUT_MS", 3000
-            ),
-            threefs_read_timeout_ms=get_env_int("THREEFS_READ_TIMEOUT_MS", 1000),
-            threefs_write_timeout_ms=get_env_int("THREEFS_WRITE_TIMEOUT_MS", 2000),
-            max_block_size_per_item=get_env_int("MAX_BLOCK_SIZE_PER_ITEM", 16),
-            threefs_read_iov_size=get_env_int("THREEFS_READ_IOV_SIZE", 1 << 32),
-            threefs_write_iov_size=get_env_int("THREEFS_WRITE_IOV_SIZE", 1 << 32),
-            memory_block_cache_size_mb=get_env_int("MEMORY_BLOCK_CACHE_SIZE_MB", 0),
-            memory_block_cache_sync_timeout_ms=get_env_int(
-                "MEMORY_BLOCK_CACHE_SYNC_TIMEOUT_MS", 10000
-            ),
-        )
-
-        enable_detail_log = get_env_bool("ENABLE_DETAIL_LOG", False)
-        logging.info(f"enable_detail_log = {enable_detail_log}")
-
-        # ProfilingDebugLoggingConfig
-        self.gpt_init_params.profiling_debug_logging_config = (
-            ProfilingDebugLoggingConfig(
-                trace_memory=get_env_bool("RTP_LLM_TRACE_MEMORY", False),
-                trace_malloc_stack=get_env_bool("RTP_LLM_TRACE_MALLOC_STACK", False),
-                enable_device_perf=get_env_bool("ENABLE_DEVICE_PERF", False),
-                ft_core_dump_on_exception=get_env_bool(
-                    "FT_CORE_DUMP_ON_EXCEPTION", False
-                ),
-                ft_alog_conf_path=get_env_str("FT_ALOG_CONF_PATH"),
-                log_level=get_env_str("LOG_LEVEL", "INFO"),
-                gen_timeline_sync=get_env_bool("GEN_TIMELINE_SYNC", False),
-                torch_cuda_profiler_dir=get_env_str("TORCH_CUDA_PROFILER_DIR", ""),
-                log_path=get_env_str("log_path", "logs"),
-                log_file_backup_count=get_env_int("LOG_FILE_BACKUP_COUNT", 16),
-                nccl_debug_file=get_env_str("NCCL_DEBUG_FILE", ""),
-                debug_load_server=get_env_bool("DEBUG_LOAD_SERVER", False),
-                hack_layer_num=get_env_int("HACK_LAYER_NUM", 0),
-                debug_start_fake_process=get_env_bool(
-                    "DEBUG_START_FAKE_PROCESS", False
-                ),
-                dg_print_reg_reuse=get_env_bool("DG_PRINT_REG_REUSE", False),
-                qwen_agent_debug=get_env_bool("QWEN_AGENT_DEBUG", False),
-                disable_dpc_random=get_env_bool("DISABLE_DPC_RANDOM", False),
-                enable_detail_log=get_env_bool("ENABLE_DETAIL_LOG", False),
-                check_nan=get_env_bool("CHECK_NAN", False),
-            )
-        )
-        # HWKernelConfig
-        self.gpt_init_params.hw_kernel_config = HWKernelConfig(
-            deep_gemm_num_sm=get_env_int("DEEP_GEMM_NUM_SM"),
-            arm_gemm_use_kai=get_env_bool("ARM_GEMM_USE_KAI"),
-            enable_stable_scatter_add=get_env_bool("ENABLE_STABLE_SCATTER_ADD", False),
-            enable_multi_block_mode=get_env_bool("ENABLE_MULTI_BLOCK_MODE", True),
-            rocm_hipblaslt_config=get_env_str(
-                "ROCM_HIPBLASLT_CONFIG", "gemm_config.csv"
-            ),
-            use_swizzleA = (
-                get_env_bool("USE_SWIZZLEA", False)
-            ),
-            ft_disable_custom_ar=get_env_bool("FT_DISABLE_CUSTOM_AR", True),
-            enable_cuda_graph=get_env_bool("ENABLE_CUDA_GRAPH", False),
-            enable_cuda_graph_debug_mode=get_env_bool(
-                "ENABLE_CUDA_GRAPH_DEBUG_MODE", False
-            ),
-            use_aiter_pa=get_env_bool("USE_AITER_PA", True),
-            use_asm_pa=get_env_bool("USE_ASM_PA", True),
-            enable_native_cuda_graph=get_env_bool("ENABLE_NATIVE_CUDA_GRAPH", False),
-            num_native_cuda_graph=get_env_int("NUM_NATIVE_CUDA_GRAPH", 200),
-        )
-
-        # DeviceResourceConfig
-        self.gpt_init_params.device_resource_config = DeviceResourceConfig(
-            device_reserve_memory_bytes=get_env_int("DEVICE_RESERVE_MEMORY_BYTES", 0),
-            host_reserve_memory_bytes=get_env_int(
-                "HOST_RESERVE_MEMORY_BYTES", 4 * 1024 * 1024 * 1024
-            ),
-            overlap_math_sm_count=get_env_int("OVERLAP_MATH_SM_COUNT", 0),
-            overlap_comm_type=get_env_int("OVERLAP_COMM_TYPE", 0),
-            m_split=get_env_int("M_SPLIT", 0),
-            enable_comm_overlap=get_env_bool("ENABLE_COMM_OVERLAP", True),
-            enable_layer_micro_batch=get_env_int("ENABLE_LAYER_MICRO_BATCH", 0),
-            not_use_default_stream=get_env_bool("NOT_USE_DEFAULT_STREAM", False),
-        )
-
-        # MoeConfig
-        self.gpt_init_params.moe_config = MoeConfig(
-            use_deepep_moe=get_env_bool("USE_DEEPEP_MOE", False),
-            use_deepep_internode=get_env_bool("USE_DEEPEP_INTERNODE", False),
-            use_deepep_low_latency=get_env_bool("USE_DEEPEP_LOW_LATENCY", True),
-            use_deepep_p2p_low_latency=get_env_bool(
-                "USE_DEEPEP_P2P_LOW_LATENCY", False
-            ),
-            fake_balance_expert=get_env_bool("FAKE_BALANCE_EXPERT", False),
-            eplb_control_step=get_env_int("EPLB_CONTROL_STEP", 100),
-            eplb_test_mode=get_env_bool("EPLB_TEST_MODE", False),
-            hack_moe_expert=get_env_bool("HACK_MOE_EXPERT", False),
-            eplb_balance_layer_per_step=get_env_int("EPLB_BALANCE_LAYER_PER_STEP", 1),
-            deep_ep_num_sm=get_env_int("DEEP_EP_NUM_SM", 0),
-            max_moe_normal_masked_token_num=get_env_int(
-                "RTP_LLM_MAX_MOE_NORMAL_MASKED_TOKEN_NUM", 1024
-            ),
-        )
-
-        # ModelSpecificConfig
-        self.gpt_init_params.model_specific_config = ModelSpecificConfig(
-            max_lora_model_size=get_env_int("MAX_LORA_MODEL_SIZE"),
-            load_python_model=get_env_bool("LOAD_PYTHON_MODEL", False),
-        )
-
-        # ServiceDiscoveryConfig
-        self.gpt_init_params.service_discovery_config = ServiceDiscoveryConfig(
-            use_local=get_env_bool("USE_LOCAL"),
-            remote_rpc_server_ip=get_env_str("REMOTE_RPC_SERVER_IP"),
-            decode_cm2_config=get_env_str("RTP_LLM_DECODE_CM2_CONFIG"),
-            remote_vit_server_ip=get_env_str("REMOTE_VIT_SERVER_IP"),
-            multimodal_part_cm2_config=get_env_str(
-                "RTP_LLM_MULTIMODAL_PART_CM2_CONFIG"
-            ),
-            # TODO(yinzhi): fix it
-            # remote_backend_ip=get_env_str("REMOTE_BACKEND_IP"),
-            # backend_cm2_config=get_env_str("RTP_LLM_BACKEND_CM2_CONFIG"),
-        )
-
-        # SchedulerConfig
-        self.gpt_init_params.scheduler_config = SchedulerConfig(
-            use_batch_decode_scheduler=get_env_bool("USE_BATCH_DECODE_SCHEDULER"),
-            use_gather_batch_scheduler=get_env_bool("USE_GATHER_BATCH_SCHEDULER"),
-        )
-        if (
-            self.gpt_init_params.scheduler_config.use_gather_batch_scheduler
-            and self.gpt_init_params.scheduler_config.use_batch_decode_scheduler
-        ):
-            raise ValueError(
-                "use_gather_batch_scheduler and use_batch_decode_scheduler cannot be true at the same time"
-            )
-
-        # BatchDecodeSchedulerConfig
-        self.gpt_init_params.batch_decode_scheduler_config = BatchDecodeSchedulerConfig(
-            batch_decode_scheduler_batch_size=get_env_int(
-                "BATCH_DECODE_SCHEDULER_BATCH_SIZE", 1
-            ),
-            batch_decode_scheduler_warmup_type=get_env_int(
-                "BATCH_DECODE_SCHEDULER_WARMUP_TYPE", 0
-            ),
-        )
-
-        # FIFOSchedulerConfig
-        self.gpt_init_params.fifo_scheduler_config = FIFOSchedulerConfig(
-            max_context_batch_size=get_env_int("MAX_CONTEXT_BATCH_SIZE", 1),
-            scheduler_reserve_resource_ratio=get_env_int(
-                "SCHEDULER_RESERVE_RESOURCE_RATIO", 5
-            ),
-            enable_fast_gen=get_env_bool("ENABLE_FAST_GEN", False),
-            enable_partial_fallback=get_env_bool("ENABLE_PARTIAL_FALLBACK", False),
-            fast_gen_context_budget=get_env_int("FAST_GEN_MAX_CONTEXT_LEN", 0),
-        )
-
-        # SpeculativeExecutionConfig
-        self.gpt_init_params.sp_config = SpeculativeExecutionConfig(
-            sp_model_type=get_env_str("SP_MODEL_TYPE", ""),
-            sp_type=get_env_str("SP_TYPE", ""),
-            sp_min_token_match=get_env_int("SP_MIN_TOKEN_MATCH", 2),
-            sp_max_token_match=get_env_int("SP_MAX_TOKEN_MATCH", 2),
-            tree_decode_config=get_env_str("TREE_DECODE_CONFIG", ""),
-            gen_num_per_cycle=get_env_int("GEN_NUM_PER_CIRCLE", 1),
-            force_stream_sample=get_env_bool("FORCE_STREAM_SAMPLE", False),
-            force_score_context_attention=get_env_bool(
-                "FORCE_SCORE_CONTEXT_ATTENTION", True
-            ),
-        )
-
-        # MiscellaneousConfig
-        self.gpt_init_params.misc_config = MiscellaneousConfig(
-            disable_pdl=get_env_bool("DISABLE_PDL", True),
-            aux_string=get_env_str("AUX_STRING", ""),
-        )
-
-        # ArpcConfig
-        self.gpt_init_params.arpc_config = ArpcConfig(
-            threadNum=get_env_int("ARPC_THREAD_NUM", 10),
-            queueNum=get_env_int("ARPC_QUEUE_NUM", 50),
-            ioThreadNum=get_env_int("ARPC_IO_THREAD_NUM", 2),
-        )
-
-        # PD Seperation
-        self.decode_entrance = get_env_bool("DECODE_ENTRANCE", False)
-
-    def update_config_with_sparse_config(self, ckpt_path: str):
-        sparse_config_file = None
-        sparse_config = None
-        if os.path.exists(os.path.join(ckpt_path, "config.json")):
-            sparse_config_file = os.path.join(ckpt_path, "config.json")
-        if self.py_env_configs.sparse_config.sparse_config_file:
-            sparse_config_file = self.py_env_configs.sparse_config.sparse_config_file
-
-        if sparse_config_file is not None:
-            logging.info(f"read sparse config from: {sparse_config_file}")
-            with open(sparse_config_file, "r") as reader:
-                sparse_config_json = json.loads(reader.read())
-                sparse_config = SparseConfig.from_dict(sparse_config_json)
-
-        if sparse_config and sparse_config.check():
-            self.layer_num = sparse_config.layer_num
-            self.layer_head_num = sparse_config.layer_head_num
-            self.layer_head_num_kv = sparse_config.layer_head_num
-            self.layer_inter_size = sparse_config.layer_inter_size
-            self.is_sparse_head = True
-
-    def update_inter_padding_size(self, tp_size: int, ep_size: int, dp_size: int):
-        if tp_size * dp_size != ep_size:
-            raise ValueError(
-                f"tp_size:{tp_size} * dp_size:{dp_size} != ep_size:{ep_size}"
-            )
-        # new tp_size just only for moe
-        if self.quant_algo.isGroupwise():
-            align_size = tp_size * self.quant_algo.getGroupSize()
-            moe_align_size = self.quant_algo.getGroupSize()
-        else:
-            align_size = tp_size * 64
-            moe_align_size = 64
-            if self.quant_algo.isFp8PTPC():
-                moe_align_size = 128
-        if self.layer_inter_size:
-            layer_inter_padding_size = []
-            for idx in range(len(self.layer_inter_size)):
-                inter_size = self.layer_inter_size[idx]
-                layer_inter_padding_size.append(
-                    inter_size
-                    + (
-                        get_pad_size(inter_size, align_size)
-                        if (self.quant_algo.isQuant() or self.gpt_init_params.hw_kernel_config.use_swizzleA)
-                        else 0
-                    )
-                )
-            self.layer_inter_padding_size = layer_inter_padding_size
-        self.inter_padding_size = self.inter_size + (
-            get_pad_size(self.inter_size, align_size)
-            if (self.quant_algo.isQuant() or self.gpt_init_params.hw_kernel_config.use_swizzleA)
-            else 0
-        )
-        if self.head_num_kv <= 0:
-            self.head_num_kv = self.head_num
-        if self.inter_padding_size <= 0:
-            self.inter_padding_size = self.inter_size
-
-        if self.moe_inter_padding_size <= 0:
-            self.moe_inter_padding_size = self.inter_size
-        if self.moe_inter_padding_size > 0:
-            moe_align_size = moe_align_size if self.quant_algo.isQuant() else 8
-            self.moe_inter_padding_size = self.moe_inter_padding_size + (
-                get_pad_size(self.moe_inter_padding_size, moe_align_size)
-            )
-
-        logging.info(
-            f"update_inter_padding_size: {self.inter_padding_size}, moe_inter_padding_size: {self.moe_inter_padding_size}, layer_inter_size: {self.layer_inter_size}"
-        )
-
-    def update_task_prompt_tokens_id(self, tokenizer):
-        if self.multi_task_prompt:
-            for info in self.multi_task_prompt:
-                task_id: str = str(info["task_id"])
-                prompt: str = info["prompt"]
-                tokens_id = tokenizer.encode(prompt)
-                self.insertMultiTaskPromptTokens(task_id, tokens_id)
-
-    def update_tokenizer_special_tokens(self, tokenizer):
-        self.special_tokens.stop_words_id_list += tokenizer.stop_words_id_list
-        self.special_tokens.stop_words_str_list += tokenizer.stop_words_str_list
-        self.special_tokens.eos_token_id = tokenizer.eos_token_id
-
-    def update_task_prompt_config(self):
-        prompt_file_path = self.kv_cache_config.multi_task_prompt
-        if prompt_file_path == "":
-            self.multi_task_prompt = None
-        else:
-            with open(prompt_file_path, "r") as reader:
-                multi_task_prompt = json.loads(reader.read(), strict=False)
-                self.multi_task_prompt = multi_task_prompt
-                return
-
-        prompt_str = self.kv_cache_config.multi_task_prompt_str
-        if prompt_str == "":
-            self.multi_task_prompt = None
-        else:
-            self.multi_task_prompt = json.loads(prompt_str, strict=False)
-            return
-
-    def update_task_type_use_kvcache(self):
-        self.task_type = check_task_type(self.ckpt_path)
-        self.setTaskType(self.task_type.value)
-        self.use_kvcache = self.task_type == TaskType.LANGUAGE_MODEL
-        logging.info(
-            f"model task type: {self.task_type}, use_kvcache: {self.use_kvcache}"
-        )
-
-    def update_common(
-        self,
-        ckpt_path: str,
-        lora_infos: Optional[Dict[str, str]],
-        ptuning_path: Optional[str],
-        tokenizer_path: str,
-        quantization: str,
-        data_type: str,
-        kv_cache_type: str,
-        max_seq_len: int,
-        seq_size_per_block: int,
-        gen_num_per_circle: int,
-        ref_module: Optional[torch.nn.Module] = None,
-        ref_dict: Dict[str, torch.Tensor] = {},
-        parallel_info: ParallelInfo = g_parallel_info,
-        config_mode: ConfigMode = ConfigMode.ComplexMode,
-        gang_info: Optional[GangInfo] = None,
-    ):
-
-        self._init_precision_config(ckpt_path, quantization, data_type, kv_cache_type)
-
-        self.tp_size = parallel_info.tp_size
-        self.tp_rank = parallel_info.tp_rank
-        self.ep_size = parallel_info.ep_size
-        self.ep_rank = parallel_info.ep_rank
-        self.dp_size = parallel_info.dp_size
-        self.dp_rank = parallel_info.dp_rank
-        self.ffn_tp_rank = parallel_info.ffn_tp_rank
-        self.ffn_tp_size = parallel_info.ffn_tp_size
-        self.enable_sp = parallel_info.ffn_sp_size > 1
-        self.local_rank = parallel_info.local_rank
-        self.use_all_gather = (
-            bool(int(os.environ.get("USE_ALL_GATHER", 0)))
-            and self.gpt_init_params.moe_config.use_deepep_low_latency == False
-        )
-        logging.info(f"use_all_gather: {self.use_all_gather}")
-
-        self.eplb_update_time = self.py_env_configs.py_eplb_config.eplb_update_time
-        self.eplb_mode = EplbMode.__members__[
-            self.py_env_configs.py_eplb_config.eplb_mode
-        ]
-        self.enable_eplb = self.eplb_mode != EplbMode.NONE
-
-        self.phy_exp_num = (
-            self.py_env_configs.py_eplb_config.redundant_expert + self.expert_num
-        )
-        logging.info(f"phy_exp_num: {self.phy_exp_num}")
-
-        if gang_info is not None:
-            self.num_nodes = gang_info.num_nodes
-        else:
-            try:
-                self.num_nodes = get_gang_info().num_nodes
-            except:
-                self.num_nodes = 1
-
-        self.ckpt_path = ckpt_path
-        self.lora_infos = lora_infos
-        self.tokenizer_path = tokenizer_path
-
-        self.gen_num_per_circle = gen_num_per_circle
-        self.ptuning_path = ptuning_path
-        self.ref_module = ref_module
-        self.ref_dict = ref_dict
-        if max_seq_len != 0:
-            self.max_seq_len = max_seq_len
-        if self.max_seq_len < 1:
-            # frontend not load ckpt config max_seq_len, use default 8192 or env
-            self.max_seq_len = 8192
-        logging.info(f"max_seq_len: {self.max_seq_len}")
-
-        self.update_task_type_use_kvcache()
-
-        if StaticConfig.ffn_disaggregate_config.enable_ffn_disaggregate:
-            # 暂时先限制tp=1, 更多支持在python版本实现
-            assert (
-                g_parallel_info.tp_size == 1 and g_parallel_info.world_size > 1
-            ), "enable_ffn_disaggregate must be used in dp = 1 world_size > 1"
-            attention_dp_size = g_parallel_info.world_size - 1
-            attention_tp_size = 1
-            ffn_tp_size = 1
-            assert (
-                attention_tp_size == ffn_tp_size
-            ), "attention_tp_size must be equal to ffn_tp_size"
-            self.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate = True
-            self.gpt_init_params.ffn_disaggregate_config.attention_tp_size = (
-                attention_tp_size
-            )
-            self.gpt_init_params.ffn_disaggregate_config.attention_dp_size = (
-                attention_dp_size
-            )
-            self.gpt_init_params.ffn_disaggregate_config.ffn_tp_size = ffn_tp_size
-            # TODO: remove it, ffn dp is stupid
-            self.gpt_init_params.ffn_disaggregate_config.ffn_dp_size = 1
-            self.gpt_init_params.ffn_disaggregate_config.is_ffn_rank = (
-                g_parallel_info.world_rank >= attention_tp_size * attention_dp_size
-            )
-
-        logging.info(f"config_mode = {config_mode}")
-        if config_mode == ConfigMode.SimpleMode:
-            return
-
-        self.update_worker_addrs()
-        self.update_config_with_sparse_config(ckpt_path)
-        self.update_inter_padding_size(self.tp_size, self.ep_size, self.dp_size)
-        self.update_task_prompt_config()
-
-        load_cutlass_gemm_config(self.quant_algo)
-
-        hack_layer_num = self.profiling_debug_logging_config.hack_layer_num
-        if hack_layer_num:
-            logging.info(f"hack layernum to {hack_layer_num}")
-            self.layer_num = hack_layer_num
-
-        self.seq_size_per_block = closest_power_of_2(
-            int(max(seq_size_per_block, self.max_seq_len // 128))
-        )  # must be 2^n
-        if self.py_env_configs.py_kv_cache_config.seq_size_per_block != -1:
-            self.seq_size_per_block = int(
-                self.py_env_configs.py_kv_cache_config.seq_size_per_block
-            )
-
-        logging.info(f"seq_size_per_block: {self.seq_size_per_block}")
-        self.max_generate_batch_size = (
-            self.py_env_configs.concurrency_config.concurrency_limit
-        )
-
-        logging.info(f"max_generate_batch_size: {self.max_generate_batch_size}")
-        self.max_context_batch_size = self.fifo_scheduler_config.max_context_batch_size
-        logging.info(f"max_context_batch_size: {self.max_context_batch_size}")
-        self.reserve_runtime_mem_mb = (
-            self.py_env_configs.py_device_resource_config.reserver_runtime_mem_mb
-        )
-        logging.info(f"reserve_runtime_mem_mb: {self.reserve_runtime_mem_mb}")
-        self.kv_cache_mem_mb = self.py_env_configs.py_kv_cache_config.kv_cache_mem_mb
-        logging.info(f"kv_cache_mem_mb: {self.kv_cache_mem_mb}")
-        self.block_nums = self.py_env_configs.py_kv_cache_config.test_block_num
-        logging.info(f"block_nums: {self.block_nums}")
-        self.enable_partial_fallback = (
-            self.fifo_scheduler_config.enable_partial_fallback
-        )
-        logging.info(f"enable_partial_fallback: {self.enable_partial_fallback}")
-        self.enable_fast_gen = self.fifo_scheduler_config.enable_fast_gen
-        logging.info(f"enable_fast_gen: {self.enable_fast_gen}")
-        self.warm_up = bool(self.py_env_configs.engine_config.warm_up)
-        logging.info(f"warm_up: {self.warm_up}")
-        self.warm_up_with_loss = bool(
-            self.py_env_configs.engine_config.warm_up_with_loss
-        )
-        logging.info(f"warm_up_with_loss: {self.warm_up_with_loss}")
-
-        self.fast_gen_max_context_len = (
-            1024
-            if self.fifo_scheduler_config.fast_gen_context_budget == -1
-            else self.fifo_scheduler_config.fast_gen_context_budget
-        )
-        logging.info(f"fast_gen_max_context_len: {self.fast_gen_max_context_len}")
-
-        self.max_rpc_timeout_ms = int(os.environ.get("MAX_RPC_TIMEOUT_MS", 0))
-        logging.info(f"max_rpc_timeout_ms: {self.max_rpc_timeout_ms}")
-
-        self.max_batch_tokens_size = int(
-            os.environ.get(
-                "MAX_BATCH_TOKENS_SIZE", self.max_context_batch_size * self.max_seq_len
-            )
-        )
-        logging.info(f"max_batch_tokens_size: {self.max_batch_tokens_size}")
-
-        if self.role_type in [RoleType.PREFILL]:
-            self.prefill_retry_times = (
-                self.py_env_configs.pd_separation_config.prefill_retry_times
-            )
-            logging.info(f"prefill_retry_times: {self.prefill_retry_times}")
-            self.prefill_retry_timeout_ms = (
-                self.py_env_configs.pd_separation_config.prefill_retry_timeout_ms
-            )
-            logging.info(f"prefill_retry_timeout_ms: {self.prefill_retry_timeout_ms}")
-            self.prefill_max_wait_timeout_ms = (
-                self.py_env_configs.pd_separation_config.prefill_max_wait_timeout_ms
-            )
-            logging.info(
-                f"prefill_max_wait_timeout_ms: {self.prefill_max_wait_timeout_ms}"
-            )
-
-        if self.role_type in [RoleType.PREFILL, RoleType.DECODE]:
-            self.cache_store_rdma_mode = (
-                self.gpt_init_params.cache_store_config.cache_store_rdma_mode
-            )
-            logging.info(f"cache_store_rdma_mode: {self.cache_store_rdma_mode}")
-
-            self.load_cache_timeout_ms = (
-                self.py_env_configs.pd_separation_config.load_cache_timeout_ms
-            )
-            logging.info(f"load_cache_timeout_ms: {self.load_cache_timeout_ms}")
-
-            self.decode_retry_times = (
-                self.py_env_configs.pd_separation_config.decode_retry_times
-            )
-            logging.info(f"decode_retry_times: {self.decode_retry_times}")
-            self.decode_retry_timeout_ms = (
-                self.py_env_configs.pd_separation_config.decode_retry_timeout_ms
-            )
-            logging.info(f"decode_retry_timeout_ms: {self.decode_retry_timeout_ms}")
-
-            self.rdma_connect_retry_times = (
-                self.py_env_configs.pd_separation_config.rdma_connect_retry_times
-            )
-            logging.info(f"rdma_connect_retry_times: {self.rdma_connect_retry_times}")
-
-            self.decode_polling_kv_cache_step_ms = (
-                self.py_env_configs.pd_separation_config.decode_polling_kv_cache_step_ms
-            )
-            logging.info(
-                f"decode_polling_kv_cache_step_ms: {self.decode_polling_kv_cache_step_ms}"
-            )
-
-            self.decode_polling_call_prefill_ms = int(
-                os.environ.get("DECODE_POLLING_CALL_PREFILL_MS", 30)
-            )
-            logging.info(
-                f"decode_polling_call_prefill_ms: {self.decode_polling_call_prefill_ms}"
-            )
-
-            self.decode_entrance = bool(
-                self.py_env_configs.pd_separation_config.decode_entrance
-            )
-            logging.info(f"decode_entrance: {self.decode_entrance}")
-
-        self.scheduler_reserve_resource_ratio = int(
-            os.environ.get("SCHEDUlER_RESERVE_RESOURCE_RATIO", 5)
-        )
-        logging.info(
-            f"scheduler_reserve_resource_ratio: {self.scheduler_reserve_resource_ratio}"
-        )
-        self.reuse_cache = self.py_env_configs.py_kv_cache_config.reuse_cache
-        logging.info(f"reuse_cache: {self.reuse_cache}")
-        self.pre_allocate_op_mem = bool(int(os.environ.get("PRE_ALLOCATE_OP_MEM", 1)))
-        logging.info(f"pre_allocate_op_mem: {self.pre_allocate_op_mem}")
-        logging.info(f"tp_split_emb_and_lm_head: {self.tp_split_emb_and_lm_head}")
-
-        # use environment variables to update stop_words_str and stop_words_id
-        env_stop_words_str = self.py_env_configs.generate_env_config.stop_words_str
-        env_stop_words_id = self.py_env_configs.generate_env_config.stop_words_list
-        env_stop_words_str_list = (
-            json.loads(env_stop_words_str) if env_stop_words_str else []
-        )
-        env_stop_words_id_list = (
-            json.loads(env_stop_words_id) if env_stop_words_id else []
-        )
-        env_force_stop = self.py_env_configs.generate_env_config.force_stop_words
-        if env_force_stop:
-            self.special_tokens.stop_words_str_list = env_stop_words_str_list
-            self.special_tokens.stop_words_id_list = env_stop_words_id_list
-        else:
-            self.special_tokens.stop_words_str_list = (
-                self.special_tokens.stop_words_str_list + env_stop_words_str_list
-            )
-            self.special_tokens.stop_words_id_list = (
-                self.special_tokens.stop_words_id_list + env_stop_words_id_list
-            )
-
-        logging.info(
-            f"use stop_words_str_list [{self.special_tokens.stop_words_str_list }],"
-            f" stop_words_id_list [{self.special_tokens.stop_words_id_list}]"
-        )
-
-        model_override_args = json.loads(
-            StaticConfig.model_config.json_model_override_args
-        )
-        if model_override_args:
-            if "rope_scaling" in model_override_args:
-                # be consistent with RopeStyle
-                rope_type = {
-                    "no": 0,
-                    "base": 1,
-                    "glm2": 2,
-                    "dynamicntk": 3,
-                    "qwendynamicntk": 4,
-                    "yarn": 5,
-                    "llama3": 6,
-                    "mrope": 7,
-                }
-                rope_override_args = model_override_args["rope_scaling"]
-                assert (
-                    "type" in rope_override_args
-                    and rope_override_args["type"] in rope_type
-                )
-                self.rotary_embedding_style = rope_type[rope_override_args["type"]]
-                if rope_override_args["type"] == "yarn":
-                    assert (
-                        "factor" in rope_override_args
-                        and "original_max_position_embeddings" in rope_override_args
-                    )
-                    self.rotary_embedding_scale = rope_override_args["factor"]
-                    self.org_embedding_max_pos = rope_override_args[
-                        "original_max_position_embeddings"
-                    ]
-                    self.rotary_factor1 = rope_override_args.get("beta_slow", 1.0)
-                    self.rotary_factor2 = rope_override_args.get("beta_fast", 1.0)
-                    mscale = rope_override_args.get("mscale", 1.0)
-                    self.rotary_embedding_mscale = float(
-                        (
-                            1.0
-                            if self.rotary_embedding_scale <= 1
-                            else 0.1 * math.log(self.rotary_embedding_scale) + 1.0
-                        )
-                        * mscale
-                    )
-                    self.rotary_embedding_extrapolation_factor = rope_override_args.get(
-                        "extrapolation_factor", 1.0
-                    )
-
-                logging.info(
-                    f"rotary_embedding_style: {self.rotary_embedding_style}, "
-                    f"rotary_embedding_scale: {self.rotary_embedding_scale}, "
-                    f"org_embedding_max_pos: {self.org_embedding_max_pos}, "
-                    f"rotary_factor1: {self.rotary_factor1}, "
-                    f"rotary_factor2: {self.rotary_factor2}, "
-                    f"rotary_embedding_mscale: {self.rotary_embedding_mscale}, "
-                    f"rotary_embedding_extrapolation_factor: {self.rotary_embedding_extrapolation_factor}"
-                )
-
-    def _init_precision_config(
-        self,
-        ckpt_path: str,
-        quantization: str,
-        data_type_str: Optional[str],
-        kv_cache_dtype_str: Optional[str],
-    ):
-        quant_config = self._load_quant_config_from_ckpt(ckpt_path)
-        if not quant_config:
-            if quantization:
-                quant_config = init_quant_config(quantization)
-                logging.info(f"need_load_quant by {quant_config.get_method()}")
-        if quant_config:
-            self.quant_algo.setQuantAlgo(
-                quant_config.get_algo().lower(),
-                quant_config.bits,
-                quant_config.group_size(),
-            )
-
-        # Verify the data_type
-        data_type, kv_cache_data_type = self._get_and_verify_dtype(
-            quant_config, data_type_str, kv_cache_dtype_str
-        )
-
-        self.quant_config = quant_config
-        self.data_type = data_type.to_str()
-        self.kv_cache_data_type = kv_cache_data_type.to_str()
-        logging.info(
-            f"quant_config: {self.quant_config}, data_type:{self.data_type}, kv_cache_data_type: {self.kv_cache_data_type}"
-        )
-
-    @staticmethod
-    def _load_quant_config_from_ckpt(ckpt_path: str) -> Optional[QuantizationConfig]:
-        quant_config_path = os.path.join(ckpt_path, "smoothquant.ini")
-        if os.path.exists(quant_config_path):
-            return QuantizationConfig.from_config(
-                {
-                    "bits": 0,
-                    "method": "smooth_quant",
-                    "group_size": 0,
-                    "is_quanted": True,
-                }
-            )
-
-        per_tensor_config_path = os.path.join(ckpt_path, "pertensorquant.ini")
-
-        if os.path.exists(per_tensor_config_path):
-            return QuantizationConfig.from_config(
-                {
-                    "bits": 0,
-                    "method": "pertensor_quant",
-                    "group_size": 0,
-                    "is_quanted": True,
-                }
-            )
-
-        config_path = os.path.join(ckpt_path, "config.json")
-        if not os.path.exists(config_path):
-            return None
-
-        config_json = json.load(open(config_path))
-        quant_config = None
-        quant_method = None
-        if config_json.get("quantization_config", None):
-            quant_config = config_json["quantization_config"]
-            quant_method = quant_config["quant_method"].lower()
-
-        if config_json.get("quantization", None):
-            quant_config = config_json["quantization"]
-            quant_method = quant_config["quant_algo"].lower()
-        if quant_config is None:
-            return None
-
-        group_size = quant_config["group_size"] if "group_size" in quant_config else 0
-        bits = quant_config["bits"] if "bits" in quant_config else 0
-        if quant_method == "fp8":
-            bits = 8
-            if "weight_block_size" in quant_config:
-                weight_block = quant_config.get("weight_block_size")
-                assert isinstance(weight_block, list) and all(
-                    element == weight_block[0] for element in weight_block
-                ), f"weight_block_size: {weight_block} must be same"
-                group_size = weight_block[0]
-                quant_method = Fp8BlockWiseQuantConfig.get_method()
-        if quant_method == "compressed-tensors":
-            config_groups = quant_config["config_groups"]
-            weights_config = config_groups["group_0"]["weights"]
-            activation_config = config_groups["group_0"]["input_activations"]
-            bits = weights_config["num_bits"]
-            if (
-                weights_config["type"] == "float"
-                and bits == 8
-                and weights_config["strategy"] == "channel"
-            ):
-                quant_method = Fp8PerChannelCompressedQuantConfig.get_method()
-            elif (
-                weights_config["type"] == "float"
-                and bits == 8
-                and weights_config["strategy"] == "tensor"
-            ):
-                quant_method = Fp8PerTensorCompressedQuantConfig.get_method()
-                return Fp8PerTensorCompressedQuantConfig.from_config(
-                    {
-                        "bits": bits,
-                        "method": quant_method,
-                        "group_size": group_size,
-                        "is_quanted": True,
-                        "dynamic": activation_config["dynamic"],
-                        "act_scale_suffix": ".input_scale",
-                        "weight_scale_suffix": ".weight_scale",
-                    }
-                )
-
-        return QuantizationConfig.from_config(
-            {
-                "bits": bits,
-                "method": quant_method,
-                "group_size": group_size,
-                "is_quanted": True,
-            }
-        )
-
-    def _get_and_verify_dtype(
-        self, quant_config: QuantizationConfig, data_type_str, kv_cache_dtype_str
-    ):
-        data_type: WEIGHT_TYPE = None
-        config_dtype = (
-            WEIGHT_TYPE.from_str(self.config_dtype) if self.config_dtype else None
-        )
-        if data_type_str:
-            data_type = WEIGHT_TYPE.from_str(data_type_str)
-            logging.info(f"set data_type by args: {data_type}")
-
-        if not data_type or data_type == WEIGHT_TYPE.AUTO:
-            data_type = config_dtype if config_dtype else WEIGHT_TYPE.FP16
-            logging.info(
-                f"data_type is not set or it's auto,we will use config_dtype:{config_dtype} or {WEIGHT_TYPE.FP16}"
-            )
-        if quant_config and isinstance(quant_config, Fp8BlockWiseQuantConfig):
-            data_type = WEIGHT_TYPE.BF16  # now fp8_block_wise only support bf16
-            logging.info(f"now fp8_block_wise only support bf16")
-        elif quant_config and quant_config.get_method().lower() in [
-            "smooth_quant",
-            "omni_quant",
-        ]:
-            data_type = WEIGHT_TYPE.FP16
-
-        if config_dtype and data_type != config_dtype:
-            if data_type == WEIGHT_TYPE.FP32:
-                # Upcasting to float32 is allowed.
-                logging.info("Upcasting %s to %s.", config_dtype, data_type)
-            elif config_dtype == WEIGHT_TYPE.FP32:
-                # Downcasting from float32 to float16 or bfloat16 is allowed.
-                logging.info("Downcasting %s to %s.", config_dtype, data_type)
-            else:
-                # Casting between float16 and bfloat16 is allowed with a warning.
-                logging.warning("Casting %s to %s.", config_dtype, data_type)
-
-        kv_cache_data_type: Optional[WEIGHT_TYPE] = (
-            WEIGHT_TYPE.from_str(kv_cache_dtype_str)
-            if kv_cache_dtype_str
-            else data_type
-        )
-        if quant_config and quant_config.get_method().lower() == "fp8":
-            kv_cache_data_type = WEIGHT_TYPE.FP8
-
-        if kv_cache_data_type == WEIGHT_TYPE.AUTO:
-            kv_cache_data_type: WEIGHT_TYPE = data_type
-
-        if quant_config:
-            quant_config.verify_compute_dtype_and_kv_cache_dtype(
-                data_type.to_torch_dtype(), kv_cache_data_type.to_torch_dtype()
-            )
-        return (data_type, kv_cache_data_type)
-
-    def get_params_dict(self):
-        res: Dict[str, Any] = {}
-        for name in updated_params:
-            res[name] = eval("self." + name)
-        return res
-
-    def eval_model_size(self):
-        layer_param_bytes = 2
-        if self.quant_algo.getWeightBits() == 8:
-            layer_param_bytes = 1
-        elif self.quant_algo.getWeightBits() == 4:
-            layer_param_bytes = 0.54
-
-        model_size = (
-            self.word_emb_param_count * 2
-            + self.layer_weight_param_count * layer_param_bytes
-            + self.gpt_init_params.hidden_size * layer_param_bytes
-            + self.word_emb_param_count * 2
-        )  # maybe some model donot have lm_head
-
-        kv_cache_mem_size = self._eval_kv_cache_mem_size()
-        runtime_buffer = self._eval_runtime_buffer_mem_size()
-        total_size = model_size + kv_cache_mem_size + runtime_buffer
-        logging.info(
-            f"total_size(Bytes): {total_size}, model_size:{model_size}, kv_cache_mem_size:{kv_cache_mem_size}, runtime_buffer:{runtime_buffer}"
-        )
-        return total_size
-
-    def _eval_kv_cache_mem_size(self):
-        if self.task_type != TaskType.LANGUAGE_MODEL:
-            return 0
-        kv_cache_bytes = (
-            1
-            if self.kv_cache_data_type
-            in [WEIGHT_TYPE.FP8.to_str(), WEIGHT_TYPE.INT8.to_str()]
-            else 2
-        )
-        kv_cache_size = (
-            2
-            * self.layer_num
-            * self.head_num_kv
-            * self.size_per_head
-            * kv_cache_bytes
-            * self.max_seq_len
-        )
-        return kv_cache_size
-
-    def _eval_runtime_buffer_mem_size(self):
-        input_buffer = self.max_seq_len * self.gpt_init_params.hidden_size
-        qkv_gemm_buffer_size = (
-            self.max_seq_len
-            * (self.head_num_kv * 2 + self.head_num_kv)
-            * self.size_per_head
-        )
-        attn_buffer_size = self.max_seq_len * self.gpt_init_params.hidden_size
-        ffn_export_num = self.expert_num if self.gpt_init_params.moe_k else 1
-        ffn_w_count = 1 if self.activation_type == "gelu" else 2
-        ffn_buffer = (
-            self.max_seq_len * self.gpt_init_params.hidden_size
-            + ffn_w_count * self.max_seq_len * self.inter_size
-        ) * ffn_export_num
-        return input_buffer + qkv_gemm_buffer_size + attn_buffer_size + ffn_buffer
-
-    @property
-    def model_param_count(self):
-        return (
-            self.word_emb_param_count * 2
-            + self.layer_weight_param_count
-            + self.gpt_init_params.hidden_size
-        )
-
-    @property
-    def word_emb_param_count(self):
-        return self.vocab_size * self.gpt_init_params.hidden_size
-
-    @property
-    def layer_weight_param_count(self):
-        hidden_size = self.gpt_init_params.hidden_size
-
-        layer_weight_param_count = 0
-        # qkv
-        if self.layer_head_num and isinstance(self.layer_head_num, list):
-            for head_num in self.layer_head_num:
-                layer_weight_param_count = (
-                    layer_weight_param_count
-                    + head_num * self.size_per_head * hidden_size * 3
-                )
-        elif self.head_num_kv != self.head_num:
-            layer_weight_param_count = (
-                layer_weight_param_count
-                + self.layer_num * hidden_size * hidden_size
-                + self.layer_num * (self.head_num_kv * self.size_per_head) * 2
-            )
-        else:
-            layer_weight_param_count = (
-                layer_weight_param_count
-                + self.layer_num * hidden_size * hidden_size * 3
-            )
-
-        # attn_o_w
-        if self.layer_head_num and isinstance(self.layer_head_num, list):
-            for head_num in self.layer_head_num:
-                layer_weight_param_count = (
-                    layer_weight_param_count
-                    + head_num * self.size_per_head * hidden_size
-                )
-        else:
-            layer_weight_param_count = (
-                layer_weight_param_count + self.layer_num * hidden_size * hidden_size
-            )
-
-        # ffn w1, w2, w3
-        ffn_export_num = self.expert_num if self.expert_num > 0 else 1
-        ffn_w_count = 2 if self.activation_type == "gelu" else 3
-        if self.layer_inter_size and isinstance(self.layer_inter_size, list):
-            for layer_inter_size in self.layer_inter_size:
-                if self.moe_style == 1:
-                    layer_weight_param_count = (
-                        layer_weight_param_count
-                        + layer_inter_size * hidden_size * ffn_w_count * ffn_export_num
-                    )
-                else:
-                    layer_weight_param_count = (
-                        layer_weight_param_count
-                        + layer_inter_size * hidden_size * ffn_w_count
-                    )
-                    if self.moe_style == 2:
-                        layer_weight_param_count = (
-                            layer_weight_param_count
-                            + self.moe_inter_padding_size
-                            * hidden_size
-                            * ffn_w_count
-                            * ffn_export_num
-                        )
-
-        else:
-            if self.moe_style == 1:
-                layer_weight_param_count = (
-                    layer_weight_param_count
-                    + self.layer_num
-                    * self.inter_size
-                    * hidden_size
-                    * ffn_w_count
-                    * ffn_export_num
-                )
-            else:
-                layer_weight_param_count = (
-                    layer_weight_param_count
-                    + self.layer_num * self.inter_size * hidden_size * ffn_w_count
-                )
-                if self.moe_style == 2:
-                    layer_weight_param_count = (
-                        layer_weight_param_count
-                        + len(self.moe_layer_index)
-                        * self.moe_inter_padding_size
-                        * hidden_size
-                        * ffn_w_count
-                        * ffn_export_num
-                    )
-
-        if ffn_export_num > 1:
-            layer_weight_param_count = (
-                layer_weight_param_count
-                + len(self.moe_layer_index) * hidden_size * ffn_export_num
-            )
-        # other small tensor
-        layer_weight_param_count = (
-            layer_weight_param_count + self.layer_num * hidden_size * 11
-        )
-        return layer_weight_param_count
diff --git a/rtp_llm/config/kv_cache_config.py b/rtp_llm/config/kv_cache_config.py
new file mode 100644
index 000000000..35333f2c3
--- /dev/null
+++ b/rtp_llm/config/kv_cache_config.py
@@ -0,0 +1,44 @@
+"""Python wrapper for KVCacheConfig with additional convenience methods."""
+import json
+from typing import Any, Optional
+
+from rtp_llm.ops import KVCacheConfig as CppKVCacheConfig
+
+
+class KVCacheConfig(CppKVCacheConfig):
+    """Python wrapper for C++ KVCacheConfig with additional convenience methods."""
+
+    def load_and_update_task_prompt_config(self, tokenizer: Optional[Any] = None) -> None:
+        """Load task prompt configuration and update token IDs if tokenizer is provided.
+        
+        This method combines the functionality of load_task_prompt_config and
+        update_task_prompt_tokens_id. It loads the configuration from either
+        multi_task_prompt file or multi_task_prompt_str, stores it in
+        multi_task_prompt_config, and if a tokenizer is provided, updates the
+        token IDs for each task prompt.
+        
+        Args:
+            tokenizer: Optional tokenizer instance with encode method. If provided,
+                     will update task prompt tokens from the loaded configuration.
+        """
+        # Load task prompt configuration
+        prompt_file_path = self.multi_task_prompt
+        if prompt_file_path != "":
+            with open(prompt_file_path, "r") as reader:
+                self.multi_task_prompt_config = json.loads(reader.read(), strict=False)
+        elif self.multi_task_prompt_str != "":
+            self.multi_task_prompt_config = json.loads(self.multi_task_prompt_str, strict=False)
+        else:
+            self.multi_task_prompt_config = None
+            return
+        
+        # Update task prompt tokens if tokenizer is provided
+        if tokenizer and self.multi_task_prompt_config:
+            multi_task_prompt = self.multi_task_prompt_config
+            if isinstance(multi_task_prompt, list):
+                for info in multi_task_prompt:
+                    task_id: str = str(info["task_id"])
+                    prompt: str = info["prompt"]
+                    tokens_id = tokenizer.encode(prompt)
+                    self.insertMultiTaskPromptTokens(task_id, tokens_id)
+
diff --git a/rtp_llm/config/log_config.py b/rtp_llm/config/log_config.py
index 035822265..30041d8fb 100644
--- a/rtp_llm/config/log_config.py
+++ b/rtp_llm/config/log_config.py
@@ -1,42 +1,53 @@
-from rtp_llm.config.py_config_modules import StaticConfig
+from typing import Dict, Any
+import os
 
-## reserve this env
-world_rank = StaticConfig.parallelism_distributed_config.world_rank
-LOGGING_CONFIG = {
-    "version": 1,
-    "disable_existing_loggers": False,
-    "formatters": {
-        "default": {
-            "format": "[%(name)s][%(asctime)s.%(msecs)03d][%(process)d][%(threadName)s][%(pathname)s:%(funcName)s():%(lineno)s][%(levelname)s] %(message)s",
-            "datefmt": "%Y-%m-%d %H:%M:%S",  # 只包含到秒，毫秒在 fmt 中处理
-        },
-    },
-    "handlers": {
-        "file_handler": {
-            "formatter": "default",
-            "class": "logging.handlers.RotatingFileHandler",
-            "filename": f"logs/main_{world_rank}.log",
-            "maxBytes": 256 * 1024 * 1024,
-            "backupCount": 20,
-        },
-        "route_file_handler": {
-            "formatter": "default",
-            "class": "logging.handlers.RotatingFileHandler",
-            "filename": f"logs/route.log",
-            "maxBytes": 256 * 1024 * 1024,
-            "backupCount": 20,
+
+def get_logging_config(world_rank: int = 0) -> Dict[str, Any]:
+    """Generate logging configuration with specified world_rank.
+    
+    Args:
+        world_rank: The world rank for the log file name.
+        
+    Returns:
+        Dictionary containing logging configuration.
+    """
+    return {
+        "version": 1,
+        "disable_existing_loggers": False,
+        "formatters": {
+            "default": {
+                "format": "[%(name)s][%(asctime)s.%(msecs)03d][%(process)d][%(threadName)s][%(pathname)s:%(funcName)s():%(lineno)s][%(levelname)s] %(message)s",
+                "datefmt": "%Y-%m-%d %H:%M:%S",  # 只包含到秒，毫秒在 fmt 中处理
+            },
         },
-    },
-    "loggers": {
-        "": {
-            "handlers": ["file_handler"],
-            "level": "INFO",
-            "propagate": True,
+        "handlers": {
+            "file_handler": {
+                "formatter": "default",
+                "class": "logging.handlers.RotatingFileHandler",
+                "filename": f"logs/main_{world_rank}.log",
+                "maxBytes": 256 * 1024 * 1024,
+                "backupCount": 20,
+            },
+            "route_file_handler": {
+                "formatter": "default",
+                "class": "logging.handlers.RotatingFileHandler",
+                "filename": f"logs/route.log",
+                "maxBytes": 256 * 1024 * 1024,
+                "backupCount": 20,
+            },
         },
-        "route_logger": {
-            "handlers": ["route_file_handler"],
-            "level": "INFO",
-            "propagate": False,
+        "loggers": {
+            "": {
+                "handlers": ["file_handler"],
+                "level": "INFO",
+                "propagate": True,
+            },
+            "route_logger": {
+                "handlers": ["route_file_handler"],
+                "level": "INFO",
+                "propagate": False,
+            },
         },
-    },
-}
+    }
+
+LOGGING_CONFIG = get_logging_config(int(os.environ.get('WORLD_RANK', '0')))
diff --git a/rtp_llm/config/model_args.py b/rtp_llm/config/model_args.py
new file mode 100644
index 000000000..afb66e1ef
--- /dev/null
+++ b/rtp_llm/config/model_args.py
@@ -0,0 +1,99 @@
+"""ModelArgs class for storing model-related arguments from server_args.
+
+This class is a simple container for user-provided model configuration arguments
+that are parsed from command-line arguments and environment variables.
+"""
+
+from typing import Optional
+
+
+class ModelArgs:
+    """Simple container for model arguments parsed from server_args.
+    
+    This class contains all user-provided model configuration arguments.
+    These arguments are used to populate ModelConfig after the model architecture
+    is determined by the model's _create_config method.
+    """
+    
+    def __init__(self):
+        """Initialize ModelArgs with default values."""
+        # Paths
+        self.ckpt_path: str = ""
+        self.tokenizer_path: str = ""
+        self.ptuning_path: Optional[str] = None
+        self.extra_data_path: str = ""
+        self.local_extra_data_path: Optional[str] = None
+        self.original_checkpoint_path: Optional[str] = None
+        
+        # Model type and task
+        self.model_type: str = ""
+        self.task_type: Optional[str] = None
+        
+        # Data types and computation
+        self.act_type: str = ""
+        self.use_float32: bool = False
+        
+        # Sequence length
+        self.max_seq_len: Optional[int] = None
+        
+        # MLA config
+        self.mla_ops_type: Optional[str] = None
+        
+        # Model override args
+        self.json_model_override_args: str = "{}"
+
+def apply_model_args_to_config(model_args: ModelArgs, model_config) -> None:
+    """Apply ModelArgs to a ModelConfig instance.
+    
+    This function sets all user-provided arguments to the ModelConfig,
+    overwriting any default values set by _create_config.
+    
+    Args:
+        model_args: ModelArgs instance containing user-provided arguments
+        model_config: ModelConfig instance to update
+    """
+    # Set paths
+    if model_args.ckpt_path:
+        model_config.ckpt_path = model_args.ckpt_path
+    if model_args.tokenizer_path:
+        model_config.tokenizer_path = model_args.tokenizer_path
+    if model_args.ptuning_path is not None:
+        model_config.ptuning_path = model_args.ptuning_path
+    if model_args.extra_data_path:
+        model_config.extra_data_path = model_args.extra_data_path
+    if model_args.local_extra_data_path is not None:
+        model_config.local_extra_data_path = model_args.local_extra_data_path
+    if model_args.original_checkpoint_path is not None:
+        model_config.original_checkpoint_path = model_args.original_checkpoint_path
+    
+    # Set model type and task type
+    if model_args.model_type:
+        model_config.model_type = model_args.model_type
+    if model_args.task_type:
+        # Convert string to enum if needed
+        from rtp_llm.ops import TaskType
+        try:
+            model_config.task_type = TaskType[model_args.task_type]
+        except KeyError:
+            # If enum conversion fails, keep as string (will be handled by C++ binding)
+            pass
+    
+    # Set data types
+    if model_args.act_type:
+        model_config.act_type = model_args.act_type
+    model_config.use_float32 = model_args.use_float32
+    
+    # Set sequence length
+    if model_args.max_seq_len is not None and model_args.max_seq_len > 0:
+        model_config.max_seq_len = model_args.max_seq_len
+
+    # Set MLA ops type (C++ binding handles string to enum conversion)
+    if model_args.mla_ops_type:
+        model_config.mla_ops_type = model_args.mla_ops_type
+
+    # Apply model override args
+    if model_args.json_model_override_args and model_args.json_model_override_args != "{}":
+        model_config.json_model_override_args = model_args.json_model_override_args
+        # Apply override args to model_config
+        model_config.apply_override_args(model_args.json_model_override_args)
+
diff --git a/rtp_llm/config/model_config.py b/rtp_llm/config/model_config.py
new file mode 100644
index 000000000..b1d8336d3
--- /dev/null
+++ b/rtp_llm/config/model_config.py
@@ -0,0 +1,701 @@
+import json
+import logging
+import math
+from typing import Any, Dict, Optional
+
+from rtp_llm.config.quant_config import (
+    Fp8BlockWiseQuantConfig,
+    QuantizationConfig,
+    init_quant_config,
+)
+from rtp_llm.ops import TaskType
+from libth_transformer_config import ModelConfig as CppModelConfig
+from rtp_llm.utils.weight_type import WEIGHT_TYPE
+from rtp_llm.utils.gemm_utils.cutlass_config import load_cutlass_gemm_config
+
+class VitParameters:
+    """Vit parameters for multimodal models."""
+    # config includes origin vit config in ckpt/config.json
+    config: Dict[str, Any] = {}
+    special_token_ids: Dict[str, Any] = {}
+    special_tokens: Dict[str, Any] = {}
+    vit_weights: Any = None
+
+def get_pad_size(size: int, align_size: int):
+    """Calculate padding size to align to align_size."""
+    return (align_size - (size % align_size)) % align_size
+
+class ModelConfig(CppModelConfig):
+    def eval_model_size(self) -> float:
+        """
+        Evaluate total model size including weights, KV cache, and runtime buffers.
+        All required parameters (quant_algo, task_type, vocab_size) are obtained from self.
+        
+        Returns:
+            Total model size in bytes
+        """
+        quant_algo = self.quant_algo
+        vocab_size = self.vocab_size
+        
+        # Get task_type from self.task_type (enum) and convert to TaskType
+        task_type_enum = self.task_type
+        # task_type_enum is already C++ TaskType enum
+        task_type = task_type_enum
+        
+        layer_param_bytes = 2
+        if quant_algo.getWeightBits() == 8:
+            layer_param_bytes = 1
+        elif quant_algo.getWeightBits() == 4:
+            layer_param_bytes = 0.54
+
+        model_size = (
+            self.word_emb_param_count(vocab_size) * 2
+            + self.layer_weight_param_count() * layer_param_bytes
+            + self.hidden_size * layer_param_bytes
+            + self.word_emb_param_count(vocab_size) * 2
+        )  # maybe some model donot have lm_head
+
+        kv_cache_mem_size = self._eval_kv_cache_mem_size()
+        runtime_buffer = self._eval_runtime_buffer_mem_size()
+        total_size = model_size + kv_cache_mem_size + runtime_buffer
+        logging.info(
+            f"total_size(Bytes): {total_size}, model_size:{model_size}, kv_cache_mem_size:{kv_cache_mem_size}, runtime_buffer:{runtime_buffer}"
+        )
+        return total_size
+
+    def _eval_kv_cache_mem_size(self) -> float:
+        """Evaluate KV cache memory size."""
+        if self.task_type != TaskType.LANGUAGE_MODEL:
+            return 0
+        # Get kv_cache_dtype from attn_config
+        from libth_transformer_config import KvCacheDataType
+        kv_cache_dtype_enum = self.attn_config.kv_cache_dtype
+        kv_cache_bytes = (
+            1
+            if kv_cache_dtype_enum in [KvCacheDataType.FP8, KvCacheDataType.INT8]
+            else 2
+        )
+        kv_cache_size = (
+            2
+            * self.num_layers
+            * self.attn_config.kv_head_num
+            * self.attn_config.size_per_head
+            * kv_cache_bytes
+            * self.max_seq_len
+        )
+        return kv_cache_size
+
+    def _eval_runtime_buffer_mem_size(self) -> float:
+        """Evaluate runtime buffer memory size."""
+        input_buffer = self.max_seq_len * self.hidden_size
+        qkv_gemm_buffer_size = (
+            self.max_seq_len
+            * (self.attn_config.kv_head_num * 2 + self.attn_config.kv_head_num)
+            * self.attn_config.size_per_head
+        )
+        attn_buffer_size = self.max_seq_len * self.hidden_size
+        ffn_export_num = self.expert_num if self.moe_k else 1
+        # Use isGatedActivation() to determine if we need 2 weights (gated) or 1 weight (non-gated like GELU)
+        ffn_w_count = 2 if self.isGatedActivation() else 1
+        ffn_buffer = (
+            self.max_seq_len * self.hidden_size
+            + ffn_w_count * self.max_seq_len * self.inter_size
+        ) * ffn_export_num
+        return input_buffer + qkv_gemm_buffer_size + attn_buffer_size + ffn_buffer
+
+    def model_param_count(self) -> int:
+        """
+        Calculate total model parameter count.
+        vocab_size is obtained from self.vocab_size.
+        
+        Returns:
+            Total parameter count
+        """
+        vocab_size = self.vocab_size
+        return (
+            self.word_emb_param_count(vocab_size) * 2
+            + self.layer_weight_param_count()
+            + self.hidden_size
+        )
+
+    def word_emb_param_count(self, vocab_size: int) -> int:
+        """
+        Calculate word embedding parameter count.
+        
+        Args:
+            vocab_size: Vocabulary size
+            
+        Returns:
+            Word embedding parameter count
+        """
+        return vocab_size * self.hidden_size
+
+    def layer_weight_param_count(self) -> int:
+        """
+        Calculate layer weight parameter count.
+        
+        Returns:
+            Layer weight parameter count
+        """
+        hidden_size = self.hidden_size
+
+        layer_weight_param_count = 0
+        # qkv
+        if self.attn_config.kv_head_num != self.attn_config.head_num:
+            layer_weight_param_count = (
+                layer_weight_param_count
+                + self.num_layers * hidden_size * hidden_size
+                + self.num_layers * (self.attn_config.kv_head_num * self.attn_config.size_per_head) * 2
+            )
+        else:
+            layer_weight_param_count = (
+                layer_weight_param_count
+                + self.num_layers * hidden_size * hidden_size * 3
+            )
+
+        # attn_o_w
+        layer_weight_param_count = (
+            layer_weight_param_count + self.num_layers * self.attn_config.head_num * hidden_size * hidden_size
+        )
+
+        # ffn w1, w2, w3
+        ffn_export_num = self.expert_num if self.expert_num > 0 else 1
+        # Use isGatedActivation() to determine if we need 3 weights (gated) or 2 weights (non-gated like GELU)
+        ffn_w_count = 3 if self.isGatedActivation() else 2
+        if self.moe_style == 1:
+            layer_weight_param_count = (
+                layer_weight_param_count
+                + self.num_layers
+                * self.inter_size
+                * hidden_size
+                * ffn_w_count
+                * ffn_export_num
+            )
+        elif self.moe_style == 2:
+            layer_weight_param_count = (
+                layer_weight_param_count
+                + len(self.moe_layer_index)
+                * self.moe_inter_padding_size
+                * hidden_size
+                * ffn_w_count)
+        else:
+            layer_weight_param_count = (
+                layer_weight_param_count + self.num_layers * self.inter_size * hidden_size * ffn_w_count
+            )
+
+        if ffn_export_num > 1:
+            layer_weight_param_count = (
+                layer_weight_param_count
+                + len(self.moe_layer_index) * hidden_size * ffn_export_num
+            )
+        # other small tensor
+        layer_weight_param_count = (
+            layer_weight_param_count + self.num_layers * hidden_size * 11
+        )
+        return layer_weight_param_count
+
+    def apply_rope_scaling_override(self, model_override_args: Dict[str, Any]) -> None:
+        """
+        Apply rope_scaling configuration from model_override_args.
+        
+        Args:
+            model_override_args: Dictionary containing model override arguments
+        """
+        if not model_override_args or "rope_scaling" not in model_override_args:
+            return
+        
+        # be consistent with RopeStyle
+        rope_type = {
+            "no": 0,
+            "base": 1,
+            "glm2": 2,
+            "dynamicntk": 3,
+            "qwendynamicntk": 4,
+            "yarn": 5,
+            "llama3": 6,
+            "mrope": 7,
+        }
+        rope_override_args = model_override_args["rope_scaling"]
+        assert (
+            "type" in rope_override_args
+            and rope_override_args["type"] in rope_type
+        ), f"Invalid rope_scaling type: {rope_override_args.get('type')}"
+        
+        self.rope_config.style = rope_type[rope_override_args["type"]]
+        
+        if rope_override_args["type"] == "yarn":
+            assert (
+                "factor" in rope_override_args
+                and "original_max_position_embeddings" in rope_override_args
+            ), "yarn rope_scaling requires 'factor' and 'original_max_position_embeddings'"
+            
+            self.rope_config.scale = rope_override_args["factor"]
+            self.rope_config.max_pos = rope_override_args["original_max_position_embeddings"]
+            self.rope_config.factor1 = rope_override_args.get("beta_slow", 1.0)
+            self.rope_config.factor2 = rope_override_args.get("beta_fast", 1.0)
+            mscale = rope_override_args.get("mscale", 1.0)
+            self.rope_config.mscale = float(
+                (
+                    1.0
+                    if self.rope_config.scale <= 1
+                    else 0.1 * math.log(self.rope_config.scale) + 1.0
+                )
+                * mscale
+            )
+            self.rope_config.extrapolation_factor = rope_override_args.get(
+                "extrapolation_factor", 1.0
+            )
+            
+            logging.info(
+                f"Applied rope_scaling (yarn): "
+                f"style: {self.rope_config.style}, "
+                f"scale: {self.rope_config.scale}, "
+                f"max_pos: {self.rope_config.max_pos}, "
+                f"factor1: {self.rope_config.factor1}, "
+                f"factor2: {self.rope_config.factor2}, "
+                f"mscale: {self.rope_config.mscale}, "
+                f"extrapolation_factor: {self.rope_config.extrapolation_factor}"
+            )
+        else:
+            logging.info(
+                f"Applied rope_scaling: style: {self.rope_config.style}"
+            )
+
+    def __init__(self, *args, **kwargs):
+        """Initialize ModelConfig with quant_algo member and default values."""
+        super().__init__(*args, **kwargs)
+        # Additional Python-only fields
+        self.is_mtp: bool = False
+        self.normalize_lm_head_weight: bool = False
+        self.has_lm_head_bias: bool = False
+        self.tie_word_embeddings: bool = False
+        # Model loading related fields
+        # ptuning_path is now in C++ ModelConfig (as std::string, default "")
+        self.quantization: str = ""  # Quantization method string (e.g., "INT8", "FP8", etc.)
+        # mm_related_params will be set to VitParameters() if needed
+        self.mm_related_params: Any = None
+        self.src_quantization_bit: int = 0
+        self.config_dtype: Optional[str] = None
+
+        
+        # Model metadata fields (merged from function parameters)
+        self.template_type: Optional[Any] = None  # TemplateType enum
+        self.model_name: str = ""  # Model name (also set to engine_config.runtime_config.model_name)
+
+    def update_inter_padding_size(
+        self, tp_size: int, ep_size: int, dp_size: int, hw_kernel_config: Any
+    ) -> None:
+        """
+        Update inter_padding_size and moe_inter_padding_size based on quantization and hardware config.
+        
+        Args:
+            tp_size: Tensor parallel size
+            ep_size: Expert parallel size
+            dp_size: Data parallel size
+            hw_kernel_config: Hardware kernel config (used for use_swizzleA check)
+        """
+        if tp_size * dp_size != ep_size:
+            raise ValueError(
+                f"tp_size:{tp_size} * dp_size:{dp_size} != ep_size:{ep_size}"
+            )
+        # new tp_size just only for moe
+        if self.quant_algo.isGroupwise():
+            align_size = tp_size * self.quant_algo.getGroupSize()
+            moe_align_size = self.quant_algo.getGroupSize()
+        else:
+            align_size = tp_size * 64
+            moe_align_size = 64
+            if self.quant_algo.isFp8PTPC():
+                moe_align_size = 128
+        
+        self.inter_padding_size = self.inter_size + (
+            get_pad_size(self.inter_size, align_size)
+            if (self.quant_algo.isQuant() or hw_kernel_config.use_swizzleA)
+            else 0
+        )
+        
+        if self.attn_config.kv_head_num <= 0:
+            self.attn_config.kv_head_num = self.attn_config.head_num
+        if self.inter_padding_size <= 0:
+            self.inter_padding_size = self.inter_size
+
+        if self.moe_inter_padding_size <= 0:
+            self.moe_inter_padding_size = self.inter_size
+        if self.moe_inter_padding_size > 0:
+            moe_align_size = moe_align_size if self.quant_algo.isQuant() else 8
+            self.moe_inter_padding_size = self.moe_inter_padding_size + (
+                get_pad_size(self.moe_inter_padding_size, moe_align_size)
+            )
+
+        logging.info(
+            f"update_inter_padding_size: {self.inter_padding_size}, moe_inter_padding_size: {self.moe_inter_padding_size}"
+        )
+
+    def setup_paths(
+        self,
+        ckpt_path: str,
+        tokenizer_path: str,
+        ptuning_path: Optional[str],
+        max_seq_len: int,
+    ) -> None:
+        """Setup paths and max_seq_len in ModelConfig.
+        
+        Args:
+            ckpt_path: Checkpoint path
+            tokenizer_path: Tokenizer path
+            ptuning_path: Optional p-tuning path
+            max_seq_len: Maximum sequence length
+        """
+        # Use C++ fields directly (no Python fields)
+        self.ckpt_path = ckpt_path
+        self.tokenizer_path = tokenizer_path
+        # ptuning_path is now in C++ ModelConfig
+        self.ptuning_path = ptuning_path if ptuning_path else ""
+        if max_seq_len != 0:
+            self.max_seq_len = max_seq_len
+        if self.max_seq_len < 1:
+            # frontend not load ckpt config max_seq_len, use default 8192 or env
+            self.max_seq_len = 8192
+        logging.info(f"max_seq_len: {self.max_seq_len}")
+
+    def apply_override_args(self, json_model_override_args: str) -> None:
+        """Apply model override arguments to ModelConfig.
+        
+        Args:
+            json_model_override_args: JSON string with model override arguments
+        """
+        model_override_args = json.loads(json_model_override_args)
+        if model_override_args:
+            # Apply rope_scaling override via py_model_config
+            self.apply_rope_scaling_override(model_override_args)
+
+    def init_precision_config(self, kv_cache_config: Optional[Any] = None):
+        """Initialize precision configuration from checkpoint and quantization settings.
+        
+        This method:
+        1. Loads quant_config from checkpoint or quantization string
+        2. Sets quant_algo if quant_config exists
+        3. Initializes data_type from act_type (or config_dtype if act_type is empty)
+        4. Sets attn_config.kv_cache_dtype based on kv_cache_config (if provided)
+        5. Gets kv_cache_data_type from attn_config.kv_cache_dtype
+        6. Applies quantization-specific overrides
+        7. Validates configuration with quant_config
+        8. Sets final data_type and kv_cache_data_type
+        
+        Args:
+            kv_cache_config: Optional KVCacheConfig to set attn_config.kv_cache_dtype
+        """
+        # Load quant_config
+        quant_config = QuantizationConfig.load_from_ckpt(self.ckpt_path)
+        if not quant_config:
+            if self.quantization:
+                quant_config = init_quant_config(self.quantization)
+                logging.info(f"need_load_quant by {quant_config.get_method()}")
+        
+        # Set quant_algo if quant_config exists
+        if quant_config:
+            self.quant_algo.setQuantAlgo(
+                quant_config.get_algo().lower(),
+                quant_config.bits,
+                quant_config.group_size(),
+            )
+
+        # Initialize data_type: first try act_type, then config_dtype, finally default to FP16
+        data_type: Optional[WEIGHT_TYPE] = None
+        if self.act_type:
+            data_type = WEIGHT_TYPE.from_str(self.act_type)
+            logging.info(f"Initializing data_type from act_type: {data_type}")
+        else:
+            # Parse config_dtype if available
+            config_dtype_parsed = None
+            if self.config_dtype:
+                config_dtype_parsed = WEIGHT_TYPE.from_str(self.config_dtype)
+                logging.info(f"act_type is empty, using config_dtype: {config_dtype_parsed}")
+            
+            if config_dtype_parsed:
+                data_type = config_dtype_parsed
+            else:
+                data_type = WEIGHT_TYPE.FP16
+                logging.info(f"act_type and config_dtype are both empty, using default: {data_type}")
+
+        # Apply quantization-specific overrides
+        if quant_config and isinstance(quant_config, Fp8BlockWiseQuantConfig):
+            original_data_type = data_type
+            data_type = WEIGHT_TYPE.BF16
+            logging.info(
+                f"Overriding data_type from {original_data_type} to {data_type} "
+                f"because fp8_block_wise quantization only supports BF16"
+            )
+        elif quant_config and quant_config.get_method().lower() in ["smooth_quant", "omni_quant"]:
+            original_data_type = data_type
+            data_type = WEIGHT_TYPE.FP16
+            logging.info(
+                f"Overriding data_type from {original_data_type} to {data_type} "
+                f"because {quant_config.get_method()} quantization requires FP16"
+            )
+
+        # Set attn_config.kv_cache_dtype based on kv_cache_config
+        from libth_transformer_config import KvCacheDataType
+        if kv_cache_config is not None:
+            if kv_cache_config.int8_kv_cache:
+                self.attn_config.kv_cache_dtype = KvCacheDataType.INT8
+                logging.info("Setting attn_config.kv_cache_dtype to INT8 based on kv_cache_config.int8_kv_cache")
+            elif kv_cache_config.fp8_kv_cache:
+                self.attn_config.kv_cache_dtype = KvCacheDataType.FP8
+                logging.info("Setting attn_config.kv_cache_dtype to FP8 based on kv_cache_config.fp8_kv_cache")
+            else:
+                self.attn_config.kv_cache_dtype = KvCacheDataType.BASE
+                logging.info("Setting attn_config.kv_cache_dtype to BASE (default, no int8/fp8 kv_cache specified)")
+
+        # Get kv_cache_data_type from attn_config.kv_cache_dtype and set it directly
+        if self.attn_config.kv_cache_dtype == KvCacheDataType.INT8:
+            kv_cache_data_type = WEIGHT_TYPE.INT8
+        elif self.attn_config.kv_cache_dtype == KvCacheDataType.FP8:
+            kv_cache_data_type = WEIGHT_TYPE.FP8
+        else:  # BASE
+            kv_cache_data_type = data_type
+        logging.info(
+            f"Initializing kv_cache_data_type from attn_config.kv_cache_dtype "
+            f"({self.attn_config.kv_cache_dtype}): {kv_cache_data_type}"
+        )
+
+        # Validate configuration with quant_config
+        if quant_config:
+            logging.info(
+                f"Validating precision configuration with quant_config: "
+                f"data_type={data_type}, kv_cache_data_type={kv_cache_data_type}"
+            )
+            quant_config.verify_compute_dtype_and_kv_cache_dtype(
+                data_type.to_torch_dtype(), kv_cache_data_type.to_torch_dtype()
+            )
+            logging.info("Precision configuration validation passed")
+
+        # Set final data_type and kv_cache_data_type
+        # This uses ModelConfig's __setattr__ which handles string-to-enum conversion
+        self.data_type = data_type.to_str()        
+        # Store kv_cache_data_type as WEIGHT_TYPE enum for use in model_weight_info
+        self.kv_cache_data_type = kv_cache_data_type
+        # Store quant_config as instance attribute for later use
+        self.quant_config = quant_config
+        
+        # Print final type results
+        logging.info(
+            f"Final precision configuration - "
+            f"quant_config: {quant_config}, "
+            f"data_type: {self.data_type}, "
+            f"kv_cache_data_type: {self.kv_cache_data_type}, "
+            f"attn_config.kv_cache_dtype: {self.attn_config.kv_cache_dtype}"
+        )
+
+
+def get_task_type_from_ckpt_path(
+    ckpt_path: str,
+    embedding_config: Optional[Any] = None,
+) -> TaskType:
+    """
+    Get task_type from checkpoint path or use provided task_type.
+    
+    Args:
+        ckpt_path: Checkpoint path
+        embedding_config: Optional EmbeddingConfig for embedding task detection
+        task_type: Optional TaskType (if already set, return it)
+        
+    Returns:
+        TaskType enum value
+    """
+    
+    # Determine task_type from checkpoint path and embedding_config
+    import os
+    from rtp_llm.utils.util import get_config_from_path
+    
+    def _is_dense_embedding_task(ckpt_path: str) -> bool:
+        def _check_is_sentence_transformer_repo() -> bool:
+            if os.path.exists(
+                os.path.join(ckpt_path, "config_sentence_transformers.json")
+            ):
+                return True
+            module_file_path = os.path.join(ckpt_path, "modules.json")
+            if os.path.exists(module_file_path):
+                with open(module_file_path, "r") as reader:
+                    content = reader.read()
+                if "sentence_transformers" in content:
+                    return True
+            return False
+
+        return (
+            (embedding_config and embedding_config.embedding_model == 1)
+            or _check_is_sentence_transformer_repo()
+        )
+
+    def _is_classifier_task(ckpt_path: str) -> bool:
+        config_json = get_config_from_path(ckpt_path)
+        if not config_json:
+            return False
+        if "architectures" in config_json and len(config_json["architectures"]) > 0:
+            model_type = config_json["architectures"][0]
+            if "SequenceClassification" in model_type:
+                return True
+        return False
+
+    if _is_dense_embedding_task(ckpt_path):
+        return TaskType.DENSE_EMBEDDING
+    elif _is_classifier_task(ckpt_path):
+        return TaskType.SEQ_CLASSIFICATION
+    else:
+        return TaskType.LANGUAGE_MODEL
+
+def update_stop_words_from_env(special_tokens, generate_env_config) -> None:
+    """
+    Update stop_words_str and stop_words_id from environment variables.
+    
+    Args:
+        special_tokens: SpecialTokens object to update
+        generate_env_config: GenerateEnvConfig object containing stop_words configuration
+    """
+    env_stop_words_str = generate_env_config.stop_words_str
+    env_stop_words_id = generate_env_config.stop_words_list
+    env_stop_words_str_list = (
+        json.loads(env_stop_words_str) if env_stop_words_str else []
+    )
+    env_stop_words_id_list = (
+        json.loads(env_stop_words_id) if env_stop_words_id else []
+    )
+    env_force_stop = generate_env_config.force_stop_words
+    if env_force_stop:
+        special_tokens.stop_words_str_list = env_stop_words_str_list
+        special_tokens.stop_words_id_list = env_stop_words_id_list
+    else:
+        special_tokens.stop_words_str_list = (
+            special_tokens.stop_words_str_list + env_stop_words_str_list
+        )
+        special_tokens.stop_words_id_list = (
+            special_tokens.stop_words_id_list + env_stop_words_id_list
+        )
+
+    logging.info(
+        f"use stop_words_str_list [{special_tokens.stop_words_str_list }],"
+        f" stop_words_id_list [{special_tokens.stop_words_id_list}]"
+    )
+
+
+def update_tokenizer_special_tokens(special_tokens, tokenizer: Any) -> None:
+    """Update special tokens from tokenizer to ModelConfig.
+    
+    Args:
+        special_tokens: SpecialTokens object to update
+        tokenizer: Tokenizer instance with stop_words_id_list, stop_words_str_list, and eos_token_id
+    """
+    special_tokens.stop_words_id_list += tokenizer.stop_words_id_list
+    special_tokens.stop_words_str_list += tokenizer.stop_words_str_list
+    special_tokens.eos_token_id = tokenizer.eos_token_id
+
+# ============================================================================
+# ModelConfig setup and initialization functions
+# ============================================================================
+
+def build_py_model_config(
+    py_model_config: ModelConfig,  # ModelConfig instance to build
+    model_args: Any,  # ModelArgs from py_env_configs
+    kv_cache_config,
+    py_hw_kernel_config: Any,  # HWKernelConfig
+    profiling_debug_logging_config: Any,  # ProfilingDebugLoggingConfig
+    parallelism_config,
+    embedding_config: Optional[Any] = None,  # EmbeddingConfig (optional, for check_task_type)
+) -> None:
+    """Build and initialize ModelConfig from model_args.
+    
+    This function initializes ModelConfig after EngineConfig is initialized.
+    It copies values from model_args to py_model_config, then sets up model-specific fields.
+    
+    Args:
+        py_model_config: ModelConfig instance to build
+        model_args: ModelArgs instance from py_env_configs (contains user-provided arguments)
+        kv_cache_config: KVCacheConfig for task_type and use_kvcache
+        py_hw_kernel_config: HWKernelConfig for inter_padding_size
+        profiling_debug_logging_config: ProfilingDebugLoggingConfig for hack_layer_num
+        parallelism_config: ParallelismConfig for inter_padding_size
+        embedding_config: Optional EmbeddingConfig (for check_task_type)
+    """
+    from rtp_llm.config.model_args import apply_model_args_to_config
+    
+    # Apply model_args to py_model_config
+    apply_model_args_to_config(model_args, py_model_config)
+    
+    # Initialize precision configuration (uses self.ckpt_path and self.quantization)
+    # This will initialize data_type from act_type (or config_dtype), set attn_config.kv_cache_dtype
+    # from kv_cache_config, and validate with quant_config
+    py_model_config.init_precision_config(kv_cache_config=kv_cache_config)
+    
+    ckpt_path = py_model_config.ckpt_path
+    tokenizer_path = py_model_config.tokenizer_path
+    ptuning_path = py_model_config.ptuning_path
+    max_seq_len = py_model_config.max_seq_len
+    py_model_config.setup_paths(
+        ckpt_path,
+        tokenizer_path,
+        ptuning_path,
+        max_seq_len,
+    )
+    
+    task_type = get_task_type_from_ckpt_path(
+        py_model_config.ckpt_path,
+        embedding_config)
+    # Convert TaskType enum to string for C++ binding (setter expects string)
+    task_type_str = task_type.name if hasattr(task_type, 'name') else str(task_type)
+    py_model_config.task_type = task_type_str
+    py_model_config.use_kvcache = task_type == TaskType.LANGUAGE_MODEL
+    logging.info(
+        f"model task type: {task_type}, use_kvcache: {py_model_config.use_kvcache}"
+    )
+    
+    # Update inter_padding_size (needs parallelism_config)
+    py_model_config.update_inter_padding_size(
+        parallelism_config.tp_size,
+        parallelism_config.ep_size,
+        parallelism_config.dp_size,
+        py_hw_kernel_config,
+    )
+
+    # Load cutlass gemm config
+    load_cutlass_gemm_config(py_model_config.quant_algo)
+    
+    # Apply hack_layer_num if needed
+    hack_layer_num = profiling_debug_logging_config.hack_layer_num
+    if hack_layer_num:
+        logging.info(f"hack layernum to {hack_layer_num}")
+        py_model_config.num_layers = hack_layer_num
+    
+    # Apply model override args
+    if py_model_config.json_model_override_args:
+        py_model_config.apply_override_args(py_model_config.json_model_override_args)
+
+    logging.info("py_model_config: %s", py_model_config.to_string())
+
+def get_quantization_from_params(env_params: Dict[str, str]) -> Optional[str]:
+    """Get quantization setting from environment parameters.
+    
+    Replaces LegacyModelConfig.get_quantization_from_params().
+    
+    Args:
+        env_params: Dictionary of environment parameters
+        
+    Returns:
+        Quantization string or None
+    """
+    QUANTIZATION_KEY = "QUANTIZATION"
+    WEIGHT_TYPE = "WEIGHT_TYPE"
+    INT8_MODE = "INT8_MODE"
+    
+    if (not env_params.get(QUANTIZATION_KEY)) and (
+        env_params.get(WEIGHT_TYPE, "").upper() == "INT8"
+        or int(env_params.get(INT8_MODE, "0")) == 1
+    ):
+        quantization = "INT8"
+    else:
+        quantization = env_params.get(QUANTIZATION_KEY)
+    return quantization
+
+
+# Add as a static method to ModelConfig for backward compatibility
+ModelConfig.get_quantization_from_params = staticmethod(get_quantization_from_params)
+
diff --git a/rtp_llm/config/py_config_modules.py b/rtp_llm/config/py_config_modules.py
index ba8f51d43..a3d800cc5 100644
--- a/rtp_llm/config/py_config_modules.py
+++ b/rtp_llm/config/py_config_modules.py
@@ -1,19 +1,29 @@
-import logging
-import os
 from typing import Optional
 
+from rtp_llm.config.model_args import ModelArgs
+from rtp_llm.config.kv_cache_config import KVCacheConfig
+
 from rtp_llm.ops import (
     ConcurrencyConfig,
+    DeviceResourceConfig,
+    EPLBConfig,
     FfnDisAggregateConfig,
     FMHAConfig,
+    HWKernelConfig,
+    KVCacheConfig,
     MiscellaneousConfig,
     ModelSpecificConfig,
-    ParallelismDistributedConfig,
+    MoeConfig,
+    PDSepConfig,
+    ParallelismConfig,
     ProfilingDebugLoggingConfig,
     RoleType,
+    RuntimeConfig,
+    SpeculativeExecutionConfig,
+    CacheStoreConfig,
+    ArpcConfig,
+    VitSeparation,
 )
-from rtp_llm.utils.fuser import MountRwMode, fetch_remote_file_to_local
-from rtp_llm.utils.weight_type import WEIGHT_TYPE
 
 DEFAULT_START_PORT = 8088
 MASTER_INFO_PORT_NUM = 11
@@ -21,24 +31,6 @@ MIN_WORKER_INFO_PORT_NUM = 7
 WORKER_INFO_PORT_NUM = MIN_WORKER_INFO_PORT_NUM
 
 
-def get_env_int(name: str, default: int = -1):
-    v = os.environ.get(name, None)
-    return int(v) if v is not None and v != "" else default
-
-
-def get_env_str(name: str, default: str = ""):
-    v = os.environ.get(name, None)
-    return v if v is not None else default
-
-
-def get_env_bool(name: str, default: bool = False):
-    # in fact, we can always get value from env, if that's not specified, we return default value
-    v = os.environ.get(name, None)
-    if v is None or v == "":
-        return default
-    return v.lower() == "1" or v.lower() == "on" or v.lower() == "true"
-
-
 class ServerConfig:
     def __init__(self):
         self.frontend_server_count = 4
@@ -46,18 +38,10 @@ class ServerConfig:
         self.timeout_keep_alive = 5
         self.frontend_server_id = 0
         self.rank_id = 0
+        self.worker_info_port_num: int = MIN_WORKER_INFO_PORT_NUM
 
-    def update_from_env(self):
-        self.frontend_server_count = int(
-            os.environ.get("FRONTEND_SERVER_COUNT", self.frontend_server_count)
-        )
-        self.start_port = int(os.environ.get("START_PORT", self.start_port))
-        self.timeout_keep_alive = int(
-            os.environ.get("TIMEOUT_KEEP_ALIVE", self.timeout_keep_alive)
-        )
-        self.frontend_server_id = int(
-            os.environ.get("FRONTEND_SERVER_ID", self.frontend_server_id)
-        )
+    # update_from_args 方法已不再需要
+    # 配置绑定现在通过声明式 bind_to 参数在 add_argument 时自动处理
 
     def to_string(self):
         return (
@@ -65,144 +49,37 @@ class ServerConfig:
             f"start_port: {self.start_port}\n"
             f"timeout_keep_alive: {self.timeout_keep_alive}\n"
             f"frontend_server_id: {self.frontend_server_id}\n"
-            f"rank_id: {self.rank_id}"
+            f"rank_id: {self.rank_id}\n"
+            f"worker_info_port_num: {self.worker_info_port_num}"
         )
 
 
-class ModelConfig:
+class PyMiscellaneousConfig:
+    """Python wrapper for C++ MiscellaneousConfig with additional Python-only fields."""
     def __init__(self):
-        self.extra_data_path: str = ""
-        self.local_extra_data_path: Optional[str] = None
-        self.tokenizer_path: str = ""
-        self.act_type: str = "FP16"
-        self.use_float32: bool = False
-        self.original_checkpoint_path: Optional[str] = None
-        self.mla_ops_type: str = "AUTO"
-        self.ft_plugin_path: Optional[str] = None
-        self.weight_type: Optional[str] = None
-
-        self.task_type: Optional[str] = None
-        self.model_type: str = ""
-        self.checkpoint_path: str = ""
+        self.misc_config = MiscellaneousConfig()
+        # Additional Python-only fields
         self.oss_endpoint: str = ""
-        self.ptuning_path: Optional[str] = None
-        self.openai_api_key = "EMPTY"
-
         self.openai_api_key: str = "EMPTY"
         self.dashscope_api_key: str = "EMPTY"
         self.dashscope_http_url: Optional[str] = None
         self.dashscope_websocket_url: Optional[str] = None
-        self.json_model_override_args: str = "{}"
-
-    def update_from_env(self):
-        self.extra_data_path = os.environ.get("EXTRA_DATA_PATH", self.extra_data_path)
-        self.local_extra_data_path = os.environ.get(
-            "LOCAL_EXTRA_DATA_PATH", self.local_extra_data_path
-        )
-        self.tokenizer_path = os.environ.get("TOKENIZER_PATH", self.tokenizer_path)
-        if int(os.environ.get("USE_FLOAT32", 0) == "1"):
-            self.act_type = WEIGHT_TYPE.FP32.to_str()
-            logging.info(f"set data_type = WEIGHT_TYPE.FP32 by USE_FLOAT32 == 1")
-        else:
-            self.act_type = os.environ.get("ACT_TYPE", self.act_type)
-        self.use_float32 = get_env_bool("USE_FLOAT32", self.use_float32)
-        self.original_checkpoint_path = os.environ.get(
-            "ORIGINAL_CHECKPOINT_PATH", self.original_checkpoint_path
-        )
-        self.mla_ops_type = os.environ.get("MLA_OPS_TYPE", self.mla_ops_type)
-        self.ft_plugin_path = os.environ.get("FT_PLUGIN_PATH", self.ft_plugin_path)
-        self.weight_type = os.environ.get("WEIGHT_TYPE", self.weight_type)
-        self.task_type = os.environ.get("TASK_TYPE", self.task_type)
-        self.model_type = os.environ.get("MODEL_TYPE", self.model_type)
-        self.checkpoint_path = os.environ.get("CHECKPOINT_PATH", self.checkpoint_path)
-        self.tokenizer_path = os.environ.get("TOKENIZER_PATH", self.checkpoint_path)
-        self.oss_endpoint = os.environ.get("OSS_ENDPOINT", self.oss_endpoint)
-        self.ptuning_path = os.environ.get("PTUNING_PATH", self.ptuning_path)
-        self.openai_api_key = os.environ.get("OPENAI_API_KEY", self.openai_api_key)
-        self.dashscope_api_key = os.environ.get(
-            "DASHSCOPE_API_KEY", self.dashscope_api_key
-        )
-        self.dashscope_http_url = os.environ.get(
-            "DASHSCOPE_HTTP_URL", self.dashscope_http_url
-        )
-        self.dashscope_websocket_url = os.environ.get(
-            "DASHSCOPE_WEBSOCKET_URL", self.dashscope_websocket_url
-        )
-        self.json_model_override_args = os.environ.get(
-            "JSON_MODEL_OVERRIDE_ARGS", self.json_model_override_args
-        )
 
     def to_string(self):
         return (
-            f"extra_data_path: {self.extra_data_path}\n"
-            f"local_extra_data_path: {self.local_extra_data_path}\n"
-            f"tokenizer_path: {self.tokenizer_path}\n"
-            f"act_type: {self.act_type}\n"
-            f"use_float32: {self.use_float32}\n"
-            f"original_checkpoint_path: {self.original_checkpoint_path}\n"
-            f"mla_ops_type: {self.mla_ops_type}\n"
-            f"ft_plugin_path: {self.ft_plugin_path}\n"
-            f"weight_type: {self.weight_type}\n"
-            f"task_type: {self.task_type}\n"
-            f"model_type: {self.model_type}\n"
-            f"checkpoint_path: {self.checkpoint_path}\n"
+            self.misc_config.to_string() + "\n"
             f"oss_endpoint: {self.oss_endpoint}\n"
-            f"ptuning_path: {self.ptuning_path}\n"
             f"openai_api_key: {self.openai_api_key}\n"
             f"dashscope_api_key: {self.dashscope_api_key}\n"
             f"dashscope_http_url: {self.dashscope_http_url}\n"
-            f"dashscope_websocket_url: {self.dashscope_websocket_url}\n"
-            f"json_model_override_args: {self.json_model_override_args}"
+            f"dashscope_websocket_url: {self.dashscope_websocket_url}"
         )
 
-
-# Todo: 合并到c++的SpeculativeExecutionConfig
-class PySpeculativeExecutionConfig:
-    def __init__(self):
-        self.gen_num_per_circle: int = 5
-        self.sp_quantization: Optional[str] = None
-        self.sp_checkpoint_path: Optional[str] = None
-        self.sp_type: Optional[str] = None
-        self.sp_model_type: Optional[str] = None
-        self.sp_kv_cache_dtype: Optional[str] = None
-
-    def update_from_env(self):
-        self.gen_num_per_circle = int(
-            os.environ.get("GEN_NUM_PER_CIRCLE", self.gen_num_per_circle)
-        )
-        sp_int8_mode = int(os.environ.get("SP_INT8_MODE", 0))
-        self.sp_quantization = os.environ.get("SP_QUANTIZATION", self.sp_quantization)
-        if sp_int8_mode and not self.sp_quantization:
-            self.sp_quantization = WEIGHT_TYPE.INT8.to_str()
-
-        self.sp_kv_cache_dtype = os.environ.get("SP_KV_CACHE_DTYPE", None)
-        self.sp_checkpoint_path = os.environ.get(
-            "SP_CHECKPOINT_PATH", self.sp_checkpoint_path
-        )
-        self.sp_type = os.environ.get("SP_TYPE", self.sp_type)
-        self.sp_model_type = os.environ.get("SP_MODEL_TYPE", self.sp_model_type)
-
-    def to_string(self):
-        return (
-            f"gen_num_per_circle: {self.gen_num_per_circle}\n"
-            f"sp_quantization: {self.sp_quantization}\n"
-            f"sp_checkpoint_path: {self.sp_checkpoint_path}\n"
-            f"sp_type: {self.sp_type}\n"
-            f"sp_model_type: {self.sp_model_type}"
-            f"sp_kv_cache_dtype: {self.sp_kv_cache_dtype}\n"
-            f"sp_checkpoint_path: {self.sp_checkpoint_path}"
-        )
-
-
 class LoraConfig:
     def __init__(self):
         self.lora_info: str = "{}"
         self.merge_lora: bool = True
 
-    def update_from_env(self):
-        self.lora_info = os.environ.get("LORA_INFO", self.lora_info)
-        self.merge_lora = get_env_bool("MERGE_LORA", self.merge_lora)
-
     def to_string(self):
         return f"lora_info: {self.lora_info}\n" f"merge_lora: {self.merge_lora}\n"
 
@@ -215,18 +92,6 @@ class LoadConfig:
         # seem like it's a third-party pkg environment, but we reserve it temporar
         self.load_ckpt_num_process: int = 0
 
-    def update_from_env(self):
-        self.phy2log_path = os.environ.get("PHY2LOG_PATH", self.phy2log_path)
-        self.converter_num_per_gpu = int(
-            os.environ.get("CONVERTER_NUM_PER_GPU", self.converter_num_per_gpu)
-        )
-        self.tokenizers_parallelism = get_env_bool(
-            "TOKENIZERS_PARALLELISM", self.tokenizers_parallelism
-        )
-        self.load_ckpt_num_process = int(
-            os.environ.get("LOAD_CKPT_NUM_PROCESS", self.load_ckpt_num_process)
-        )
-
     def to_string(self):
         return (
             f"phy2log_path: {self.phy2log_path}\n"
@@ -243,20 +108,6 @@ class RenderConfig:
         self.default_tool_use_template_key: str = "tool_use"
         self.llava_chat_template: str = ""
 
-    def update_from_env(self):
-        self.model_template_type = os.environ.get(
-            "MODEL_TEMPLATE_TYPE", self.model_template_type
-        )
-        self.default_chat_template_key = os.environ.get(
-            "DEFAULT_CHAT_TEMPLATE_KEY", self.default_chat_template_key
-        )
-        self.default_tool_use_template_key = os.environ.get(
-            "DEFAULT_TOOL_USE_TEMPLATE_KEY", self.default_tool_use_template_key
-        )
-        self.llava_chat_template = os.environ.get(
-            "LLAVA_CHAT_TEMPLATE", self.llava_chat_template
-        )
-
     def to_string(self):
         return (
             f"model_template_type: {self.model_template_type}\n"
@@ -266,6 +117,7 @@ class RenderConfig:
         )
 
 
+
 class GangConfig:
     def __init__(self):
         self.fake_gang_env: bool = False
@@ -279,29 +131,6 @@ class GangConfig:
         self.json_gang_parts: Optional[str] = None
         self.leader_address: Optional[str] = None
 
-    def update_from_env(self):
-        self.fake_gang_env = get_env_bool("FAKE_GANG_ENV", self.fake_gang_env)
-        self.gang_annocation_path = os.environ.get(
-            "GANG_ANNOCATION_PATH", self.gang_annocation_path
-        )
-        self.gang_config_string = os.environ.get(
-            "GANG_CONFIG_STRING", self.gang_config_string
-        )
-        self.zone_name = os.environ.get("ZONE_NAME", self.zone_name)
-        self.distribute_config_file = os.environ.get(
-            "DISTRIBUTE_CONFIG_FILE", self.distribute_config_file
-        )
-        self.dist_barrier_timeout = int(
-            os.environ.get("DIST_BARRIER_TIMEOUT", self.dist_barrier_timeout)
-        )
-        self.gang_sleep_time = int(
-            os.environ.get("GANG_SLEEP_TIME", self.gang_sleep_time)
-        )
-        self.gang_timeout_min = int(
-            os.environ.get("GANG_TIMEOUT_MIN", self.gang_timeout_min)
-        )
-        self.json_gang_parts = os.environ.get("JSON_GANG_PARTS", self.json_gang_parts)
-        self.leader_address = os.environ.get("LEADER_ADDRESS", self.leader_address)
 
     def to_string(self):
         return (
@@ -317,10 +146,10 @@ class GangConfig:
             f"lead_address: {self.leader_address}\n"
         )
 
-
 class VitConfig:
     def __init__(self):
-        self.vit_separation: int = 0
+        self.vit_separation: VitSeparation = VitSeparation.VIT_SEPARATION_LOCAL
+        self.vit_run_batch: int = 1  # Batch size for VIT processing
         self.vit_trt: int = 0
         self.trt_cache_enabled: int = 0
         self.trt_cache_path: Optional[str] = None
@@ -333,32 +162,6 @@ class VitConfig:
         self.igraph_table_name: str = ""
         self.default_key: Optional[str] = None
 
-    def update_from_env(self):
-        self.vit_separation = int(os.environ.get("VIT_SEPARATION", self.vit_separation))
-        self.vit_trt = int(os.environ.get("VIT_TRT", self.vit_trt))
-        self.trt_cache_enabled = int(
-            os.environ.get("TRT_CACHE_ENABLED", self.trt_cache_enabled)
-        )
-        self.trt_cache_path = os.environ.get("TRT_CACHE_PATH", self.trt_cache_path)
-        self.download_headers = os.environ.get(
-            "DOWNLOAD_HEADERS", self.download_headers
-        )
-        self.mm_cache_item_num = int(
-            os.environ.get("MM_CACHE_ITEM_NUM", self.mm_cache_item_num)
-        )
-        self.url_cache_item_num = int(
-            os.environ.get("url_cache_item_num", self.url_cache_item_num)
-        )
-        self.use_igraph_cache = get_env_bool("USE_IGRAPH_CACHE", self.use_igraph_cache)
-        self.igraph_search_dom = get_env_str(
-            "IGRAPH_SEARCH_DOM", self.igraph_search_dom
-        )
-        self.igraph_vipserver = get_env_int("IGRAPH_VIPSERVER", self.igraph_vipserver)
-        self.igraph_table_name = get_env_str(
-            "IGRAPH_TABLE_NAME", self.igraph_table_name
-        )
-        self.default_key = os.environ.get("IGRAPH_DEFAULT_KEY", self.default_key)
-
     def to_string(self):
         return (
             f"vit_separation: {self.vit_separation}\n"
@@ -387,20 +190,6 @@ class GenerateEnvConfig:
         self.think_start_tag: str = "<think>\n"
         self.generation_config_path: Optional[str] = None
 
-    def update_from_env(self):
-        self.think_end_tag = os.environ.get("THINK_END_TAG", self.think_end_tag)
-        self.think_end_token_id = int(
-            os.environ.get("THINK_END_TOKEN_ID", self.think_end_token_id)
-        )
-        self.think_mode = int(os.environ.get("THINK_MODE", self.think_mode))
-        self.force_stop_words = get_env_bool("FORCE_STOP_WORDS", self.force_stop_words)
-        self.stop_words_list = os.environ.get("STOP_WORDS_LIST", self.stop_words_list)
-        self.stop_words_str = os.environ.get("STOP_WORDS_STR", self.stop_words_str)
-        self.think_start_tag = os.environ.get("THINK_START_TAG", self.think_start_tag)
-        self.generation_config_path = os.environ.get(
-            "GENERATION_CONFIG_PATH", self.generation_config_path
-        )
-
     def to_string(self):
         return (
             f"think_end_tag: {self.think_end_tag}\n"
@@ -413,213 +202,45 @@ class GenerateEnvConfig:
             f"generation_config_path: {self.generation_config_path}"
         )
 
-
 class QuantizationConfig:
     def __init__(self):
         self.int8_mode: int = 0
         self.quantization: str = ""
 
-    def update_from_env(self):
-        self.int8_mode = int(os.environ.get("INT8_MODE", self.int8_mode))
-        self.quantization = os.environ.get("QUANTIZATION", self.quantization)
-        if self.int8_mode and not self.quantization:
-            self.quantization = WEIGHT_TYPE.INT8.to_str()
-
     def to_string(self):
         return f"int8_mode: {self.int8_mode}\n" f"quantization: {self.quantization}"
 
-
-class PyEplbConfig:
-    def __init__(self):
-        self.eplb_mode: str = "NONE"
-        self.eplb_update_time: int = 5000
-        self.redundant_expert: int = 0
-        self.hack_ep_single_entry: int = 0
-        self.balance_method: str = "mix"
-        self.eplb_force_repack: int = 0
-        self.eplb_stats_window_size: int = 10
-
-    def update_from_env(self):
-        self.eplb_mode = os.environ.get("EPLB_MODE", self.eplb_mode)
-        self.eplb_update_time = int(
-            os.environ.get("EPLB_UPDATE_TIME", self.eplb_update_time)
-        )
-        self.redundant_expert = int(
-            os.environ.get("REDUNDANT_EXPERT", self.redundant_expert)
-        )
-        self.hack_ep_single_entry = int(
-            os.environ.get("HACK_EP_SINGLE_ENTRY", self.hack_ep_single_entry)
-        )
-        self.balance_method = os.environ.get("BALANCE_METHOD", self.balance_method)
-        self.eplb_force_repack = int(
-            os.environ.get("EPLB_FORCE_REPACK", self.eplb_force_repack)
-        )
-        self.eplb_stats_window_size = int(
-            os.environ.get("EPLB_STATS_WINDOW_SIZE", self.eplb_stats_window_size)
-        )
-
-    def to_string(self):
-        return (
-            f"eplb_mode: {self.eplb_mode}\n"
-            f"eplb_update_time: {self.eplb_update_time}\n"
-            f"redundant_expert: {self.redundant_expert}\n"
-            f"hack_ep_single_entry: {self.hack_ep_single_entry}\n"
-            f"balance_method: {self.balance_method}\n"
-            f"eplb_force_repack: {self.eplb_force_repack}\n"
-            f"eplb_stats_window_size: {self.eplb_stats_window_size}"
-        )
-
-
-class PyKvCacheConfig:
-    def __init__(self):
-        self.int8_kv_cache: int = 0
-        self.fp8_kv_cache: int = 0
-        self.kv_cache_mem_mb: int = -1
-        self.seq_size_per_block: int = -1
-        self.test_block_num: int = 0
-        self.use_block_cache: Optional[int] = None
-        self.blockwise_use_fp8_kv_cache: int = 0
-        self.kv_cache_dtype: Optional[str] = None
-        self.reuse_cache: bool = False
-
-    def update_from_env(self):
-        self.int8_kv_cache = int(os.environ.get("INT8_KV_CACHE", self.int8_kv_cache))
-        self.fp8_kv_cache = int(os.environ.get("FP8_KV_CACHE", self.fp8_kv_cache))
-        self.kv_cache_mem_mb = int(
-            os.environ.get("KV_CACHE_MEM_MB", self.kv_cache_mem_mb)
-        )
-        self.seq_size_per_block = int(
-            os.environ.get("SEQ_SIZE_PER_BLOCK", self.seq_size_per_block)
-        )
-        self.test_block_num = int(os.environ.get("TEST_BLOCK_NUM", self.test_block_num))
-        use_block_cache = os.environ.get("USE_BLOCK_CACHE")
-        if use_block_cache is not None:
-            self.use_block_cache = int(use_block_cache)
-        self.blockwise_use_fp8_kv_cache = int(
-            os.environ.get(
-                "BLOCKWISE_USE_FP8_KV_CACHE", self.blockwise_use_fp8_kv_cache
-            )
-        )
-        self.kv_cache_dtype = os.environ.get("KV_CACHE_DTYPE", None)
-        if self.int8_kv_cache:
-            self.kv_cache_dtype = WEIGHT_TYPE.INT8.to_str()
-        elif self.blockwise_use_fp8_kv_cache or self.fp8_kv_cache:
-            self.kv_cache_dtype = WEIGHT_TYPE.FP8.to_str()
-        elif int(os.environ.get("USE_FLOAT32", 0)):
-            self.kv_cache_dtype = WEIGHT_TYPE.FP32.to_str()
-        if not self.kv_cache_dtype:
-            self.kv_cache_dtype = WEIGHT_TYPE.AUTO.to_str()
-        self.reuse_cache = get_env_bool("REUSE_CACHE", self.reuse_cache)
-
-    def to_string(self):
-        return (
-            f"int8_kv_cache: {self.int8_kv_cache}\n"
-            f"fp8_kv_cache: {self.fp8_kv_cache}\n"
-            f"kv_cache_dtype: {self.kv_cache_dtype}\n"
-            f"kv_cache_mem_mb: {self.kv_cache_mem_mb}\n"
-            f"seq_size_per_block: {self.seq_size_per_block}\n"
-            f"test_block_num: {self.test_block_num}\n"
-            f"use_block_cache: {self.use_block_cache}\n"
-            f"blockwise_use_fp8_kv_cache: {self.blockwise_use_fp8_kv_cache}\n"
-            f"reuse_cache: {self.reuse_cache}"
-        )
-
-
-class PyDeviceResourceConfig:
-    def __init__(self):
-        self.reserver_runtime_mem_mb: int = 1024
-        self.specify_gpu_arch: str = ""
-        self.acext_gemm_config_dir: Optional[str] = None
-        self.device_reserve_memory_bytes: int = 0
-
-    def update_from_env(self):
-        self.reserver_runtime_mem_mb = int(
-            os.environ.get("RESERVER_RUNTIME_MEM_MB", self.reserver_runtime_mem_mb)
-        )
-        self.specify_gpu_arch = os.environ.get(
-            "SPECIFY_GPU_ARCH", self.specify_gpu_arch
-        )
-        self.acext_gemm_config_dir = os.environ.get(
-            "ACEXT_GEMM_CONFIG_DIR", self.acext_gemm_config_dir
-        )
-        self.device_reserve_memory_bytes = int(
-            os.environ.get(
-                "DEVICE_RESERVE_MEMORY_BYTES", self.device_reserve_memory_bytes
-            )
-        )
-
-    def to_string(self):
-        return (
-            f"reserver_runtime_mem_mb: {self.reserver_runtime_mem_mb}\n"
-            f"specify_gpu_arch: {self.specify_gpu_arch}\n"
-            f"acext_gemm_config_dir: {self.acext_gemm_config_dir}\n"
-            f"device_reserve_memory_bytes: {self.device_reserve_memory_bytes}"
-        )
-
-
-class SparseConfig:
-    def __init__(self):
-        self.sparse_config_file: Optional[str] = None
-
-    def update_from_env(self):
-        self.sparse_config_file = os.environ.get(
-            "SPARSE_CONFIG_FILE", self.sparse_config_file
-        )
-
-    def to_string(self):
-        return f"sparse_config_file: {self.sparse_config_file}"
-
-
-class EngineConfig:
-    def __init__(self):
-        self.warm_up: int = 1
-        self.warm_up_with_loss: int = 0
-        self.max_seq_len: int = 0
-
-    def update_from_env(self):
-        self.warm_up = int(os.environ.get("WARM_UP", self.warm_up))
-        self.warm_up_with_loss = int(
-            os.environ.get("WARM_UP_WITH_LOSS", self.warm_up_with_loss)
-        )
-        self.max_seq_len = int(os.environ.get("MAX_SEQ_LEN", self.max_seq_len))
-
-    def to_string(self):
-        return (
-            f"warm_up: {self.warm_up}\n"
-            f"warm_up_with_loss: {self.warm_up_with_loss}\n"
-            f"max_seq_len: {self.max_seq_len}"
-        )
-
-
 class EmbeddingConfig:
     def __init__(self):
         self.embedding_model: int = 0
         self.extra_input_in_mm_embedding = ""
 
-    def update_from_env(self):
-        self.embedding_model = int(
-            os.environ.get("EMBEDDING_MODEL", self.embedding_model)
-        )
-        self.extra_input_in_mm_embedding = os.environ.get(
-            "EXTRA_INPUT_IN_MM_EMBEDDING", self.extra_input_in_mm_embedding
-        )
-
     def to_string(self):
         return (
             f"embedding_model: {self.embedding_model}\n"
             f"extra_input_in_mm_embedding: {self.extra_input_in_mm_embedding}"
         )
-
-
 class RoleConfig:
     def __init__(self):
-        self.role_type: RoleType = RoleType.PDFUSION
-
-    def update_from_env(self):
-        self.role_type = self._trans_role_type(os.environ.get("ROLE_TYPE", ""))
+        self._role_type: RoleType = RoleType.PDFUSION
+
+    @property
+    def role_type(self) -> RoleType:
+        """Get role_type as RoleType enum."""
+        return self._role_type
+
+    @role_type.setter
+    def role_type(self, value):
+        """Set role_type, accepting either RoleType enum or string."""
+        if isinstance(value, str):
+            self._role_type = RoleConfig._trans_role_type(value)
+        elif isinstance(value, RoleType):
+            self._role_type = value
+        else:
+            raise TypeError(f"role_type must be RoleType enum or str, got {type(value)}")
 
     def to_string(self):
-        return f"role_type: {self.role_type.name}"
+        return f"role_type: {self._role_type.name}"
 
     @staticmethod
     def _trans_role_type(role_type: str) -> RoleType:
@@ -637,180 +258,20 @@ class RoleConfig:
         else:
             return RoleType.PDFUSION
 
-
-class PdSeparationConfig:
-    def __init__(self):
-        # Prefill related configuration
-        self.prefill_retry_times: int = 0
-        self.prefill_retry_timeout_ms: int = 20
-        self.prefill_max_wait_timeout_ms: int = 600 * 1000
-
-        # Decode related configuration
-        self.decode_retry_times: int = 100
-        self.decode_retry_timeout_ms: int = 100
-        self.decode_polling_kv_cache_step_ms: int = 30
-        self.decode_entrance: int = 0
-
-        # RDMA related configuration
-        self.rdma_connect_retry_times: int = 0
-        self.load_cache_timeout_ms: int = 5000
-
-    def update_from_env(self):
-        # Prefill related configuration
-        self.prefill_retry_times = int(
-            os.environ.get("PREFILL_RETRY_TIMES", self.prefill_retry_times)
-        )
-        self.prefill_retry_timeout_ms = int(
-            os.environ.get("PREFILL_RETRY_TIMEOUT_MS", self.prefill_retry_timeout_ms)
-        )
-        self.prefill_max_wait_timeout_ms = int(
-            os.environ.get(
-                "PREFILL_MAX_WAIT_TIMEOUT_MS", self.prefill_max_wait_timeout_ms
-            )
-        )
-
-        # Decode related configuration
-        self.decode_retry_times = int(
-            os.environ.get("DECODE_RETRY_TIMES", self.decode_retry_times)
-        )
-        self.decode_retry_timeout_ms = int(
-            os.environ.get("DECODE_RETRY_TIMEOUT_MS", self.decode_retry_timeout_ms)
-        )
-        self.decode_polling_kv_cache_step_ms = int(
-            os.environ.get(
-                "DECODE_POLLING_KV_CACHE_STEP_MS", self.decode_polling_kv_cache_step_ms
-            )
-        )
-        self.decode_entrance = int(
-            os.environ.get("DECODE_ENTRANCE", self.decode_entrance)
-        )
-
-        # RDMA related configuration
-        self.rdma_connect_retry_times = int(
-            os.environ.get("RDMA_CONNECT_RETRY_TIMES", self.rdma_connect_retry_times)
-        )
-        self.load_cache_timeout_ms = int(
-            os.environ.get("LOAD_CACHE_TIMEOUT_MS", self.load_cache_timeout_ms)
-        )
-
-    def to_string(self):
-        return (
-            f"prefill_retry_times: {self.prefill_retry_times}\n"
-            f"prefill_retry_timeout_ms: {self.prefill_retry_timeout_ms}\n"
-            f"prefill_max_wait_timeout_ms: {self.prefill_max_wait_timeout_ms}\n"
-            f"decode_retry_times: {self.decode_retry_times}\n"
-            f"decode_retry_timeout_ms: {self.decode_retry_timeout_ms}\n"
-            f"decode_polling_kv_cache_step_ms: {self.decode_polling_kv_cache_step_ms}\n"
-            f"decode_entrance: {self.decode_entrance}\n"
-            f"rdma_connect_retry_times: {self.rdma_connect_retry_times}\n"
-            f"load_cache_timeout_ms: {self.load_cache_timeout_ms}"
-        )
-
-
-class WorkerConfig:
-    def __init__(self):
-        self.worker_info_port_num: int = MIN_WORKER_INFO_PORT_NUM
-
-    def update_from_env(self):
-        self.worker_info_port_num = int(
-            os.environ.get("WORKER_INFO_PORT_NUM", self.worker_info_port_num)
-        )
-
-    def to_string(self):
-        return f"worker_info_port_num: {self.worker_info_port_num}"
-
-
 class JITConfig:
     def __init__(self):
         self.remote_jit_dir: str = ""
 
-    def update_from_env(self):
-        self.remote_jit_dir = os.environ.get("REMOTE_JIT_DIR", self.remote_jit_dir)
-        os.environ["REMOTE_JIT_DIR"] = fetch_remote_file_to_local(
-            self.remote_jit_dir, MountRwMode.RWMODE_RW
-        )
-
     def to_string(self):
         return f"remote_jit_dir: {self.remote_jit_dir}"
 
-
-class PyHwKernelConfig:
-    def __init__(self):
-        self.deep_gemm_num_sm: int = -1
-        self.arm_gemm_use_kai: bool = False
-        self.enable_stable_scatter_add: bool = False
-        self.enable_multi_block_mode: bool = True
-        self.ft_disable_custom_ar: bool = True
-        self.rocm_hipblaslt_config: str = "gemm_config.csv"
-        self.use_swizzleA = False
-        self.enable_cuda_graph: bool = False
-        self.enable_cuda_graph_debug_mode: bool = False
-        self.use_aiter_pa: bool = True
-        self.use_asm_pa: bool = True
-        self.enable_native_cuda_graph: bool = False
-        self.num_native_cuda_graph: int = 200
-
-    def update_from_env(self):
-        self.deep_gemm_num_sm = get_env_int("DEEP_GEMM_NUM_SM", self.deep_gemm_num_sm)
-        self.arm_gemm_use_kai = get_env_bool("ARM_GEMM_USE_KAI", self.arm_gemm_use_kai)
-        self.enable_stable_scatter_add = get_env_bool(
-            "ENABLE_STABLE_SCATTER_ADD", self.enable_stable_scatter_add
-        )
-        self.enable_multi_block_mode = get_env_bool(
-            "ENABLE_MULTI_BLOCK_MODE", self.enable_multi_block_mode
-        )
-        self.ft_disable_custom_ar = get_env_bool(
-            "FT_DISABLE_CUSTOM_AR", self.ft_disable_custom_ar
-        )
-        self.rocm_hipblaslt_config = get_env_str(
-            "ROCM_HIPBLASLT_CONFIG", self.rocm_hipblaslt_config
-        )
-        self.use_swizzleA = get_env_bool(
-            "USE_SWIZZLEA", self.use_swizzleA
-        )
-        self.enable_cuda_graph = get_env_bool(
-            "ENABLE_CUDA_GRAPH", self.enable_cuda_graph
-        )
-        self.enable_cuda_graph_debug_mode = get_env_bool(
-            "ENABLE_CUDA_GRAPH_DEBUG_MODE", self.enable_cuda_graph_debug_mode
-        )
-        self.use_aiter_pa = get_env_bool("USE_AITER_PA", self.use_aiter_pa)
-        self.use_asm_pa = get_env_bool("USE_ASM_PA", self.use_asm_pa)
-        self.enable_native_cuda_graph = get_env_bool(
-            "ENABLE_NATIVE_CUDA_GRAPH", self.enable_native_cuda_graph
-        )
-        self.num_native_cuda_graph = get_env_int(
-            "NUM_NATIVE_CUDA_GRAPH", self.num_native_cuda_graph
-        )
-
-    def to_string(self):
-        return (
-            f"deep_gemm_num_sm: {self.deep_gemm_num_sm}\n"
-            f"arm_gemm_use_kai: {self.arm_gemm_use_kai}\n"
-            f"enable_stable_scatter_add: {self.enable_stable_scatter_add}\n"
-            f"enable_multi_block_mode: {self.enable_multi_block_mode}\n"
-            f"ft_disable_custom_ar: {self.ft_disable_custom_ar}\n"
-            f"rocm_hipblaslt_config: {self.rocm_hipblaslt_config}\n"
-            f"use_swizzleA: {self.use_swizzleA}\n"
-            f"enable_cuda_graph: {self.enable_cuda_graph}\n"
-            f"enable_cuda_graph_debug_mode: {self.enable_cuda_graph_debug_mode}\n"
-            f"use_aiter_pa: {self.use_aiter_pa}\n"
-            f"use_asm_pa: {self.use_asm_pa}\n"
-            f"enable_native_cuda_graph: {self.enable_native_cuda_graph}\n"
-            f"num_native_cuda_graph: {self.num_native_cuda_graph}"
-        )
-
-
 class PyEnvConfigs:
     def __init__(self):
         self.server_config: ServerConfig = ServerConfig()
         self.profiling_debug_config: ProfilingDebugLoggingConfig = (
             ProfilingDebugLoggingConfig()
         )
-        self.model_config: ModelConfig = ModelConfig()
-        self.py_speculative_execution_config: PySpeculativeExecutionConfig = (
-            PySpeculativeExecutionConfig()
-        )
+        self.model_args: ModelArgs = ModelArgs()
         self.lora_config: LoraConfig = LoraConfig()
         self.load_config: LoadConfig = LoadConfig()
         self.render_config: RenderConfig = RenderConfig()
@@ -818,59 +279,33 @@ class PyEnvConfigs:
         self.vit_config: VitConfig = VitConfig()
         self.generate_env_config: GenerateEnvConfig = GenerateEnvConfig()
         self.quantization_config: QuantizationConfig = QuantizationConfig()
-        self.py_eplb_config: PyEplbConfig = PyEplbConfig()
-        self.py_kv_cache_config: PyKvCacheConfig = PyKvCacheConfig()
-        self.py_device_resource_config: PyDeviceResourceConfig = (
-            PyDeviceResourceConfig()
-        )
-        self.sparse_config: SparseConfig = SparseConfig()
-        self.engine_config: EngineConfig = EngineConfig()
+        self.py_eplb_config: EPLBConfig = EPLBConfig()
+        self.kv_cache_config: KVCacheConfig = KVCacheConfig()
+        self.device_resource_config: DeviceResourceConfig = DeviceResourceConfig()
+        self.runtime_config: RuntimeConfig = RuntimeConfig()
+        # EngineConfig has been merged into RuntimeConfig and ModelConfig
+        # warm_up and warm_up_with_loss are in RuntimeConfig
+        # max_seq_len is in ModelConfig
         self.embedding_config: EmbeddingConfig = EmbeddingConfig()
-        self.worker_config: WorkerConfig = WorkerConfig()
         self.role_config: RoleConfig = RoleConfig()
-        self.pd_separation_config: PdSeparationConfig = PdSeparationConfig()
-        self.parallelism_distributed_config: ParallelismDistributedConfig = (
-            ParallelismDistributedConfig()
+        self.pd_separation_config: PDSepConfig = PDSepConfig()
+        self.parallelism_config: ParallelismConfig = (
+            ParallelismConfig()
         )
         self.ffn_disaggregate_config: FfnDisAggregateConfig = FfnDisAggregateConfig()
         self.model_specific_config = ModelSpecificConfig()
         self.fmha_config = FMHAConfig()
-        self.misc_config = MiscellaneousConfig()
+        self.misc_config = PyMiscellaneousConfig()
         self.concurrency_config = ConcurrencyConfig()
         self.jit_config = JITConfig()
-        self.py_hw_kernel_config = PyHwKernelConfig()
-
-    def update_from_env(self):
-        self.server_config.update_from_env()
-        self.profiling_debug_config.update_from_env()
-        self.model_config.update_from_env()
-        self.py_speculative_execution_config.update_from_env()
-        self.lora_config.update_from_env()
-        self.load_config.update_from_env()
-        self.render_config.update_from_env()
-        self.gang_config.update_from_env()
-        self.vit_config.update_from_env()
-        self.generate_env_config.update_from_env()
-        self.quantization_config.update_from_env()
-        self.py_eplb_config.update_from_env()
-        self.py_kv_cache_config.update_from_env()
-        self.py_device_resource_config.update_from_env()
-        self.sparse_config.update_from_env()
-        self.engine_config.update_from_env()
-        self.embedding_config.update_from_env()
-        self.worker_config.update_from_env()
-        self.role_config.update_from_env()
-        self.pd_separation_config.update_from_env()
-        # in gpt model parameters, we should update it from g_parallel_info
-        self.parallelism_distributed_config.update_from_env()
-        self.model_specific_config.update_from_env()
-        self.fmha_config.update_from_env()
-        self.misc_config.update_from_env()
-        self.concurrency_config.update_from_env()
-        self.ffn_disaggregate_config.update_from_env()
-        self.jit_config.update_from_env()
-        self.py_hw_kernel_config.update_from_env()
-        logging.info(self.to_string())
+        self.py_hw_kernel_config: HWKernelConfig = HWKernelConfig()
+        self.moe_config = MoeConfig()
+        self.sp_config = SpeculativeExecutionConfig()
+        self.cache_store_config = CacheStoreConfig()
+        self.arpc_config = ArpcConfig()
+
+    # update_from_args 方法已不再需要
+    # 配置绑定现在通过声明式 bind_to 参数在 add_argument 时自动处理
 
     def to_string(self):
         return (
@@ -879,8 +314,8 @@ class PyEnvConfigs:
             + self.profiling_debug_config.to_string()
             + "\n\n"
             "[model_config]\n" + self.model_config.to_string() + "\n\n"
-            "[py_speculative_execution_config]\n"
-            + self.py_speculative_execution_config.to_string()
+            "[sp_config]\n"
+            + self.sp_config.to_string()
             + "\n\n"
             "[lora_config]\n" + self.lora_config.to_string() + "\n\n"
             "[load_config]\n" + self.load_config.to_string() + "\n\n"
@@ -890,14 +325,12 @@ class PyEnvConfigs:
             "[generate_env_config]\n" + self.generate_env_config.to_string() + "\n\n"
             "[quantization_config]\n" + self.quantization_config.to_string() + "\n\n"
             "[py_eplb_config]\n" + self.py_eplb_config.to_string() + "\n\n"
-            "[py_kv_cache_config]\n" + self.py_kv_cache_config.to_string() + "\n\n"
-            "[py_device_resource_config]\n"
-            + self.py_device_resource_config.to_string()
+            "[kv_cache_config]\n" + self.kv_cache_config.to_string() + "\n\n"
+            "[device_resource_config]\n"
+            + self.device_resource_config.to_string()
             + "\n\n"
             "[sparse_config]\n" + self.sparse_config.to_string() + "\n\n"
-            "[engine_config]\n" + self.engine_config.to_string() + "\n\n"
             "[embedding_config]\n" + self.embedding_config.to_string() + "\n\n"
-            "[worker_config]\n" + self.worker_config.to_string() + "\n\n"
             "[role_config]\n" + self.role_config.to_string() + "\n\n"
             "[pd_separation_config]\n" + self.pd_separation_config.to_string() + "\n\n"
             "[parallelism_distributed_config]\n"
@@ -911,16 +344,10 @@ class PyEnvConfigs:
             "[concurrency_config]\n" + self.concurrency_config.to_string() + "\n\n"
             "[jit_config]\n" + self.jit_config.to_string() + "\n\n"
             "[py_hw_kernel_config]\n" + self.py_hw_kernel_config.to_string() + "\n\n"
+            "[moe_config]\n" + self.moe_config.to_string() + "\n\n"
+            "[sp_config]\n" + self.sp_config.to_string() + "\n\n"
+            "[cache_store_config]\n" + self.cache_store_config.to_string() + "\n\n"
+            "[runtime_config]\n" + self.runtime_config.to_string() + "\n\n"
+            "[batch_decode_scheduler_config]\n" + self.runtime_config.batch_decode_scheduler_config.to_string() + "\n\n"
+            "[fifo_scheduler_config]\n" + self.runtime_config.fifo_scheduler_config.to_string() + "\n\n"
         )
-
-
-# some configs are from static method or global method, etc, we collect them in `StaticConfig`, but in-none-static methods,
-# we should use configs alone. This design can make the codes of this project more clear. All configs
-# should be retrived from `StaticConfig` or a top-down `PyEnvConfigs`. Notably, we don't modify smoke
-# test envs and that's necessary.
-StaticConfig = PyEnvConfigs()
-StaticConfig.update_from_env()
-
-# The envs we reserve below:
-# 1. weights convert: because we don't use it in our project.
-# 2. smoke test.
diff --git a/rtp_llm/config/quant_config.py b/rtp_llm/config/quant_config.py
index d2cb1a76c..551d1bb79 100644
--- a/rtp_llm/config/quant_config.py
+++ b/rtp_llm/config/quant_config.py
@@ -1,8 +1,9 @@
 import json
+import os
 import weakref
 from abc import ABC, abstractmethod
 from enum import Enum
-from typing import Any, Dict, List
+from typing import Any, Dict, List, Optional
 
 import torch
 
@@ -93,6 +94,106 @@ class QuantizationConfig(ABC):
     def group_size(self) -> int:
         return self._group_size
 
+    @classmethod
+    def load_from_ckpt(cls, ckpt_path: str) -> Optional["QuantizationConfig"]:
+        """
+        Load quantization config from checkpoint directory.
+        
+        Args:
+            ckpt_path: Path to checkpoint directory
+            
+        Returns:
+            QuantizationConfig instance if found, None otherwise
+        """
+        quant_config_path = os.path.join(ckpt_path, "smoothquant.ini")
+        if os.path.exists(quant_config_path):
+            return cls.from_config(
+                {
+                    "bits": 0,
+                    "method": "smooth_quant",
+                    "group_size": 0,
+                    "is_quanted": True,
+                }
+            )
+
+        per_tensor_config_path = os.path.join(ckpt_path, "pertensorquant.ini")
+
+        if os.path.exists(per_tensor_config_path):
+            return cls.from_config(
+                {
+                    "bits": 0,
+                    "method": "pertensor_quant",
+                    "group_size": 0,
+                    "is_quanted": True,
+                }
+            )
+
+        config_path = os.path.join(ckpt_path, "config.json")
+        if not os.path.exists(config_path):
+            return None
+
+        config_json = json.load(open(config_path))
+        quant_config = None
+        quant_method = None
+        if config_json.get("quantization_config", None):
+            quant_config = config_json["quantization_config"]
+            quant_method = quant_config["quant_method"].lower()
+
+        if config_json.get("quantization", None):
+            quant_config = config_json["quantization"]
+            quant_method = quant_config["quant_algo"].lower()
+        if quant_config is None:
+            return None
+
+        group_size = quant_config["group_size"] if "group_size" in quant_config else 0
+        bits = quant_config["bits"] if "bits" in quant_config else 0
+        if quant_method == "fp8":
+            bits = 8
+            if "weight_block_size" in quant_config:
+                weight_block = quant_config.get("weight_block_size")
+                assert isinstance(weight_block, list) and all(
+                    element == weight_block[0] for element in weight_block
+                ), f"weight_block_size: {weight_block} must be same"
+                group_size = weight_block[0]
+                quant_method = Fp8BlockWiseQuantConfig.get_method()
+        if quant_method == "compressed-tensors":
+            config_groups = quant_config["config_groups"]
+            weights_config = config_groups["group_0"]["weights"]
+            activation_config = config_groups["group_0"]["input_activations"]
+            bits = weights_config["num_bits"]
+            if (
+                weights_config["type"] == "float"
+                and bits == 8
+                and weights_config["strategy"] == "channel"
+            ):
+                quant_method = Fp8PerChannelCompressedQuantConfig.get_method()
+            elif (
+                weights_config["type"] == "float"
+                and bits == 8
+                and weights_config["strategy"] == "tensor"
+            ):
+                quant_method = Fp8PerTensorCompressedQuantConfig.get_method()
+                return Fp8PerTensorCompressedQuantConfig.from_config(
+                    {
+                        "bits": bits,
+                        "method": quant_method,
+                        "group_size": group_size,
+                        "is_quanted": True,
+                        "dynamic": activation_config["dynamic"],
+                        "act_scale_suffix": ".input_scale",
+                        "weight_scale_suffix": ".weight_scale",
+                    }
+                )
+
+        return cls.from_config(
+            {
+                "bits": bits,
+                "method": quant_method,
+                "group_size": group_size,
+                "is_quanted": True,
+            }
+        )
+
 
 class WeightOnlyInt8PerChannelQuantConfig(QuantizationConfig):
     def __init__(self):
diff --git a/rtp_llm/config/task_type.py b/rtp_llm/config/task_type.py
deleted file mode 100644
index 3208043ad..000000000
--- a/rtp_llm/config/task_type.py
+++ /dev/null
@@ -1,73 +0,0 @@
-import os
-from enum import Enum
-
-from rtp_llm.config.py_config_modules import StaticConfig
-from rtp_llm.utils.util import get_config_from_path
-
-"""
-embedding: return model embedding output
-language_model: return token by lm_head
-sequence_classfication: return label and score by classifier
-"""
-
-
-class TaskType(Enum):
-    DENSE_EMBEDDING = "DENSE_EMBEDDING"
-    ALL_EMBEDDING = "ALL_EMBEDDING"
-    SPARSE_EMBEDDING = "SPARSE_EMBEDDING"
-    COLBERT_EMBEDDING = "COLBERT_EMBEDDING"
-    LANGUAGE_MODEL = "LANGUAGE_MODEL"
-    SEQ_CLASSIFICATION = "SEQ_CLASSIFICATION"
-    RERANKER = "RERANKER"
-    LINEAR_SOFTMAX = "LINEAR_SOFTMAX"
-    PLUGIN_TASK = "PLUGIN_TASK"
-    BGE_M3 = "BGE_M3"
-
-    @staticmethod
-    def from_str(task_type: str):
-        for val in TaskType:
-            if val.value == task_type:
-                return val
-        raise Exception(f"unknown task type: {task_type}")
-
-
-def check_task_type(ckpt_path: str):
-    def _is_dense_embedding_task(ckpt_path: str) -> bool:
-        def _check_is_sentence_transformer_repo() -> bool:
-            if os.path.exists(
-                os.path.join(ckpt_path, "config_sentence_transformers.json")
-            ):
-                return True
-            module_file_path = os.path.join(ckpt_path, "modules.json")
-            if os.path.exists(module_file_path):
-                with open(module_file_path, "r") as reader:
-                    content = reader.read()
-                if "sentence_transformers" in content:
-                    return True
-            return False
-
-        return (
-            StaticConfig.embedding_config.embedding_model == 1
-            or _check_is_sentence_transformer_repo()
-        )
-
-    def _is_classifier_task(ckpt_path: str) -> bool:
-        config_json = get_config_from_path(ckpt_path)
-        if not config_json:
-            return False
-        if "architectures" in config_json and len(config_json["architectures"]) > 0:
-            model_type = config_json["architectures"][0]
-            if "SequenceClassification" in model_type:
-                return True
-        return False
-
-    # from_env
-    if StaticConfig.model_config.task_type:
-        return TaskType.from_str(StaticConfig.model_config.task_type)
-    # from config
-    elif _is_dense_embedding_task(ckpt_path):
-        return TaskType.DENSE_EMBEDDING
-    elif _is_classifier_task(ckpt_path):
-        return TaskType.SEQ_CLASSIFICATION
-    else:
-        return TaskType.LANGUAGE_MODEL
diff --git a/rtp_llm/cpp/api_server/ChatService.h b/rtp_llm/cpp/api_server/ChatService.h
index fd4a8ac86..cb7146695 100644
--- a/rtp_llm/cpp/api_server/ChatService.h
+++ b/rtp_llm/cpp/api_server/ChatService.h
@@ -21,12 +21,12 @@ public:
                 const std::shared_ptr<autil::AtomicCounter>&    request_counter,
                 const std::shared_ptr<Tokenizer>&               tokenizer,
                 const std::shared_ptr<ChatRender>&              render,
-                rtp_llm::GptInitParameter                       params,
+                const ModelConfig&                             model_config,
                 const std::shared_ptr<ApiServerMetricReporter>& metric_reporter):
         engine_(engine),
         mm_processor_(mm_processor),
         request_counter_(request_counter),
-        openai_endpoint_(new OpenaiEndpoint(tokenizer, render, params)),
+        openai_endpoint_(new OpenaiEndpoint(tokenizer, render, model_config)),
         metric_reporter_(metric_reporter) {}
     ~ChatService() = default;
 
diff --git a/rtp_llm/cpp/api_server/HttpApiServer.cc b/rtp_llm/cpp/api_server/HttpApiServer.cc
index 1d365089c..19abd8661 100644
--- a/rtp_llm/cpp/api_server/HttpApiServer.cc
+++ b/rtp_llm/cpp/api_server/HttpApiServer.cc
@@ -10,15 +10,15 @@
 
 namespace rtp_llm {
 
-void HttpApiServer::init_controller(const rtp_llm::GptInitParameter& params) {
-    bool block = params.concurrency_config.concurrency_with_block;
-    RTP_LLM_LOG_INFO("Get concurrency_with_block: %d from GptInitParameter.",
-                     params.concurrency_config.concurrency_with_block);
-    if (params.tp_rank_ == 0) {
-        int limit = params.concurrency_config.concurrency_limit;
+void HttpApiServer::init_controller(const ConcurrencyConfig& concurrency_config, const ParallelismConfig& parallelism_config) {
+    bool block = concurrency_config.concurrency_with_block;
+    RTP_LLM_LOG_INFO("Get concurrency_with_block: %d from ConcurrencyConfig.",
+                     concurrency_config.concurrency_with_block);
+    if (parallelism_config.tp_rank == 0) {
+        int limit = concurrency_config.concurrency_limit;
         RTP_LLM_LOG_INFO("CONCURRENCY_LIMIT to %d", limit);
         controller_ = std::make_shared<ConcurrencyController>(limit, block);
-    } else /* if (params.tp_size_ != 1) */ {
+    } else /* if (parallelism_config.tp_size != 1) */ {
         RTP_LLM_LOG_INFO("use gang cluster and is worker, set CONCURRENCY_LIMIT to 99");
         controller_ = std::make_shared<ConcurrencyController>(99, block);
     }
@@ -209,7 +209,7 @@ bool HttpApiServer::registerTokenizerService() {
 
 bool HttpApiServer::registerChatService() {
     chat_service_.reset(
-        new ChatService(engine_, mm_processor_, request_counter_, tokenizer_, render_, params_, metric_reporter_));
+        new ChatService(engine_, mm_processor_, request_counter_, tokenizer_, render_, params_.model_config_, metric_reporter_));
     auto chat_completions_callback = [active_request_count = active_request_count_,
                                       chat_service         = chat_service_,
                                       controller           = controller_,
@@ -266,7 +266,7 @@ bool HttpApiServer::registerInferenceService() {
         return false;
     }
     inference_service_.reset(new InferenceService(
-        engine_, mm_processor_, request_counter_, token_processor_, controller_, params_, metric_reporter_));
+        engine_, mm_processor_, request_counter_, token_processor_, controller_, params_.model_config_, metric_reporter_));
     auto inference_internal_callback =
         [active_request_count = active_request_count_, inference_service = inference_service_](
             std::unique_ptr<http_server::HttpResponseWriter> writer, const http_server::HttpRequest& request) -> void {
diff --git a/rtp_llm/cpp/api_server/HttpApiServer.h b/rtp_llm/cpp/api_server/HttpApiServer.h
index 04630869a..04e0f43c1 100644
--- a/rtp_llm/cpp/api_server/HttpApiServer.h
+++ b/rtp_llm/cpp/api_server/HttpApiServer.h
@@ -17,6 +17,7 @@
 #include "rtp_llm/cpp/api_server/InferenceService.h"
 #include "rtp_llm/cpp/api_server/EmbeddingService.h"
 #include "rtp_llm/cpp/api_server/LoraService.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 namespace rtp_llm {
 
@@ -37,14 +38,13 @@ public:
         engine_(engine),
         mm_processor_(mm_processor),
         addr_(address),
-        engine_init_param_(params),
-        params_(params.gpt_init_parameter),
+        params_(params),
         token_processor_(new TokenProcessor(token_processor)),
         metrics_reporter_(params.metrics_reporter) {
         is_embedding_ = false;
         active_request_count_.reset(new autil::AtomicCounter());
         request_counter_.reset(new autil::AtomicCounter());
-        init_controller(params_);
+        init_controller(params_.concurrency_config, params_.parallelism_config);
     }
 
     // embedding engine
@@ -52,12 +52,12 @@ public:
                   std::shared_ptr<MultimodalProcessor> mm_processor,
                   const EngineInitParams&              params,
                   py::object                           custom_module):
-        engine_init_param_(params), params_(params.gpt_init_parameter), metrics_reporter_(params.metrics_reporter) {
+        params_(params), metrics_reporter_(params.metrics_reporter) {
         is_embedding_       = true;
         embedding_endpoint_ = std::make_shared<EmbeddingEndpoint>(embedding_engine, mm_processor, custom_module);
         active_request_count_.reset(new autil::AtomicCounter());
         request_counter_.reset(new autil::AtomicCounter());
-        init_controller(params_);
+        init_controller(params_.concurrency_config, params_.parallelism_config);
     }
 
     ~HttpApiServer() = default;
@@ -77,7 +77,7 @@ public:
     }
 
 private:
-    void init_controller(const rtp_llm::GptInitParameter& params);
+    void init_controller(const ConcurrencyConfig& concurrency_config, const ParallelismConfig& parallelism_config);
 
 private:
     bool registerServices();
@@ -101,8 +101,7 @@ private:
     std::shared_ptr<MultimodalProcessor> mm_processor_;
     std::string                          addr_;
 
-    const EngineInitParams&                engine_init_param_;
-    const rtp_llm::GptInitParameter&       params_;
+    const EngineInitParams&               params_;
     std::shared_ptr<ConcurrencyController> controller_;
     std::shared_ptr<TokenProcessor>        token_processor_;
 
diff --git a/rtp_llm/cpp/api_server/InferenceService.cc b/rtp_llm/cpp/api_server/InferenceService.cc
index 499037c87..013e25752 100644
--- a/rtp_llm/cpp/api_server/InferenceService.cc
+++ b/rtp_llm/cpp/api_server/InferenceService.cc
@@ -50,7 +50,7 @@ void InferenceParsedRequest::extractRequestUrls(const RawRequest& req, Inference
 
 void InferenceParsedRequest::extractRequestGenerateConfigs(RawRequest&                            req,
                                                            InferenceParsedRequest&                pr,
-                                                           const rtp_llm::GptInitParameter&       params,
+                                                           const ModelConfig&                     model_config,
                                                            const std::shared_ptr<TokenProcessor>& token_processor) {
     if (req.generate_config.has_value()) {
         auto& config = req.generate_config.value();
@@ -58,7 +58,7 @@ void InferenceParsedRequest::extractRequestGenerateConfigs(RawRequest&
             throw HttpApiServerException(HttpApiServerException::ERROR_INPUT_FORMAT_ERROR,
                                          "request is non_stream but use incremental decoder");
         }
-        config.addSpecialTokens(params.special_tokens_);
+        config.addSpecialTokens(model_config.special_tokens);
         if (config.sp_advice_prompt.empty() == false) {
             config.sp_advice_prompt_token_ids = token_processor->encode(config.sp_advice_prompt);
         }
@@ -87,7 +87,7 @@ void InferenceParsedRequest::extractRequestGenerateConfigs(RawRequest&
 }
 
 InferenceParsedRequest InferenceParsedRequest::extractRequest(const std::string&                     body,
-                                                              const rtp_llm::GptInitParameter&       params,
+                                                              const ModelConfig&                     model_config,
                                                               const std::shared_ptr<TokenProcessor>& token_processor) {
     RawRequest req;
     FromJsonString(req, body);
@@ -100,7 +100,7 @@ InferenceParsedRequest InferenceParsedRequest::extractRequest(const std::string&
 
     InferenceParsedRequest::extractRequestTexts(req, pr);
     InferenceParsedRequest::extractRequestUrls(req, pr);
-    InferenceParsedRequest::extractRequestGenerateConfigs(req, pr, params, token_processor);
+    InferenceParsedRequest::extractRequestGenerateConfigs(req, pr, model_config, token_processor);
 
     return pr;
 }
@@ -110,14 +110,14 @@ InferenceService::InferenceService(const std::shared_ptr<EngineBase>&
                                    const std::shared_ptr<autil::AtomicCounter>&    request_counter,
                                    const std::shared_ptr<TokenProcessor>&          token_processor,
                                    const std::shared_ptr<ConcurrencyController>&   controller,
-                                   const rtp_llm::GptInitParameter&                params,
+                                   const ModelConfig&                              model_config,
                                    const std::shared_ptr<ApiServerMetricReporter>& metric_reporter):
     engine_(engine),
     mm_processor_(mm_processor),
     token_processor_(token_processor),
     request_counter_(request_counter),
     controller_(controller),
-    params_(params),
+    model_config_(model_config),
     metric_reporter_(metric_reporter) {}
 
 void checkMasterWorker(bool isInternal) {
@@ -163,7 +163,7 @@ void InferenceService::inferResponse(int64_t
     autil::StageTime iterate_stage_timer;
     auto             start_time_ms = autil::TimeUtility::currentTimeInMilliSeconds();
     const auto       body          = request.GetBody();
-    auto             req           = InferenceParsedRequest::extractRequest(body, params_, token_processor_);
+    auto             req           = InferenceParsedRequest::extractRequest(body, model_config_, token_processor_);
     if (metric_reporter_) {
         metric_reporter_->reportQpsMetric(req.source);
     }
diff --git a/rtp_llm/cpp/api_server/InferenceService.h b/rtp_llm/cpp/api_server/InferenceService.h
index a5196e397..d9df70bbc 100644
--- a/rtp_llm/cpp/api_server/InferenceService.h
+++ b/rtp_llm/cpp/api_server/InferenceService.h
@@ -4,8 +4,8 @@
 
 #include "rtp_llm/cpp/multimodal_processor/MultimodalProcessor.h"
 #include "rtp_llm/cpp/normal_engine/NormalEngine.h"
-#include "rtp_llm/cpp/engine_base/EngineInitParams.h"
 #include "rtp_llm/cpp/engine_base/ProposeModelEngineInitParams.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 
 #include "rtp_llm/cpp/api_server/http_server/http_server/HttpResponseWriter.h"
 #include "rtp_llm/cpp/api_server/http_server/http_server/HttpRequest.h"
@@ -20,13 +20,13 @@ namespace rtp_llm {
 
 struct InferenceParsedRequest {
     static InferenceParsedRequest extractRequest(const std::string&                     body,
-                                                 const rtp_llm::GptInitParameter&       params,
+                                                 const ModelConfig&                     model_config,
                                                  const std::shared_ptr<TokenProcessor>& token_processor);
     static void                   extractRequestTexts(const RawRequest& req, InferenceParsedRequest& pr);
     static void                   extractRequestUrls(const RawRequest& req, InferenceParsedRequest& pr);
     static void                   extractRequestGenerateConfigs(RawRequest&                            req,
                                                                 InferenceParsedRequest&                pr,
-                                                                const rtp_llm::GptInitParameter&       params,
+                                                                const ModelConfig&                     model_config,
                                                                 const std::shared_ptr<TokenProcessor>& token_processor);
 
     bool                                         batch_infer;
@@ -45,7 +45,7 @@ public:
                      const std::shared_ptr<autil::AtomicCounter>&    request_counter,
                      const std::shared_ptr<TokenProcessor>&          token_processor,
                      const std::shared_ptr<ConcurrencyController>&   controller,
-                     const rtp_llm::GptInitParameter&                params,
+                     const ModelConfig&                              model_config,
                      const std::shared_ptr<ApiServerMetricReporter>& metric_reporter);
     ~InferenceService() = default;
 
@@ -88,7 +88,7 @@ private:
     std::shared_ptr<TokenProcessor>          token_processor_;
     std::shared_ptr<autil::AtomicCounter>    request_counter_;
     std::shared_ptr<ConcurrencyController>   controller_;
-    rtp_llm::GptInitParameter                params_;
+    ModelConfig                              model_config_;
     std::shared_ptr<ApiServerMetricReporter> metric_reporter_;
 };
 
diff --git a/rtp_llm/cpp/api_server/openai/OpenaiEndpoint.cc b/rtp_llm/cpp/api_server/openai/OpenaiEndpoint.cc
index df6ad53fc..17a695cf8 100644
--- a/rtp_llm/cpp/api_server/openai/OpenaiEndpoint.cc
+++ b/rtp_llm/cpp/api_server/openai/OpenaiEndpoint.cc
@@ -6,18 +6,18 @@ namespace rtp_llm {
 
 OpenaiEndpoint::OpenaiEndpoint(const std::shared_ptr<Tokenizer>&  tokenizer,
                                const std::shared_ptr<ChatRender>& chat_render,
-                               const rtp_llm::GptInitParameter&   params):
-    tokenizer_(tokenizer), chat_render_(chat_render), model_config_(params) {
+                               const ModelConfig&                 model_config):
+    tokenizer_(tokenizer), chat_render_(chat_render), model_config_(model_config) {
 
-    max_seq_len_ = model_config_.max_seq_len_;
+    max_seq_len_ = model_config_.max_seq_len;
 
     std::optional<int> res;
     if (tokenizer_ && tokenizer_->isPreTrainedTokenizer()) {
         res = tokenizer_->getEosTokenId();
     }
-    eos_token_id_ = res.value_or(model_config_.special_tokens_.eos_token_id_);
+    eos_token_id_ = res.value_or(model_config_.special_tokens.eos_token_id);
 
-    for (const auto& vec : model_config_.special_tokens_.stop_words_id_list_) {
+    for (const auto& vec : model_config_.special_tokens.stop_words_id_list) {
         std::vector<int> tmpVec;
         for (int64_t val : vec) {
             tmpVec.push_back(static_cast<int>(val));
@@ -86,9 +86,9 @@ std::shared_ptr<GenerateConfig> OpenaiEndpoint::extract_generation_config(const
     if (req.logprobs.has_value()) {
         config.return_all_probs = req.logprobs.value();
     }
-    config.addSpecialTokens(model_config_.special_tokens_);
+    config.addSpecialTokens(model_config_.special_tokens);
 
-    auto select_tokens_id = tokenizer_->convertSelectTokens(config.select_tokens_str, model_config_.vocab_size_);
+    auto select_tokens_id = tokenizer_->convertSelectTokens(config.select_tokens_str, model_config_.vocab_size);
     config.select_tokens_id.insert(config.select_tokens_id.begin(), select_tokens_id.begin(), select_tokens_id.end());
     if (config.sp_advice_prompt.empty() == false) {
         config.sp_advice_prompt_token_ids = tokenizer_->encode(config.sp_advice_prompt);
diff --git a/rtp_llm/cpp/api_server/openai/OpenaiEndpoint.h b/rtp_llm/cpp/api_server/openai/OpenaiEndpoint.h
index 03f3535cc..14ab24683 100644
--- a/rtp_llm/cpp/api_server/openai/OpenaiEndpoint.h
+++ b/rtp_llm/cpp/api_server/openai/OpenaiEndpoint.h
@@ -2,7 +2,7 @@
 
 #include <memory>
 
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 #include "rtp_llm/cpp/api_server/openai/ChatRender.h"
 #include "rtp_llm/cpp/api_server/tokenizer/Tokenizer.h"
 
@@ -14,7 +14,7 @@ class OpenaiEndpoint {
 public:
     OpenaiEndpoint(const std::shared_ptr<Tokenizer>&  tokenizer,
                    const std::shared_ptr<ChatRender>& chat_render,
-                   const rtp_llm::GptInitParameter&   params);
+                   const ModelConfig&                 model_config);
     virtual ~OpenaiEndpoint() {}
 
 public:
@@ -33,7 +33,7 @@ private:
 
     std::shared_ptr<Tokenizer>  tokenizer_;
     std::shared_ptr<ChatRender> chat_render_;
-    rtp_llm::GptInitParameter   model_config_;
+    ModelConfig                 model_config_;
 };
 
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/api_server/openai/test/BUILD b/rtp_llm/cpp/api_server/openai/test/BUILD
index 07db9a0d2..f8630eada 100644
--- a/rtp_llm/cpp/api_server/openai/test/BUILD
+++ b/rtp_llm/cpp/api_server/openai/test/BUILD
@@ -7,7 +7,7 @@ test_copts = [
 ] + copts()
 
 test_deps = [
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
     "@com_google_googletest//:gtest",
     "@com_google_googletest//:gtest_main",
     "@local_config_cuda//cuda:cuda_headers",
diff --git a/rtp_llm/cpp/api_server/openai/test/OpenaiEndpointTest.cc b/rtp_llm/cpp/api_server/openai/test/OpenaiEndpointTest.cc
index eac6ff7b7..0f1fb080c 100644
--- a/rtp_llm/cpp/api_server/openai/test/OpenaiEndpointTest.cc
+++ b/rtp_llm/cpp/api_server/openai/test/OpenaiEndpointTest.cc
@@ -32,41 +32,42 @@ protected:
 };
 
 TEST_F(OpenaiEndpointTest, Constructor_TokenizerIsNull) {
-    auto openai_endpoint = std::make_shared<OpenaiEndpoint>(nullptr, nullptr, rtp_llm::GptInitParameter());
+    ModelConfig model_config;
+    auto openai_endpoint = std::make_shared<OpenaiEndpoint>(nullptr, nullptr, model_config);
     EXPECT_TRUE(openai_endpoint->stop_word_ids_list_.empty());
     EXPECT_TRUE(openai_endpoint->stop_words_list_.empty());
 }
 
 TEST_F(OpenaiEndpointTest, Constructor_IsPreTrainedTokenizer) {
-    rtp_llm::GptInitParameter param;
-    param.special_tokens_.eos_token_id_ = 5;
+    ModelConfig model_config;
+    model_config.special_tokens.eos_token_id = 5;
 
     EXPECT_CALL(*mock_tokenizer_, isPreTrainedTokenizer).WillOnce(Return(true));
     EXPECT_CALL(*mock_tokenizer_, getEosTokenId).WillOnce(Return(10));
 
-    auto openai_endpoint = std::make_shared<OpenaiEndpoint>(tokenizer_, nullptr, param);
+    auto openai_endpoint = std::make_shared<OpenaiEndpoint>(tokenizer_, nullptr, model_config);
     EXPECT_EQ(openai_endpoint->eos_token_id_, 10);
     EXPECT_TRUE(openai_endpoint->stop_word_ids_list_.empty());
     EXPECT_TRUE(openai_endpoint->stop_words_list_.empty());
 }
 
 TEST_F(OpenaiEndpointTest, Constructor_IsNotPreTrainedTokenizer) {
-    rtp_llm::GptInitParameter param;
-    param.special_tokens_.eos_token_id_ = 5;
+    ModelConfig model_config;
+    model_config.special_tokens.eos_token_id = 5;
 
     EXPECT_CALL(*mock_tokenizer_, isPreTrainedTokenizer).WillOnce(Return(false));
     EXPECT_CALL(*mock_tokenizer_, getEosTokenId).Times(0);
 
-    auto openai_endpoint = std::make_shared<OpenaiEndpoint>(tokenizer_, nullptr, param);
+    auto openai_endpoint = std::make_shared<OpenaiEndpoint>(tokenizer_, nullptr, model_config);
     EXPECT_EQ(openai_endpoint->eos_token_id_, 5);
     EXPECT_TRUE(openai_endpoint->stop_word_ids_list_.empty());
     EXPECT_TRUE(openai_endpoint->stop_words_list_.empty());
 }
 
 TEST_F(OpenaiEndpointTest, Constructor_RenderIsNotNull) {
-    rtp_llm::GptInitParameter param;
-    param.special_tokens_.eos_token_id_       = 5;
-    param.special_tokens_.stop_words_id_list_ = {{1, 2, 3}, {4, 5, 6}};
+    ModelConfig model_config;
+    model_config.special_tokens.eos_token_id       = 5;
+    model_config.special_tokens.stop_words_id_list = {{1, 2, 3}, {4, 5, 6}};
 
     EXPECT_CALL(*mock_tokenizer_, isPreTrainedTokenizer).WillOnce(Return(true));
     EXPECT_CALL(*mock_tokenizer_, getEosTokenId).WillOnce(Return(10));
@@ -76,7 +77,7 @@ TEST_F(OpenaiEndpointTest, Constructor_RenderIsNotNull) {
 
     std::vector<std::vector<int>> ids_list;
     {
-        for (const auto& list : param.special_tokens_.stop_words_id_list_) {
+        for (const auto& list : model_config.special_tokens.stop_words_id_list) {
             std::vector<int> temp(list.size());
             std::transform(list.begin(), list.end(), temp.begin(), [](int64_t val) { return static_cast<int>(val); });
             ids_list.push_back(temp);
@@ -90,7 +91,7 @@ TEST_F(OpenaiEndpointTest, Constructor_RenderIsNotNull) {
         word_list.push_back(word);
     }
 
-    auto openai_endpoint = std::make_shared<OpenaiEndpoint>(tokenizer_, render_, param);
+    auto openai_endpoint = std::make_shared<OpenaiEndpoint>(tokenizer_, render_, model_config);
     EXPECT_EQ(openai_endpoint->eos_token_id_, 10);
     EXPECT_EQ(openai_endpoint->stop_word_ids_list_, ids_list);
     EXPECT_EQ(openai_endpoint->stop_words_list_, word_list);
@@ -100,8 +101,8 @@ TEST_F(OpenaiEndpointTest, ExtractGenerationConfig) {
     EXPECT_CALL(*mock_tokenizer_, isPreTrainedTokenizer).WillOnce(Return(false));
     EXPECT_CALL(*mock_render_, get_all_extra_stop_word_ids_list).WillOnce(Return(std::vector<std::vector<int>>()));
 
-    rtp_llm::GptInitParameter param;
-    auto                      openai_endpoint = std::make_shared<OpenaiEndpoint>(tokenizer_, render_, param);
+    ModelConfig model_config;
+    auto                      openai_endpoint = std::make_shared<OpenaiEndpoint>(tokenizer_, render_, model_config);
 
     ChatCompletionRequest req;
     req.stream      = false;
@@ -129,12 +130,14 @@ TEST_F(OpenaiEndpointTest, ExtractGenerationConfig) {
 
 TEST_F(OpenaiEndpointTest, GetChatRender) {
     {
-        auto openai_endpoint = std::make_shared<OpenaiEndpoint>(nullptr, nullptr, rtp_llm::GptInitParameter());
+        ModelConfig model_config;
+        auto openai_endpoint = std::make_shared<OpenaiEndpoint>(nullptr, nullptr, model_config);
         EXPECT_EQ(openai_endpoint->getChatRender(), nullptr);
     }
     {
         EXPECT_CALL(*mock_render_, get_all_extra_stop_word_ids_list).WillOnce(Return(std::vector<std::vector<int>>()));
-        auto openai_endpoint = std::make_shared<OpenaiEndpoint>(nullptr, render_, rtp_llm::GptInitParameter());
+        ModelConfig model_config;
+        auto openai_endpoint = std::make_shared<OpenaiEndpoint>(nullptr, render_, model_config);
         EXPECT_EQ(openai_endpoint->getChatRender(), render_);
     }
 }
diff --git a/rtp_llm/cpp/api_server/test/BUILD b/rtp_llm/cpp/api_server/test/BUILD
index d7402981f..fac86a684 100644
--- a/rtp_llm/cpp/api_server/test/BUILD
+++ b/rtp_llm/cpp/api_server/test/BUILD
@@ -16,7 +16,7 @@ cc_library(
 test_deps = [
     "//rtp_llm/cpp/devices/testing:device_test_utils",
     "//rtp_llm/cpp/models:models",
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
     ":test_headers",
     "@com_google_googletest//:gtest",
     "@com_google_googletest//:gtest_main",
diff --git a/rtp_llm/cpp/api_server/test/ChatServiceTest.cc b/rtp_llm/cpp/api_server/test/ChatServiceTest.cc
index b71d5b8f4..93ae009f7 100644
--- a/rtp_llm/cpp/api_server/test/ChatServiceTest.cc
+++ b/rtp_llm/cpp/api_server/test/ChatServiceTest.cc
@@ -41,11 +41,12 @@ protected:
         // 所以需要 mock
         MockWhenConstructOpenaiEndPoint();
 
+        ModelConfig model_config;
         chat_service_ = std::make_shared<ChatService>(
-            engine, nullptr, request_counter, tokenizer, render, rtp_llm::GptInitParameter(), metric_reporter);
+            engine, nullptr, request_counter, tokenizer, render, model_config, metric_reporter);
 
         // mock OpenaiEndpoint 方便测试
-        mock_openai_endpoint_ = std::make_shared<MockOpenaiEndpoint>(tokenizer, render, rtp_llm::GptInitParameter());
+        mock_openai_endpoint_ = std::make_shared<MockOpenaiEndpoint>(tokenizer, render, model_config);
         auto openai_endpoint  = std::dynamic_pointer_cast<OpenaiEndpoint>(mock_openai_endpoint_);
         chat_service_->openai_endpoint_ = openai_endpoint;
 
@@ -85,10 +86,11 @@ protected:
         input->input_ids = std::make_shared<rtp_llm::Buffer>(
             rtp_llm::MemoryType::MEMORY_CPU, rtp_llm::DataType::TYPE_INT32, shape, data_.data());
 
-        rtp_llm::GptInitParameter param;
-        param.max_seq_len_ = data_.size();
+        ModelConfig model_config;
+        RuntimeConfig runtime_config;
+        model_config.max_seq_len = data_.size();
 
-        auto mock_stream = std::make_shared<MockGenerateStream>(input, param);
+        auto mock_stream = std::make_shared<MockGenerateStream>(input, model_config, runtime_config);
         return mock_stream;
     }
 
diff --git a/rtp_llm/cpp/api_server/test/GenerateStreamWrapperTest.cc b/rtp_llm/cpp/api_server/test/GenerateStreamWrapperTest.cc
index a0faab4c0..9d0c569d0 100644
--- a/rtp_llm/cpp/api_server/test/GenerateStreamWrapperTest.cc
+++ b/rtp_llm/cpp/api_server/test/GenerateStreamWrapperTest.cc
@@ -22,10 +22,11 @@ std::shared_ptr<MockGenerateStream> CreateMockGenerateStream() {
     input->input_ids = std::make_shared<rtp_llm::Buffer>(
         rtp_llm::MemoryType::MEMORY_CPU, rtp_llm::DataType::TYPE_INT32, shape, fake_token_ids.data());
 
-    rtp_llm::GptInitParameter param;
-    param.max_seq_len_ = fake_token_ids.size();
+    ModelConfig model_config;
+    RuntimeConfig runtime_config;
+    model_config.max_seq_len = fake_token_ids.size();
 
-    auto mock_stream = std::make_shared<MockGenerateStream>(input, param);
+    auto mock_stream = std::make_shared<MockGenerateStream>(input, model_config, runtime_config);
     return mock_stream;
 }
 
diff --git a/rtp_llm/cpp/api_server/test/InferenceServiceTest.cc b/rtp_llm/cpp/api_server/test/InferenceServiceTest.cc
index 2a226cd90..f1e3b3f3b 100644
--- a/rtp_llm/cpp/api_server/test/InferenceServiceTest.cc
+++ b/rtp_llm/cpp/api_server/test/InferenceServiceTest.cc
@@ -33,12 +33,12 @@ protected:
         mock_metric_reporter_ = std::make_shared<MockApiServerMetricReporter>();
         auto metric_reporter  = std::dynamic_pointer_cast<ApiServerMetricReporter>(mock_metric_reporter_);
 
-        rtp_llm::GptInitParameter params;
+        ModelConfig model_config;
         auto                      request_counter = std::make_shared<autil::AtomicCounter>();
         auto                      controller      = std::make_shared<ConcurrencyController>(1, false);
 
         inference_service_ = std::make_shared<InferenceService>(
-            engine, nullptr, request_counter, token_processor, controller, params, metric_reporter);
+            engine, nullptr, request_counter, token_processor, controller, model_config, metric_reporter);
     }
     void TearDown() override {}
 
@@ -60,10 +60,11 @@ protected:
         input->input_ids = std::make_shared<rtp_llm::Buffer>(
             rtp_llm::MemoryType::MEMORY_CPU, rtp_llm::DataType::TYPE_INT32, shape, data_.data());
 
-        rtp_llm::GptInitParameter param;
-        param.max_seq_len_ = data_.size();
+        ModelConfig model_config;
+        RuntimeConfig runtime_config;
+        model_config.max_seq_len = data_.size();
 
-        auto mock_stream = std::make_shared<MockGenerateStream>(input, param);
+        auto mock_stream = std::make_shared<MockGenerateStream>(input, model_config, runtime_config);
         return mock_stream;
     }
 
diff --git a/rtp_llm/cpp/api_server/test/mock/MockGenerateStream.h b/rtp_llm/cpp/api_server/test/mock/MockGenerateStream.h
index 66fbb63fc..8c13f836b 100644
--- a/rtp_llm/cpp/api_server/test/mock/MockGenerateStream.h
+++ b/rtp_llm/cpp/api_server/test/mock/MockGenerateStream.h
@@ -4,13 +4,14 @@
 #include <gtest/gtest.h>
 
 #include "rtp_llm/cpp/engine_base/stream/GenerateStream.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 namespace rtp_llm {
 
 class MockGenerateStream: public GenerateStream {
 public:
-    MockGenerateStream(const std::shared_ptr<GenerateInput>& input, const rtp_llm::GptInitParameter& param):
-        GenerateStream(input, param, ResourceContext{}, nullptr) {}
+    MockGenerateStream(const std::shared_ptr<GenerateInput>& input, const ModelConfig& model_config, const RuntimeConfig& runtime_config):
+        GenerateStream(input, model_config, runtime_config, ResourceContext{}, nullptr) {}
     ~MockGenerateStream() override = default;
 
 public:
diff --git a/rtp_llm/cpp/api_server/test/mock/MockOpenaiEndpoint.h b/rtp_llm/cpp/api_server/test/mock/MockOpenaiEndpoint.h
index f586e5327..9ee2fb4ce 100644
--- a/rtp_llm/cpp/api_server/test/mock/MockOpenaiEndpoint.h
+++ b/rtp_llm/cpp/api_server/test/mock/MockOpenaiEndpoint.h
@@ -11,8 +11,8 @@ class MockOpenaiEndpoint: public OpenaiEndpoint {
 public:
     MockOpenaiEndpoint(const std::shared_ptr<Tokenizer>&  tokenizer,
                        const std::shared_ptr<ChatRender>& chat_render,
-                       const rtp_llm::GptInitParameter&   params):
-        OpenaiEndpoint(tokenizer, chat_render, params) {}
+                       const ModelConfig&                 model_config):
+        OpenaiEndpoint(tokenizer, chat_render, model_config) {}
     ~MockOpenaiEndpoint() override = default;
 
 public:
diff --git a/rtp_llm/cpp/cache/BUILD b/rtp_llm/cpp/cache/BUILD
index 0249850b6..d97813d6a 100644
--- a/rtp_llm/cpp/cache/BUILD
+++ b/rtp_llm/cpp/cache/BUILD
@@ -46,6 +46,7 @@ cc_library(
         "@com_google_absl//absl/status:statusor",
         "//:rtp_compute_ops",
         "//rtp_llm/cpp/disaggregate/cache_store:cache_store",
+        "//rtp_llm/cpp/config:model_config",
     ] + select({
         "//:enable_3fs": [
             ":storage_3fs",
diff --git a/rtp_llm/cpp/cache/CacheConfigCreator.cc b/rtp_llm/cpp/cache/CacheConfigCreator.cc
index dcab6b842..885ef1ae6 100644
--- a/rtp_llm/cpp/cache/CacheConfigCreator.cc
+++ b/rtp_llm/cpp/cache/CacheConfigCreator.cc
@@ -4,51 +4,56 @@
 
 namespace rtp_llm {
 
-CacheConfig CacheConfigCreator::createBasicConfig(const rtp_llm::GptInitParameter& param, bool is_mtp) {
-    int        local_head_num_kv = (param.head_num_kv_ > 1) ? param.head_num_kv_ / param.tp_size_ : param.head_num_kv_;
+CacheConfig CacheConfigCreator::createBasicConfig(const ModelConfig& model_config,
+                                                  const ParallelismConfig& parallelism_config,
+                                                  bool is_mtp) {
+    int        local_head_num_kv = (model_config.attn_config.kv_head_num > 1) ? model_config.attn_config.kv_head_num / parallelism_config.tp_size : model_config.attn_config.kv_head_num;
     const auto device_prop       = rtp_llm::DeviceFactory::getDefaultDevice()->getDeviceProperties();
-    auto       dtype             = param.kv_cache_data_type_;
+    auto       dtype             = model_config.attn_config.kv_cache_dtype == KvCacheDataType::INT8 ? rtp_llm::DataType::TYPE_INT8 : (model_config.attn_config.kv_cache_dtype == KvCacheDataType::FP8 ? rtp_llm::DataType::TYPE_FP8_E4M3 : model_config.data_type);
     if (device_prop.type == rtp_llm::DeviceType::ArmCpu) {
         // Arm attention operator support FP32 data type only
-        dtype = param.kv_cache_data_type_ == rtp_llm::DataType::TYPE_INT8 ? rtp_llm::TYPE_INT8 : rtp_llm::TYPE_FP32;
+        dtype = model_config.attn_config.kv_cache_dtype == KvCacheDataType::INT8 ? rtp_llm::TYPE_INT8 : rtp_llm::TYPE_FP32;
     }
-    auto layer_num = param.num_layers_;
+    auto layer_num = model_config.num_layers;
     if (is_mtp) {
         layer_num = 1;
     }
 
-    if (param.use_mla_ && param.mla_ops_type_ != rtp_llm::MlaOpsType::MHA) {
-        return CacheConfig(MlaCacheParam{(uint)param.num_layers_,
+    if (model_config.attn_config.use_mla && model_config.mla_ops_type != rtp_llm::MlaOpsType::MHA) {
+        return CacheConfig(MlaCacheParam{(uint)model_config.num_layers,
                                          (uint)0,
-                                         (uint)(param.kv_lora_rank_ + param.rope_head_dim_),
+                                         (uint)(model_config.attn_config.kv_lora_rank + model_config.attn_config.rope_head_dim),
                                          (uint)0,
-                                         (uint)param.seq_size_per_block_,
+                                         (uint)model_config.attn_config.tokens_per_block,
                                          dtype});
     }
 
     return CacheConfig(KVCacheParam{(uint)layer_num,
                                     (uint)0,
                                     (uint)local_head_num_kv,
-                                    (uint)param.size_per_head_,
-                                    (uint)param.seq_size_per_block_,
+                                    (uint)model_config.attn_config.size_per_head,
+                                    (uint)model_config.attn_config.tokens_per_block,
                                     dtype});
 }
 
-size_t CacheConfigCreator::getDefaultRuntimeMemorySize(const rtp_llm::GptInitParameter& params) {
-    auto reserve_runtime_mem_bytes = params.reserve_runtime_mem_mb_ * 1024 * 1024;
-    RTP_LLM_LOG_INFO("GptInitParameter has reserve_runtime_mem_mb_=%ld", params.reserve_runtime_mem_mb_);
+size_t CacheConfigCreator::getDefaultRuntimeMemorySize(const RuntimeConfig& runtime_config,
+                                                       const ParallelismConfig& parallelism_config,
+                                                       const ModelConfig& model_config,
+                                                       const SpeculativeExecutionConfig* sp_config) {
+    auto reserve_runtime_mem_bytes = runtime_config.reserve_runtime_mem_mb * 1024 * 1024;
+    RTP_LLM_LOG_INFO("RuntimeConfig has reserve_runtime_mem_mb=%ld", runtime_config.reserve_runtime_mem_mb);
 
-    const auto minimal_runtime_bytes = 256L * 1024 * 1024 * std::max(4, 8 / (int)params.tp_size_);
+    const auto minimal_runtime_bytes = 256L * 1024 * 1024 * std::max(4, 8 / (int)parallelism_config.tp_size);
     if (reserve_runtime_mem_bytes < minimal_runtime_bytes) {
         RTP_LLM_LOG_INFO("tp_size %d needs at least %d MiB memory for runtime by default, "
                          "but only %ld MiB reserved memory set by config. adjust to minimal value.",
-                         params.tp_size_,
+                         parallelism_config.tp_size,
                          minimal_runtime_bytes / 1024 / 1024,
                          reserve_runtime_mem_bytes / 1024 / 1024);
         reserve_runtime_mem_bytes = minimal_runtime_bytes;
     }
 
-    if (params.is_multimodal_) {
+    if (model_config.mm_model_config.is_multimodal) {
         const auto minimal_runtime_required = 2L * 1024 * 1024 * 1024;  // 2 GiB
         if (reserve_runtime_mem_bytes < minimal_runtime_required) {
             reserve_runtime_mem_bytes = minimal_runtime_required;
@@ -59,7 +64,7 @@ size_t CacheConfigCreator::getDefaultRuntimeMemorySize(const rtp_llm::GptInitPar
         }
     }
 
-    if (params.enable_speculative_decoding_) {
+    if (sp_config && sp_config->sp_type != "none" && !sp_config->sp_type.empty()) {
         const auto minimal_runtime_required = 2L * 1024 * 1024 * 1024;  // 2 GiB
         if (reserve_runtime_mem_bytes < minimal_runtime_required) {
             reserve_runtime_mem_bytes = minimal_runtime_required;
@@ -73,15 +78,18 @@ size_t CacheConfigCreator::getDefaultRuntimeMemorySize(const rtp_llm::GptInitPar
     return reserve_runtime_mem_bytes;
 }
 
-size_t CacheConfigCreator::getKVCacheMemorySize(const rtp_llm::GptInitParameter&   params,
+size_t CacheConfigCreator::getKVCacheMemorySize(const RuntimeConfig& runtime_config,
+                                                const KVCacheConfig& kv_cache_config,
+                                                const ModelConfig& model_config,
+                                                const ParallelismConfig& parallelism_config,
                                                 const std::optional<WarmUpResult>& warm_up_result) {
     const auto device                       = rtp_llm::DeviceFactory::getDefaultDevice();
     size_t     device_reserved_memory_bytes = device->getDeviceStatus().device_memory_status.preserved_bytes;
     size_t     runtime_required_bytes       = 0;
 
-    if (params.kv_cache_mem_mb_ > 0) {
-        RTP_LLM_LOG_INFO("GptInitParameter explicitly specified kv cache memory size %ld MiB", params.kv_cache_mem_mb_);
-        return params.kv_cache_mem_mb_ * 1024 * 1024;
+    if (kv_cache_config.kv_cache_mem_mb > 0) {
+        RTP_LLM_LOG_INFO("KVCacheConfig explicitly specified kv cache memory size %ld MiB", kv_cache_config.kv_cache_mem_mb);
+        return kv_cache_config.kv_cache_mem_mb * 1024 * 1024;
     }
 
     if (warm_up_result) {
@@ -94,7 +102,7 @@ size_t CacheConfigCreator::getKVCacheMemorySize(const rtp_llm::GptInitParameter&
                 std::min(device_reserved_memory_bytes, warm_up_result->device_reserved_bytes);
         }
 
-        size_t env_runtime_required_bytes = getDefaultRuntimeMemorySize(params);
+        size_t env_runtime_required_bytes = getDefaultRuntimeMemorySize(runtime_config, parallelism_config, model_config, nullptr);
         runtime_required_bytes            = std::max(env_runtime_required_bytes, warm_up_result->max_used_memory);
 
         RTP_LLM_LOG_INFO(
@@ -104,13 +112,13 @@ size_t CacheConfigCreator::getKVCacheMemorySize(const rtp_llm::GptInitParameter&
             env_runtime_required_bytes / 1024 / 1024,
             runtime_required_bytes / 1024 / 1024);
     } else {
-        runtime_required_bytes = getDefaultRuntimeMemorySize(params);
+        runtime_required_bytes = getDefaultRuntimeMemorySize(runtime_config, parallelism_config, model_config, nullptr);
         RTP_LLM_LOG_INFO("warm up result not available, use default runtime memory size %ld MiB",
                          runtime_required_bytes / 1024 / 1024);
     }
 
     size_t sample_need_mem =
-        (size_t)params.max_generate_batch_size_ * params.vocab_size_ * 4 * 8;  // just estimated value
+        (size_t)runtime_config.max_generate_batch_size * model_config.vocab_size * 4 * 8;  // just estimated value
     RTP_LLM_LOG_INFO("sampler needs %ld MiB memory, model runtime needs %ld MiB memory, take max value.",
                      sample_need_mem / 1024 / 1024,
                      runtime_required_bytes / 1024 / 1024);
@@ -126,16 +134,19 @@ size_t CacheConfigCreator::getKVCacheMemorySize(const rtp_llm::GptInitParameter&
     return kv_cache_mem_size;
 }
 
-CacheConfig CacheConfigCreator::createConfig(const rtp_llm::GptInitParameter&   param,
+CacheConfig CacheConfigCreator::createConfig(const ModelConfig& model_config,
+                                             const ParallelismConfig& parallelism_config,
+                                             const RuntimeConfig& runtime_config,
+                                             const KVCacheConfig& kv_cache_config,
                                              const std::optional<WarmUpResult>& warm_up_result) {
-    CacheConfig config     = CacheConfigCreator::createBasicConfig(param);
+    CacheConfig config     = CacheConfigCreator::createBasicConfig(model_config, parallelism_config);
     uint32_t    block_nums = 0;
 
-    if (param.block_nums_ > 0) {
-        RTP_LLM_LOG_INFO("GptInitParameter explicitly specified kv cache block num %d", param.block_nums_);
-        block_nums = param.block_nums_;
+    if (kv_cache_config.test_block_num > 0) {
+        RTP_LLM_LOG_INFO("KVCacheConfig explicitly specified kv cache block num %d", kv_cache_config.test_block_num);
+        block_nums = kv_cache_config.test_block_num;
     } else {
-        const auto kv_cache_mem_size = getKVCacheMemorySize(param, warm_up_result);
+        const auto kv_cache_mem_size = getKVCacheMemorySize(runtime_config, kv_cache_config, model_config, parallelism_config, warm_up_result);
         block_nums                   = kv_cache_mem_size / config.block_size;
     }
     RTP_LLM_CHECK_WITH_INFO(block_nums > 0,
@@ -146,32 +157,39 @@ CacheConfig CacheConfigCreator::createConfig(const rtp_llm::GptInitParameter&
     const auto kv_cache_seq_len = block_nums * config.seq_size_per_block;
     config.block_nums           = block_nums;
     RTP_LLM_LOG_INFO("kv cache block nums is %u, allows storing %ld tokens", block_nums, kv_cache_seq_len);
-    if (kv_cache_seq_len < param.max_seq_len_) {
+    if (kv_cache_seq_len < model_config.max_seq_len) {
         RTP_LLM_LOG_WARNING("kv cache block nums %u can only store %ld tokens, less than max_seq_len %ld, "
                             "this is dangerous, consider decrease max_seq_len",
                             block_nums,
                             kv_cache_seq_len,
-                            param.max_seq_len_);
+                            model_config.max_seq_len);
     }
     return config;
 }
 
 std::tuple<CacheConfig, CacheConfig>
-CacheConfigCreator::createSpConfig(const rtp_llm::GptInitParameter&   score_param,
-                                   const rtp_llm::GptInitParameter&   propose_param,
+CacheConfigCreator::createSpConfig(const ModelConfig& score_model_config,
+                                   const ParallelismConfig& score_parallelism_config,
+                                   const RuntimeConfig& score_runtime_config,
+                                   const KVCacheConfig& score_kv_cache_config,
+                                   const SpeculativeExecutionConfig& sp_config,
+                                   const ModelConfig& propose_model_config,
+                                   const ParallelismConfig& propose_parallelism_config,
+                                   const RuntimeConfig& propose_runtime_config,
+                                   const KVCacheConfig& propose_kv_cache_config,
                                    const std::optional<WarmUpResult>& warm_up_result,
-                                   bool                               is_mtp   = false,
-                                   bool                               is_eagle = false) {
-    CacheConfig score_config = CacheConfigCreator::createBasicConfig(score_param);
+                                   bool is_mtp,
+                                   bool is_eagle) {
+    CacheConfig score_config = CacheConfigCreator::createBasicConfig(score_model_config, score_parallelism_config);
 
-    CacheConfig propose_config = CacheConfigCreator::createBasicConfig(propose_param, is_mtp);
+    CacheConfig propose_config = CacheConfigCreator::createBasicConfig(propose_model_config, propose_parallelism_config, is_mtp);
     size_t      block_nums     = 0;
-    if (score_param.block_nums_ > 0) {
-        block_nums = score_param.block_nums_;
+    if (score_kv_cache_config.test_block_num > 0) {
+        block_nums = score_kv_cache_config.test_block_num;
     } else {
-        const auto kv_cache_mem_size = CacheConfigCreator::getKVCacheMemorySize(score_param, warm_up_result);
+        const auto kv_cache_mem_size = CacheConfigCreator::getKVCacheMemorySize(score_runtime_config, score_kv_cache_config, score_model_config, score_parallelism_config, warm_up_result);
         if (is_mtp) {
-            auto cache_num = propose_param.gen_num_per_circle_;
+            auto cache_num = sp_config.gen_num_per_cycle;
             if (is_eagle) {
                 cache_num = 1;
             }
diff --git a/rtp_llm/cpp/cache/CacheConfigCreator.h b/rtp_llm/cpp/cache/CacheConfigCreator.h
index bdf76336d..5994f4cb3 100644
--- a/rtp_llm/cpp/cache/CacheConfigCreator.h
+++ b/rtp_llm/cpp/cache/CacheConfigCreator.h
@@ -5,24 +5,42 @@
 #include "absl/status/statusor.h"
 #include "rtp_llm/cpp/cache/CacheConfig.h"
 #include "rtp_llm/cpp/cache/WarmUpResult.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 namespace rtp_llm {
 
 class CacheConfigCreator {
 public:
-    static CacheConfig createBasicConfig(const rtp_llm::GptInitParameter& param, bool is_mtp = false);
-    static CacheConfig createConfig(const rtp_llm::GptInitParameter&   param,
+    static CacheConfig createBasicConfig(const ModelConfig& model_config,
+                                         const ParallelismConfig& parallelism_config,
+                                         bool is_mtp = false);
+    static CacheConfig createConfig(const ModelConfig& model_config,
+                                    const ParallelismConfig& parallelism_config,
+                                    const RuntimeConfig& runtime_config,
+                                    const KVCacheConfig& kv_cache_config,
                                     const std::optional<WarmUpResult>& warm_up_result = std::nullopt);
-    static std::tuple<CacheConfig, CacheConfig> createSpConfig(const rtp_llm::GptInitParameter&   score_param,
-                                                               const rtp_llm::GptInitParameter&   propose_param,
+    static std::tuple<CacheConfig, CacheConfig> createSpConfig(const ModelConfig& score_model_config,
+                                                               const ParallelismConfig& score_parallelism_config,
+                                                               const RuntimeConfig& score_runtime_config,
+                                                               const KVCacheConfig& score_kv_cache_config,
+                                                               const SpeculativeExecutionConfig& sp_config,
+                                                               const ModelConfig& propose_model_config,
+                                                               const ParallelismConfig& propose_parallelism_config,
+                                                               const RuntimeConfig& propose_runtime_config,
+                                                               const KVCacheConfig& propose_kv_cache_config,
                                                                const std::optional<WarmUpResult>& warm_up_result,
-                                                               bool                               is_mtp,
-                                                               bool                               is_eagle);
+                                                               bool is_mtp,
+                                                               bool is_eagle);
 
 private:
-    static size_t getDefaultRuntimeMemorySize(const rtp_llm::GptInitParameter& param);
-    static size_t getKVCacheMemorySize(const rtp_llm::GptInitParameter&   param,
+    static size_t getDefaultRuntimeMemorySize(const RuntimeConfig& runtime_config,
+                                               const ParallelismConfig& parallelism_config,
+                                               const ModelConfig& model_config,
+                                               const SpeculativeExecutionConfig* sp_config = nullptr);
+    static size_t getKVCacheMemorySize(const RuntimeConfig& runtime_config,
+                                       const KVCacheConfig& kv_cache_config,
+                                       const ModelConfig& model_config,
+                                       const ParallelismConfig& parallelism_config,
                                        const std::optional<WarmUpResult>& warm_up_result = std::nullopt);
 };
 
diff --git a/rtp_llm/cpp/cache/CacheManager.cc b/rtp_llm/cpp/cache/CacheManager.cc
index 5b1b6b0cd..7d8982681 100644
--- a/rtp_llm/cpp/cache/CacheManager.cc
+++ b/rtp_llm/cpp/cache/CacheManager.cc
@@ -15,10 +15,6 @@
 #include "rtp_llm/cpp/core/Buffer.h"
 #include "rtp_llm/cpp/core/Types.h"
 #include "rtp_llm/cpp/core/torch_utils/BufferTorchUtils.h"
-#ifdef ENABLE_FP8
-#include <cuda_fp8.h>
-#endif
-
 using namespace std;
 
 namespace rtp_llm {
@@ -27,13 +23,17 @@ CacheManager::CacheManager(const CacheConfig&                 config,
                            rtp_llm::DeviceBase*               device,
                            bool                               warmup,
                            const kmonitor::MetricsReporterPtr metrics_reporter,
-                           const GptInitParameter&            params):
+                           const KVCacheConfig&                kv_cache_config,
+                           const ParallelismConfig&            parallelism_config,
+                           const RuntimeConfig&                runtime_config):
     config_(config),
     seq_size_per_block_(config.seq_size_per_block),
     block_cache_(config.seq_size_per_block),
     device_(device),
     metrics_reporter_(metrics_reporter),
-    params_(params) {
+    kv_cache_config_(kv_cache_config),
+    parallelism_config_(parallelism_config),
+    runtime_config_(runtime_config) {
 
     if (warmup) {
         config_.block_nums = 1;
@@ -51,18 +51,18 @@ CacheManager::CacheManager(const CacheConfig&                 config,
     if (metrics_reporter_) {
         metrics_reporter_thread_ = std::thread(&CacheManager::reportMetricsLoop, this);
     }
-    if (params_.kv_cache_config.enable_3fs) {
+    if (kv_cache_config_.enable_3fs) {
         enable_dist_kvcache_ = initDistKvCache();
         if (!enable_dist_kvcache_) {
             RTP_LLM_FAIL("dist kv cache init failed");
         }
     }
 
-    if (params_.kv_cache_config.memory_block_cache_size_mb > 0) {
+    if (kv_cache_config_.memory_block_cache_size_mb > 0) {
         int64_t block_nums =
-            (int64_t)params_.kv_cache_config.memory_block_cache_size_mb * 1024 * 1024 / config_.block_size;
+            (int64_t)kv_cache_config_.memory_block_cache_size_mb * 1024 * 1024 / config_.block_size;
         RTP_LLM_LOG_INFO("init memory block cache, size: %d MB, block nums: %ld",
-                         params_.kv_cache_config.memory_block_cache_size_mb,
+                         kv_cache_config_.memory_block_cache_size_mb,
                          block_nums);
 
         auto memory_block_cache_config       = config_;
@@ -70,13 +70,14 @@ CacheManager::CacheManager(const CacheConfig&                 config,
         memory_block_cache_config.refresh();
 
         memory_block_cache_ = std::make_shared<MemoryBlockCache>(
-            memory_block_cache_config, device_, allocator_.get(), params_, metrics_reporter_);
+            memory_block_cache_config, device_, allocator_.get(), parallelism_config_, kv_cache_config_, runtime_config_, metrics_reporter_);
         if (!memory_block_cache_->init()) {
             RTP_LLM_FAIL("memory block cache init failed");
         }
     }
 }
 
+
 void CacheManager::regUserMr(size_t model_id) {
     allocator_->regUserMr(model_id);
 }
@@ -148,6 +149,10 @@ const CacheConfig& CacheManager::cacheConfig() const {
     return config_;
 }
 
+const KVCacheConfig& CacheManager::kvCacheConfig() const {
+    return kv_cache_config_;
+}
+
 const BlockCache& CacheManager::blockCache() const {
     return block_cache_;
 }
@@ -458,19 +463,19 @@ KVCacheAllocator::BlockAddrInfo CacheManager::convertIndexToAddr(int block_index
 
 bool CacheManager::initDistKvCache() {
     DistKvCacheInitParams init_params;
-    init_params.match_timeout_ms         = params_.kv_cache_config.match_timeout_ms;
-    init_params.rpc_get_cache_timeout_ms = params_.kv_cache_config.rpc_get_cache_timeout_ms;
-    init_params.rpc_put_cache_timeout_ms = params_.kv_cache_config.rpc_put_cache_timeout_ms;
-    init_params.max_block_size_per_item  = params_.kv_cache_config.max_block_size_per_item;
-    if (params_.kv_cache_config.enable_3fs) {
+    init_params.match_timeout_ms         = kv_cache_config_.match_timeout_ms;
+    init_params.rpc_get_cache_timeout_ms = kv_cache_config_.rpc_get_cache_timeout_ms;
+    init_params.rpc_put_cache_timeout_ms = kv_cache_config_.rpc_put_cache_timeout_ms;
+    init_params.max_block_size_per_item  = kv_cache_config_.max_block_size_per_item;
+    if (kv_cache_config_.enable_3fs) {
         DistStorage3FSInitParams init_params_3fs;
-        init_params_3fs.read_iov_size                      = params_.kv_cache_config.threefs_read_iov_size;
-        init_params_3fs.write_iov_size                     = params_.kv_cache_config.threefs_write_iov_size;
-        init_params_3fs.read_timeout_ms                    = params_.kv_cache_config.threefs_read_timeout_ms;
-        init_params_3fs.write_timeout_ms                   = params_.kv_cache_config.threefs_write_timeout_ms;
+        init_params_3fs.read_iov_size                      = kv_cache_config_.threefs_read_iov_size;
+        init_params_3fs.write_iov_size                     = kv_cache_config_.threefs_write_iov_size;
+        init_params_3fs.read_timeout_ms                    = kv_cache_config_.threefs_read_timeout_ms;
+        init_params_3fs.write_timeout_ms                   = kv_cache_config_.threefs_write_timeout_ms;
         init_params.storage_manager_params.init_params_3fs = init_params_3fs;
     }
-    auto dist_kvcache = std::make_shared<DistKvCache>(this, params_, metrics_reporter_);
+    auto dist_kvcache = std::make_shared<DistKvCache>(this, parallelism_config_, runtime_config_, metrics_reporter_);
     if (!dist_kvcache->init(init_params)) {
         RTP_LLM_LOG_WARNING("dist kvcache init failed!!!");
         return false;
diff --git a/rtp_llm/cpp/cache/CacheManager.h b/rtp_llm/cpp/cache/CacheManager.h
index f162902a1..548f9dc9c 100644
--- a/rtp_llm/cpp/cache/CacheManager.h
+++ b/rtp_llm/cpp/cache/CacheManager.h
@@ -98,10 +98,13 @@ public:
                  rtp_llm::DeviceBase*               device,
                  bool                               warmup           = false,
                  const kmonitor::MetricsReporterPtr metrics_reporter = nullptr,
-                 const GptInitParameter&            params           = GptInitParameter{});
+                 const KVCacheConfig&               kv_cache_config  = KVCacheConfig{},
+                 const ParallelismConfig&           parallelism_config = ParallelismConfig{},
+                 const RuntimeConfig&                runtime_config = RuntimeConfig{});
     ~CacheManager();
 
     const CacheConfig&                     cacheConfig() const;
+    const KVCacheConfig&                   kvCacheConfig() const;
     size_t                                 freeBlockNums() const;
     size_t                                 availableBlockNums() const;
     KVCacheInfo                            getKVCacheInfo(int64_t latest_version, bool need_cache_keys) const;
@@ -201,7 +204,9 @@ protected:
 
     std::mutex mutex_;
 
-    const GptInitParameter             params_;
+    const KVCacheConfig                kv_cache_config_;
+    const ParallelismConfig            parallelism_config_;
+    const RuntimeConfig                runtime_config_;
     std::map<std::string, std::string> lora_info_map_;
     bool                               enable_dist_kvcache_{false};
     std::shared_ptr<DistKvCache>       dist_kvcache_;
diff --git a/rtp_llm/cpp/cache/DistKvCache.cc b/rtp_llm/cpp/cache/DistKvCache.cc
index 1db6b0117..6859d2cff 100644
--- a/rtp_llm/cpp/cache/DistKvCache.cc
+++ b/rtp_llm/cpp/cache/DistKvCache.cc
@@ -17,9 +17,10 @@ inline std::size_t hashString(const std::string& str) {
 }
 
 DistKvCache::DistKvCache(CacheManager*                       cache_manager,
-                         const GptInitParameter&             gpt_init_params,
+                         const ParallelismConfig&            parallelism_config,
+                         const RuntimeConfig&                 runtime_config,
                          const kmonitor::MetricsReporterPtr& metrics_reporter):
-    cache_manager_(cache_manager), gpt_init_params_(gpt_init_params), metrics_reporter_(metrics_reporter) {}
+    cache_manager_(cache_manager), parallelism_config_(parallelism_config), runtime_config_(runtime_config), metrics_reporter_(metrics_reporter) {}
 
 DistKvCache::~DistKvCache() {
     RTP_LLM_LOG_INFO("DistKvCache destructor");
@@ -52,7 +53,7 @@ bool DistKvCache::init(const DistKvCacheInitParams& init_params) {
     }
     const auto& init_params_3fs = storage_params.init_params_3fs.value();
     planner_                    = std::make_unique<DefaultDistKvCachePlanner>(
-        cache_manager_, gpt_init_params_, init_params_3fs, metrics_reporter_);
+        cache_manager_, cache_manager_->kvCacheConfig(), init_params_3fs, metrics_reporter_);
 
     storage_params.lookup_timeout_ms = init_params.match_timeout_ms;
     storage_params.get_timeout_ms    = init_params.rpc_get_cache_timeout_ms;
@@ -98,8 +99,8 @@ bool DistKvCache::initDefaultMetas() {
     default_metas_["SEQ_SIZE_PER_BLOCK"] = std::to_string(cache_config.seq_size_per_block);
     default_metas_["DTYPE"]              = std::to_string(static_cast<int>(cache_config.dtype));
     default_metas_["USE_MLA"]            = std::to_string(static_cast<int>(cache_config.use_mla));
-    default_metas_["TP_SIZE"]            = std::to_string(gpt_init_params_.tp_size_);
-    default_metas_["TP_RANK"]            = std::to_string(gpt_init_params_.tp_rank_);
+    default_metas_["TP_SIZE"]            = std::to_string(parallelism_config_.tp_size);
+    default_metas_["TP_RANK"]            = std::to_string(parallelism_config_.tp_rank);
     default_metas_["LAYOUT_VERSION"]     = "v2";
 
     auto biz_name = autil::EnvUtil::getEnv("BIZ_NAME", std::string(""));
@@ -158,7 +159,7 @@ int32_t DistKvCache::matchForAllRank(const std::vector<int64_t>&        cache_ke
 
         int32_t match_len = static_cast<int32_t>(cache_keys.size());
         auto    metas     = extra_metas;
-        for (int i = 0; i < shared_this->gpt_init_params_.tp_size_; i++) {
+        for (int i = 0; i < shared_this->parallelism_config_.tp_size; i++) {
             metas["TP_RANK"] = std::to_string(i);
             auto ret         = shared_this->match(cache_keys, ignore_block_num, request_id, metas, stop);
             if (ret < match_len) {
@@ -457,7 +458,7 @@ bool DistKvCache::syncCallAllRank(const std::vector<int64_t>&        cache_keys,
                                   int64_t                            request_id,
                                   std::map<std::string, std::string> extra_metas,
                                   DistKvCache::OpType                op_type) const {
-    const auto& grpc_workers = gpt_init_params_.worker_grpc_addrs_;
+    const auto& grpc_workers = runtime_config_.worker_grpc_addrs;
     if (grpc_workers.empty()) {
         RTP_LLM_LOG_WARNING("rpc get cache failed, grpc workers is empty, request: %ld", request_id);
         return false;
diff --git a/rtp_llm/cpp/cache/DistKvCache.h b/rtp_llm/cpp/cache/DistKvCache.h
index 4c34be50e..9751373c3 100644
--- a/rtp_llm/cpp/cache/DistKvCache.h
+++ b/rtp_llm/cpp/cache/DistKvCache.h
@@ -34,7 +34,8 @@ struct DistKvCacheInitParams {
 class DistKvCache: public std::enable_shared_from_this<DistKvCache> {
 public:
     DistKvCache(CacheManager*                       cache_manager,
-                const GptInitParameter&             gpt_init_params,
+                const ParallelismConfig&             parallelism_config,
+                const RuntimeConfig&                runtime_config,
                 const kmonitor::MetricsReporterPtr& metrics_reporter = nullptr);
     virtual ~DistKvCache();
 
@@ -93,7 +94,8 @@ private:
 
 private:
     CacheManager*                cache_manager_{nullptr};
-    const GptInitParameter       gpt_init_params_;
+    const ParallelismConfig      parallelism_config_;
+    const RuntimeConfig          runtime_config_;
     kmonitor::MetricsReporterPtr metrics_reporter_;
 
     std::map<std::string, std::string> default_metas_;
diff --git a/rtp_llm/cpp/cache/DistKvCachePlanner.cc b/rtp_llm/cpp/cache/DistKvCachePlanner.cc
index b7694a523..b6b629a6d 100644
--- a/rtp_llm/cpp/cache/DistKvCachePlanner.cc
+++ b/rtp_llm/cpp/cache/DistKvCachePlanner.cc
@@ -7,11 +7,11 @@
 namespace rtp_llm {
 
 DefaultDistKvCachePlanner::DefaultDistKvCachePlanner(CacheManager*                       cache_manager,
-                                                     const GptInitParameter&             gpt_init_params,
+                                                     const KVCacheConfig&               kv_cache_config,
                                                      const DistStorage3FSInitParams&     init_params_3fs,
                                                      const kmonitor::MetricsReporterPtr& metrics_reporter):
     cache_manager_(cache_manager),
-    gpt_init_params_(gpt_init_params),
+    kv_cache_config_(kv_cache_config),
     init_params_3fs_(init_params_3fs),
     metrics_reporter_(metrics_reporter) {}
 
@@ -79,7 +79,7 @@ std::vector<DistStorage::Item> DefaultDistKvCachePlanner::layout(const std::vect
         item_block_count++;
         item_keys.push_back(cache_keys[i]);
 
-        if (item_block_count >= gpt_init_params_.kv_cache_config.max_block_size_per_item && item_keys.size() > 0) {
+        if (item_block_count >= kv_cache_config_.max_block_size_per_item && item_keys.size() > 0) {
             item->key = std::to_string(item_keys.front()) + "_" + std::to_string(item_keys.back());
             item->metas["ITEM_KEY"] = item->key;
             RTP_LLM_LOG_DEBUG("push item: %s", item->key.c_str());
diff --git a/rtp_llm/cpp/cache/DistKvCachePlanner.h b/rtp_llm/cpp/cache/DistKvCachePlanner.h
index 8ce99ed2c..5b0c60289 100644
--- a/rtp_llm/cpp/cache/DistKvCachePlanner.h
+++ b/rtp_llm/cpp/cache/DistKvCachePlanner.h
@@ -2,7 +2,7 @@
 
 #include "kmonitor/client/MetricsReporter.h"
 #include "rtp_llm/cpp/cache/DistStorage.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 
 namespace rtp_llm {
 
@@ -25,7 +25,7 @@ class CacheManager;
 class DefaultDistKvCachePlanner: public DistKvCachePlanner {
 public:
     DefaultDistKvCachePlanner(CacheManager*                       cache_manager,
-                              const GptInitParameter&             gpt_init_params,
+                              const KVCacheConfig&               kv_cache_config,
                               const DistStorage3FSInitParams&     init_params_3fs,
                               const kmonitor::MetricsReporterPtr& metrics_reporter);
 
@@ -45,7 +45,7 @@ private:
 
 private:
     CacheManager*                  cache_manager_ = nullptr;
-    const GptInitParameter         gpt_init_params_;
+    const KVCacheConfig           kv_cache_config_;
     const DistStorage3FSInitParams init_params_3fs_;
     kmonitor::MetricsReporterPtr   metrics_reporter_;
 };
diff --git a/rtp_llm/cpp/cache/KVCacheAllocator.h b/rtp_llm/cpp/cache/KVCacheAllocator.h
index 09b5266ac..e0106c156 100644
--- a/rtp_llm/cpp/cache/KVCacheAllocator.h
+++ b/rtp_llm/cpp/cache/KVCacheAllocator.h
@@ -121,8 +121,6 @@ private:
     int64_t mr_cost_time_ms_ = 0;
 
     mutable std::mutex mutex_;
-
-    const GptInitParameter params_;
 };
 
 typedef std::shared_ptr<KVCacheAllocator> KVCacheAllocatorPtr;
diff --git a/rtp_llm/cpp/cache/MemoryBlockCache.cc b/rtp_llm/cpp/cache/MemoryBlockCache.cc
index 62cc10cd9..49fe57900 100644
--- a/rtp_llm/cpp/cache/MemoryBlockCache.cc
+++ b/rtp_llm/cpp/cache/MemoryBlockCache.cc
@@ -20,12 +20,16 @@ namespace rtp_llm {
 MemoryBlockCache::MemoryBlockCache(const CacheConfig&                  config,
                                    rtp_llm::DeviceBase*                device,
                                    KVCacheAllocator*                   gpu_kvcache_allocator,
-                                   const GptInitParameter&             gpt_init_params,
+                                   const ParallelismConfig&            parallelism_config,
+                                   const KVCacheConfig&                kv_cache_config,
+                                   const RuntimeConfig&                runtime_config,
                                    const kmonitor::MetricsReporterPtr& metrics_reporter):
     config_(config),
     device_(device),
     gpu_kvcache_allocator_(gpu_kvcache_allocator),
-    gpt_init_params_(gpt_init_params),
+    parallelism_config_(parallelism_config),
+    kv_cache_config_(kv_cache_config),
+    runtime_config_(runtime_config),
     metrics_reporter_(metrics_reporter) {}
 
 MemoryBlockCache::~MemoryBlockCache() {
@@ -52,7 +56,7 @@ bool MemoryBlockCache::init() {
     block_lru_cache_ = std::make_unique<BlockLRUCache>(config_.block_nums, config_.seq_size_per_block);
 
     // 初始化RPC连接池
-    if (gpt_init_params_.tp_size_ > 1) {
+    if (parallelism_config_.tp_size > 1) {
         rpc_pool_ = std::make_shared<RPCPool>();
     }
 
@@ -60,8 +64,8 @@ bool MemoryBlockCache::init() {
         metrics_reporter_thread_ = std::thread(&MemoryBlockCache::reportMetricsLoop, this);
     }
     RTP_LLM_LOG_INFO("memory block cache init success, memory size %d mb, sync timeout %d ms, config: %s",
-                     gpt_init_params_.kv_cache_config.memory_block_cache_size_mb,
-                     gpt_init_params_.kv_cache_config.memory_block_cache_sync_timeout_ms,
+                     kv_cache_config_.memory_block_cache_size_mb,
+                     kv_cache_config_.memory_block_cache_sync_timeout_ms,
                      config_.debugString().c_str());
     return true;
 }
@@ -267,7 +271,7 @@ bool MemoryBlockCache::copyKVData(const std::vector<int>& memory_block_indices,
     device_->noBlockCopy({dst_buffers, src_buffers});
 
     recordCopyMetrics(true, timer.done_us(), direction);
-    if (timer.done_us() / 1000 > gpt_init_params_.kv_cache_config.memory_block_cache_sync_timeout_ms) {
+    if (timer.done_us() / 1000 > kv_cache_config_.memory_block_cache_sync_timeout_ms) {
         RTP_LLM_LOG_INFO(
             "memory block cache done, %s copy timeout, request: %ld, block size: %zu, copy buffer count: %zu, latency: %ld ms",
             direction == CopyDirection::FROM_GPU ? "from GPU" : "to GPU",
@@ -313,7 +317,7 @@ bool MemoryBlockCache::copyKVDataForAllRank(const std::vector<int>& memory_block
                                             const std::vector<int>& gpu_block_indices,
                                             CopyDirection           direction,
                                             int64_t                 request_id) {
-    if (gpt_init_params_.tp_size_ <= 1) {
+    if (parallelism_config_.tp_size <= 1) {
         // 单TP场景，直接调用单TP版本
         auto ret = copyKVData(memory_block_indices, gpu_block_indices, direction, request_id);
         return ret;
@@ -331,7 +335,7 @@ bool MemoryBlockCache::copyKVDataForAllRank(const std::vector<int>& memory_block
     bool success = syncRpcCallForAllRank(gpu_block_indices,
                                          memory_block_indices,
                                          op_type,
-                                         gpt_init_params_.kv_cache_config.memory_block_cache_sync_timeout_ms,
+                                         kv_cache_config_.memory_block_cache_sync_timeout_ms,
                                          request_id);
     if (success) {
         return true;
@@ -361,7 +365,7 @@ bool MemoryBlockCache::syncRpcCallForAllRank(const std::vector<int>& gpu_block_i
                                              int                     timeout_ms,
                                              int64_t                 request_id) {
     // 多TP场景，使用gRPC同步所有rank
-    const auto& grpc_workers = gpt_init_params_.worker_grpc_addrs_;
+    const auto& grpc_workers = runtime_config_.worker_grpc_addrs;
     if (grpc_workers.empty() || !rpc_pool_) {
         RTP_LLM_LOG_WARNING("sync RPC call for all rank failed, grpc workers empty or rpc pool/thread pool null");
         return false;
diff --git a/rtp_llm/cpp/cache/MemoryBlockCache.h b/rtp_llm/cpp/cache/MemoryBlockCache.h
index f13b3d498..15f28a735 100644
--- a/rtp_llm/cpp/cache/MemoryBlockCache.h
+++ b/rtp_llm/cpp/cache/MemoryBlockCache.h
@@ -19,6 +19,7 @@
 #include "rtp_llm/cpp/model_rpc/RPCPool.h"
 #include "rtp_llm/cpp/core/Types.h"
 #include "rtp_llm/cpp/metrics/RtpLLMMetrics.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "autil/LockFreeThreadPool.h"
 
 namespace rtp_llm {
@@ -34,7 +35,9 @@ public:
     MemoryBlockCache(const CacheConfig&                  config,
                      rtp_llm::DeviceBase*                device,
                      KVCacheAllocator*                   gpu_kvcache_allocator,
-                     const GptInitParameter&             gpt_init_params,
+                     const ParallelismConfig&            parallelism_config,
+                     const KVCacheConfig&                kv_cache_config,
+                     const RuntimeConfig&                runtime_config,
                      const kmonitor::MetricsReporterPtr& metrics_reporter = nullptr);
     ~MemoryBlockCache();
 
@@ -108,7 +111,9 @@ private:
 
     // 多TP同步相关
     std::shared_ptr<RPCPool> rpc_pool_;
-    GptInitParameter         gpt_init_params_;
+    ParallelismConfig        parallelism_config_;
+    KVCacheConfig            kv_cache_config_;
+    RuntimeConfig            runtime_config_;
 
     // 指标相关
     bool                                       stop_ = false;
diff --git a/rtp_llm/cpp/cache/perf_test/KVCacheReadPerfTest.h b/rtp_llm/cpp/cache/perf_test/KVCacheReadPerfTest.h
index 4618668ed..f6237b58c 100644
--- a/rtp_llm/cpp/cache/perf_test/KVCacheReadPerfTest.h
+++ b/rtp_llm/cpp/cache/perf_test/KVCacheReadPerfTest.h
@@ -7,6 +7,7 @@
 #include "rtp_llm/cpp/cache/DistKvCache.h"
 #include "rtp_llm/cpp/cache/perf_test/KVCacheOptionBase.h"
 #include "rtp_llm/cpp/metrics/RtpLLMMetrics.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 extern std::atomic<bool> g_stop_flag;
 
@@ -73,9 +74,11 @@ public:
 
         auto             device   = createDevice();
         auto             reporter = createMetricsReporter();
-        GptInitParameter gpt_init_params;
-        gpt_init_params.model_name_ = "TestModel";
-        auto cache_manager = std::make_shared<CacheManager>(cache_config, device, false, reporter, gpt_init_params);
+        KVCacheConfig kv_cache_config;
+        ParallelismConfig parallelism_config;
+        RuntimeConfig runtime_config;
+        runtime_config.model_name = "TestModel";
+        auto cache_manager = std::make_shared<CacheManager>(cache_config, device, false, reporter, kv_cache_config, parallelism_config, runtime_config);
         const auto kvcache = cache_manager->kvCacheBuffer();
 
         DistStorage3FSInitParams storage_3fs_init_params;
@@ -83,7 +86,7 @@ public:
         DistKvCacheInitParams dist_kvcache_init_params;
         dist_kvcache_init_params.storage_manager_params.init_params_3fs = storage_3fs_init_params;
 
-        auto dist_kvcache = std::make_shared<DistKvCache>(cache_manager.get(), gpt_init_params, reporter);
+        auto dist_kvcache = std::make_shared<DistKvCache>(cache_manager.get(), parallelism_config, runtime_config, reporter);
         if (!dist_kvcache->init(dist_kvcache_init_params)) {
             RTP_LLM_LOG_ERROR("dist kvcache init failed");
             return;
@@ -207,7 +210,33 @@ private:
     }
 
     DeviceBase* createDevice() const {
-        DeviceFactory::initDevices(GptInitParameter());
+        ParallelismConfig parallelism_config;
+        ModelConfig model_config;
+        EPLBConfig eplb_config;
+        FMHAConfig fmha_config;
+        DeviceResourceConfig device_resource_config;
+        MoeConfig moe_config;
+        SpeculativeExecutionConfig sp_config;
+        MiscellaneousConfig misc_config;
+        ProfilingDebugLoggingConfig profiling_debug_logging_config;
+        HWKernelConfig hw_kernel_config;
+        ConcurrencyConfig concurrency_config;
+        FfnDisAggregateConfig ffn_disaggregate_config;
+        RuntimeConfig runtime_config;
+        DeviceFactory::initDevices(
+            parallelism_config,
+            model_config,
+            eplb_config,
+            fmha_config,
+            device_resource_config,
+            moe_config,
+            sp_config,
+            misc_config,
+            profiling_debug_logging_config,
+            hw_kernel_config,
+            concurrency_config,
+            ffn_disaggregate_config,
+            runtime_config);
         return DeviceFactory::getDefaultDevice();
     }
 
diff --git a/rtp_llm/cpp/cache/perf_test/KVCacheWritePerfTest.h b/rtp_llm/cpp/cache/perf_test/KVCacheWritePerfTest.h
index c412b8959..b0e88b270 100644
--- a/rtp_llm/cpp/cache/perf_test/KVCacheWritePerfTest.h
+++ b/rtp_llm/cpp/cache/perf_test/KVCacheWritePerfTest.h
@@ -7,6 +7,7 @@
 #include "rtp_llm/cpp/cache/DistKvCache.h"
 #include "rtp_llm/cpp/cache/perf_test/KVCacheOptionBase.h"
 #include "rtp_llm/cpp/metrics/RtpLLMMetrics.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 extern std::atomic<bool> g_stop_flag;
 
@@ -83,9 +84,11 @@ public:
 
         auto             device   = createDevice();
         auto             reporter = createMetricsReporter();
-        GptInitParameter gpt_init_params;
-        gpt_init_params.model_name_ = "TestModel";
-        auto cache_manager = std::make_shared<CacheManager>(cache_config, device, false, reporter, gpt_init_params);
+        KVCacheConfig kv_cache_config;
+        ParallelismConfig parallelism_config;
+        RuntimeConfig runtime_config;
+        runtime_config.model_name = "TestModel";
+        auto cache_manager = std::make_shared<CacheManager>(cache_config, device, false, reporter, kv_cache_config, parallelism_config, runtime_config);
 
         if (g_stop_flag.load()) {
             return;
@@ -105,7 +108,7 @@ public:
         DistKvCacheInitParams dist_kvcache_init_params;
         dist_kvcache_init_params.storage_manager_params.init_params_3fs = storage_3fs_init_params;
 
-        auto dist_kvcache = std::make_shared<DistKvCache>(cache_manager.get(), gpt_init_params, reporter);
+        auto dist_kvcache = std::make_shared<DistKvCache>(cache_manager.get(), parallelism_config, runtime_config, reporter);
         if (!dist_kvcache->init(dist_kvcache_init_params)) {
             RTP_LLM_LOG_ERROR("dist kvcache init failed");
             return;
@@ -193,7 +196,33 @@ private:
     }
 
     DeviceBase* createDevice() const {
-        DeviceFactory::initDevices(GptInitParameter());
+        ParallelismConfig parallelism_config;
+        ModelConfig model_config;
+        EPLBConfig eplb_config;
+        FMHAConfig fmha_config;
+        DeviceResourceConfig device_resource_config;
+        MoeConfig moe_config;
+        SpeculativeExecutionConfig sp_config;
+        MiscellaneousConfig misc_config;
+        ProfilingDebugLoggingConfig profiling_debug_logging_config;
+        HWKernelConfig hw_kernel_config;
+        ConcurrencyConfig concurrency_config;
+        FfnDisAggregateConfig ffn_disaggregate_config;
+        RuntimeConfig runtime_config;
+        DeviceFactory::initDevices(
+            parallelism_config,
+            model_config,
+            eplb_config,
+            fmha_config,
+            device_resource_config,
+            moe_config,
+            sp_config,
+            misc_config,
+            profiling_debug_logging_config,
+            hw_kernel_config,
+            concurrency_config,
+            ffn_disaggregate_config,
+            runtime_config);
         return DeviceFactory::getDefaultDevice();
     }
 
diff --git a/rtp_llm/cpp/cache/test/BUILD b/rtp_llm/cpp/cache/test/BUILD
index 1086f555e..e4c5a44df 100644
--- a/rtp_llm/cpp/cache/test/BUILD
+++ b/rtp_llm/cpp/cache/test/BUILD
@@ -8,7 +8,7 @@ test_copts = [
 test_deps = [
     "//rtp_llm/cpp/devices/testing:device_test_utils",
     "//rtp_llm/cpp/devices/cuda_impl:cuda_impl",
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
     "//rtp_llm/cpp/cache",
     "@com_google_googletest//:gtest",
     "@com_google_googletest//:gtest_main",
diff --git a/rtp_llm/cpp/cache/test/CacheConfigCreatorTest.cc b/rtp_llm/cpp/cache/test/CacheConfigCreatorTest.cc
index bd9c6b74c..ffcfb6ebf 100644
--- a/rtp_llm/cpp/cache/test/CacheConfigCreatorTest.cc
+++ b/rtp_llm/cpp/cache/test/CacheConfigCreatorTest.cc
@@ -5,7 +5,7 @@
 #include "rtp_llm/cpp/cache/CacheConfigCreator.h"
 #include "rtp_llm/cpp/core/Types.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 
 #include <chrono>
 #include <memory>
@@ -27,21 +27,25 @@ class CacheConfigCreatorTest: public DeviceTestBase {
 };
 
 TEST_F(CacheConfigCreatorTest, testGetKVCacheMemorySize) {
-    GptInitParameter param;
-    param.reserve_runtime_mem_mb_ = 2;
-    param.kv_cache_mem_mb_        = 10;
+    RuntimeConfig runtime_config;
+    runtime_config.reserve_runtime_mem_mb = 2;
+    KVCacheConfig kv_cache_config;
+    kv_cache_config.kv_cache_mem_mb = 10;
+    ModelConfig model_config;
+    ParallelismConfig parallelism_config;
+    MMModelConfig mm_model_config;
     CacheConfigCreator creator;
-    auto               result1 = creator.getKVCacheMemorySize(param);
+    auto               result1 = creator.getKVCacheMemorySize(runtime_config, kv_cache_config, model_config, parallelism_config, mm_model_config);
     ASSERT_EQ(10 * 1024 * 1024, result1);
 
-    param.kv_cache_mem_mb_ = 0;
-    auto result2           = creator.getKVCacheMemorySize(param);
+    kv_cache_config.kv_cache_mem_mb = 0;
+    auto result2           = creator.getKVCacheMemorySize(runtime_config, kv_cache_config, model_config, parallelism_config, mm_model_config);
     ASSERT_TRUE(result2 > 0);
 
-    param.reserve_runtime_mem_mb_ = 200000;
+    runtime_config.reserve_runtime_mem_mb = 200000;
     std::string exception         = "";
     try {
-        creator.getKVCacheMemorySize(param);
+        creator.getKVCacheMemorySize(runtime_config, kv_cache_config, model_config, parallelism_config, mm_model_config);
     } catch (const std::exception& e) {
         exception = e.what();
         printf("exception: %s", e.what());
@@ -50,26 +54,25 @@ TEST_F(CacheConfigCreatorTest, testGetKVCacheMemorySize) {
 }
 
 TEST_F(CacheConfigCreatorTest, testCreateConfig) {
-    GptInitParameter param;
-    param.block_nums_         = 200;
-    param.num_layers_         = 1;
-    param.head_num_kv_        = 4;
-    param.size_per_head_      = 128;
-    param.seq_size_per_block_ = 8;
+    ModelConfig model_config;
+    model_config.num_layers         = 1;
+    model_config.attn_config.kv_head_num = 4;
+    model_config.attn_config.size_per_head = 128;
+    model_config.attn_config.tokens_per_block = 8;
+    MMModelConfig mm_model_config;
+    ParallelismConfig parallelism_config;
+    RuntimeConfig runtime_config;
+    KVCacheConfig kv_cache_config;
+    kv_cache_config.kv_cache_mem_mb = 0; // Use default calculation
     CacheConfigCreator creator;
-    auto               result1 = creator.createConfig(param);
-    ASSERT_EQ(result1.block_nums, 200);
+    auto               result1 = creator.createConfig(model_config, mm_model_config, parallelism_config, runtime_config, kv_cache_config);
+    ASSERT_TRUE(result1.block_nums > 0);
     ASSERT_EQ(result1.local_head_num_kv, 4);
 
-    param.block_nums_ = 0;
-    auto result2      = creator.createConfig(param);
-    ASSERT_TRUE(result2.block_nums > 0);
-    ASSERT_EQ(result2.local_head_num_kv, 4);
-
-    param.kv_cache_mem_mb_ = 32;
-    param.head_num_kv_     = 1024;
-    auto result3           = creator.createConfig(param);
-    ASSERT_EQ(result3.block_nums, 8);
+    kv_cache_config.kv_cache_mem_mb = 32;
+    model_config.attn_config.kv_head_num = 1024;
+    auto result3           = creator.createConfig(model_config, mm_model_config, parallelism_config, runtime_config, kv_cache_config);
+    ASSERT_TRUE(result3.block_nums > 0);
 }
 
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/cache/test/MemoryBlockCacheTest.cc b/rtp_llm/cpp/cache/test/MemoryBlockCacheTest.cc
index 75f142dce..f115341b5 100644
--- a/rtp_llm/cpp/cache/test/MemoryBlockCacheTest.cc
+++ b/rtp_llm/cpp/cache/test/MemoryBlockCacheTest.cc
@@ -8,6 +8,7 @@
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
 #include "rtp_llm/cpp/core/Buffer.h"
 #include "rtp_llm/cpp/core/Types.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 // 定义MemoryBlockCacheOp枚举（如果不存在的话）
 namespace rtp_llm {
@@ -25,15 +26,15 @@ protected:
     void SetUp() override {
         DeviceTestBase::SetUp();
 
-        // 创建单TP的GptInitParameter
-        gpt_init_params_.tp_size_ = 1;
-        gpt_init_params_.tp_rank_ = 0;
+        // 创建单TP的配置
+        parallelism_config_.tp_size = 1;
+        parallelism_config_.tp_rank = 0;
 
         gpu_allocator_ = std::make_unique<KVCacheAllocator>(gpu_config_, device_);
         ASSERT_TRUE(gpu_allocator_->init());
 
-        memory_cache_ =
-            std::make_unique<MemoryBlockCache>(cpu_config_, device_, gpu_allocator_.get(), gpt_init_params_);
+        memory_cache_ = std::make_unique<MemoryBlockCache>(
+            cpu_config_, device_, gpu_allocator_.get(), parallelism_config_, kv_cache_config_, runtime_config_);
         ASSERT_TRUE(memory_cache_->init());
     }
 
@@ -216,7 +217,9 @@ protected:
     CacheConfig                       gpu_config_ = initGPUConfig();
     CacheConfig                       cpu_config_ = initCPUConfig();
     std::unique_ptr<KVCacheAllocator> gpu_allocator_;
-    GptInitParameter                  gpt_init_params_;
+    ParallelismConfig                 parallelism_config_;
+    KVCacheConfig                     kv_cache_config_;
+    RuntimeConfig                     runtime_config_;
     std::unique_ptr<MemoryBlockCache> memory_cache_;
     int64_t                           request_id_ = 0;
 };
@@ -228,8 +231,8 @@ TEST_F(MemoryBlockCacheTest, ConstructorAndInitTest) {
     EXPECT_NE(memory_cache_, nullptr);
 
     // 测试单TP配置
-    EXPECT_EQ(gpt_init_params_.tp_size_, 1);
-    EXPECT_EQ(gpt_init_params_.tp_rank_, 0);
+    EXPECT_EQ(parallelism_config_.tp_size, 1);
+    EXPECT_EQ(parallelism_config_.tp_rank, 0);
     EXPECT_EQ(memory_cache_->size(), 0);
     EXPECT_EQ(memory_cache_->capacity(), 7);
 }
@@ -402,7 +405,7 @@ TEST_F(MemoryBlockCacheTest, LRUEvictionTest) {
     // 创建一个较小的配置来测试淘汰
     CacheConfig small_config(KVCacheParam({2, 4, 4, 64, 1, rtp_llm::TYPE_FP32}));  // 只有3个block
     auto        small_memory_cache =
-        std::make_unique<MemoryBlockCache>(small_config, device_, gpu_allocator_.get(), gpt_init_params_);
+        std::make_unique<MemoryBlockCache>(small_config, device_, gpu_allocator_.get(), parallelism_config_, kv_cache_config_, runtime_config_);
     ASSERT_TRUE(small_memory_cache->init());
     ASSERT_EQ(3, small_memory_cache->capacity());
 
diff --git a/rtp_llm/cpp/cache/test/mock/MockDistKvCache.h b/rtp_llm/cpp/cache/test/mock/MockDistKvCache.h
index 078c31de2..9ea1d589c 100644
--- a/rtp_llm/cpp/cache/test/mock/MockDistKvCache.h
+++ b/rtp_llm/cpp/cache/test/mock/MockDistKvCache.h
@@ -9,6 +9,7 @@
 #include <vector>
 
 #include "rtp_llm/cpp/cache/DistKvCache.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 namespace rtp_llm {
 
@@ -16,11 +17,12 @@ using MapStrStr = std::map<std::string, std::string>;
 
 class MockDistKvCache: public DistKvCache {
 public:
-    MockDistKvCache(): DistKvCache(nullptr, GptInitParameter{}, nullptr) {}
+    MockDistKvCache(): DistKvCache(nullptr, ParallelismConfig{}, RuntimeConfig{}, nullptr) {}
     MockDistKvCache(CacheManager*                       cache_manager,
-                    const GptInitParameter&             gpt_init_params,
+                    const ParallelismConfig&            parallelism_config,
+                    const RuntimeConfig&                runtime_config,
                     const kmonitor::MetricsReporterPtr& metrics_reporter):
-        DistKvCache(cache_manager, gpt_init_params, metrics_reporter) {}
+        DistKvCache(cache_manager, parallelism_config, runtime_config, metrics_reporter) {}
     ~MockDistKvCache() override = default;
 
 public:
diff --git a/rtp_llm/cpp/config/BUILD b/rtp_llm/cpp/config/BUILD
index f0d8a9c5a..74d6e7ab0 100644
--- a/rtp_llm/cpp/config/BUILD
+++ b/rtp_llm/cpp/config/BUILD
@@ -10,32 +10,50 @@ cc_library(
     copts = copts(),
 )
 
+cc_library(
+    name = "role_types",
+    hdrs = ["RoleTypes.h"],
+    visibility = ["//visibility:public"],
+    copts = copts(),
+)
+
 cc_library(
     name = "config_modules",
     hdrs = ["ConfigModules.h"],
     srcs = ["ConfigModules.cc"],
     deps = [
         "@havenask//aios/autil:env_util",
+        ":role_types",
+        ":eplb_config",
     ],
     copts = copts(),
 )
 
 cc_library(
-    name = "gpt_init_params",
-    srcs = [
-        "GptInitParameter.cc",
-    ],
-    hdrs = [
-        "GptInitParameter.h",
-    ],
-    deps = [
-        "//rtp_llm/cpp/utils:core_utils",
-        "//rtp_llm/cpp/core:types",
-        "//rtp_llm/cpp/model_utils:model_utils",
-        "//rtp_llm/cpp/config:config_modules",
-        "//rtp_llm/cpp/models:eplb_config",
-    ] + torch_deps(),
+    name = "eplb_config",
+    hdrs = ["EplbConfig.h"],
+    visibility = ["//visibility:public"],
     copts = copts(),
+)
+
+cc_library(
+    name = "special_tokens",
+    hdrs = ["SpecialTokens.h"],
     visibility = ["//visibility:public"],
+    copts = copts(),
 )
 
+cc_library(
+    name = "model_config",
+    hdrs = ["ModelConfig.h"],
+    srcs = ["ModelConfig.cc"],
+    deps = [
+        ":eplb_config",
+        ":special_tokens",
+        ":config_modules",
+        "//rtp_llm/cpp/model_utils:model_utils",
+        "//rtp_llm/cpp/core:types",
+    ],
+    visibility = ["//visibility:public"],
+    copts = copts(),
+)
\ No newline at end of file
diff --git a/rtp_llm/cpp/config/ConfigModules.cc b/rtp_llm/cpp/config/ConfigModules.cc
index 3dbab7e1a..e721df45b 100644
--- a/rtp_llm/cpp/config/ConfigModules.cc
+++ b/rtp_llm/cpp/config/ConfigModules.cc
@@ -7,35 +7,8 @@
 
 namespace rtp_llm {
 
-std::string to_lower(const std::string& s) {
-    std::string result = s;
-    std::transform(result.begin(), result.end(), result.begin(), [](unsigned char c) { return std::tolower(c); });
-    return result;
-}
-
-bool bool_from_env_for_test(std::string env_name, bool default_value) {
-    const char* val = getenv(env_name.c_str());
-    if (!val) {
-        return default_value;
-    }
-    std::string lower = to_lower(val);
-    return lower == "1" || lower == "on" || lower == "true";
-}
-
-// ParallelismDistributedConfig
-void ParallelismDistributedConfig::update_from_env_for_test() {
-    tp_size          = autil::EnvUtil::getEnv("TP_SIZE", 1);
-    ep_size          = autil::EnvUtil::getEnv("EP_SIZE", 1);
-    dp_size          = autil::EnvUtil::getEnv("DP_SIZE", 1);
-    pp_size          = autil::EnvUtil::getEnv("PP_SIZE", 1);
-    world_size       = autil::EnvUtil::getEnv("WORLD_SIZE", 1);
-    world_rank       = autil::EnvUtil::getEnv("WORLD_RANK", 0);
-    local_world_size = autil::EnvUtil::getEnv("LOCAL_WORLD_SIZE", 1);
-    ffn_sp_size      = autil::EnvUtil::getEnv("FFN_SP_SIZE", 1);
-}
-
-// ParallelismDistributedConfig
-std::string ParallelismDistributedConfig::to_string() const {
+// ParallelismConfig
+std::string ParallelismConfig::to_string() const {
     std::ostringstream oss;
     oss << "tp_size: " << tp_size << "\n"
         << "ep_size: " << ep_size << "\n"
@@ -44,16 +17,24 @@ std::string ParallelismDistributedConfig::to_string() const {
         << "world_rank: " << world_rank << "\n"
         << "pp_size: " << pp_size << "\n"
         << "local_world_size: " << local_world_size << "\n"
-        << "ffn_sp_size" << ffn_sp_size;
+        << "ffn_sp_size: " << ffn_sp_size << "\n"
+        << "tp_rank: " << tp_rank << "\n"
+        << "ep_rank: " << ep_rank << "\n"
+        << "dp_rank: " << dp_rank << "\n"
+        << "ffn_tp_size: " << ffn_tp_size << "\n"
+        << "ffn_tp_rank: " << ffn_tp_rank << "\n"
+        << "enable_sp: " << enable_sp << "\n"
+        << "nccl_ip: " << nccl_ip << "\n"
+        << "tp_nccl_port: " << tp_nccl_port << "\n"
+        << "dp_tp_nccl_port: " << dp_tp_nccl_port << "\n"
+        << "ffn_tp_nccl_port: " << ffn_tp_nccl_port << "\n"
+        << "th_nccl_port: " << th_nccl_port << "\n"
+        << "http_port: " << http_port << "\n"
+        << "model_rpc_port: " << model_rpc_port;
     return oss.str();
 }
 
 // ConcurrencyConfig
-void ConcurrencyConfig::update_from_env_for_test() {
-    concurrency_with_block = bool_from_env_for_test("CONCURRENCY_WITH_BLOCK", false);
-    concurrency_limit      = autil::EnvUtil::getEnv("CONCURRENCY_LIMIT", 32);
-}
-
 std::string ConcurrencyConfig::to_string() const {
     std::ostringstream oss;
     oss << "concurrency_with_block: " << concurrency_with_block << "\n"
@@ -62,19 +43,6 @@ std::string ConcurrencyConfig::to_string() const {
 }
 
 // FMHAConfig
-void FMHAConfig::update_from_env_for_test() {
-    enable_fmha                   = bool_from_env_for_test("ENABLE_FMHA", true);
-    enable_trt_fmha               = bool_from_env_for_test("ENABLE_TRT_FMHA", true);
-    enable_paged_trt_fmha         = bool_from_env_for_test("ENABLE_PAGED_TRT_FMHA", true);
-    enable_open_source_fmha       = bool_from_env_for_test("ENABLE_OPENSOURCE_FMHA", true);
-    enable_paged_open_source_fmha = bool_from_env_for_test("ENABLE_PAGED_OPEN_SOURCE_FMHA", true);
-    enable_trtv1_fmha             = bool_from_env_for_test("ENABLE_TRTV1_FMHA", true);
-    fmha_perf_instrument          = bool_from_env_for_test("FMHA_PERF_INSTRUMENT", false);
-    fmha_show_params              = bool_from_env_for_test("FMHA_SHOW_PARAMS", false);
-    disable_flash_infer           = bool_from_env_for_test("DISABLE_FLASH_INFER", false);
-    enable_xqa                    = bool_from_env_for_test("ENABLE_XQA", true);
-}
-
 std::string FMHAConfig::to_string() const {
     std::ostringstream oss;
     oss << "enable_fmha: " << enable_fmha << "\n"
@@ -91,21 +59,12 @@ std::string FMHAConfig::to_string() const {
 }
 
 // KVCacheConfig
-void KVCacheConfig::update_from_env_for_test() {
-    reuse_cache                        = bool_from_env_for_test("REUSE_CACHE", false);
-    multi_task_prompt                  = autil::EnvUtil::getEnv("MULTI_TASK_PROMPT", "");
-    multi_task_prompt_str              = autil::EnvUtil::getEnv("MULTI_TASK_PROMPT_STR", "");
-    enable_3fs                         = bool_from_env_for_test("ENABLE_3FS", false);
-    match_timeout_ms                   = autil::EnvUtil::getEnv("MATCH_TIMEOUT_MS", 1000);
-    rpc_get_cache_timeout_ms           = autil::EnvUtil::getEnv("RPC_GET_CACHE_TIMEOUT_MS", 2000);
-    rpc_put_cache_timeout_ms           = autil::EnvUtil::getEnv("RPC_PUT_CACHE_TIMEOUT_MS", 2000);
-    threefs_read_timeout_ms            = autil::EnvUtil::getEnv("THREEFS_READ_TIMEOUT_MS", 1000);
-    threefs_write_timeout_ms           = autil::EnvUtil::getEnv("THREEFS_WRITE_TIMEOUT_MS", 2000);
-    max_block_size_per_item            = autil::EnvUtil::getEnv("MAX_BLOCK_SIZE_PER_ITEM", 16);
-    threefs_read_iov_size              = autil::EnvUtil::getEnv("THREEFS_READ_IOV_SIZE", 1LL << 32);   // 4GB
-    threefs_write_iov_size             = autil::EnvUtil::getEnv("THREEFS_WRITE_IOV_SIZE", 1LL << 32);  // 4GB
-    memory_block_cache_size_mb         = autil::EnvUtil::getEnv("MEMORY_BLOCK_CACHE_SIZE_MB", 0);
-    memory_block_cache_sync_timeout_ms = autil::EnvUtil::getEnv("MEMORY_BLOCK_CACHE_SYNC_TIMEOUT_MS", 10000);
+void KVCacheConfig::insertMultiTaskPromptTokens(std::string task_id, std::vector<int64_t> tokens_id) {
+    std::vector<int> new_tokens_id;  // to convert tokens of type int64_t to type int32_t
+    for (auto token_id : tokens_id) {
+        new_tokens_id.push_back(token_id);
+    }
+    multi_task_prompt_tokens[task_id] = new_tokens_id;
 }
 
 std::string KVCacheConfig::to_string() const {
@@ -113,6 +72,7 @@ std::string KVCacheConfig::to_string() const {
     oss << "reuse_cache: " << reuse_cache << "\n"
         << "multi_task_prompt: " << multi_task_prompt << "\n"
         << "multi_task_prompt_str: " << multi_task_prompt_str << "\n"
+        << "multi_task_prompt_tokens: " << (multi_task_prompt_tokens.empty() ? "empty" : "non-empty") << "\n"
         << "enable_3fs: " << enable_3fs << "\n"
         << "match_timeout_ms: " << match_timeout_ms << "\n"
         << "rpc_get_cache_timeout_ms: " << rpc_get_cache_timeout_ms << "\n"
@@ -123,32 +83,18 @@ std::string KVCacheConfig::to_string() const {
         << "threefs_read_iov_size: " << threefs_read_iov_size << "\n"
         << "threefs_write_iov_size: " << threefs_write_iov_size << "\n"
         << "memory_block_cache_size_mb: " << memory_block_cache_size_mb << "\n"
-        << "memory_block_cache_sync_timeout_ms: " << memory_block_cache_sync_timeout_ms;
+        << "memory_block_cache_sync_timeout_ms: " << memory_block_cache_sync_timeout_ms << "\n"
+        << "int8_kv_cache: " << int8_kv_cache << "\n"
+        << "fp8_kv_cache: " << fp8_kv_cache << "\n"
+        << "kv_cache_mem_mb: " << kv_cache_mem_mb << "\n"
+        << "seq_size_per_block: " << seq_size_per_block << "\n"
+        << "test_block_num: " << test_block_num << "\n"
+        << "use_block_cache: " << use_block_cache << "\n"
+        << "blockwise_use_fp8_kv_cache: " << blockwise_use_fp8_kv_cache;
     return oss.str();
 }
 
 // ProfilingDebugLoggingConfig
-void ProfilingDebugLoggingConfig::update_from_env_for_test() {
-    trace_memory              = bool_from_env_for_test("RTP_LLM_TRACE_MEMORY", false);
-    trace_malloc_stack        = bool_from_env_for_test("RTP_LLM_TRACE_MALLOC_STACK", false);
-    enable_device_perf        = bool_from_env_for_test("ENABLE_DEVICE_PERF", false);
-    ft_core_dump_on_exception = bool_from_env_for_test("FT_CORE_DUMP_ON_EXCEPTION", false);
-    ft_alog_conf_path         = autil::EnvUtil::getEnv("FT_ALOG_CONF_PATH", "");
-    log_level                 = autil::EnvUtil::getEnv("LOG_LEVEL", "INFO");
-    gen_timeline_sync         = bool_from_env_for_test("GEN_TIMELINE_SYNC", false);
-    torch_cuda_profiler_dir   = autil::EnvUtil::getEnv("TORCH_CUDA_PROFILER_DIR", "");
-    log_path                  = autil::EnvUtil::getEnv("LOG_PATH", "logs");
-    log_file_backup_count     = autil::EnvUtil::getEnv("LOG_FILE_BACKUP_COUNT", 16);
-    nccl_debug_file           = autil::EnvUtil::getEnv("NCCL_DEBUG_FILE", "");
-    debug_load_server         = bool_from_env_for_test("DEBUG_LOAD_SERVER", false);
-    hack_layer_num            = autil::EnvUtil::getEnv("HACK_LAYER_NUM", 0);
-    debug_start_fake_process  = bool_from_env_for_test("DEBUG_START_FAKE_PROCESS", false);
-    dg_print_reg_reuse        = bool_from_env_for_test("DG_PRINT_REG_REUSE", false);
-    qwen_agent_debug          = bool_from_env_for_test("QWEN_AGENT_DEBUG", false);
-    disable_dpc_random        = bool_from_env_for_test("DISABLE_DPC_RANDOM", false);
-    check_nan                 = bool_from_env_for_test("CHECK_NAN", false);
-}
-
 std::string ProfilingDebugLoggingConfig::to_string() const {
     std::ostringstream oss;
     oss << "trace_memory: " << trace_memory << "\n"
@@ -173,22 +119,6 @@ std::string ProfilingDebugLoggingConfig::to_string() const {
 }
 
 // HWKernelConfig
-void HWKernelConfig::update_from_env_for_test() {
-    deep_gemm_num_sm             = autil::EnvUtil::getEnv("DEEP_GEMM_NUM_SM", -1);
-    arm_gemm_use_kai             = bool_from_env_for_test("ARM_GEMM_USE_KAI", false);
-    enable_stable_scatter_add    = bool_from_env_for_test("ENABLE_STABLE_SCATTER_ADD", false);
-    enable_multi_block_mode      = bool_from_env_for_test("ENABLE_MULTI_BLOCK_MODE", true);
-    ft_disable_custom_ar         = bool_from_env_for_test("FT_DISABLE_CUSTOM_AR", true);
-    rocm_hipblaslt_config        = autil::EnvUtil::getEnv("ROCM_HIPBLASLT_CONFIG", "gemm_config.csv");
-    use_swizzleA                 = bool_from_env_for_test("USE_SWIZZLEA", false);
-    enable_cuda_graph            = bool_from_env_for_test("ENABLE_CUDA_GRAPH", false);
-    enable_cuda_graph_debug_mode = bool_from_env_for_test("ENABLE_CUDA_GRAPH_DEBUG_MODE", false);
-    use_aiter_pa                 = bool_from_env_for_test("USE_AITER_PA", true);
-    use_asm_pa                   = bool_from_env_for_test("USE_ASM_PA", true);
-    enable_native_cuda_graph     = bool_from_env_for_test("ENABLE_NATIVE_CUDA_GRAPH", false);
-    num_native_cuda_graph        = autil::EnvUtil::getEnv("NUM_NATIVE_CUDA_GRAPH", 200);
-}
-
 std::string HWKernelConfig::to_string() const {
     std::ostringstream oss;
     oss << "deep_gemm_num_sm: " << deep_gemm_num_sm << "\n"
@@ -199,26 +129,15 @@ std::string HWKernelConfig::to_string() const {
         << "rocm_hipblaslt_config: " << rocm_hipblaslt_config << "\n"
         << "use_swizzleA: " << use_swizzleA << "\n"
         << "enable_cuda_graph: " << enable_cuda_graph << "\n"
-        << "enable_cuda_graph_debug_mode" << enable_cuda_graph_debug_mode << "\n"
+        << "enable_cuda_graph_debug_mode: " << enable_cuda_graph_debug_mode << "\n"
         << "use_aiter_pa: " << use_aiter_pa << "\n"
         << "use_asm_pa: " << use_asm_pa << "\n"
-        << "enable_native_cuda_graph" << enable_native_cuda_graph << "\n"
-        << "num_native_cuda_graph" << num_native_cuda_graph << "\n";
+        << "enable_native_cuda_graph: " << enable_native_cuda_graph << "\n"
+        << "num_native_cuda_graph: " << num_native_cuda_graph;
     return oss.str();
 }
 
 // DeviceResourceConfig
-void DeviceResourceConfig::update_from_env_for_test() {
-    device_reserve_memory_bytes = autil::EnvUtil::getEnv("DEVICE_RESERVE_MEMORY_BYTES", 0);
-    host_reserve_memory_bytes   = autil::EnvUtil::getEnv("HOST_RESERVE_MEMORY_BYTES", 4LL * 1024 * 1024 * 1024);
-    overlap_math_sm_count       = autil::EnvUtil::getEnv("OVERLAP_MATH_SM_COUNT", 0);
-    overlap_comm_type           = autil::EnvUtil::getEnv("OVERLAP_COMM_TYPE", 0);
-    m_split                     = autil::EnvUtil::getEnv("M_SPLIT", 0);
-    enable_comm_overlap         = bool_from_env_for_test("ENABLE_COMM_OVERLAP", true);
-    enable_layer_micro_batch    = autil::EnvUtil::getEnv("ENABLE_LAYER_MICRO_BATCH", 0);
-    not_use_default_stream      = bool_from_env_for_test("NOT_USE_DEFAULT_STREAM", false);
-}
-
 std::string DeviceResourceConfig::to_string() const {
     std::ostringstream oss;
     oss << "device_reserve_memory_bytes: " << device_reserve_memory_bytes << "\n"
@@ -232,20 +151,6 @@ std::string DeviceResourceConfig::to_string() const {
 }
 
 // MoeConfig
-void MoeConfig::update_from_env_for_test() {
-    use_deepep_moe                  = bool_from_env_for_test("USE_DEEPEP_MOE", false);
-    use_deepep_internode            = bool_from_env_for_test("USE_DEEPEP_INTERNODE", false);
-    use_deepep_low_latency          = bool_from_env_for_test("USE_DEEPEP_LOW_LATENCY", true);
-    use_deepep_p2p_low_latency      = bool_from_env_for_test("USE_DEEPEP_P2P_LOW_LATENCY", false);
-    fake_balance_expert             = bool_from_env_for_test("FAKE_BALANCE_EXPERT", false);
-    eplb_control_step               = autil::EnvUtil::getEnv("EPLB_CONTROL_STEP", 100);
-    eplb_test_mode                  = bool_from_env_for_test("EPLB_TEST_MODE", false);
-    hack_moe_expert                 = bool_from_env_for_test("HACK_MOE_EXPERT", false);
-    eplb_balance_layer_per_step     = autil::EnvUtil::getEnv("EPLB_BALANCE_LAYER_PER_STEP", 1);
-    deep_ep_num_sm                  = autil::EnvUtil::getEnv("DEEP_EP_NUM_SM", 0);
-    max_moe_normal_masked_token_num = autil::EnvUtil::getEnv("RTP_LLM_MAX_MOE_NORMAL_MASKED_TOKEN_NUM", 1024);
-}
-
 std::string MoeConfig::to_string() const {
     std::ostringstream oss;
     oss << "use_deepep_moe: " << use_deepep_moe << "\n"
@@ -253,21 +158,14 @@ std::string MoeConfig::to_string() const {
         << "use_deepep_low_latency: " << use_deepep_low_latency << "\n"
         << "use_deepep_p2p_low_latency: " << use_deepep_p2p_low_latency << "\n"
         << "fake_balance_expert: " << fake_balance_expert << "\n"
-        << "eplb_control_step: " << eplb_control_step << "\n"
-        << "eplb_test_mode: " << eplb_test_mode << "\n"
         << "hack_moe_expert: " << hack_moe_expert << "\n"
-        << "eplb_balance_layer_per_step: " << eplb_balance_layer_per_step << "\n"
         << "deep_ep_num_sm: " << deep_ep_num_sm << "\n"
-        << "max_moe_normal_masked_token_num: " << max_moe_normal_masked_token_num;
+        << "max_moe_normal_masked_token_num: " << max_moe_normal_masked_token_num << "\n"
+        << "use_all_gather: " << use_all_gather;
     return oss.str();
 }
 
 // ModelSpecificConfig
-void ModelSpecificConfig::update_from_env_for_test() {
-    max_lora_model_size = autil::EnvUtil::getEnv("MAX_LORA_MODEL_SIZE", -1);
-    load_python_model   = bool_from_env_for_test("LOAD_PYTHON_MODEL", false);
-}
-
 std::string ModelSpecificConfig::to_string() const {
     std::ostringstream oss;
     oss << "max_lora_model_size: " << max_lora_model_size << "\n";
@@ -276,17 +174,6 @@ std::string ModelSpecificConfig::to_string() const {
 }
 
 // SpeculativeExecutionConfig
-void SpeculativeExecutionConfig::update_from_env_for_test() {
-    sp_model_type                 = autil::EnvUtil::getEnv("SP_MODEL_TYPE", "");
-    sp_type                       = autil::EnvUtil::getEnv("SP_TYPE", "");
-    sp_min_token_match            = autil::EnvUtil::getEnv("SP_MIN_TOKEN_MATCH", 2);
-    sp_max_token_match            = autil::EnvUtil::getEnv("SP_MAX_TOKEN_MATCH", 2);
-    tree_decode_config            = autil::EnvUtil::getEnv("TREE_DECODE_CONFIG", "");
-    gen_num_per_cycle             = autil::EnvUtil::getEnv("GEN_NUM_PER_CIRCLE", 1);
-    force_stream_sample           = autil::EnvUtil::getEnv("FORCE_STREAM_SAMPLE", false);
-    force_score_context_attention = autil::EnvUtil::getEnv("FORCE_SCORE_CONTEXT_ATTENTION", true);
-}
-
 std::string SpeculativeExecutionConfig::to_string() const {
     std::ostringstream oss;
     oss << "sp_model_type: " << sp_model_type << "\n"
@@ -296,45 +183,34 @@ std::string SpeculativeExecutionConfig::to_string() const {
         << "tree_decode_config: " << tree_decode_config << "\n"
         << "gen_num_per_cycle: " << gen_num_per_cycle << "\n"
         << "force_stream_sample: " << force_stream_sample << "\n"
-        << "force_score_context_attention: " << force_score_context_attention;
+        << "force_score_context_attention: " << force_score_context_attention << "\n"
+        << "sp_quantization: " << sp_quantization << "\n"
+        << "sp_checkpoint_path: " << sp_checkpoint_path;
     return oss.str();
 }
 
-// ServiceDiscoveryConfig
-void ServiceDiscoveryConfig::update_from_env_for_test() {
-    use_local                  = bool_from_env_for_test("USE_LOCAL", false);
-    remote_rpc_server_ip       = autil::EnvUtil::getEnv("REMOTE_RPC_SERVER_IP", "");
-    decode_cm2_config          = autil::EnvUtil::getEnv("RTP_LLM_DECODE_CM2_CONFIG", "");
-    remote_vit_server_ip       = autil::EnvUtil::getEnv("REMOTE_VIT_SERVER_IP", "");
-    multimodal_part_cm2_config = autil::EnvUtil::getEnv("RTP_LLM_MULTIMODAL_PART_CM2_CONFIG", "");
-    remote_backend_ip          = autil::EnvUtil::getEnv("REMOTE_BACKEND_IP", "");
-    backend_cm2_config         = autil::EnvUtil::getEnv("BACKEND_CM2_CONFIG", "");
-}
-
-std::string ServiceDiscoveryConfig::to_string() const {
+// VitConfig
+std::string VitConfig::to_string() const {
     std::ostringstream oss;
-    oss << "use_local: " << use_local << "\n"
-        << "remote_rpc_server_ip: " << remote_rpc_server_ip << "\n"
-        << "decode_cm2_config: " << decode_cm2_config << "\n"
-        << "remote_vit_server_ip: " << remote_vit_server_ip << "\n"
-        << "multimodal_part_cm2_config: " << multimodal_part_cm2_config << "\n"
-        << "remote_backend_ip: " << remote_backend_ip << "\n"
-        << "backend_cm2_config: " << backend_cm2_config;
+    std::string vit_sep_str;
+    switch (vit_separation) {
+        case VitSeparation::VIT_SEPARATION_LOCAL:
+            vit_sep_str = "LOCAL";
+            break;
+        case VitSeparation::VIT_SEPARATION_ROLE:
+            vit_sep_str = "ROLE";
+            break;
+        case VitSeparation::VIT_SEPARATION_REMOTE:
+            vit_sep_str = "REMOTE";
+            break;
+        default:
+            vit_sep_str = "UNKNOWN(" + std::to_string(static_cast<int>(vit_separation)) + ")";
+            break;
+    }
+    oss << "vit_separation: " << vit_sep_str;
     return oss.str();
 }
 
-// CacheStoreConfig
-void CacheStoreConfig::update_from_env_for_test() {
-    cache_store_rdma_mode        = bool_from_env_for_test("CACHE_STORE_RDMA_MODE", false);
-    wrr_available_ratio          = autil::EnvUtil::getEnv("WRR_AVAILABLE_RATIO", 80);
-    rank_factor                  = autil::EnvUtil::getEnv("RANK_FACTOR", 0);
-    thread_count                 = autil::EnvUtil::getEnv("CACHE_STORE_THREAD_COUNT", 16);
-    rdma_connect_timeout_ms      = autil::EnvUtil::getEnv("CACHE_STORE_RDMA_CONNECT_TIMEOUT_MS", 250);
-    rdma_qp_count_per_connection = autil::EnvUtil::getEnv("CACHE_STORE_RDMA_CONNECTION_COUNT_PER_CONNECTION", 2);
-    messager_io_thread_count     = autil::EnvUtil::getEnv("MESSAGER_IO_THREAD_COUNT", 2);
-    messager_worker_thread_count = autil::EnvUtil::getEnv("MESSAGER_WORKER_THREAD_COUNT", 16);
-}
-
 // CacheStoreConfig
 std::string CacheStoreConfig::to_string() const {
     std::ostringstream oss;
@@ -349,57 +225,57 @@ std::string CacheStoreConfig::to_string() const {
     return oss.str();
 }
 
-// SchedulerConfig
-void SchedulerConfig::update_from_env_for_test() {
-    use_batch_decode_scheduler = bool_from_env_for_test("USE_BATCH_DECODE_SCHEDULER", false);
-}
-
-std::string SchedulerConfig::to_string() const {
+// MiscellaneousConfig
+std::string MiscellaneousConfig::to_string() const {
     std::ostringstream oss;
-    oss << "use_batch_decode_scheduler: " << use_batch_decode_scheduler << "\n";
+    oss << "disable_pdl" << disable_pdl << "\n"
+        << "aux_string: " << aux_string << "\n";
     return oss.str();
 }
 
 // BatchDecodeSchedulerConfig
-void BatchDecodeSchedulerConfig::update_from_env_for_test() {
-    batch_decode_scheduler_batch_size = autil::EnvUtil::getEnv("BATCH_DECODE_SCHEDULER_BATCH_SIZE", 1);
-}
-
 std::string BatchDecodeSchedulerConfig::to_string() const {
     std::ostringstream oss;
-    oss << "batch_decode_scheduler_batch_size: " << batch_decode_scheduler_batch_size << "\n";
+    oss << "batch_decode_scheduler_batch_size: " << batch_decode_scheduler_batch_size << "\n"
+        << "batch_decode_scheduler_warmup_type: " << batch_decode_scheduler_warmup_type;
     return oss.str();
 }
 
 // FIFOSchedulerConfig
-void FIFOSchedulerConfig::update_from_env_for_test() {
-    max_context_batch_size           = autil::EnvUtil::getEnv("MAX_CONTEXT_BATCH_SIZE", 1);
-    scheduler_reserve_resource_ratio = autil::EnvUtil::getEnv("SCHEDULER_RESERVE_RESOURCE_RATIO", 5);
-    enable_fast_gen                  = bool_from_env_for_test("ENABLE_FAST_GEN", false);
-    enable_partial_fallback          = bool_from_env_for_test("ENABLE_PARTIAL_FALLBACK", false);
-    fast_gen_context_budget          = autil::EnvUtil::getEnv("FAST_GEN_MAX_CONTEXT_LEN", -1);
-}
-
 std::string FIFOSchedulerConfig::to_string() const {
     std::ostringstream oss;
-    oss << "max_context_batch_size: " << max_context_batch_size << "\n"
-        << "scheduler_reserve_resource_ratio: " << scheduler_reserve_resource_ratio << "\n"
-        << "enable_fast_gen: " << enable_fast_gen << "\n"
+    oss << "enable_fast_gen: " << enable_fast_gen << "\n"
         << "enable_partial_fallback: " << enable_partial_fallback << "\n"
-        << "fast_gen_context_budget: " << fast_gen_context_budget;
+        << "fast_gen_context_budget: " << fast_gen_context_budget << "\n"
+        << "max_context_batch_size: " << max_context_batch_size << "\n"
+        << "scheduler_reserve_resource_ratio: " << scheduler_reserve_resource_ratio << "\n"
+        << "fast_gen_max_context_len: " << fast_gen_max_context_len << "\n"
+        << "max_batch_tokens_size: " << max_batch_tokens_size;
     return oss.str();
 }
 
-// MiscellaneousConfig
-void MiscellaneousConfig::update_from_env_for_test() {
-    disable_pdl = bool_from_env_for_test("DISABLE_PDL", true);
-    aux_string  = autil::EnvUtil::getEnv("AUX_STRING", "");
-}
-
-std::string MiscellaneousConfig::to_string() const {
+// RuntimeConfig
+std::string RuntimeConfig::to_string() const {
     std::ostringstream oss;
-    oss << "disable_pdl" << disable_pdl << "\n"
-        << "aux_string: " << aux_string << "\n";
+    oss << "max_generate_batch_size: " << max_generate_batch_size << "\n"
+        << "pre_allocate_op_mem: " << pre_allocate_op_mem << "\n"
+        << "max_block_size_per_item: " << max_block_size_per_item << "\n"
+        << "reserve_runtime_mem_mb: " << reserve_runtime_mem_mb << "\n"
+        << "warm_up: " << warm_up << "\n"
+        << "warm_up_with_loss: " << warm_up_with_loss << "\n"
+        << "use_batch_decode_scheduler: " << use_batch_decode_scheduler << "\n"
+        << "use_gather_batch_scheduler: " << use_gather_batch_scheduler << "\n"
+        << "batch_decode_scheduler_config: {\n" << batch_decode_scheduler_config.to_string() << "\n}\n"
+        << "fifo_scheduler_config: {\n" << fifo_scheduler_config.to_string() << "\n}\n"
+        << "model_name: " << model_name << "\n"
+        << "worker_grpc_addrs: [";
+    for (size_t i = 0; i < worker_grpc_addrs.size(); ++i) {
+        oss << worker_grpc_addrs[i];
+        if (i < worker_grpc_addrs.size() - 1) oss << ", ";
+    }
+    oss << "]\n"
+        << "specify_gpu_arch: " << specify_gpu_arch << "\n"
+        << "acext_gemm_config_dir: " << acext_gemm_config_dir;
     return oss.str();
 }
 
@@ -413,15 +289,6 @@ std::string ArpcConfig::to_string() const {
 }
 
 // FfnDisAggregateConfig
-void FfnDisAggregateConfig::update_from_env_for_test() {
-    enable_ffn_disaggregate = bool_from_env_for_test("ENABLE_FFN_DISAGGREGATE", false);
-    attention_tp_size       = autil::EnvUtil::getEnv("ATTENTION_TP_SIZE", 1);
-    attention_dp_size       = autil::EnvUtil::getEnv("ATTENTION_DP_SIZE", 1);
-    ffn_tp_size             = autil::EnvUtil::getEnv("FFN_TP_SIZE", 1);
-    ffn_dp_size             = autil::EnvUtil::getEnv("FFN_DP_SIZE", 1);
-    is_ffn_rank             = bool_from_env_for_test("IS_FFN_RANK", false);
-}
-
 std::string FfnDisAggregateConfig::to_string() const {
     std::ostringstream oss;
     oss << "enable_ffn_disaggregate: " << enable_ffn_disaggregate << "\n";
@@ -433,4 +300,29 @@ std::string FfnDisAggregateConfig::to_string() const {
     return oss.str();
 }
 
+// PDSepConfig
+std::string PDSepConfig::to_string() const {
+    std::ostringstream oss;
+    oss << "role_type: " << (int)role_type << "\n"
+        << "cache_store_rdma_mode: " << cache_store_rdma_mode << "\n"
+        << "cache_store_listen_port: " << cache_store_listen_port << "\n"
+        << "cache_store_connect_port: " << cache_store_connect_port << "\n"
+        << "cache_store_rdma_listen_port: " << cache_store_rdma_listen_port << "\n"
+        << "cache_store_rdma_connect_port: " << cache_store_rdma_connect_port << "\n"
+        << "remote_rpc_server_port: " << remote_rpc_server_port << "\n"
+        << "prefill_retry_times: " << prefill_retry_times << "\n"
+        << "prefill_retry_timeout_ms: " << prefill_retry_timeout_ms << "\n"
+        << "prefill_max_wait_timeout_ms: " << prefill_max_wait_timeout_ms << "\n"
+        << "decode_retry_times: " << decode_retry_times << "\n"
+        << "decode_retry_timeout_ms: " << decode_retry_timeout_ms << "\n"
+        << "decode_polling_kv_cache_step_ms: " << decode_polling_kv_cache_step_ms << "\n"
+        << "decode_polling_call_prefill_ms: " << decode_polling_call_prefill_ms << "\n"
+        << "rdma_connect_retry_times: " << rdma_connect_retry_times << "\n"
+        << "load_cache_timeout_ms: " << load_cache_timeout_ms << "\n"
+        << "max_rpc_timeout_ms: " << max_rpc_timeout_ms << "\n"
+        << "worker_port_offset: " << worker_port_offset << "\n"
+        << "decode_entrance: " << decode_entrance;
+    return oss.str();
+}
+
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/config/ConfigModules.h b/rtp_llm/cpp/config/ConfigModules.h
index c0c8f9626..865d807d8 100644
--- a/rtp_llm/cpp/config/ConfigModules.h
+++ b/rtp_llm/cpp/config/ConfigModules.h
@@ -1,27 +1,58 @@
 #pragma once
 #include <string>
 #include <sstream>
+#include <map>
+#include <vector>
+#include "rtp_llm/cpp/config/RoleTypes.h"
 
 namespace rtp_llm {
 
-struct ParallelismDistributedConfig {
-    int         tp_size          = 1;
-    int         ep_size          = 1;
-    int         dp_size          = 1;
-    int         pp_size          = 1;
-    int         world_size       = 1;
-    int         world_rank       = 0;
-    int         local_world_size = 1;
-    int         ffn_sp_size      = 1;
+struct FfnDisAggregateConfig {
+    bool        enable_ffn_disaggregate = false;
+    int         attention_tp_size       = 1;
+    int         attention_dp_size       = 1;
+    int         ffn_tp_size             = 1;
+    int         ffn_dp_size             = 1;
+    bool        is_ffn_rank             = false;
+    std::string to_string() const;
+    bool        is_ffn_service() const {
+        return enable_ffn_disaggregate && is_ffn_rank;
+    }
+};
+
+struct ParallelismConfig {
+    int64_t     tp_size          = 1;
+    int64_t     ep_size          = 1;
+    int64_t     dp_size          = 1;
+    int64_t     pp_size          = 1;
+    int64_t     world_size       = 1;
+    int64_t     world_rank       = 0;
+    int64_t     local_world_size = 1;
+    int64_t     ffn_sp_size      = 1;
+    int64_t     tp_rank          = 0;
+    int64_t     ep_rank          = 0;
+    int64_t     dp_rank          = 0;
+    int64_t     ffn_tp_size      = 1;
+    int64_t     ffn_tp_rank      = 0;
+    bool        enable_sp        = false;
+
+    std::string nccl_ip          = "";
+    int64_t     tp_nccl_port     = 0;
+    int64_t     dp_tp_nccl_port  = 0;
+    int64_t     ffn_tp_nccl_port = 0;
+    int64_t     th_nccl_port     = 0;  // General NCCL port for compatibility
+    int64_t     http_port        = 0;
+    int64_t     model_rpc_port   = 0;
+
+    FfnDisAggregateConfig ffn_disaggregate_config;  // FFN disaggregate configuration
+
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct ConcurrencyConfig {
     bool        concurrency_with_block = false;
     int         concurrency_limit      = 32;
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct FMHAConfig {
@@ -36,13 +67,13 @@ struct FMHAConfig {
     bool        disable_flash_infer           = false;
     bool        enable_xqa                    = true;
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct KVCacheConfig {
     bool        reuse_cache                        = false;
     std::string multi_task_prompt                  = "";
     std::string multi_task_prompt_str              = "";
+    std::map<std::string, std::vector<int>> multi_task_prompt_tokens;
     bool        enable_3fs                         = false;
     int         match_timeout_ms                   = 1000;
     int         rpc_get_cache_timeout_ms           = 2000;
@@ -54,8 +85,16 @@ struct KVCacheConfig {
     int64_t     threefs_write_iov_size             = 1LL << 32;  // 4GB
     int64_t     memory_block_cache_size_mb         = 0;
     int64_t     memory_block_cache_sync_timeout_ms = 10000;
+    // Fields merged from PyKvCacheConfig
+    int         int8_kv_cache                      = 0;
+    int         fp8_kv_cache                      = 0;
+    int64_t     kv_cache_mem_mb                    = -1;
+    int         seq_size_per_block                 = 64;
+    int         test_block_num                     = 0;
+    int         use_block_cache                    = -1;  // -1 means not set, use Optional<int> equivalent
+    int         blockwise_use_fp8_kv_cache         = 0;
+    void        insertMultiTaskPromptTokens(std::string task_id, std::vector<int64_t> tokens_id);
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct ProfilingDebugLoggingConfig {
@@ -80,7 +119,6 @@ struct ProfilingDebugLoggingConfig {
     bool        check_nan                 = false;
 
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct HWKernelConfig {
@@ -98,7 +136,6 @@ struct HWKernelConfig {
     bool        enable_native_cuda_graph     = false;
     int         num_native_cuda_graph        = 200;
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct DeviceResourceConfig {
@@ -111,7 +148,6 @@ struct DeviceResourceConfig {
     int         enable_layer_micro_batch    = 0;
     bool        not_use_default_stream      = false;
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct MoeConfig {
@@ -120,21 +156,17 @@ struct MoeConfig {
     bool        use_deepep_low_latency          = true;
     bool        use_deepep_p2p_low_latency      = false;
     bool        fake_balance_expert             = false;
-    int         eplb_control_step               = 100;
-    bool        eplb_test_mode                  = false;
     bool        hack_moe_expert                 = false;
-    int         eplb_balance_layer_per_step     = 1;
     int         deep_ep_num_sm                  = 0;
     int         max_moe_normal_masked_token_num = 1024;
+    bool        use_all_gather                  = false;
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct ModelSpecificConfig {
     int64_t     max_lora_model_size = -1;
     bool        load_python_model   = false;
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct SpeculativeExecutionConfig {
@@ -146,20 +178,15 @@ struct SpeculativeExecutionConfig {
     int64_t     gen_num_per_cycle             = 1;
     bool        force_stream_sample           = false;
     bool        force_score_context_attention = true;
+    // Fields merged from PySpeculativeExecutionConfig
+    std::string sp_quantization               = "";
+    std::string sp_checkpoint_path            = "";
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
-struct ServiceDiscoveryConfig {
-    bool        use_local = false;
-    std::string remote_rpc_server_ip;
-    std::string decode_cm2_config;
-    std::string remote_vit_server_ip;
-    std::string multimodal_part_cm2_config;
-    std::string remote_backend_ip;
-    std::string backend_cm2_config;
+struct VitConfig {
+    VitSeparation vit_separation = VitSeparation::VIT_SEPARATION_LOCAL;
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct CacheStoreConfig {
@@ -172,39 +199,80 @@ struct CacheStoreConfig {
     int         messager_io_thread_count     = 2;
     int         messager_worker_thread_count = 16;
     std::string to_string() const;
-    void        update_from_env_for_test();
-};
-
-struct SchedulerConfig {
-    bool        use_batch_decode_scheduler = false;
-    bool        use_gather_batch_scheduler = false;
-    std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct BatchDecodeSchedulerConfig {
     int64_t batch_decode_scheduler_batch_size = 1;
     // 0: use decode warmup, others: use prefill warmup
-    int64_t     batch_decode_scheduler_warmup_type = 0;
+    int64_t batch_decode_scheduler_warmup_type = 0;
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct FIFOSchedulerConfig {
-    int64_t     max_context_batch_size           = 1;
-    int         scheduler_reserve_resource_ratio = 5;
-    bool        enable_fast_gen                  = false;
-    bool        enable_partial_fallback          = false;
-    int64_t     fast_gen_context_budget          = -1;
+    bool    enable_fast_gen                  = false;
+    bool    enable_partial_fallback          = false;
+    int64_t fast_gen_context_budget          = -1;
+    int64_t max_context_batch_size           = 1;
+    int64_t scheduler_reserve_resource_ratio = 5;
+    int64_t fast_gen_max_context_len         = 0;
+    int64_t max_batch_tokens_size            = 0;
+    std::string to_string() const;
+};
+
+struct RuntimeConfig {
+    int64_t max_generate_batch_size = 1;
+
+    bool    pre_allocate_op_mem     = true;
+    int64_t max_block_size_per_item = 16;
+
+    int64_t reserve_runtime_mem_mb           = 0;
+    bool    warm_up                          = false;
+    bool    warm_up_with_loss                = false;
+
+    // Scheduler configuration
+    bool    use_batch_decode_scheduler = false;
+    bool    use_gather_batch_scheduler = false;
+    BatchDecodeSchedulerConfig batch_decode_scheduler_config;
+    FIFOSchedulerConfig fifo_scheduler_config;
+
+    std::string                  model_name                  = "";
+    std::vector<std::string>    worker_grpc_addrs;
+
+    // Fields merged from PyDeviceResourceConfig
+    std::string specify_gpu_arch      = "";
+    std::string acext_gemm_config_dir = "";
+
+    std::string to_string() const;
+};
+
+struct PDSepConfig {
+    RoleType role_type                       = RoleType::PDFUSION;
+    bool     cache_store_rdma_mode           = true;
+    int64_t  cache_store_listen_port         = 0;
+    int64_t  cache_store_connect_port        = 0;
+    int64_t  cache_store_rdma_listen_port    = 0;
+    int64_t  cache_store_rdma_connect_port   = 0;
+    int64_t  remote_rpc_server_port          = 0;
+    int64_t  prefill_retry_times             = 0;
+    int64_t  prefill_retry_timeout_ms        = 20;
+    int64_t  prefill_max_wait_timeout_ms     = 600 * 1000;
+    int64_t  decode_retry_times              = 100;
+    int64_t  decode_retry_timeout_ms         = 100;
+    int64_t  decode_polling_kv_cache_step_ms = 30;
+    int64_t  decode_polling_call_prefill_ms  = 30;
+    int64_t  rdma_connect_retry_times        = 0;
+    int64_t  load_cache_timeout_ms           = 5000;
+    int64_t  max_rpc_timeout_ms              = 0;
+    int64_t  worker_port_offset              = 0;
+    bool     decode_entrance                 = false;
+
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 struct MiscellaneousConfig {
     bool        disable_pdl = true;
     std::string aux_string  = "";
     std::string to_string() const;
-    void        update_from_env_for_test();
 };
 
 class ParallelInfo final {
@@ -292,16 +360,15 @@ public:
     }
     // only for test
     void reload() {
-        ParallelismDistributedConfig parallelism_distributed_config;
-        parallelism_distributed_config.update_from_env_for_test();
-        tp_size_ = parallelism_distributed_config.tp_size;
+        ParallelismConfig parallelism_config;
+        tp_size_ = parallelism_config.tp_size;
         // in fact pipeline parallelism is not supported yet
-        pp_size_          = parallelism_distributed_config.pp_size;
-        ep_size_          = parallelism_distributed_config.ep_size;
-        dp_size_          = parallelism_distributed_config.dp_size;
-        world_size_       = parallelism_distributed_config.world_size;
-        world_rank_       = parallelism_distributed_config.world_rank;
-        local_world_size_ = parallelism_distributed_config.local_world_size;
+        pp_size_          = parallelism_config.pp_size;
+        ep_size_          = parallelism_config.ep_size;
+        dp_size_          = parallelism_config.dp_size;
+        world_size_       = parallelism_config.world_size;
+        world_rank_       = parallelism_config.world_rank;
+        local_world_size_ = parallelism_config.local_world_size;
     }
 
 private:
@@ -314,20 +381,6 @@ private:
     int local_world_size_;
 };
 
-struct FfnDisAggregateConfig {
-    bool        enable_ffn_disaggregate = false;
-    int         attention_tp_size       = 1;
-    int         attention_dp_size       = 1;
-    int         ffn_tp_size             = 1;
-    int         ffn_dp_size             = 1;
-    bool        is_ffn_rank             = false;
-    std::string to_string() const;
-    void        update_from_env_for_test();
-    bool        is_ffn_service() const {
-        return enable_ffn_disaggregate && is_ffn_rank;
-    }
-};
-
 struct ArpcConfig {
     int         threadNum   = 10;
     int         queueNum    = 50;
@@ -335,7 +388,4 @@ struct ArpcConfig {
     std::string to_string() const;
 };
 
-std::string to_lower(const std::string& s);
-bool        bool_from_env_for_test(std::string env_name, bool default_value);
-
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/config/EplbConfig.h b/rtp_llm/cpp/config/EplbConfig.h
new file mode 100644
index 000000000..65cb71837
--- /dev/null
+++ b/rtp_llm/cpp/config/EplbConfig.h
@@ -0,0 +1,72 @@
+#pragma once
+#include <string>
+#include <vector>
+
+namespace rtp_llm {
+
+enum class EplbMode {
+    NONE,
+    STATS,  // stats, only
+    EPLB,   // load balance, only
+    ALL     // stats + load balance
+};
+
+struct EPLBConfig {
+    int64_t          eplb_update_time = 5000;
+    EplbMode         eplb_mode        = EplbMode::NONE;
+    int64_t          redundant_expert = 0;
+    std::string      balance_method = "mix";
+    int64_t          eplb_force_repack = 0;
+    int64_t          eplb_stats_window_size = 10;
+    int              eplb_control_step = 100;
+    bool             eplb_test_mode = false;
+    int              eplb_balance_layer_per_step = 1;
+    template<typename... CheckModes>
+    bool checkEplbMode(EplbMode mode, CheckModes... modes) {
+        return ((mode == modes) || ...);
+    }
+
+    std::vector<int> toList() const {
+        std::vector<int> list;
+        list.push_back((int)eplb_mode);
+        list.push_back((int)eplb_update_time);
+        return list;
+    }
+
+    static EPLBConfig fromList(const int* list) {
+        EPLBConfig data;
+        data.eplb_mode = (EplbMode)list[0];
+        data.eplb_update_time = list[1];
+        return data;
+    }
+
+    // Getter methods
+    bool enable_eplb() const {
+        return eplb_mode != EplbMode::NONE;
+    }
+    
+    int64_t phy_exp_num(int64_t expert_num) const {
+        return redundant_expert + expert_num;
+    }
+
+    std::string to_string() const {
+        std::ostringstream oss;
+        oss << "enable_eplb: " << enable_eplb() << "\n"
+            << "eplb_update_time: " << eplb_update_time << "\n"
+            << "eplb_mode: " << (int)eplb_mode << "\n"
+            << "redundant_expert: " << redundant_expert << "\n"
+            << "balance_method: " << balance_method << "\n"
+            << "eplb_force_repack: " << eplb_force_repack << "\n"
+            << "eplb_stats_window_size: " << eplb_stats_window_size << "\n"
+            << "eplb_control_step: " << eplb_control_step << "\n"
+            << "eplb_test_mode: " << eplb_test_mode << "\n"
+            << "eplb_balance_layer_per_step: " << eplb_balance_layer_per_step;
+        return oss.str();
+    }
+};
+
+}  // namespace rtp_llm
+
+
+
+
diff --git a/rtp_llm/cpp/config/GptInitParameter.cc b/rtp_llm/cpp/config/GptInitParameter.cc
deleted file mode 100644
index a1a5f91da..000000000
--- a/rtp_llm/cpp/config/GptInitParameter.cc
+++ /dev/null
@@ -1,223 +0,0 @@
-#include "rtp_llm/cpp/config/GptInitParameter.h"
-#include "autil/Log.h"
-
-namespace rtp_llm {
-
-namespace py = pybind11;
-
-SpecialTokens::SpecialTokens() {}
-
-GptInitParameter::GptInitParameter() {}
-
-GptInitParameter::GptInitParameter(int64_t head_num,
-                                   int64_t size_per_head,
-                                   int64_t num_layers,
-                                   int64_t max_seq_len,
-                                   int64_t vocab_size,
-                                   int64_t hidden_size):
-    head_num_(head_num),
-    size_per_head_(size_per_head),
-    num_layers_(num_layers),
-    hidden_size_(hidden_size),
-    max_seq_len_(max_seq_len),
-    vocab_size_(vocab_size) {}
-
-void GptInitParameter::insertMultiTaskPromptTokens(std::string task_id, std::vector<int64_t> tokens_id) {
-    std::vector<int> new_tokens_id;  // to convert tokens of type int64_t to type int32_t
-    for (auto token_id : tokens_id) {
-        new_tokens_id.push_back(token_id);
-    }
-    multi_task_prompt_tokens_[task_id] = new_tokens_id;
-}
-
-void GptInitParameter::setLayerNormType() {
-    layernorm_type_ = getLayerNormType(layernorm_type_str_);
-}
-
-void GptInitParameter::setNormType() {
-    norm_type_ = getNormType(norm_type_str_);
-}
-
-void GptInitParameter::setTaskType(std::string task) {
-    if (task == "DENSE_EMBEDDING") {
-        task_type_ = TaskType::DENSE_EMBEDDING;
-    } else if (task == "ALL_EMBEDDING") {
-        task_type_ = TaskType::ALL_EMBEDDING;
-    } else if (task == "SPARSE_EMBEDDING") {
-        task_type_ = TaskType::SPARSE_EMBEDDING;
-    } else if (task == "COLBERT_EMBEDDING") {
-        task_type_ = TaskType::COLBERT_EMBEDDING;
-    } else if (task == "LANGUAGE_MODEL") {
-        task_type_ = TaskType::LANGUAGE_MODEL;
-    } else if (task == "SEQ_CLASSIFICATION") {
-        task_type_ = TaskType::SEQ_CLASSIFICATION;
-    } else if (task == "RERANKER") {
-        task_type_ = TaskType::RERANKER;
-    } else if (task == "LINEAR_SOFTMAX") {
-        task_type_ = TaskType::LINEAR_SOFTMAX;
-    } else if (task == "BGE_M3") {
-        task_type_ = TaskType::BGE_M3;
-    } else {
-        RTP_LLM_CHECK_WITH_INFO(false, "unkown task type: " + task);
-    }
-}
-
-void GptInitParameter::setActivationType() {
-    activation_type_ = getActivationType(activation_type_str_);
-}
-
-void GptInitParameter::setDataType() {
-    data_type_ = getDataType(data_type_str_);
-}
-
-void GptInitParameter::setKvCacheDataType() {
-    kv_cache_data_type_ = getDataType(kv_cache_data_type_str_);
-}
-
-bool GptInitParameter::isGatedActivation() const {
-    return rtp_llm::isGatedActivation(activation_type_);
-}
-
-bool GptInitParameter::isKvCacheQuant() const {
-    return kv_cache_data_type_ == DataType::TYPE_FP8_E4M3 || kv_cache_data_type_ == DataType::TYPE_INT8;
-}
-
-void QuantAlgo::setQuantAlgo(const std::string& quant_method, int64_t bits, int64_t group_size) {
-    if (quant_method == "gptq") {
-        quant_method_ = GptQ;
-        weight_bits_  = bits;
-        group_size_   = group_size;
-    } else if (quant_method == "awq") {
-        quant_method_ = Awq;
-        weight_bits_  = bits;
-        group_size_   = group_size;
-    } else if (quant_method == "weight_only_per_col") {
-        quant_method_ = WeightOnlyPerCol;
-        weight_bits_  = bits;
-        if (weight_bits_ != 8) {
-            throw std::invalid_argument("invalid weight_bits: " + std::to_string(weight_bits_));
-        }
-    } else if (quant_method == "smooth_quant") {
-        quant_method_ = SmoothQuant;
-        weight_bits_  = 8;
-    } else if (quant_method == "omni_quant") {
-        quant_method_ = OmniQuant;
-        weight_bits_  = 8;
-    } else if (quant_method == "pertensor_quant") {
-        quant_method_ = PerTensorQuant;
-        weight_bits_  = 8;
-    } else if (quant_method == "fp8" || quant_method == "fp8_dynamic_per_tensor") {
-        quant_method_ = FP8Quant;
-        weight_bits_  = 8;
-        group_size_   = group_size;
-    } else if (quant_method == "fp8-perchannel-compressed-tensors") {
-        quant_method_ = FP8PTPC;
-        weight_bits_  = 8;
-    } else {
-        throw std::invalid_argument("unknown quant_method: " + quant_method);
-    }
-    if (weight_bits_ != 4 && weight_bits_ != 8) {
-        throw std::invalid_argument("invalid weight_bits: " + std::to_string(weight_bits_));
-    }
-    if (group_size_ != 0 && group_size_ != 64 && group_size_ != 128) {
-        throw std::invalid_argument("invalid group_size: " + std::to_string(group_size_));
-    }
-}
-
-void GptInitParameter::showDebugInfo() const {
-    std::ostringstream oss;
-    oss << "\n========== ParallelismDistributedConfig ==========\n"
-        << parallelism_distributed_config.to_string() << "\n"
-        << "========== ConcurrencyConfig ==========\n"
-        << concurrency_config.to_string() << "\n"
-        << "========== FMHAConfig ==========\n"
-        << fmha_config.to_string() << "\n"
-        << "========== KVCacheConfig ==========\n"
-        << kv_cache_config.to_string() << "\n"
-        << "========== ProfilingDebugLoggingConfig ==========\n"
-        << profiling_debug_logging_config.to_string() << "\n"
-        << "========== HWKernelConfig ==========\n"
-        << hw_kernel_config.to_string() << "\n"
-        << "========== DeviceResourceConfig ==========\n"
-        << device_resource_config.to_string() << "\n"
-        << "========== MoeConfig ==========\n"
-        << moe_config.to_string() << "\n"
-        << "========== ModelSpecificConfig ==========\n"
-        << model_specific_config.to_string() << "\n"
-        << "========== SpeculativeExecutionConfig ==========\n"
-        << sp_config.to_string() << "\n"
-        << "========== ServiceDiscoveryConfig ==========\n"
-        << service_discovery_config.to_string() << "\n"
-        << "========== CacheStoreConfig ==========\n"
-        << cache_store_config.to_string() << "\n"
-        << "========== SchedulerConfig ==========\n"
-        << scheduler_config.to_string() << "\n"
-        << "========== BatchDecodeSchedulerConfig ==========\n"
-        << batch_decode_scheduler_config.to_string() << "\n"
-        << "========== FIFOSchedulerConfig ==========\n"
-        << fifo_scheduler_config.to_string() << "\n"
-        << "========== MiscellaneousConfig ==========\n"
-        << misc_config.to_string() << "\n"
-        << "========== ArpcConfig ==========\n"
-        << arpc_config.to_string() << "\n"
-        << "========== FfnDisAggregateConfig ==========\n"
-        << ffn_disaggregate_config.to_string() << "\n";
-    RTP_LLM_LOG_INFO(oss.str());
-}
-
-RopeConfig GptInitParameter::getRopeConfig() const {
-    RopeConfig rope_config;
-    rope_config.style                = (RopeStyle)rotary_embedding_style_;
-    rope_config.dim                  = rotary_embedding_dim_;
-    rope_config.base                 = rotary_embedding_base_;
-    rope_config.scale                = rotary_embedding_scale_;
-    rope_config.max_pos              = org_embedding_max_pos_;
-    rope_config.factor1              = rotary_factor1_;
-    rope_config.factor2              = rotary_factor2_;
-    rope_config.mscale               = rotary_embedding_mscale_;
-    rope_config.offset               = rotary_embedding_offset_;
-    rope_config.index_factor         = position_id_len_factor_;
-    rope_config.extrapolation_factor = rotary_embedding_extrapolation_factor_;
-    if (rope_config.style == RopeStyle::Mrope) {
-        rope_config.mrope_dim1 = mrope_section_[0];
-        rope_config.mrope_dim2 = mrope_section_[1];
-        rope_config.mrope_dim3 = mrope_section_[2];
-    }
-    return rope_config;
-}
-
-KvCacheDataType loadKvCacheDataTypeFromDataType(rtp_llm::DataType type) {
-    if (type == rtp_llm::DataType::TYPE_INT8) {
-        return KvCacheDataType::INT8;
-    } else if (type == rtp_llm::DataType::TYPE_FP8_E4M3) {
-        return KvCacheDataType::FP8;
-    } else {
-        return KvCacheDataType::BASE;
-    }
-}
-
-AttentionConfigs GptInitParameter::getAttentionConfigs() const {
-    AttentionConfigs attention_config{head_num_ > 1 ? (size_t)head_num_ / tp_size_ : 1,
-                                      head_num_kv_ > 1 ? (size_t)head_num_kv_ / tp_size_ : 1,
-                                      (size_t)size_per_head_,
-                                      (size_t)hidden_size_,
-                                      getRopeConfig(),
-                                      (size_t)seq_size_per_block_,
-                                      is_causal_ ? rtp_llm::AttentionMaskType::causalMask :
-                                                   rtp_llm::AttentionMaskType::noMask,
-                                      1.0,
-                                      // if qk_norm or use embedding model, fuse add bias in gemm
-                                      qk_norm_ || (rotary_embedding_style_ == 0 && !use_kvcache_) ? false : true,
-                                      false,
-                                      use_mla_,
-                                      (size_t)q_lora_rank_,
-                                      (size_t)kv_lora_rank_,
-                                      (size_t)nope_head_dim_,
-                                      (size_t)rope_head_dim_,
-                                      (size_t)v_head_dim_,
-                                      softmax_extra_scale_,
-                                      loadKvCacheDataTypeFromDataType(kv_cache_data_type_)};
-    return attention_config;
-}
-
-}  // namespace rtp_llm
diff --git a/rtp_llm/cpp/config/GptInitParameter.h b/rtp_llm/cpp/config/GptInitParameter.h
deleted file mode 100644
index 9934764b1..000000000
--- a/rtp_llm/cpp/config/GptInitParameter.h
+++ /dev/null
@@ -1,337 +0,0 @@
-#pragma once
-
-#include "rtp_llm/cpp/model_utils/layernorm_types.h"
-#include "rtp_llm/cpp/model_utils/activation_types.h"
-#include "rtp_llm/cpp/model_utils/quantization.h"
-#include "rtp_llm/cpp/model_utils/RopeConfig.h"
-#include "rtp_llm/cpp/model_utils/MlaConfig.h"
-#include "rtp_llm/cpp/model_utils/QuantInfo.h"
-#include "rtp_llm/cpp/model_utils/AttentionConfig.h"
-#include "rtp_llm/cpp/core/Types.h"
-#include "rtp_llm/cpp/models/eplb/EplbConfig.h"
-#include "rtp_llm/cpp/config/ConfigModules.h"
-#include <pybind11/pybind11.h>
-#include <pybind11/stl.h>
-#include <vector>
-#include <map>
-
-namespace rtp_llm {
-
-enum TaskType {
-    DENSE_EMBEDDING    = 0,
-    ALL_EMBEDDING      = 1,
-    SPARSE_EMBEDDING   = 2,
-    COLBERT_EMBEDDING  = 3,
-    LANGUAGE_MODEL     = 4,
-    SEQ_CLASSIFICATION = 5,
-    RERANKER           = 6,
-    LINEAR_SOFTMAX     = 7,
-    BGE_M3             = 8
-};
-
-enum RoleType {
-    PDFUSION = 0,
-    PREFILL  = 1,
-    DECODE   = 2,
-    VIT      = 3,
-    FRONTEND = 4
-};
-
-class RoleAddr {
-public:
-    RoleType    role;
-    std::string ip;
-    int         http_port;
-    int         grpc_port;
-
-    RoleAddr(RoleType type, std::string ip, int http_port, int grpc_port):
-        role(type), ip(ip), http_port(http_port), grpc_port(grpc_port) {}
-};
-
-struct RoleSpecialTokens {
-public:
-    std::vector<int64_t> token_ids_;
-    std::vector<int64_t> eos_token_ids_;
-};
-
-struct SpecialTokens {
-public:
-    SpecialTokens();
-    int64_t                           bos_token_id_           = -1;
-    int64_t                           eos_token_id_           = 0;
-    int64_t                           pad_token_id_           = 0;
-    int64_t                           decoder_start_token_id_ = -1;
-    RoleSpecialTokens                 user_;
-    RoleSpecialTokens                 assistant_;
-    RoleSpecialTokens                 system_;
-    std::vector<std::vector<int64_t>> stop_words_id_list_;
-    std::vector<std::string>          stop_words_str_list_;
-};
-
-class GptInitParameter {
-public:
-    // model variant params used in ft
-    int64_t head_num_               = 0;
-    int64_t head_num_kv_            = -1;
-    int64_t size_per_head_          = 0;
-    int64_t inter_size_             = 0;
-    int64_t inter_padding_size_     = -1;
-    int64_t moe_inter_padding_size_ = -1;
-    int64_t num_layers_             = 0;
-    int64_t num_valid_layer_        = 0;
-    int64_t hidden_size_            = 0;
-
-    // mla extra params
-    bool       use_mla_       = false;
-    int64_t    q_lora_rank_   = 0;
-    int64_t    kv_lora_rank_  = 0;
-    int64_t    nope_head_dim_ = 0;
-    int64_t    rope_head_dim_ = 0;
-    int64_t    v_head_dim_    = 0;
-    MlaOpsType mla_ops_type_  = MlaOpsType::AUTO;
-
-    // rope config for deepseek
-    double deepseek_rope_mscale_    = 1.0;
-    double deepseek_mscale_all_dim_ = 1.0;
-
-    // deepseek moe extra params
-    int64_t moe_n_group_    = 1;
-    int64_t moe_topk_group_ = 1;
-
-    double routed_scaling_factor_ = 1.0;  // used in deepseek v2 and glm4 moe
-
-    // in sparse, those params might vary among layers
-    bool                 is_sparse_head_           = false;
-    std::vector<int64_t> layer_head_num_           = {};
-    std::vector<int64_t> layer_head_num_kv_        = {};
-    std::vector<int64_t> layer_inter_size_         = {};
-    std::vector<int64_t> layer_inter_padding_size_ = {};
-
-    double         layernorm_eps_          = 1e-5;
-    std::string    layernorm_type_str_     = "pre_layernorm";
-    std::string    norm_type_str_          = "layernorm";
-    std::string    activation_type_str_    = "Gelu";
-    std::string    data_type_str_          = "fp16";
-    std::string    kv_cache_data_type_str_ = "fp16";
-    LayerNormType  layernorm_type_         = LayerNormType::pre_layernorm;
-    NormType       norm_type_              = NormType::layernorm;
-    TaskType       task_type_              = TaskType::LANGUAGE_MODEL;
-    ActivationType activation_type_        = ActivationType::Gelu;
-    DataType       data_type_              = DataType::TYPE_FP16;
-    DataType       kv_cache_data_type_     = DataType::TYPE_FP16;
-
-    int64_t rotary_embedding_dim_                  = 0;
-    int64_t rotary_embedding_style_                = 0;
-    int64_t position_ids_style_                    = 0;
-    float   rotary_embedding_base_                 = 10000.f;
-    double  rotary_embedding_scale_                = 1.0;
-    double  rotary_factor1_                        = 1.0;
-    double  rotary_factor2_                        = 1.0;
-    double  partial_rotary_factor_                 = 1.0;
-    int64_t org_embedding_max_pos_                 = 0;
-    double  rotary_embedding_mscale_               = 1.0;
-    int64_t rotary_embedding_offset_               = 0;
-    double  rotary_embedding_extrapolation_factor_ = 1.0;
-    // for Gemma, hidden_states = hidden_states * (hidden_size**0.5)
-    double               input_embedding_scalar_ = 1;
-    double               residual_scalar_        = 1;
-    float                softmax_extra_scale_    = 1.0f;
-    std::vector<int64_t> mrope_section_          = {};
-
-    bool   use_logn_attn_ = false;
-    double q_scaling_     = 1;
-    bool   qk_norm_       = false;
-
-    bool    use_cross_attn_       = false;
-    int64_t cross_attn_input_len_ = 0;
-
-    bool use_norm_input_residual_    = false;
-    bool use_norm_attn_out_residual_ = false;
-
-    int64_t local_rank_ = 0;
-
-    int64_t max_seq_len_                = 0;
-    int64_t max_batch_tokens_size_      = 0;
-    int64_t vocab_size_                 = 0;
-    int64_t input_vocab_size_           = 0;  // 0 if not set
-    int64_t type_vocab_size_            = 0;
-    int64_t embedding_size_             = 0;
-    int64_t expert_num_                 = 0;
-    int64_t moe_k_                      = 0;
-    bool    moe_normalize_expert_scale_ = false;
-    // 0 for no moe; 1 for all layer moe; 2 for partial layer moe
-    int64_t moe_style_ = 0;
-    // 0 for softmax; 1 for sigmoid
-    int64_t              scoring_func_    = 0;
-    std::vector<int64_t> moe_layer_index_ = {};
-
-    // EPLB
-    bool             enable_eplb_      = false;
-    int64_t          phy_exp_num_      = 0;  // number of physical experts
-    int64_t          eplb_update_time_ = 5000;
-    EplbMode         eplb_mode_        = EplbMode::NONE;
-    pybind11::object py_eplb_;
-
-    bool   has_positional_encoding_    = false;
-    bool   has_pre_decoder_layernorm_  = false;
-    bool   has_post_decoder_layernorm_ = false;
-    bool   has_lm_head_                = true;
-    bool   use_attention_linear_bias_  = false;
-    bool   use_fp32_to_compute_logit_  = false;
-    bool   add_bias_linear_            = false;
-    bool   has_moe_norm_               = false;
-    double logit_scale_                = 1.0;
-    bool   is_causal_                  = true;
-    bool   use_kvcache_                = true;
-
-    std::string tokenizer_path_    = "";
-    std::string ckpt_path_         = "";
-    int64_t     pre_seq_len_       = 0;
-    bool        prefix_projection_ = false;
-    bool        using_hf_sampling_ = false;
-
-    SpecialTokens special_tokens_;
-    QuantAlgo     quant_algo_;
-
-    // async mode config
-    int64_t max_generate_batch_size_ = 1;
-    int64_t max_context_batch_size_  = 1;
-    int64_t gen_num_per_circle_      = 1;
-
-    bool                              is_multimodal_          = false;
-    std::vector<std::vector<int64_t>> mm_sep_tokens_          = {};
-    bool                              include_sep_tokens_     = false;
-    int64_t                           mm_position_ids_style_  = 0;  // 0 for default; 1 for chatglm4v; 2 for qwen2 vl
-    int64_t                           position_id_len_factor_ = 1;
-
-    bool    pre_allocate_op_mem_     = true;
-    int64_t seq_size_per_block_      = 8;
-    int64_t max_block_size_per_item_ = 16;
-
-    int64_t block_nums_                       = 0;
-    int64_t scheduler_reserve_resource_ratio_ = 5;
-    int64_t reserve_runtime_mem_mb_           = 0;
-    int64_t kv_cache_mem_mb_                  = 0;
-    bool    reuse_cache_                      = false;
-    bool    enable_partial_fallback_          = false;
-    bool    enable_fast_gen_                  = false;
-    bool    warm_up_                          = false;
-    bool    warm_up_with_loss_                = false;
-    int64_t fast_gen_max_context_len_         = 0;
-    bool    reverse_e_h_norm_                 = false;
-
-    std::string nccl_ip_          = "";
-    bool        use_all_gather_   = false;
-    int64_t     tp_nccl_port_     = 0;
-    int64_t     dp_tp_nccl_port_  = 0;
-    int64_t     ffn_tp_nccl_port_ = 0;
-    int64_t     http_port_        = 0;
-    int64_t     model_rpc_port_   = 0;
-    int64_t     tp_size_          = 1;
-    int64_t     tp_rank_          = 0;
-    int64_t     ep_size_          = 1;
-    int64_t     ep_rank_          = 0;
-    int64_t     dp_size_          = 1;
-    int64_t     dp_rank_          = 0;
-    int64_t     ffn_tp_size_      = 1;
-    int64_t     ffn_tp_rank_      = 0;
-    bool        enable_sp_        = false;
-
-    int64_t world_size_ = 1;
-
-    // pd speration
-    RoleType role_type_                       = RoleType::PDFUSION;
-    bool     cache_store_rdma_mode_           = true;
-    int64_t  cache_store_listen_port_         = 0;
-    int64_t  cache_store_connect_port_        = 0;
-    int64_t  cache_store_rdma_listen_port_    = 0;
-    int64_t  cache_store_rdma_connect_port_   = 0;
-    int64_t  remote_rpc_server_port_          = 0;
-    int64_t  prefill_retry_times_             = 0;
-    int64_t  prefill_retry_timeout_ms_        = 0;
-    int64_t  prefill_max_wait_timeout_ms_     = 0;
-    int64_t  decode_retry_times_              = 0;
-    int64_t  decode_retry_timeout_ms_         = 0;
-    int64_t  decode_polling_kv_cache_step_ms_ = 0;
-    int64_t  decode_polling_call_prefill_ms_  = 0;
-    int64_t  rdma_connect_retry_times_        = 0;
-    int64_t  load_cache_timeout_ms_           = 0;
-    int64_t  max_rpc_timeout_ms_              = 0;
-    int64_t  worker_port_offset_              = 0;
-    bool     decode_entrance_                 = false;
-
-    std::map<std::string, std::vector<int>> multi_task_prompt_tokens_;
-
-    // 0 for no sep, 1 for server, 2 for client
-    int64_t vit_separation_              = 0;
-    bool    enable_speculative_decoding_ = false;
-
-    std::string model_name_ = "";
-
-    // multi machine
-    std::vector<std::string> worker_addrs_;
-    std::vector<std::string> worker_grpc_addrs_;
-    // GlobalConfig replication
-
-    ParallelismDistributedConfig parallelism_distributed_config;
-    ConcurrencyConfig            concurrency_config;
-    FMHAConfig                   fmha_config;
-    KVCacheConfig                kv_cache_config;
-    ProfilingDebugLoggingConfig  profiling_debug_logging_config;
-    HWKernelConfig               hw_kernel_config;
-    DeviceResourceConfig         device_resource_config;
-    MoeConfig                    moe_config;
-    ModelSpecificConfig          model_specific_config;
-    SpeculativeExecutionConfig   sp_config;
-    ServiceDiscoveryConfig       service_discovery_config;
-    CacheStoreConfig             cache_store_config;
-    SchedulerConfig              scheduler_config;
-    BatchDecodeSchedulerConfig   batch_decode_scheduler_config;
-    FIFOSchedulerConfig          fifo_scheduler_config;
-    MiscellaneousConfig          misc_config;
-    ArpcConfig                   arpc_config;
-    FfnDisAggregateConfig        ffn_disaggregate_config;
-
-    GptInitParameter();
-
-    GptInitParameter(int64_t head_num,
-                     int64_t size_per_head,
-                     int64_t num_layers,
-                     int64_t max_seq_len,
-                     int64_t vocab_size,
-                     int64_t hidden_size);
-
-    void update_from_env_for_test() {
-        parallelism_distributed_config.update_from_env_for_test();
-        concurrency_config.update_from_env_for_test();
-        fmha_config.update_from_env_for_test();
-        kv_cache_config.update_from_env_for_test();
-        profiling_debug_logging_config.update_from_env_for_test();
-        hw_kernel_config.update_from_env_for_test();
-        device_resource_config.update_from_env_for_test();
-        moe_config.update_from_env_for_test();
-        model_specific_config.update_from_env_for_test();
-        sp_config.update_from_env_for_test();
-        service_discovery_config.update_from_env_for_test();
-        cache_store_config.update_from_env_for_test();
-        scheduler_config.update_from_env_for_test();
-        batch_decode_scheduler_config.update_from_env_for_test();
-        fifo_scheduler_config.update_from_env_for_test();
-        misc_config.update_from_env_for_test();
-    }
-
-    void             showDebugInfo() const;
-    void             insertMultiTaskPromptTokens(std::string task_id, std::vector<int64_t> tokens_id);
-    void             setLayerNormType();
-    void             setNormType();
-    void             setTaskType(std::string task);
-    void             setActivationType();
-    void             setDataType();
-    void             setKvCacheDataType();
-    bool             isGatedActivation() const;
-    RopeConfig       getRopeConfig() const;
-    AttentionConfigs getAttentionConfigs() const;
-    bool             isKvCacheQuant() const;
-};
-
-}  // namespace rtp_llm
diff --git a/rtp_llm/cpp/config/ModelConfig.cc b/rtp_llm/cpp/config/ModelConfig.cc
new file mode 100644
index 000000000..7eeedf6fd
--- /dev/null
+++ b/rtp_llm/cpp/config/ModelConfig.cc
@@ -0,0 +1,288 @@
+#include "rtp_llm/cpp/config/ModelConfig.h"
+#include "autil/Log.h"
+#include "rtp_llm/cpp/model_utils/layernorm_types.h"
+#include "rtp_llm/cpp/model_utils/activation_types.h"
+#include "rtp_llm/cpp/core/Types.h"
+#include <sstream>
+#include <string>
+
+namespace rtp_llm {
+
+// Helper functions to convert enums to strings
+static std::string taskTypeToString(TaskType task_type) {
+    switch (task_type) {
+        case TaskType::DENSE_EMBEDDING:
+            return "DENSE_EMBEDDING";
+        case TaskType::ALL_EMBEDDING:
+            return "ALL_EMBEDDING";
+        case TaskType::SPARSE_EMBEDDING:
+            return "SPARSE_EMBEDDING";
+        case TaskType::COLBERT_EMBEDDING:
+            return "COLBERT_EMBEDDING";
+        case TaskType::LANGUAGE_MODEL:
+            return "LANGUAGE_MODEL";
+        case TaskType::SEQ_CLASSIFICATION:
+            return "SEQ_CLASSIFICATION";
+        case TaskType::RERANKER:
+            return "RERANKER";
+        case TaskType::LINEAR_SOFTMAX:
+            return "LINEAR_SOFTMAX";
+        case TaskType::BGE_M3:
+            return "BGE_M3";
+        default:
+            return "UNKNOWN(" + std::to_string(static_cast<int>(task_type)) + ")";
+    }
+}
+
+static std::string mlaOpsTypeToString(MlaOpsType mla_ops_type) {
+    switch (mla_ops_type) {
+        case MlaOpsType::AUTO:
+            return "AUTO";
+        case MlaOpsType::MHA:
+            return "MHA";
+        case MlaOpsType::FLASH_INFER:
+            return "FLASH_INFER";
+        case MlaOpsType::FLASH_MLA:
+            return "FLASH_MLA";
+        default:
+            return "UNKNOWN(" + std::to_string(static_cast<int>(mla_ops_type)) + ")";
+    }
+}
+
+static std::string quantMethodToString(QuantMethod quant_method) {
+    switch (quant_method) {
+        case QuantMethod::None:
+            return "None";
+        case QuantMethod::WeightOnlyPerCol:
+            return "WeightOnlyPerCol";
+        case QuantMethod::GptQ:
+            return "GptQ";
+        case QuantMethod::Awq:
+            return "Awq";
+        case QuantMethod::SmoothQuant:
+            return "SmoothQuant";
+        case QuantMethod::OmniQuant:
+            return "OmniQuant";
+        case QuantMethod::PerTensorQuant:
+            return "PerTensorQuant";
+        case QuantMethod::FP8Quant:
+            return "FP8Quant";
+        case QuantMethod::FP8PTPC:
+            return "FP8PTPC";
+        default:
+            return "UNKNOWN(" + std::to_string(static_cast<int>(quant_method)) + ")";
+    }
+}
+
+// Setter methods with validation (throw exception on invalid input)
+void ModelConfig::set_layer_norm_type(std::string layernorm_type_str) {
+    try {
+        layernorm_type = getLayerNormType(layernorm_type_str);
+    } catch (...) {
+        throw std::runtime_error("Invalid layernorm_type: " + layernorm_type_str);
+    }
+}
+
+void ModelConfig::set_norm_type(std::string norm_type_str) {
+    try {
+        norm_type = getNormType(norm_type_str);
+    } catch (...) {
+        throw std::runtime_error("Invalid norm_type: " + norm_type_str);
+    }
+}
+
+void ModelConfig::set_task_type(std::string task) {
+    if (task == "DENSE_EMBEDDING") {
+        task_type = TaskType::DENSE_EMBEDDING;
+    } else if (task == "ALL_EMBEDDING") {
+        task_type = TaskType::ALL_EMBEDDING;
+    } else if (task == "SPARSE_EMBEDDING") {
+        task_type = TaskType::SPARSE_EMBEDDING;
+    } else if (task == "COLBERT_EMBEDDING") {
+        task_type = TaskType::COLBERT_EMBEDDING;
+    } else if (task == "LANGUAGE_MODEL") {
+        task_type = TaskType::LANGUAGE_MODEL;
+    } else if (task == "SEQ_CLASSIFICATION") {
+        task_type = TaskType::SEQ_CLASSIFICATION;
+    } else if (task == "RERANKER") {
+        task_type = TaskType::RERANKER;
+    } else if (task == "LINEAR_SOFTMAX") {
+        task_type = TaskType::LINEAR_SOFTMAX;
+    } else if (task == "BGE_M3") {
+        task_type = TaskType::BGE_M3;
+    } else {
+        throw std::runtime_error("Invalid task_type: " + task);
+    }
+}
+
+void ModelConfig::set_activation_type(std::string activation_type_str) {
+    try {
+        activation_type = getActivationType(activation_type_str);
+    } catch (...) {
+        throw std::runtime_error("Invalid activation_type: " + activation_type_str);
+    }
+}
+
+void ModelConfig::set_data_type(std::string data_type_str) {
+    try {
+        data_type = getDataType(data_type_str);
+    } catch (const std::runtime_error& e) {
+        throw std::runtime_error("Invalid data_type: " + data_type_str + " - " + e.what());
+    }
+}
+
+void ModelConfig::set_kv_cache_data_type(std::string kv_cache_data_type_str) {
+    try {
+        DataType dtype = getDataType(kv_cache_data_type_str);
+        if (dtype == DataType::TYPE_INT8) {
+            attn_config.kv_cache_dtype = KvCacheDataType::INT8;
+        } else if (dtype == DataType::TYPE_FP8_E4M3) {
+            attn_config.kv_cache_dtype = KvCacheDataType::FP8;
+        } else {
+            attn_config.kv_cache_dtype = KvCacheDataType::BASE;
+        }
+    } catch (const std::runtime_error& e) {
+        throw std::runtime_error("Invalid kv_cache_data_type: " + kv_cache_data_type_str + " - " + e.what());
+    }
+}
+
+void ModelConfig::set_mla_ops_type(std::string mla_ops_type_str) {
+    try {
+        mla_ops_type = getMlaOpsType(mla_ops_type_str);
+    } catch (...) {
+        throw std::runtime_error("Invalid mla_ops_type: " + mla_ops_type_str);
+    }
+}
+
+bool ModelConfig::isGatedActivation() const {
+    return rtp_llm::isGatedActivation(activation_type);
+}
+
+bool ModelConfig::isKvCacheQuant() const {
+    return attn_config.kv_cache_dtype == KvCacheDataType::FP8 || 
+           attn_config.kv_cache_dtype == KvCacheDataType::INT8;
+}
+
+AttentionConfigs ModelConfig::getAttentionConfigs(int64_t tp_size) const {
+    AttentionConfigs config = attn_config;
+
+    config.head_num = config.head_num / tp_size;
+    config.kv_head_num = config.kv_head_num / tp_size;
+
+    // if qk_norm or use embedding model, fuse add bias in gemm
+    config.fuse_qkv_add_bias = qk_norm || (config.rope_config.style == RopeStyle::No && !use_kvcache) ? false : true;
+    return config;
+}
+
+std::string ModelConfig::to_string() const {
+    std::ostringstream oss;
+    
+    // Model variant params
+    oss << "inter_size: " << inter_size << "\n"
+        << "inter_padding_size: " << inter_padding_size << "\n"
+        << "moe_inter_padding_size: " << moe_inter_padding_size << "\n"
+        << "num_layers: " << num_layers << "\n"
+        << "hidden_size: " << hidden_size << "\n"
+        << "attn_config: {\n" << attn_config.DebugAttentionConfigStr() << "\n}\n"
+        << "mla_ops_type: " << mlaOpsTypeToString(mla_ops_type) << "\n"
+        << "deepseek_rope_mscale: " << deepseek_rope_mscale << "\n"
+        << "deepseek_mscale_all_dim: " << deepseek_mscale_all_dim << "\n"
+        << "moe_n_group: " << moe_n_group << "\n"
+        << "moe_topk_group: " << moe_topk_group << "\n"
+        << "routed_scaling_factor: " << routed_scaling_factor << "\n"
+        << "layernorm_eps: " << layernorm_eps << "\n"
+        << "layernorm_type: " << getLayerNormTypeStr(layernorm_type) << "\n"
+        << "norm_type: " << getNormTypeStr(norm_type) << "\n"
+        << "task_type: " << taskTypeToString(task_type) << "\n"
+        << "activation_type: " << getActivationTypeStr(activation_type) << "\n"
+        << "data_type: " << getDataTypeStr(data_type) << "\n"
+        << "position_ids_style: " << position_ids_style << "\n"
+        << "partial_rotary_factor: " << partial_rotary_factor << "\n"
+        << "input_embedding_scalar: " << input_embedding_scalar << "\n"
+        << "residual_scalar: " << residual_scalar << "\n"
+        << "qk_norm: " << qk_norm << "\n"
+        << "use_norm_input_residual: " << use_norm_input_residual << "\n"
+        << "use_norm_attn_out_residual: " << use_norm_attn_out_residual << "\n"
+        << "max_seq_len: " << max_seq_len << "\n"
+        << "vocab_size: " << vocab_size << "\n"
+        << "input_vocab_size: " << input_vocab_size << "\n"
+        << "type_vocab_size: " << type_vocab_size << "\n"
+        << "embedding_size: " << embedding_size << "\n"
+        << "expert_num: " << expert_num << "\n"
+        << "moe_k: " << moe_k << "\n"
+        << "moe_normalize_expert_scale: " << moe_normalize_expert_scale << "\n"
+        << "moe_style: " << moe_style << "\n"
+        << "scoring_func: " << scoring_func << "\n"
+        << "moe_layer_index: [";
+    for (size_t i = 0; i < moe_layer_index.size(); ++i) {
+        oss << moe_layer_index[i];
+        if (i < moe_layer_index.size() - 1) oss << ", ";
+    }
+    oss << "]\n"
+        << "has_positional_encoding: " << has_positional_encoding << "\n"
+        << "has_pre_decoder_layernorm: " << has_pre_decoder_layernorm << "\n"
+        << "has_post_decoder_layernorm: " << has_post_decoder_layernorm << "\n"
+        << "has_lm_head: " << has_lm_head << "\n"
+        << "use_attention_linear_bias: " << use_attention_linear_bias << "\n"
+        << "use_fp32_to_compute_logit: " << use_fp32_to_compute_logit << "\n"
+        << "add_bias_linear: " << add_bias_linear << "\n"
+        << "has_moe_norm: " << has_moe_norm << "\n"
+        << "logit_scale: " << logit_scale << "\n"
+        << "use_kvcache: " << use_kvcache << "\n"
+        << "pre_seq_len: " << pre_seq_len << "\n"
+        << "prefix_projection: " << prefix_projection << "\n"
+        << "reverse_e_h_norm: " << reverse_e_h_norm << "\n"
+        << "tokenizer_path: " << tokenizer_path << "\n"
+        << "ckpt_path: " << ckpt_path << "\n"
+        << "lora_infos: {";
+    bool first = true;
+    for (const auto& pair : lora_infos) {
+        if (!first) oss << ", ";
+        oss << pair.first << ": " << pair.second;
+        first = false;
+    }
+    oss << "}\n"
+        << "special_tokens: {\n"
+        << "  bos_token_id: " << special_tokens.bos_token_id << "\n"
+        << "  eos_token_id: " << special_tokens.eos_token_id << "\n"
+        << "  pad_token_id: " << special_tokens.pad_token_id << "\n"
+        << "  decoder_start_token_id: " << special_tokens.decoder_start_token_id << "\n"
+        << "  stop_words_id_list: [";
+    for (size_t i = 0; i < special_tokens.stop_words_id_list.size(); ++i) {
+        oss << "[";
+        for (size_t j = 0; j < special_tokens.stop_words_id_list[i].size(); ++j) {
+            oss << special_tokens.stop_words_id_list[i][j];
+            if (j < special_tokens.stop_words_id_list[i].size() - 1) oss << ", ";
+        }
+        oss << "]";
+        if (i < special_tokens.stop_words_id_list.size() - 1) oss << ", ";
+    }
+    oss << "]\n"
+        << "  stop_words_str_list: [";
+    for (size_t i = 0; i < special_tokens.stop_words_str_list.size(); ++i) {
+        oss << "\"" << special_tokens.stop_words_str_list[i] << "\"";
+        if (i < special_tokens.stop_words_str_list.size() - 1) oss << ", ";
+    }
+    oss << "]\n}\n"
+        << "quant_algo: {\n"
+        << "  method: " << quantMethodToString(quant_algo.getQuantMethod()) << "\n"
+        << "  bits: " << quant_algo.getWeightBits() << "\n"
+        << "  group_size: " << quant_algo.getGroupSize() << "\n}\n"
+        << "eplb_config: {\n" << eplb_config.to_string() << "\n}\n"
+        << "extra_data_path: " << extra_data_path << "\n"
+        << "local_extra_data_path: " << local_extra_data_path << "\n"
+        << "act_type: " << act_type << "\n"
+        << "use_float32: " << use_float32 << "\n"
+        << "original_checkpoint_path: " << original_checkpoint_path << "\n"
+        << "ft_plugin_path: " << ft_plugin_path << "\n"
+        << "model_type: " << model_type << "\n"
+        << "ptuning_path: " << ptuning_path << "\n"
+        << "json_model_override_args: " << json_model_override_args;
+    
+    return oss.str();
+}
+
+}  // namespace rtp_llm
+
+
+
diff --git a/rtp_llm/cpp/config/ModelConfig.h b/rtp_llm/cpp/config/ModelConfig.h
new file mode 100644
index 000000000..148d90301
--- /dev/null
+++ b/rtp_llm/cpp/config/ModelConfig.h
@@ -0,0 +1,156 @@
+#pragma once
+
+#include "rtp_llm/cpp/model_utils/layernorm_types.h"
+#include "rtp_llm/cpp/model_utils/activation_types.h"
+#include "rtp_llm/cpp/model_utils/quantization.h"
+#include "rtp_llm/cpp/model_utils/RopeConfig.h"
+#include "rtp_llm/cpp/model_utils/MlaConfig.h"
+#include "rtp_llm/cpp/model_utils/QuantInfo.h"
+#include "rtp_llm/cpp/model_utils/AttentionConfig.h"
+#include "rtp_llm/cpp/core/Types.h"
+#include "rtp_llm/cpp/config/EplbConfig.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/config/SpecialTokens.h"
+#include <vector>
+#include <string>
+#include <map>
+
+namespace rtp_llm {
+
+enum TaskType {
+    DENSE_EMBEDDING    = 0,
+    ALL_EMBEDDING      = 1,
+    SPARSE_EMBEDDING   = 2,
+    COLBERT_EMBEDDING  = 3,
+    LANGUAGE_MODEL     = 4,
+    SEQ_CLASSIFICATION = 5,
+    RERANKER           = 6,
+    LINEAR_SOFTMAX     = 7,
+    BGE_M3             = 8
+};
+
+class MMModelConfig {
+public:
+    bool                              is_multimodal          = false;
+    std::vector<std::vector<int64_t>> mm_sep_tokens          = {};
+    bool                              include_sep_tokens     = false;
+    int64_t                           mm_position_ids_style  = 0;  // 0 for default; 1 for chatglm4v; 2 for qwen2 vl
+};
+    
+class ModelConfig {
+public:
+    // model variant params used in ft
+    int64_t inter_size             = 0;
+    int64_t inter_padding_size     = -1;
+    int64_t moe_inter_padding_size = -1;
+    int64_t num_layers             = 0;
+    int64_t hidden_size            = 0;
+
+    // Attention configuration - contains all attention-related params
+    AttentionConfigs attn_config;
+
+    // mla ops type (not in AttentionConfigs)
+    MlaOpsType mla_ops_type  = MlaOpsType::AUTO;
+
+    // rope config for deepseek
+    double deepseek_rope_mscale    = 1.0;
+    double deepseek_mscale_all_dim = 1.0;
+
+    // deepseek moe extra params
+    int64_t moe_n_group    = 1;
+    int64_t moe_topk_group = 1;
+
+    double routed_scaling_factor = 1.0;  // used in deepseek v2 and glm4 moe
+
+    double         layernorm_eps          = 1e-5;
+    LayerNormType  layernorm_type         = LayerNormType::pre_layernorm;
+    NormType       norm_type              = NormType::rmsnorm;
+    TaskType       task_type              = TaskType::LANGUAGE_MODEL;
+    ActivationType activation_type        = ActivationType::Swiglu;
+    DataType       data_type              = DataType::TYPE_FP16;
+
+    int64_t        position_ids_style     = 0;
+    double         partial_rotary_factor  = 1.0;
+    // for Gemma, hidden_states = hidden_states * (hidden_size**0.5)
+    double               input_embedding_scalar = 1;
+    double               residual_scalar        = 1;
+
+    bool   qk_norm       = false;
+
+    bool use_norm_input_residual    = false;
+    bool use_norm_attn_out_residual = false;
+
+    int64_t max_seq_len                = 0;
+    int64_t vocab_size                 = 0;
+    int64_t input_vocab_size           = 0;  // 0 if not set
+    int64_t type_vocab_size            = 0;
+    int64_t embedding_size             = 0;
+    int64_t expert_num                 = 0;
+    int64_t moe_k                      = 0;
+    bool    moe_normalize_expert_scale = false;
+    // 0 for no moe; 1 for all layer moe; 2 for partial layer moe
+    int64_t moe_style = 0;
+    // 0 for softmax; 1 for sigmoid
+    int64_t              scoring_func    = 0;
+    std::vector<int64_t> moe_layer_index = {};
+
+    bool   has_positional_encoding    = false;
+    bool   has_pre_decoder_layernorm  = false;
+    bool   has_post_decoder_layernorm = true;
+    bool   has_lm_head                = true;
+    bool   use_attention_linear_bias  = false;
+    bool   use_fp32_to_compute_logit  = false;
+    bool   add_bias_linear            = false;
+    bool   has_moe_norm               = false;
+    double logit_scale                = 1.0;
+    bool   use_kvcache                = true;
+
+    int64_t pre_seq_len       = 0;
+    bool    prefix_projection = false;
+
+    bool reverse_e_h_norm = false;
+
+    // Model loading and quantization
+    std::string tokenizer_path    = "";
+    std::string ckpt_path         = "";
+    std::map<std::string, std::string> lora_infos = {};  // Map of lora name to path
+    SpecialTokens special_tokens;
+    QuantAlgo     quant_algo;
+
+    // EPLB configuration
+    EPLBConfig eplb_config;
+
+    // Multimodal model configuration
+    MMModelConfig mm_model_config;
+
+    // Fields merged from PyModelConfig
+    std::string extra_data_path            = "";
+    std::string local_extra_data_path      = "";
+    std::string act_type                   = "";
+    bool        use_float32                = false;
+    std::string original_checkpoint_path   = "";
+    std::string ft_plugin_path             = "";
+    std::string model_type                 = "";
+    std::string ptuning_path               = "";
+    std::string json_model_override_args   = "{}";
+
+    ModelConfig() = default;
+
+    // Setter methods with validation (throw exception on invalid input)
+    void             set_layer_norm_type(std::string layernorm_type_str);
+    void             set_norm_type(std::string norm_type_str);
+    void             set_task_type(std::string task);
+    void             set_activation_type(std::string activation_type_str);
+    void             set_data_type(std::string data_type_str);
+    void             set_kv_cache_data_type(std::string kv_cache_data_type_str);
+    void             set_mla_ops_type(std::string mla_ops_type_str);
+    bool             isGatedActivation() const;
+    AttentionConfigs getAttentionConfigs(int64_t tp_size) const;
+    bool             isKvCacheQuant() const;
+    std::string      to_string() const;
+};
+
+}  // namespace rtp_llm
+
+
+
diff --git a/rtp_llm/cpp/config/RoleTypes.h b/rtp_llm/cpp/config/RoleTypes.h
new file mode 100644
index 000000000..a3ee7cfa7
--- /dev/null
+++ b/rtp_llm/cpp/config/RoleTypes.h
@@ -0,0 +1,32 @@
+#pragma once
+#include <string>
+
+namespace rtp_llm {
+
+enum RoleType {
+    PDFUSION = 0,
+    PREFILL  = 1,
+    DECODE   = 2,
+    VIT      = 3,
+    FRONTEND = 4
+};
+
+enum VitSeparation {
+    VIT_SEPARATION_LOCAL  = 0,  // Local multimodal processing
+    VIT_SEPARATION_ROLE   = 1,  // VIT role (separated VIT process)
+    VIT_SEPARATION_REMOTE = 2   // Remote multimodal processing
+};
+
+class RoleAddr {
+public:
+    RoleType    role;
+    std::string ip;
+    int         http_port;
+    int         grpc_port;
+
+    RoleAddr(RoleType type, std::string ip, int http_port, int grpc_port):
+        role(type), ip(ip), http_port(http_port), grpc_port(grpc_port) {}
+};
+
+}  // namespace rtp_llm
+
diff --git a/rtp_llm/cpp/config/SpecialTokens.h b/rtp_llm/cpp/config/SpecialTokens.h
new file mode 100644
index 000000000..666199dee
--- /dev/null
+++ b/rtp_llm/cpp/config/SpecialTokens.h
@@ -0,0 +1,24 @@
+#pragma once
+#include <vector>
+#include <string>
+
+namespace rtp_llm {
+
+struct RoleSpecialTokens {
+    std::vector<int64_t> token_ids;
+    std::vector<int64_t> eos_token_ids;
+};
+
+struct SpecialTokens {
+    int64_t                           bos_token_id           = -1;
+    int64_t                           eos_token_id           = 0;
+    int64_t                           pad_token_id           = 0;
+    int64_t                           decoder_start_token_id = -1;
+    RoleSpecialTokens                 user;
+    RoleSpecialTokens                 assistant;
+    RoleSpecialTokens                 system;
+    std::vector<std::vector<int64_t>> stop_words_id_list;
+    std::vector<std::string>          stop_words_str_list;
+};
+
+}
\ No newline at end of file
diff --git a/rtp_llm/cpp/core/BUILD b/rtp_llm/cpp/core/BUILD
index 16f6488f7..60d35456b 100644
--- a/rtp_llm/cpp/core/BUILD
+++ b/rtp_llm/cpp/core/BUILD
@@ -162,6 +162,7 @@ cc_library(
     deps = [
         "//rtp_llm/cpp/core:allocator",
         "//rtp_llm/cpp/devices:devices_base",
+        "//rtp_llm/cpp/config:model_config",
     ] + torch_deps(),
     include_prefix = "src",
     visibility = ["//visibility:public"],
diff --git a/rtp_llm/cpp/core/Types.h b/rtp_llm/cpp/core/Types.h
index 2bcbec3c5..0225e4c74 100644
--- a/rtp_llm/cpp/core/Types.h
+++ b/rtp_llm/cpp/core/Types.h
@@ -62,17 +62,20 @@ inline DataType getDataType(const std::string& type_str) {
 }
 
 inline std::string getDataTypeStr(const DataType& data_type) {
-    std::string type_str;
-    if (data_type == TYPE_FP16) {
-        type_str = "fp16";
-    } else if (data_type == TYPE_BF16) {
-        type_str = "bf16";
-    } else if (data_type == TYPE_FP32) {
-        type_str = "fp32";
-    } else {
-        throw std::runtime_error("wrong data type");
+    switch (data_type) {
+        case TYPE_FP16:
+            return "fp16";
+        case TYPE_BF16:
+            return "bf16";
+        case TYPE_FP32:
+            return "fp32";
+        case TYPE_INT8:
+            return "int8";
+        case TYPE_FP8_E4M3:
+            return "fp8";
+        default:
+            throw std::runtime_error("Invalid DataType: " + std::to_string(static_cast<int>(data_type)));
     }
-    return type_str;
 }
 
 template<typename T>
diff --git a/rtp_llm/cpp/cuda/BUILD b/rtp_llm/cpp/cuda/BUILD
index db42f01e5..e41fd16a0 100644
--- a/rtp_llm/cpp/cuda/BUILD
+++ b/rtp_llm/cpp/cuda/BUILD
@@ -104,6 +104,8 @@ cc_library(
         "//rtp_llm/cpp/core:types",
         "//rtp_llm/cpp/core:types_hdr",
         "//rtp_llm/cpp/config:config_modules",
+        "//rtp_llm/cpp/config:model_config",
+        "//rtp_llm/cpp/config:special_tokens",
         "//rtp_llm/cpp/kernels:kernels_tensor_ops",
         "@local_config_cuda//cuda:cuda_headers",
         "@local_config_cuda//cuda:cudart",
diff --git a/rtp_llm/cpp/cuda/cuda_fmha_utils.h b/rtp_llm/cpp/cuda/cuda_fmha_utils.h
index 21216efdd..6c600ae43 100644
--- a/rtp_llm/cpp/cuda/cuda_fmha_utils.h
+++ b/rtp_llm/cpp/cuda/cuda_fmha_utils.h
@@ -4,8 +4,8 @@
 #include "rtp_llm/cpp/utils/Logger.h"
 #include "rtp_llm/cpp/cuda/cuda_host_utils.h"
 #include "rtp_llm/cpp/core/Types.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
 #include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 #include "3rdparty/trt_fused_multihead_attention/qkvToContext.h"
 #include "3rdparty/contextFusedMultiHeadAttention/fmhaRunner.h"
 #include <cublasLt.h>
@@ -25,23 +25,23 @@ namespace rtp_llm {
 class CudaFmhaUtils {
 public:
     template<typename T>
-    static bool UseTrtFMHA(const rtp_llm::GptInitParameter& gpt_init_parameter) {
-        bool use_trt_fmha = CheckUseFMHA<T>(gpt_init_parameter) && CheckQKVLengthEqual<T>(gpt_init_parameter);
+    static bool UseTrtFMHA(const FMHAConfig& fmha_config,
+                          const ModelConfig& model_config,
+                          const RuntimeConfig& runtime_config,
+                          const KVCacheConfig& kv_cache_config,
+                          const SpeculativeExecutionConfig& sp_config) {
+        bool use_trt_fmha = CheckUseFMHA<T>(fmha_config) && CheckQKVLengthEqual<T>(model_config, runtime_config, kv_cache_config, sp_config);
         if (!(is_sm8x() || is_sm90() || is_sm70())) {
             RTP_LLM_LOG_INFO("TRT FMHA is disabled for sm %d", get_sm());
             use_trt_fmha = false;
         }
-        if (gpt_init_parameter.is_sparse_head_) {
-            RTP_LLM_LOG_INFO("TRT FMHA is disabled for sparse");
-            use_trt_fmha = false;
-        }
 
-        bool fmha_env = gpt_init_parameter.fmha_config.enable_trt_fmha;
+        bool fmha_env = fmha_config.enable_trt_fmha;
         if (!fmha_env) {
             RTP_LLM_LOG_INFO("TRT FMHA is disabled for by env");
             use_trt_fmha = false;
         }
-        if (!tensorrt_llm::kernels::MHARunner::fmha_supported(gpt_init_parameter.size_per_head_, rtp_llm::get_sm())) {
+        if (!tensorrt_llm::kernels::MHARunner::fmha_supported(model_config.attn_config.size_per_head, rtp_llm::get_sm())) {
             RTP_LLM_LOG_INFO("TRT FMHA is disabled for by check fmha_supported");
             use_trt_fmha = false;
         }
@@ -49,9 +49,13 @@ public:
     }
 
     template<typename T>
-    static bool UseOldTrtFMHA(const rtp_llm::GptInitParameter& gpt_init_parameter) {
+    static bool UseOldTrtFMHA(const FMHAConfig& fmha_config,
+                             const ModelConfig& model_config,
+                             const RuntimeConfig& runtime_config,
+                             const KVCacheConfig& kv_cache_config,
+                             const SpeculativeExecutionConfig& sp_config) {
 #ifdef USE_OLD_TRT_FMHA
-        bool use_old_trt_fmha = CheckUseFMHA<T>(gpt_init_parameter) && CheckQKVLengthEqual<T>(gpt_init_parameter);
+        bool use_old_trt_fmha = CheckUseFMHA<T>(fmha_config) && CheckQKVLengthEqual<T>(model_config, runtime_config, kv_cache_config, sp_config);
         if (!use_old_trt_fmha) {
             return false;
         }
@@ -59,13 +63,13 @@ public:
             RTP_LLM_LOG_INFO("OLD TRT FMHA only support half");
             return false;
         }
-        if (gpt_init_parameter.head_num_ != gpt_init_parameter.head_num_kv_) {
+        if (model_config.attn_config.head_num != model_config.attn_config.kv_head_num) {
             RTP_LLM_LOG_INFO("OLD TRT not support head_num != head_num_kv");
             return false;
         }
         auto testRunner = FusedMHARunnerFP16v2(
-            gpt_init_parameter.head_num_, gpt_init_parameter.size_per_head_, get_sm(), gpt_init_parameter.q_scaling_);
-        if (!testRunner.fmha_supported(gpt_init_parameter.is_causal_)) {
+            model_config.attn_config.head_num, model_config.attn_config.size_per_head, get_sm(), model_config.attn_config.q_scaling);
+        if (!testRunner.fmha_supported(model_config.attn_config.is_causal)) {
             RTP_LLM_LOG_INFO("OLD TRT disabled by call fmha_supported");
             return false;
         }
@@ -77,25 +81,22 @@ public:
     }
 
     template<typename T>
-    static bool UsePagedTrtFMHA(const rtp_llm::GptInitParameter& gpt_init_parameter) {
-        bool use_paged_trt_fmha = CheckUseFMHA<T>(gpt_init_parameter);
+    static bool UsePagedTrtFMHA(const FMHAConfig& fmha_config,
+                               const ModelConfig& model_config) {
+        bool use_paged_trt_fmha = CheckUseFMHA<T>(fmha_config);
         if (!(is_sm8x() || is_sm90())) {
             RTP_LLM_LOG_INFO("Paged TRT FMHA is disabled for sm %d", get_sm());
             use_paged_trt_fmha = false;
         }
-        if (!gpt_init_parameter.use_kvcache_) {
+        if (!model_config.use_kvcache) {
             RTP_LLM_LOG_INFO("Paged TRT FMHA is disabled when not use kvcache");
             use_paged_trt_fmha = false;
         }
-        if (gpt_init_parameter.is_sparse_head_) {
-            RTP_LLM_LOG_INFO("Paged TRT FMHA is disabled for sparse");
-            use_paged_trt_fmha = false;
-        }
-        if (gpt_init_parameter.isKvCacheQuant()) {
+        if (model_config.isKvCacheQuant()) {
             RTP_LLM_LOG_INFO("Paged TRT FMHA is disabled for int8 kvcache");
             use_paged_trt_fmha = false;
         }
-        bool paged_fmha_env = gpt_init_parameter.fmha_config.enable_paged_trt_fmha;
+        bool paged_fmha_env = fmha_config.enable_paged_trt_fmha;
         if (!paged_fmha_env) {
             RTP_LLM_LOG_INFO("Paged TRT FMHA is disabled for by ENABLE_PAGED_TRT_FMHA=OFF env");
             use_paged_trt_fmha = false;
@@ -104,13 +105,17 @@ public:
     }
 
     template<typename T>
-    static bool UseOpenSourceFMHA(const rtp_llm::GptInitParameter& gpt_init_parameter) {
-        bool use_open_source_fmha = CheckUseFMHA<T>(gpt_init_parameter) && CheckQKVLengthEqual<T>(gpt_init_parameter);
+    static bool UseOpenSourceFMHA(const FMHAConfig& fmha_config,
+                                  const ModelConfig& model_config,
+                                  const RuntimeConfig& runtime_config,
+                                  const KVCacheConfig& kv_cache_config,
+                                  const SpeculativeExecutionConfig& sp_config) {
+        bool use_open_source_fmha = CheckUseFMHA<T>(fmha_config) && CheckQKVLengthEqual<T>(model_config, runtime_config, kv_cache_config, sp_config);
         if (!(is_sm8x() || is_sm90())) {
             RTP_LLM_LOG_INFO("opensource FMHA is disabled for sm %d", get_sm());
             use_open_source_fmha = false;
         }
-        bool fmha_env = gpt_init_parameter.fmha_config.enable_open_source_fmha;
+        bool fmha_env = fmha_config.enable_open_source_fmha;
         if (!fmha_env) {
             RTP_LLM_LOG_INFO("opensource FMHA is disabled for by env");
             use_open_source_fmha = false;
@@ -120,9 +125,8 @@ public:
 
 protected:
     template<typename T>
-    static bool CheckUseFMHA(const rtp_llm::GptInitParameter& params) {
-
-        bool fmha_enable = params.fmha_config.enable_fmha;
+    static bool CheckUseFMHA(const FMHAConfig& fmha_config) {
+        bool fmha_enable = fmha_config.enable_fmha;
         if (!fmha_enable) {
             RTP_LLM_LOG_INFO("FMHA is not enbaled");
             return false;
@@ -135,19 +139,22 @@ protected:
     }
 
     template<typename T>
-    static bool CheckQKVLengthEqual(const rtp_llm::GptInitParameter& params) {
-        bool               reuse_cache_env           = params.kv_cache_config.reuse_cache;
-        bool               not_prefix                = params.pre_seq_len_ == 0 && !reuse_cache_env;
-        const std::string& multi_task_prompt_env     = params.kv_cache_config.multi_task_prompt;
-        const std::string& multi_task_prompt_str_env = params.kv_cache_config.multi_task_prompt_str;
+    static bool CheckQKVLengthEqual(const ModelConfig& model_config,
+                                    const RuntimeConfig& runtime_config,
+                                    const KVCacheConfig& kv_cache_config,
+                                    const SpeculativeExecutionConfig& sp_config) {
+        bool               reuse_cache_env           = kv_cache_config.reuse_cache;
+        bool               not_prefix                = model_config.pre_seq_len == 0 && !reuse_cache_env;
+        const std::string& multi_task_prompt_env     = kv_cache_config.multi_task_prompt;
+        const std::string& multi_task_prompt_str_env = kv_cache_config.multi_task_prompt_str;
 
-        bool enable_partial_fallback_env = params.fifo_scheduler_config.enable_partial_fallback;
+        bool enable_partial_fallback_env = runtime_config.fifo_scheduler_config.enable_partial_fallback;
         if (enable_partial_fallback_env) {
             RTP_LLM_LOG_INFO("QKV length not equal: enable part fallback");
             return false;
         }
 
-        if (params.fifo_scheduler_config.enable_fast_gen) {
+        if (runtime_config.fifo_scheduler_config.enable_fast_gen) {
             RTP_LLM_LOG_INFO("QKV length not equal: enable fast gen");
             return false;
         }
@@ -156,7 +163,7 @@ protected:
             RTP_LLM_LOG_INFO("QKV length not equal: use kv cache reuse");
             return false;
         }
-        if (!params.sp_config.sp_model_type.empty()) {
+        if (!sp_config.sp_model_type.empty()) {
             RTP_LLM_LOG_INFO("QKV length not equal: use sp_model");
             return false;
         }
diff --git a/rtp_llm/cpp/cuda/cufmha/cufmha.cc b/rtp_llm/cpp/cuda/cufmha/cufmha.cc
index 8ca572f41..428caeb38 100644
--- a/rtp_llm/cpp/cuda/cufmha/cufmha.cc
+++ b/rtp_llm/cpp/cuda/cufmha/cufmha.cc
@@ -22,24 +22,24 @@ tensorrt_llm::kernels::Data_type trtDtypeConvert(DataType dtype) {
     }
 }
 
-cufmha::cufmha(DataType          dtype,
-               AttentionMaskType mtype,
-               size_t            head_num,
-               size_t            kv_head_num,
-               size_t            size_per_head,
-               size_t            seq_size_per_block,
-               float             q_scaling,
-               bool              use_linear_bias_slopes,
-               bool              can_use_trtv1_fmha,
-               bool              can_use_trtv2_fmha,
-               bool              can_use_trtv2_fmha_paged,
-               bool              can_use_open_source_fmha,
-               bool              can_use_open_source_fmha_paged,
-               bool              is_s_padded,
-               cudaStream_t      stream) {
-
-    dtype_         = dtype;
-    mtype_         = mtype;
+cufmha::cufmha(DataType dtype,
+               bool     is_causal,
+               size_t   head_num,
+               size_t   kv_head_num,
+               size_t   size_per_head,
+               size_t   seq_size_per_block,
+               float    q_scaling,
+               bool     use_linear_bias_slopes,
+               bool     can_use_trtv1_fmha,
+               bool     can_use_trtv2_fmha,
+               bool     can_use_trtv2_fmha_paged,
+               bool     can_use_open_source_fmha,
+               bool     can_use_open_source_fmha_paged,
+               bool     is_s_padded,
+               cudaStream_t stream) {
+
+    dtype_      = dtype;
+    is_causal_  = is_causal;
     head_num_      = head_num;
     kv_head_num_   = kv_head_num;
     size_per_head_ = size_per_head;
@@ -66,14 +66,14 @@ cudaStream_t cufmha::getStream() {
     return run_stream;
 }
 
-bool cufmha::checkSignature(DataType          dtype,
-                            AttentionMaskType mtype,
-                            size_t            head_num,
-                            size_t            kv_head_num,
-                            size_t            size_per_head,
-                            float             q_scaling,
-                            bool              use_linear_bias_slopes) {
-    return dtype == dtype_ && mtype == mtype_ && head_num == head_num_ && kv_head_num == kv_head_num_
+bool cufmha::checkSignature(DataType dtype,
+                            bool     is_causal,
+                            size_t   head_num,
+                            size_t   kv_head_num,
+                            size_t   size_per_head,
+                            float    q_scaling,
+                            bool     use_linear_bias_slopes) {
+    return dtype == dtype_ && is_causal == is_causal_ && head_num == head_num_ && kv_head_num == kv_head_num_
            && size_per_head == size_per_head_ && q_scaling == q_scaling_
            && use_linear_bias_slopes == use_linear_bias_slopes_;
 }
@@ -82,7 +82,7 @@ bool cufmha::initTrtV1FmhaAndCheckSupport() {
 #ifdef USE_OLD_TRT_FMHA
     trtv1_fmha_runner_.reset(new FusedMHARunnerFP16v2(head_num_, size_per_head_, get_sm(), q_scaling_));
 
-    return trtv1_fmha_runner_->fmha_supported(mtype_ == AttentionMaskType::causalMask) && (head_num_ == kv_head_num_)
+    return trtv1_fmha_runner_->fmha_supported(is_causal_) && (head_num_ == kv_head_num_)
            && (dtype_ == DataType::TYPE_FP16);
 #else
     return false;
@@ -98,7 +98,7 @@ void cufmha::runTrtV1Fmha(void*  input,
                           size_t token_num) {
 #ifdef USE_OLD_TRT_FMHA
     auto run_stream = getStream();
-    if (mtype_ == AttentionMaskType::causalMask) {
+    if (is_causal_) {
         trtv1_fmha_runner_->setup_causal_masked_fmha(seq_len, batch_size);
         trtv1_fmha_runner_->run_causal_masked_fmha(input, cu_seqlens, output, true, run_stream);
     } else {
@@ -121,7 +121,7 @@ MHARunnerFixedParams cufmha::createMHARunnerFixedParams(bool paged, bool isSPadd
     fixedParams.dataTypeOut  = trtDtypeConvert(dtype_);
     fixedParams.forceFp32Acc = false;
     fixedParams.attentionMaskType =
-        mtype_ == AttentionMaskType::causalMask ? ContextAttentionMaskType::CAUSAL : ContextAttentionMaskType::PADDING;
+        is_causal_ ? ContextAttentionMaskType::CAUSAL : ContextAttentionMaskType::PADDING;
     fixedParams.attentionInputLayout      = paged ? AttentionInputLayout::Q_PAGED_KV : AttentionInputLayout::PACKED_QKV;
     fixedParams.isSPadded                 = isSPadded;
     fixedParams.numQHeads                 = head_num_;
@@ -204,8 +204,8 @@ bool cufmha::initTrtV2FmhaAndCheckSupport() {
         trtv2_sm70_fmha_runner_.reset(new tensorrt_llm::kernels::FusedMHARunnerV2Sm70(
             trtDtypeConvert(dtype_), head_num_, size_per_head_, q_scaling_));
         return trtv2_sm70_fmha_runner_->fmha_supported()
-               && (mtype_ == AttentionMaskType::causalMask || mtype_ == AttentionMaskType::noMask)
-               && !(mtype_ == AttentionMaskType::noMask && use_linear_bias_slopes_);
+               && (is_causal_ || !is_causal_)
+               && !(!is_causal_ && use_linear_bias_slopes_);
     }
     if (get_sm() < tensorrt_llm::kernels::kSM_80) {
         RTP_LLM_LOG_INFO("cuda sm %d < 80, not support trt v2 fmha", get_sm());
@@ -215,8 +215,8 @@ bool cufmha::initTrtV2FmhaAndCheckSupport() {
     trtv2_fmha_runner_.reset(new tensorrt_llm::kernels::FusedMHARunnerV2(fixedParams));
 
     return trtv2_fmha_runner_->isFmhaSupported()
-           && (mtype_ == AttentionMaskType::causalMask || mtype_ == AttentionMaskType::noMask)
-           && !(mtype_ == AttentionMaskType::noMask && use_linear_bias_slopes_);
+           && (is_causal_ || !is_causal_)
+           && !(!is_causal_ && use_linear_bias_slopes_);
 }
 
 bool cufmha::initTrtV2FmhaPagedAndCheckSupport() {
@@ -227,13 +227,12 @@ bool cufmha::initTrtV2FmhaPagedAndCheckSupport() {
     MHARunnerFixedParams fixedParams = createMHARunnerFixedParams(true, is_s_padded_);
     trtv2_paged_fmha_runner_.reset(new tensorrt_llm::kernels::FusedMHARunnerV2(fixedParams));
     return trtv2_paged_fmha_runner_->isFmhaSupported()
-           && (mtype_ == AttentionMaskType::causalMask || mtype_ == AttentionMaskType::noMask)
-           && !(mtype_ == AttentionMaskType::noMask && use_linear_bias_slopes_);
+           && (is_causal_ || !is_causal_)
+           && !(!is_causal_ && use_linear_bias_slopes_);
 }
 
 bool cufmha::initOpenSourceFmhaAndCheckSupport() {
     return (kv_head_num_ != 0 && head_num_ % kv_head_num_ == 0)
-           && (mtype_ == AttentionMaskType::causalMask || mtype_ == AttentionMaskType::noMask)
            && ((size_per_head_ == 64) || (size_per_head_ == 96) || (size_per_head_ == 128) || (size_per_head_ == 192));
 }
 
@@ -293,7 +292,7 @@ void cufmha::runTrtV2Fmha(void*        input,
                                                   custom_mask);
         trtv2_fmha_runner_->run(runnerParams);
     } else {
-        trtv2_sm70_fmha_runner_->setup_flags(false, false, (mtype_ == AttentionMaskType::causalMask), kv_head_num_);
+        trtv2_sm70_fmha_runner_->setup_flags(false, false, is_causal_, kv_head_num_);
 
         trtv2_sm70_fmha_runner_->setup(
             batch_size, max_seq_len, max_seq_len, token_num, use_linear_bias_slopes_, false, 1, 0);
@@ -525,7 +524,7 @@ Flash_fwd_params cufmha::genFlashFwdParams(void*  q,
     flash_fwd_params.rp_dropout               = 1.f / flash_fwd_params.p_dropout;
     flash_fwd_params.scale_softmax_rp_dropout = flash_fwd_params.rp_dropout * flash_fwd_params.scale_softmax;
 
-    flash_fwd_params.is_causal = (mtype_ == AttentionMaskType::causalMask);
+    flash_fwd_params.is_causal = is_causal_;
     if (linear_bias_slopes) {
         flash_fwd_params.alibi_slopes_ptr = linear_bias_slopes;
     }
diff --git a/rtp_llm/cpp/cuda/cufmha/cufmha.h b/rtp_llm/cpp/cuda/cufmha/cufmha.h
index 4eee0111b..fda423a17 100644
--- a/rtp_llm/cpp/cuda/cufmha/cufmha.h
+++ b/rtp_llm/cpp/cuda/cufmha/cufmha.h
@@ -13,9 +13,9 @@ namespace rtp_llm {
 class cufmha {
 
 public:
-    cufmha(DataType          dtype,
-           AttentionMaskType mtype,
-           size_t            head_num,
+    cufmha(DataType dtype,
+           bool     is_causal,
+           size_t   head_num,
            size_t            kv_head_num,
            size_t            size_per_head,
            size_t            seq_size_per_block,
@@ -111,9 +111,9 @@ public:
     size_t
     getOpenSourceWorkSpaceSize(size_t batch_size, size_t seq_len_q, size_t max_seq_len_kv = 0, bool paged = false);
 
-    bool checkSignature(DataType          dtype,
-                        AttentionMaskType mtype,
-                        size_t            head_num,
+    bool checkSignature(DataType dtype,
+                        bool     is_causal,
+                        size_t   head_num,
                         size_t            kv_head_num,
                         size_t            size_per_head,
                         float             q_scaling,
@@ -177,8 +177,8 @@ private:
 #ifdef USE_OLD_TRT_FMHA
     std::unique_ptr<FusedMHARunnerFP16v2> trtv1_fmha_runner_;
 #endif
-    DataType          dtype_;
-    AttentionMaskType mtype_;
+    DataType dtype_;
+    bool     is_causal_;
 
     size_t       head_num_;
     size_t       kv_head_num_;
diff --git a/rtp_llm/cpp/devices/BUILD b/rtp_llm/cpp/devices/BUILD
index b09cec77c..add28e13e 100644
--- a/rtp_llm/cpp/devices/BUILD
+++ b/rtp_llm/cpp/devices/BUILD
@@ -32,8 +32,9 @@ cc_library(
         "//rtp_llm/cpp/utils:core_utils",
         "//rtp_llm/cpp/utils:kv_cache_utils",
         "//rtp_llm/cpp/config:static_config",
+        "//rtp_llm/cpp/config:model_config",
         "//rtp_llm/cpp/model_utils:model_utils",
-        "//rtp_llm/cpp/config:gpt_init_params",
+        "//rtp_llm/cpp/config:config_modules",
         "//rtp_llm/cpp/models/models_weight:weights_define",
         "//rtp_llm/cpp/disaggregate/cache_store:cache_store_interface",
         "//rtp_llm/cpp/models:stats",
@@ -55,9 +56,10 @@ cc_library(
         "utils/*.cc",
     ]),
     deps = [
-        "devices_base",
+        ":devices_base",
         "//rtp_llm/cpp/core:buffer_torch",
         "//rtp_llm/cpp/config:config_modules",
+        "//rtp_llm/cpp/config:model_config",
     ],
     visibility = ["//visibility:public"],
     copts = copts(),
@@ -72,6 +74,7 @@ cc_library(
         ":devices_base",
         ":device_utils",
         "//rtp_llm/cpp/config:config_modules",
+        "//rtp_llm/cpp/config:model_config",
     ],
     visibility = ["//visibility:public"],
     copts = copts(),
diff --git a/rtp_llm/cpp/devices/DeviceData.h b/rtp_llm/cpp/devices/DeviceData.h
index beafaba04..361a0dbb1 100644
--- a/rtp_llm/cpp/devices/DeviceData.h
+++ b/rtp_llm/cpp/devices/DeviceData.h
@@ -3,7 +3,8 @@
 #include <stddef.h>
 #include <string>
 #include "rtp_llm/cpp/core/Types.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 namespace rtp_llm {
 
@@ -80,9 +81,10 @@ struct DeviceInitParams {
     DeviceResourceConfig         device_resource_config;
     MoeConfig                    moe_config;
     SpeculativeExecutionConfig   sp_config;
-    FIFOSchedulerConfig          fifo_scheduler_config;
+    // FIFOSchedulerConfig fields are now in RuntimeConfig
+    RuntimeConfig                runtime_config;
     MiscellaneousConfig          misc_config;
-    ParallelismDistributedConfig parallelism_distributed_config;
+    ParallelismConfig parallelism_config;
     ProfilingDebugLoggingConfig  profile_debug_logging_config;
     ModelSpecificConfig          model_specific_config;
     ConcurrencyConfig            concurrency_config;
diff --git a/rtp_llm/cpp/devices/DeviceFactory.cc b/rtp_llm/cpp/devices/DeviceFactory.cc
index 3087f45ef..ce33158d4 100644
--- a/rtp_llm/cpp/devices/DeviceFactory.cc
+++ b/rtp_llm/cpp/devices/DeviceFactory.cc
@@ -2,6 +2,7 @@
 #include "rtp_llm/cpp/config/ConfigModules.h"
 #include "autil/EnvUtil.h"
 #include <cassert>
+#include <cstdlib>
 
 using namespace std;
 using namespace torch_ext;
@@ -44,7 +45,7 @@ GlobalDeviceParams DeviceFactory::getDefaultGlobalDeviceParams() {
     return params;
 }
 
-int64_t getDefaultDeviceReserveMemoryBytes(const GptInitParameter& params) {
+int64_t getDefaultDeviceReserveMemoryBytes() {
     auto reserve_bytes = -512L * 1024 * 1024;
     RTP_LLM_LOG_INFO("Default device reserve memory bytes: %ld", reserve_bytes);
     return reserve_bytes;
@@ -58,73 +59,97 @@ bool DeviceFactory::isAlreadyInit() {
     }
 }
 
-void DeviceFactory::initDevices(const GptInitParameter& params) {
+void DeviceFactory::initDevices(const ParallelismConfig& parallelism_config,
+                                const ModelConfig& model_config,
+                                const EPLBConfig& eplb_config,
+                                const FMHAConfig& fmha_config,
+                                const DeviceResourceConfig& device_resource_config,
+                                const MoeConfig& moe_config,
+                                const SpeculativeExecutionConfig& sp_config,
+                                const MiscellaneousConfig& misc_config,
+                                const ProfilingDebugLoggingConfig& profiling_debug_logging_config,
+                                const HWKernelConfig& hw_kernel_config,
+                                const ConcurrencyConfig& concurrency_config,
+                                const FfnDisAggregateConfig& ffn_disaggregate_config,
+                                const RuntimeConfig& runtime_config) {
     if (getCurrentDevices().size()) {
         RTP_LLM_LOG_WARNING("Devices are already initialized! will do nothing.");
         return;
     }
     auto  global_params                          = getDefaultGlobalDeviceParams();
     auto& device_params                          = global_params.device_params[0].second;
-    device_params.tp_size                        = params.tp_size_;
-    device_params.dp_size                        = params.dp_size_;
-    device_params.ep_size                        = params.ep_size_;
-    device_params.ep_rank                        = params.ep_rank_;
-    device_params.tp_rank                        = params.tp_rank_;
-    device_params.dp_rank                        = params.dp_rank_;
-    device_params.ffn_tp_size                    = params.ffn_tp_size_;
-    device_params.ffn_tp_rank                    = params.ffn_tp_rank_;
-    device_params.enable_sp                      = params.enable_sp_;
-    device_params.use_all_gather                 = params.use_all_gather_;
-    device_params.device_id                      = params.local_rank_;
-    device_params.master_ip                      = params.nccl_ip_;
-    device_params.tp_master_port                 = params.tp_nccl_port_;
-    device_params.dp_tp_master_port              = params.dp_tp_nccl_port_;
-    device_params.ffn_tp_master_port             = params.ffn_tp_nccl_port_;
-    device_params.tokens_per_block               = params.seq_size_per_block_;
-    device_params.mla_ops_type                   = params.mla_ops_type_;
-    device_params.max_seq_len                    = params.max_seq_len_;
-    device_params.hidden_size                    = params.hidden_size_;
-    device_params.num_experts                    = params.expert_num_;
-    device_params.extra_experts                  = params.phy_exp_num_ - params.expert_num_;
-    device_params.fmha_config                    = params.fmha_config;
-    device_params.device_resource_config         = params.device_resource_config;
-    device_params.moe_config                     = params.moe_config;
-    device_params.sp_config                      = params.sp_config;
-    device_params.fifo_scheduler_config          = params.fifo_scheduler_config;
-    device_params.misc_config                    = params.misc_config;
-    device_params.parallelism_distributed_config = params.parallelism_distributed_config;
-    device_params.profile_debug_logging_config   = params.profiling_debug_logging_config;
-    device_params.hw_kernel_config               = params.hw_kernel_config;
-    device_params.concurrency_config             = params.concurrency_config;
-    device_params.ffn_as_service = params.ffn_disaggregate_config.is_ffn_service();
-    device_params.max_seq_len    = params.max_seq_len_;
+    device_params.tp_size                        = parallelism_config.tp_size;
+    device_params.dp_size                        = parallelism_config.dp_size;
+    device_params.ep_size                        = parallelism_config.ep_size;
+    device_params.ep_rank                        = parallelism_config.ep_rank;
+    device_params.tp_rank                        = parallelism_config.tp_rank;
+    device_params.dp_rank                        = parallelism_config.dp_rank;
+    device_params.ffn_tp_size                    = parallelism_config.ffn_tp_size;
+    device_params.ffn_tp_rank                    = parallelism_config.ffn_tp_rank;
+    device_params.enable_sp                      = parallelism_config.enable_sp;
+    // use_all_gather is now in moe_config, but we need to ensure it's not used
+    // when use_deepep_low_latency is True
+    device_params.use_all_gather = moe_config.use_all_gather 
+                                   && !moe_config.use_deepep_low_latency;
+    // local_rank is calculated from parallelism_config
+    device_params.device_id                      = parallelism_config.world_rank % parallelism_config.local_world_size;
+    device_params.master_ip                      = parallelism_config.nccl_ip;
+    device_params.tp_master_port                 = parallelism_config.tp_nccl_port;
+    device_params.dp_tp_master_port              = parallelism_config.dp_tp_nccl_port;
+    device_params.ffn_tp_master_port             = parallelism_config.ffn_tp_nccl_port;
+    device_params.tokens_per_block               = model_config.attn_config.tokens_per_block;
+    device_params.mla_ops_type                   = model_config.mla_ops_type;
+    device_params.max_seq_len                    = model_config.max_seq_len;
+    device_params.hidden_size                    = model_config.hidden_size;
+    device_params.num_experts                    = model_config.expert_num;
+    device_params.extra_experts                  = eplb_config.phy_exp_num(model_config.expert_num) - model_config.expert_num;
+    device_params.fmha_config                    = fmha_config;
+    device_params.device_resource_config         = device_resource_config;
+    device_params.moe_config                     = moe_config;
+    device_params.sp_config                      = sp_config;
+    // FIFOSchedulerConfig fields are now in RuntimeConfig
+    device_params.runtime_config                 = runtime_config;
+    device_params.misc_config                    = misc_config;
+    device_params.parallelism_config.tp_size = parallelism_config.tp_size;
+    device_params.parallelism_config.ep_size = parallelism_config.ep_size;
+    device_params.parallelism_config.dp_size = parallelism_config.dp_size;
+    device_params.parallelism_config.pp_size = parallelism_config.pp_size;
+    device_params.parallelism_config.world_size = parallelism_config.world_size;
+    device_params.parallelism_config.world_rank = parallelism_config.world_rank;
+    device_params.parallelism_config.local_world_size = parallelism_config.local_world_size;
+    device_params.parallelism_config.ffn_sp_size = parallelism_config.ffn_sp_size;
+    device_params.profile_debug_logging_config   = profiling_debug_logging_config;
+    device_params.hw_kernel_config               = hw_kernel_config;
+    device_params.concurrency_config             = concurrency_config;
+    device_params.ffn_as_service = ffn_disaggregate_config.is_ffn_service();
+    device_params.max_seq_len    = model_config.max_seq_len;
     RTP_LLM_LOG_INFO("set overlap type to be %d", device_params.device_resource_config.overlap_comm_type);
-    device_params.m_split                 = params.device_resource_config.m_split;
-    device_params.max_generate_batch_size = params.max_generate_batch_size_;
+    device_params.m_split                 = device_resource_config.m_split;
+    device_params.max_generate_batch_size = runtime_config.max_generate_batch_size;
 
-    const auto device_mem_reserve_env = params.device_resource_config.device_reserve_memory_bytes;
+    const auto device_mem_reserve_env = device_resource_config.device_reserve_memory_bytes;
     RTP_LLM_LOG_INFO("Device reserve memory bytes from env: %ld", device_mem_reserve_env);
     device_params.device_reserve_memory_bytes =
-        device_mem_reserve_env ? device_mem_reserve_env : getDefaultDeviceReserveMemoryBytes(params);
+        device_mem_reserve_env ? device_mem_reserve_env : getDefaultDeviceReserveMemoryBytes();
     RTP_LLM_LOG_INFO("Device reserve memory bytes: %ld", device_params.device_reserve_memory_bytes);
 
-    device_params.host_reserve_memory_bytes = params.device_resource_config.host_reserve_memory_bytes;  // 4GB
+    device_params.host_reserve_memory_bytes = device_resource_config.host_reserve_memory_bytes;  // 4GB
     RTP_LLM_LOG_INFO("Host reserve memory bytes: %ld", device_params.host_reserve_memory_bytes);
 
-    device_params.enable_comm_overlap = params.device_resource_config.enable_comm_overlap;
+    device_params.enable_comm_overlap = device_resource_config.enable_comm_overlap;
     device_params.enable_layer_micro_batch =
-        static_cast<MicroBatchType>(params.device_resource_config.enable_layer_micro_batch);
+        static_cast<MicroBatchType>(device_resource_config.enable_layer_micro_batch);
     RTP_LLM_LOG_INFO("enable comm overlap: %d, enable layer micro batch: %d",
                      device_params.enable_comm_overlap,
                      device_params.enable_layer_micro_batch);
-    device_params.user_deep_gemm_num_sm  = params.hw_kernel_config.deep_gemm_num_sm;
-    device_params.use_aiter_pa           = params.hw_kernel_config.use_aiter_pa;
-    device_params.use_asm_pa             = params.hw_kernel_config.use_asm_pa;
-    device_params.use_deepep_moe         = params.moe_config.use_deepep_moe;
-    device_params.use_deepep_internode   = params.moe_config.use_deepep_internode;
-    device_params.use_deepep_low_latency = params.moe_config.use_deepep_low_latency;
-    auto sp_type                         = params.sp_config.sp_type;
-    auto sp_model_type                   = params.sp_config.sp_model_type;
+    device_params.user_deep_gemm_num_sm  = hw_kernel_config.deep_gemm_num_sm;
+    device_params.use_aiter_pa           = hw_kernel_config.use_aiter_pa;
+    device_params.use_asm_pa             = hw_kernel_config.use_asm_pa;
+    device_params.use_deepep_moe         = moe_config.use_deepep_moe;
+    device_params.use_deepep_internode   = moe_config.use_deepep_internode;
+    device_params.use_deepep_low_latency = moe_config.use_deepep_low_latency;
+    auto sp_type                         = sp_config.sp_type;
+    auto sp_model_type                   = sp_config.sp_model_type;
     RTP_LLM_LOG_INFO("device_params sp_type is %s", sp_type.c_str());
     RTP_LLM_LOG_INFO("device_params sp_model_type is %s", sp_model_type.c_str());
     if (((sp_type == "vanilla") && (sp_model_type == "mixtbstars-mtp"))
@@ -143,7 +168,7 @@ void DeviceFactory::initDevices(const GptInitParameter& params) {
                      device_params.use_deepep_moe,
                      device_params.use_deepep_low_latency);
 
-    device_params.model_specific_config = params.model_specific_config;
+    // Note: model_specific_config is now passed separately, not needed here
 
     if (!global_params.device_params.size()) {
         RTP_LLM_LOG_ERROR("No device is specified to init !");
@@ -242,7 +267,20 @@ void registerDeviceOps(py::module& m) {
         .def("preprocess_weight_scale", &DeviceExporter::preprocessWeightScale, py::arg("weight"), py::arg("scale"));
 
     m.def("get_device", &DeviceFactory::getDeviceExporter);
-    m.def("init_device", &DeviceFactory::initDevices, py::arg("params"));
+    m.def("init_device", &DeviceFactory::initDevices,
+          py::arg("parallelism_config"),
+          py::arg("model_config"),
+          py::arg("eplb_config"),
+          py::arg("fmha_config"),
+          py::arg("device_resource_config"),
+          py::arg("moe_config"),
+          py::arg("sp_config"),
+          py::arg("misc_config"),
+          py::arg("profiling_debug_logging_config"),
+          py::arg("hw_kernel_config"),
+          py::arg("concurrency_config"),
+          py::arg("ffn_disaggregate_config"),
+          py::arg("runtime_config"));
 }
 
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/devices/DeviceFactory.h b/rtp_llm/cpp/devices/DeviceFactory.h
index f194f0ad3..93b0daac4 100644
--- a/rtp_llm/cpp/devices/DeviceFactory.h
+++ b/rtp_llm/cpp/devices/DeviceFactory.h
@@ -2,7 +2,7 @@
 
 #include "rtp_llm/cpp/devices/DeviceBase.h"
 #include "rtp_llm/cpp/devices/DeviceExport.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 #include <unordered_map>
 #include <vector>
 
@@ -22,7 +22,19 @@ public:
 
 class DeviceFactory {
 public:
-    static void        initDevices(const GptInitParameter& params);
+    static void        initDevices(const ParallelismConfig& parallelism_config,
+                                   const ModelConfig& model_config,
+                                   const EPLBConfig& eplb_config,
+                                   const FMHAConfig& fmha_config,
+                                   const DeviceResourceConfig& device_resource_config,
+                                   const MoeConfig& moe_config,
+                                   const SpeculativeExecutionConfig& sp_config,
+                                   const MiscellaneousConfig& misc_config,
+                                   const ProfilingDebugLoggingConfig& profiling_debug_logging_config,
+                                   const HWKernelConfig& hw_kernel_config,
+                                   const ConcurrencyConfig& concurrency_config,
+                                   const FfnDisAggregateConfig& ffn_disaggregate_config,
+                                   const RuntimeConfig& runtime_config);
     static bool        isAlreadyInit();
     static DeviceBase* getDefaultDevice();
     static void        registerDevice(DeviceType type, DeviceCreatorType creator);
diff --git a/rtp_llm/cpp/devices/arm_impl/test/ops/GemmOpTest.cc b/rtp_llm/cpp/devices/arm_impl/test/ops/GemmOpTest.cc
index 50e2363ce..043a0ca0a 100644
--- a/rtp_llm/cpp/devices/arm_impl/test/ops/GemmOpTest.cc
+++ b/rtp_llm/cpp/devices/arm_impl/test/ops/GemmOpTest.cc
@@ -238,7 +238,6 @@ void ArmGemmOpTest::TransposeBatchGemmOP(TransposeOperation op_a,
 
 TEST_F(ArmGemmOpTest, BasicGemmOpTest) {
     HWKernelConfig hw_kernel_config;
-    hw_kernel_config.update_from_env_for_test();
     if (hw_kernel_config.arm_gemm_use_kai) {
         return;
     }
@@ -252,7 +251,6 @@ TEST_F(ArmGemmOpTest, BasicGemmOpTest) {
 
 TEST_F(ArmGemmOpTest, BasicGemmOPKaiTest) {
     HWKernelConfig hw_kernel_config;
-    hw_kernel_config.update_from_env_for_test();
     if (!hw_kernel_config.arm_gemm_use_kai) {
         return;
     }
diff --git a/rtp_llm/cpp/devices/base_tests/AttentionLayerTest.hpp b/rtp_llm/cpp/devices/base_tests/AttentionLayerTest.hpp
index 9cd48efc7..13e783543 100644
--- a/rtp_llm/cpp/devices/base_tests/AttentionLayerTest.hpp
+++ b/rtp_llm/cpp/devices/base_tests/AttentionLayerTest.hpp
@@ -84,7 +84,7 @@ void AttentionLayerTest<T>::testAttentionLayer(const CacheConfig&          cache
     const auto prefix_lengths = std::vector<int32_t>(context_lengths.size(), 0);
 
     const auto mask_tensor =
-        create_context_mask(context_lengths, attention_conf.mask_type == AttentionMaskType::causalMask)
+        create_context_mask(context_lengths, attention_conf.is_causal)
             .to(dataTypeToTorchType(dtype));
     std::cout << "mask: " << mask_tensor << std::endl;
     const auto input_buffer = tensorToBuffer(input_tensor.to(dataTypeToTorchType(dtype)));
diff --git a/rtp_llm/cpp/devices/base_tests/AttentionOpTest.hpp b/rtp_llm/cpp/devices/base_tests/AttentionOpTest.hpp
index 8723a8c1d..5290b17fb 100644
--- a/rtp_llm/cpp/devices/base_tests/AttentionOpTest.hpp
+++ b/rtp_llm/cpp/devices/base_tests/AttentionOpTest.hpp
@@ -824,7 +824,7 @@ void AttentionOpTest::flashinferPrefillOpTest(size_t        batch_size,
     auto kv_cache_block_id = allocateKVBlocks(cache_conf, kv_seq_lengths, kvcache_pad, false);
     auto kv_cache_buffer   = cache_manager_->kvCacheBuffer();
     auto attention_config  = AttentionConfigs(
-        {num_heads, num_key_value_heads, head_dim, num_heads * head_dim, rope_config, tokens_per_block, causalMask});
+        {num_heads, num_key_value_heads, head_dim, num_heads * head_dim, rope_config, tokens_per_block, true});
     attention_config.kv_cache_dtype         = KvCacheDataType::BASE;
     attention_config.skip_append_kv_cache   = true;
     BufferPtr        prefix_lengths_buf     = tensorToBuffer(prefix_lengths_host, AllocationType::HOST);
@@ -980,7 +980,7 @@ void AttentionOpTest::xqaPrefillOpTest(size_t        batch_size,
     auto kv_cache_block_id = allocateKVBlocks(cache_conf, kv_seq_lengths, kvcache_pad_fp8, false);
     auto kv_cache_buffer   = cache_manager_->kvCacheBuffer();
     auto attention_config  = AttentionConfigs(
-        {num_heads, num_key_value_heads, head_dim, num_heads * head_dim, rope_config, tokens_per_block, causalMask});
+        {num_heads, num_key_value_heads, head_dim, num_heads * head_dim, rope_config, tokens_per_block, true});
     attention_config.kv_cache_dtype         = KvCacheDataType::FP8;
     attention_config.skip_append_kv_cache   = true;
     BufferPtr        prefix_lengths_buf     = tensorToBuffer(prefix_lengths_host, AllocationType::HOST);
diff --git a/rtp_llm/cpp/devices/cpu_impl/test/ops/CpuAttentionLayerTest.cc b/rtp_llm/cpp/devices/cpu_impl/test/ops/CpuAttentionLayerTest.cc
index b774d58d4..f29706761 100644
--- a/rtp_llm/cpp/devices/cpu_impl/test/ops/CpuAttentionLayerTest.cc
+++ b/rtp_llm/cpp/devices/cpu_impl/test/ops/CpuAttentionLayerTest.cc
@@ -12,7 +12,7 @@ TEST_F(CpuAttentionLayerTestFP16, testSimpleContextAttention) {
     attention_conf.size_per_head    = 32;
     attention_conf.tokens_per_block = 4;
 
-    attention_conf.mask_type = AttentionMaskType::causalMask;
+    attention_conf.is_causal = true;
 
     attention_conf.rope_config.style   = RopeStyle::Base;
     attention_conf.rope_config.base    = 10000;
diff --git a/rtp_llm/cpp/devices/cuda_impl/BUILD b/rtp_llm/cpp/devices/cuda_impl/BUILD
index 6af61fa0b..698b671d3 100644
--- a/rtp_llm/cpp/devices/cuda_impl/BUILD
+++ b/rtp_llm/cpp/devices/cuda_impl/BUILD
@@ -35,6 +35,7 @@ cc_library(
         "//rtp_llm/cpp/core:torch_cuda_allocator",
         "//rtp_llm/cpp/core:torch_event",
         "//rtp_llm/cpp/pybind:th_utils",
+        "//rtp_llm/cpp/config:model_config",
     ] + select({
         ":use_accl_ep": [
             "//3rdparty/accl_ep:accl_ep",
diff --git a/rtp_llm/cpp/devices/cuda_impl/CudaDeepEPFfnLayer.cc b/rtp_llm/cpp/devices/cuda_impl/CudaDeepEPFfnLayer.cc
index b5d951caf..9f5612b32 100644
--- a/rtp_llm/cpp/devices/cuda_impl/CudaDeepEPFfnLayer.cc
+++ b/rtp_llm/cpp/devices/cuda_impl/CudaDeepEPFfnLayer.cc
@@ -46,7 +46,7 @@ bool CudaDevice::initDeepEPBuffer() {
     auto   nccl_param       = getNcclParam(ParallelMode::DP_AND_TP);
     size_t world_rank       = nccl_param.rank_;
     size_t world_size       = nccl_param.world_size_;
-    size_t local_world_size = init_params_.parallelism_distributed_config.local_world_size;
+    size_t local_world_size = init_params_.parallelism_config.local_world_size;
 
     int num_experts    = init_params_.num_experts + init_params_.extra_experts;
     int deep_ep_num_sm = init_params_.moe_config.deep_ep_num_sm > 0 ? init_params_.moe_config.deep_ep_num_sm : 24;
diff --git a/rtp_llm/cpp/devices/cuda_impl/CudaDevice.cc b/rtp_llm/cpp/devices/cuda_impl/CudaDevice.cc
index 607860e4e..a877d1178 100644
--- a/rtp_llm/cpp/devices/cuda_impl/CudaDevice.cc
+++ b/rtp_llm/cpp/devices/cuda_impl/CudaDevice.cc
@@ -392,7 +392,7 @@ CudaDevice::selectCuFMHARunner(const AttentionConfigs& configs, DataType attn_dt
     DataType fmha_datatype       = use_fp8_fmha_ ? DataType::TYPE_FP8_E4M3 : attn_dtype;
     for (auto& runner : cufmha_runner_pool_) {
         if (runner->checkSignature(fmha_datatype,
-                                   configs.mask_type,
+                                   configs.is_causal,
                                    configs.head_num,
                                    configs.kv_head_num,
                                    configs.size_per_head,
@@ -409,7 +409,7 @@ CudaDevice::selectCuFMHARunner(const AttentionConfigs& configs, DataType attn_dt
         bool is_s_padded = (graph_runner_ != nullptr);
         cufmha_runner_pool_.back().reset(
             new cufmha(fmha_datatype,
-                       configs.mask_type,
+                       configs.is_causal,
                        configs.head_num,
                        configs.kv_head_num,
                        configs.size_per_head,
diff --git a/rtp_llm/cpp/devices/cuda_impl/CudaFlashInfer.cc b/rtp_llm/cpp/devices/cuda_impl/CudaFlashInfer.cc
index 54842f570..4440e6ff6 100644
--- a/rtp_llm/cpp/devices/cuda_impl/CudaFlashInfer.cc
+++ b/rtp_llm/cpp/devices/cuda_impl/CudaFlashInfer.cc
@@ -349,7 +349,7 @@ bool FlashInferAttnParams::check(rtp_llm::DeviceBase*             device,
             || (attn_configs.kv_cache_dtype != KvCacheDataType::BASE
                 && attn_configs.kv_cache_dtype != KvCacheDataType::FP8)
             || (attn_configs.rope_config.style != RopeStyle::Base && attn_configs.rope_config.style != RopeStyle::No)
-            || attn_configs.mask_type != causalMask || attn_configs.q_scaling != 1.0f || attn_configs.use_logn_attn
+            || !attn_configs.is_causal || attn_configs.q_scaling != 1.0f || attn_configs.use_logn_attn
             || (size_per_head != 64 && size_per_head != 128 && size_per_head != 192)
             || (!is_prefill && group_size > 10 && group_size != 16 && group_size != 12)) {
             return false;
diff --git a/rtp_llm/cpp/devices/cuda_impl/tests/CudaAttentionLayerTest.cc b/rtp_llm/cpp/devices/cuda_impl/tests/CudaAttentionLayerTest.cc
index 8c48593af..ad3c8f6e3 100644
--- a/rtp_llm/cpp/devices/cuda_impl/tests/CudaAttentionLayerTest.cc
+++ b/rtp_llm/cpp/devices/cuda_impl/tests/CudaAttentionLayerTest.cc
@@ -41,7 +41,7 @@ TEST_F(AttentionLayerTestFp16, testSimpleContextAttention2) {
     attention_conf.kv_head_num       = 16;
     attention_conf.size_per_head     = 64;
     attention_conf.tokens_per_block  = 4;
-    attention_conf.mask_type         = AttentionMaskType::causalMask;
+    attention_conf.is_causal         = true;
     attention_conf.rope_config.style = RopeStyle::Base;
     attention_conf.rope_config.dim   = attention_conf.size_per_head;
     attention_conf.rope_config.base  = 1000000;
diff --git a/rtp_llm/cpp/devices/rocm_impl/ROCmAttentionOp.cc b/rtp_llm/cpp/devices/rocm_impl/ROCmAttentionOp.cc
index 1f200260d..29551e4e7 100644
--- a/rtp_llm/cpp/devices/rocm_impl/ROCmAttentionOp.cc
+++ b/rtp_llm/cpp/devices/rocm_impl/ROCmAttentionOp.cc
@@ -399,7 +399,7 @@ ParamsPtr FlashInferAttnParams::prepareDecodeFlashInferAttnParams(rtp_llm::Devic
     if (!cuda_device || (dtype != DataType::TYPE_FP16 && dtype != DataType::TYPE_BF16)
         || attn_configs.kv_cache_dtype != KvCacheDataType::BASE
         || (attn_configs.rope_config.style != RopeStyle::Base && attn_configs.rope_config.style != RopeStyle::No)
-        || attn_configs.mask_type != causalMask || attn_configs.q_scaling != 1.0f || attn_configs.use_logn_attn
+        || !attn_configs.is_causal || attn_configs.q_scaling != 1.0f || attn_configs.use_logn_attn
         || (size_per_head != 64 && size_per_head != 128 && size_per_head != 192)
         || (group_size > 10 && group_size != 16)) {
         return nullptr;
@@ -812,7 +812,7 @@ AttentionModuleOutput ROCmDevice::contextAttention(const AttentionModuleParams&
     }
 
     fmha_runner_->setup(
-        datatype, params.configs.mask_type, head_num, kv_head_num, size_per_head, params.configs.q_scaling);
+        datatype, params.configs.is_causal, head_num, kv_head_num, size_per_head, params.configs.q_scaling);
     // auto seq_len_round_32 = (seq_len + 31) / 32 * 32;
     // auto softmax_lse_ = allocateBuffer({DataType::TYPE_FP32, // params.output.type(),
     //                                     {batch_size, head_num, seq_len_round_32},
@@ -941,7 +941,7 @@ AttentionModuleOutput ROCmDevice::contextAttention(const AttentionModuleParams&
             auto attention_mask    = attentionMask({*lengths_host,
                                                     *prefix_lengths_host,
                                                     q_output->type(),
-                                                    params.configs.mask_type == AttentionMaskType::causalMask});
+                                                    params.configs.is_causal});
             auto softmax_qk_output = softmax({std::move(qk_output), *attention_mask, nullopt, scale, datatype});
             softmax_qk_output->updateShape(
                 {batch_size, kv_head_num, (head_num / kv_head_num) * seq_len, seq_len_with_prefix});
diff --git a/rtp_llm/cpp/devices/rocm_impl/ROCmMlaAttentionOp.cc b/rtp_llm/cpp/devices/rocm_impl/ROCmMlaAttentionOp.cc
index cd780c251..d8807bf53 100644
--- a/rtp_llm/cpp/devices/rocm_impl/ROCmMlaAttentionOp.cc
+++ b/rtp_llm/cpp/devices/rocm_impl/ROCmMlaAttentionOp.cc
@@ -286,7 +286,7 @@ AttentionModuleOutput ROCmDevice::mlaContextAttention(const MlaAttentionModulePa
     const size_t hidden_units = head_num * nope_rope_dim;
 
     fmha_runner_->setup(
-        datatype, params.configs.mask_type, head_num, head_num, nope_rope_dim, params.configs.q_scaling);
+        datatype, params.configs.is_causal, head_num, head_num, nope_rope_dim, params.configs.q_scaling);
 
     auto lse_acc_buf = allocateBuffer({DataType::TYPE_FP32, {1, 1, 1, 1}, AllocationType::DEVICE}, {"lse_acc_buf"});
 
diff --git a/rtp_llm/cpp/devices/testing/TestBase.h b/rtp_llm/cpp/devices/testing/TestBase.h
index b7b31a088..cd593db86 100644
--- a/rtp_llm/cpp/devices/testing/TestBase.h
+++ b/rtp_llm/cpp/devices/testing/TestBase.h
@@ -16,7 +16,7 @@
 #include "rtp_llm/cpp/core/torch_utils/BufferTorchUtils.h"
 #include "rtp_llm/cpp/core/Buffer.h"
 #include "rtp_llm/cpp/utils/Logger.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "rtp_llm/cpp/cache/CacheManager.h"
 #include "rtp_llm/cpp/utils/KVCacheUtils.h"
 #include "rtp_llm/cpp/cache/BatchKVCacheResource.h"
@@ -84,10 +84,36 @@ public:
     }
 
     virtual void initTestDevices() {
-        rtp_llm::GptInitParameter gpt_init_params;
-        gpt_init_params.device_resource_config.device_reserve_memory_bytes = device_reserve_memory_size_;
-        gpt_init_params.device_resource_config.host_reserve_memory_bytes   = host_reserve_memory_size_;
-        rtp_llm::DeviceFactory::initDevices(gpt_init_params);
+        rtp_llm::ParallelismConfig parallelism_config;
+        rtp_llm::ModelConfig model_config;
+        rtp_llm::EPLBConfig eplb_config;
+        rtp_llm::FMHAConfig fmha_config;
+        rtp_llm::DeviceResourceConfig device_resource_config;
+        device_resource_config.device_reserve_memory_bytes = device_reserve_memory_size_;
+        device_resource_config.host_reserve_memory_bytes   = host_reserve_memory_size_;
+        rtp_llm::MoeConfig moe_config;
+        rtp_llm::SpeculativeExecutionConfig sp_config;
+        rtp_llm::MiscellaneousConfig misc_config;
+        rtp_llm::ProfilingDebugLoggingConfig profiling_debug_logging_config;
+        rtp_llm::HWKernelConfig hw_kernel_config;
+        rtp_llm::ConcurrencyConfig concurrency_config;
+        rtp_llm::FfnDisAggregateConfig ffn_disaggregate_config;
+        rtp_llm::RuntimeConfig runtime_config;
+        
+        rtp_llm::DeviceFactory::initDevices(
+            parallelism_config,
+            model_config,
+            eplb_config,
+            fmha_config,
+            device_resource_config,
+            moe_config,
+            sp_config,
+            misc_config,
+            profiling_debug_logging_config,
+            hw_kernel_config,
+            concurrency_config,
+            ffn_disaggregate_config,
+            runtime_config);
         device_ = rtp_llm::DeviceFactory::getDefaultDevice();
     }
 
diff --git a/rtp_llm/cpp/disaggregate/cache_store/test/CacheStoreTestBase.h b/rtp_llm/cpp/disaggregate/cache_store/test/CacheStoreTestBase.h
index 4544eecd9..fcd90b8a9 100644
--- a/rtp_llm/cpp/disaggregate/cache_store/test/CacheStoreTestBase.h
+++ b/rtp_llm/cpp/disaggregate/cache_store/test/CacheStoreTestBase.h
@@ -12,7 +12,6 @@ class CacheStoreTestBase: public ::testing::Test {
 public:
     void SetUp() override {
         CacheStoreConfig cache_store_config;
-        cache_store_config.update_from_env_for_test();
         memory_util_       = createMemoryUtilImpl(cache_store_config.cache_store_rdma_mode);
         device_util_       = std::make_shared<DeviceUtil>();
         block_buffer_util_ = std::make_shared<BlockBufferUtil>(memory_util_, device_util_);
@@ -22,7 +21,6 @@ public:
 protected:
     bool initMockMemoryUtil() {
         CacheStoreConfig cache_store_config;
-        cache_store_config.update_from_env_for_test();
         mock_memory_util_ = new MockMemoryUtil(createMemoryUtilImpl(cache_store_config.cache_store_rdma_mode));
         memory_util_.reset(mock_memory_util_);
         block_buffer_util_ = std::make_shared<BlockBufferUtil>(memory_util_, device_util_);
diff --git a/rtp_llm/cpp/disaggregate/cache_store/test/test_util/DeviceUtil.cpp b/rtp_llm/cpp/disaggregate/cache_store/test/test_util/DeviceUtil.cpp
index 68b3bcc12..e3d0f9e8d 100644
--- a/rtp_llm/cpp/disaggregate/cache_store/test/test_util/DeviceUtil.cpp
+++ b/rtp_llm/cpp/disaggregate/cache_store/test/test_util/DeviceUtil.cpp
@@ -1,14 +1,40 @@
 #include "rtp_llm/cpp/disaggregate/cache_store/test/test_util/DeviceUtil.h"
 #include "rtp_llm/cpp/utils/Logger.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 using namespace std;
 
 namespace rtp_llm {
 
 DeviceUtil::DeviceUtil(const DeviceResourceConfig device_resource_config) {
-    GptInitParameter gpt_init_params;
-    gpt_init_params.device_resource_config = device_resource_config;
-    rtp_llm::DeviceFactory::initDevices(gpt_init_params);
+    rtp_llm::ParallelismConfig parallelism_config;
+    rtp_llm::ModelConfig model_config;
+    rtp_llm::EPLBConfig eplb_config;
+    rtp_llm::FMHAConfig fmha_config;
+    rtp_llm::DeviceResourceConfig device_resource_config_copy = device_resource_config;
+    rtp_llm::MoeConfig moe_config;
+    rtp_llm::SpeculativeExecutionConfig sp_config;
+    rtp_llm::MiscellaneousConfig misc_config;
+    rtp_llm::ProfilingDebugLoggingConfig profiling_debug_logging_config;
+    rtp_llm::HWKernelConfig hw_kernel_config;
+    rtp_llm::ConcurrencyConfig concurrency_config;
+    rtp_llm::FfnDisAggregateConfig ffn_disaggregate_config;
+    rtp_llm::RuntimeConfig runtime_config;
+    
+    rtp_llm::DeviceFactory::initDevices(
+        parallelism_config,
+        model_config,
+        eplb_config,
+        fmha_config,
+        device_resource_config_copy,
+        moe_config,
+        sp_config,
+        misc_config,
+        profiling_debug_logging_config,
+        hw_kernel_config,
+        concurrency_config,
+        ffn_disaggregate_config,
+        runtime_config);
     device_ = DeviceFactory::getDefaultDevice();
 }
 
diff --git a/rtp_llm/cpp/embedding_engine/BUILD b/rtp_llm/cpp/embedding_engine/BUILD
index dfc6e7a78..697cc91b1 100644
--- a/rtp_llm/cpp/embedding_engine/BUILD
+++ b/rtp_llm/cpp/embedding_engine/BUILD
@@ -15,6 +15,7 @@ cc_library(
         "//rtp_llm/cpp/engine_base/stream:generate_config",
         "//rtp_llm/cpp/engine_base:worker_status_info",
         "//rtp_llm/cpp/engine_base/stream:stream",
+        "//rtp_llm/cpp/config:eplb_config",
         "//rtp_llm/cpp/models:models",
     ],
     visibility = ["//visibility:public"]
diff --git a/rtp_llm/cpp/embedding_engine/EmbeddingEngine.cc b/rtp_llm/cpp/embedding_engine/EmbeddingEngine.cc
index 1d5987426..42281db26 100644
--- a/rtp_llm/cpp/embedding_engine/EmbeddingEngine.cc
+++ b/rtp_llm/cpp/embedding_engine/EmbeddingEngine.cc
@@ -8,10 +8,26 @@ using namespace std;
 namespace rtp_llm {
 
 EmbeddingEngine::EmbeddingEngine(const EngineInitParams& params, py::object handler):
-    params_(params.gpt_init_parameter), metrics_reporter_(params.metrics_reporter) {
-    rtp_llm::DeviceFactory::initDevices(params.gpt_init_parameter);
+    model_config_(params.model_config_),
+    parallelism_config(params.parallelism_config),
+    concurrency_config(params.concurrency_config),
+    metrics_reporter_(params.metrics_reporter) {
+    rtp_llm::DeviceFactory::initDevices(
+        params.parallelism_config,
+        params.model_config_,
+        params.eplb_config,
+        params.fmha_config,
+        params.device_resource_config,
+        params.moe_config,
+        params.sp_config,
+        params.misc_config,
+        params.profiling_debug_logging_config,
+        params.hw_kernel_config,
+        params.concurrency_config,
+        params.ffn_disaggregate_config,
+        params.runtime_config);
     executor_.reset(new EmbeddingExecutor(params, rtp_llm::DeviceFactory::getDefaultDevice(), handler));
-    scheduler_.reset(new EmbeddingScheduler(params_, metrics_reporter_));
+    scheduler_.reset(new EmbeddingScheduler(model_config_, concurrency_config, params.runtime_config, metrics_reporter_));
 
     (void)startLoop();
 }
@@ -21,9 +37,6 @@ EmbeddingEngine::~EmbeddingEngine() {
     (void)stop();
 }
 
-const rtp_llm::GptInitParameter& EmbeddingEngine::GetGptInitParameter() {
-    return params_;
-}
 
 absl::Status EmbeddingEngine::startLoop() {
     RTP_LLM_LOG_INFO("start embedding engine");
diff --git a/rtp_llm/cpp/embedding_engine/EmbeddingEngine.h b/rtp_llm/cpp/embedding_engine/EmbeddingEngine.h
index 26618a36e..2644f0f1e 100644
--- a/rtp_llm/cpp/embedding_engine/EmbeddingEngine.h
+++ b/rtp_llm/cpp/embedding_engine/EmbeddingEngine.h
@@ -43,14 +43,14 @@ public:
         return resource_context_;
     }
 
-    const rtp_llm::GptInitParameter& GetGptInitParameter();
-
 private:
     absl::Status trySaveStepError() const;
     void         loop();
 
 private:
-    const rtp_llm::GptInitParameter     params_;
+    ModelConfig                     model_config_;
+    ParallelismConfig               parallelism_config;
+    ConcurrencyConfig               concurrency_config;
     std::thread                         loop_thread_;
     std::atomic<bool>                   running_{false};
     std::unique_ptr<EmbeddingExecutor>  executor_;
diff --git a/rtp_llm/cpp/embedding_engine/EmbeddingExecutor.cc b/rtp_llm/cpp/embedding_engine/EmbeddingExecutor.cc
index 235b51042..dfec88154 100644
--- a/rtp_llm/cpp/embedding_engine/EmbeddingExecutor.cc
+++ b/rtp_llm/cpp/embedding_engine/EmbeddingExecutor.cc
@@ -57,11 +57,13 @@ EmbeddingExecutor::EmbeddingExecutor(const EngineInitParams& params, rtp_llm::De
     handler_(handler),
     handler_args_(),
     metrics_reporter_(params.metrics_reporter),
-    params_(params.gpt_init_parameter) {
+    model_config_(params.model_config_),
+    parallelism_config(params.parallelism_config),
+    eplb_config(params.eplb_config) {
     GptModelInitParams model_init_params({
         device_,
         params.gpt_weights,
-        Executor::genModelDescription(params_),
+        Executor::genModelDescription(model_config_, parallelism_config, eplb_config, params.moe_config),
         nullopt,  // no kv cache buffer for embedding executor
     });
 
@@ -73,7 +75,7 @@ EmbeddingExecutor::EmbeddingExecutor(const EngineInitParams& params, rtp_llm::De
         model_.reset(new GptModel(model_init_params));
     }
 
-    init_position_ids(params_.max_seq_len_);
+    init_position_ids(model_config_.max_seq_len);
     std::vector<std::string> handler_args;
     {
         py::gil_scoped_acquire acquire;
@@ -122,8 +124,8 @@ absl::StatusOr<GptModelInputs> EmbeddingExecutor::gatherModelInput(const std::li
     int  token_idx             = 0;
     int  batch_idx             = 0;
     int  position_bias         = 0;
-    if (params_.position_ids_style_ == 1) {
-        position_bias = params_.special_tokens_.pad_token_id_ + 1;
+    if (model_config_.position_ids_style == 1) {
+        position_bias = model_config_.special_tokens.pad_token_id + 1;
     }
 
     std::vector<rtp_llm::BufferPtr> gathered_mm_features;
diff --git a/rtp_llm/cpp/embedding_engine/EmbeddingExecutor.h b/rtp_llm/cpp/embedding_engine/EmbeddingExecutor.h
index 1707978c8..72515b5e5 100644
--- a/rtp_llm/cpp/embedding_engine/EmbeddingExecutor.h
+++ b/rtp_llm/cpp/embedding_engine/EmbeddingExecutor.h
@@ -48,7 +48,9 @@ private:
     py::handle                      torch_type_;
     rtp_llm::BufferPtr              max_position_ids_buf_;
     kmonitor::MetricsReporterPtr    metrics_reporter_ = nullptr;
-    const rtp_llm::GptInitParameter params_;
+    ModelConfig                     model_config_;
+    ParallelismConfig               parallelism_config;
+    EPLBConfig                      eplb_config;
 
     ModelRequest                     generateOldModelRequest(GptModelInputs& model_input);
     absl::StatusOr<GptModelInputs>   gatherModelInput(const std::list<EmbeddingStreamPtr>& streams) const;
diff --git a/rtp_llm/cpp/embedding_engine/EmbeddingScheduler.cc b/rtp_llm/cpp/embedding_engine/EmbeddingScheduler.cc
index 2f8e777fa..8cf8c4def 100644
--- a/rtp_llm/cpp/embedding_engine/EmbeddingScheduler.cc
+++ b/rtp_llm/cpp/embedding_engine/EmbeddingScheduler.cc
@@ -1,15 +1,16 @@
 #include "rtp_llm/cpp/embedding_engine/EmbeddingScheduler.h"
 #include "rtp_llm/cpp/metrics/RtpLLMMetrics.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
 #include "rtp_llm/cpp/utils/Logger.h"
 #include <mutex>
 
 using namespace std;
 namespace rtp_llm {
 
-EmbeddingScheduler::EmbeddingScheduler(const rtp_llm::GptInitParameter&   config,
+EmbeddingScheduler::EmbeddingScheduler(const ModelConfig& model_config,
+                                       const ConcurrencyConfig& concurrency_config,
+                                       const RuntimeConfig& runtime_config,
                                        const kmonitor::MetricsReporterPtr metrics_reporter):
-    config_(config), metrics_reporter_(metrics_reporter) {}
+    model_config_(model_config), concurrency_config_(concurrency_config), runtime_config_(runtime_config), metrics_reporter_(metrics_reporter) {}
 
 EmbeddingScheduler::~EmbeddingScheduler() {
     (void)stop();
@@ -39,7 +40,7 @@ absl::StatusOr<list<EmbeddingStreamPtr>> EmbeddingScheduler::scheduleNew() {
     auto                          it        = waiting_streams_.begin();
     while (it != waiting_streams_.end()) {
         const auto& stream = *it;
-        if (total_len + stream->inputLength() > config_.max_context_batch_size_ * config_.max_seq_len_) {
+        if (total_len + stream->inputLength() > runtime_config_.fifo_scheduler_config.max_context_batch_size * model_config_.max_seq_len) {
             break;
         }
         stream->setStart();
diff --git a/rtp_llm/cpp/embedding_engine/EmbeddingScheduler.h b/rtp_llm/cpp/embedding_engine/EmbeddingScheduler.h
index 570200b1e..ca499b727 100644
--- a/rtp_llm/cpp/embedding_engine/EmbeddingScheduler.h
+++ b/rtp_llm/cpp/embedding_engine/EmbeddingScheduler.h
@@ -2,12 +2,15 @@
 
 #include <queue>
 #include "rtp_llm/cpp/embedding_engine/EmbeddingStream.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 namespace rtp_llm {
 
 class EmbeddingScheduler {
 public:
-    explicit EmbeddingScheduler(const rtp_llm::GptInitParameter&   config,
+    explicit EmbeddingScheduler(const ModelConfig& model_config,
+                                const ConcurrencyConfig& concurrency_config,
+                                const RuntimeConfig& runtime_config,
                                 const kmonitor::MetricsReporterPtr metrics_reporter = nullptr);
 
     ~EmbeddingScheduler();
@@ -25,7 +28,9 @@ public:
 private:
     void reportMetrics(size_t new_stream_size);
 
-    const rtp_llm::GptInitParameter config_;
+    const ModelConfig& model_config_;
+    const ConcurrencyConfig& concurrency_config_;
+    const RuntimeConfig& runtime_config_;
     std::list<EmbeddingStreamPtr>   waiting_streams_;
     std::atomic<bool>               stop_ = false;
     std::mutex                      lock_;
diff --git a/rtp_llm/cpp/embedding_engine/arpc/ArpcServiceCreator.cc b/rtp_llm/cpp/embedding_engine/arpc/ArpcServiceCreator.cc
index e35caa926..1d24d5fcb 100644
--- a/rtp_llm/cpp/embedding_engine/arpc/ArpcServiceCreator.cc
+++ b/rtp_llm/cpp/embedding_engine/arpc/ArpcServiceCreator.cc
@@ -1,13 +1,15 @@
 #include <vector>
 #include <google/protobuf/service.h>
-#include "rtp_llm/cpp/config/GptInitParameter.h"
 #include "rtp_llm/cpp/embedding_engine/EmbeddingEngine.h"
 #include "rtp_llm/cpp/embedding_engine/arpc/ArpcServiceCreator.h"
 
 namespace rtp_llm {
 
 std::unique_ptr<::google::protobuf::Service>
-createEmbeddingArpcService(const rtp_llm::GptInitParameter&              gpt_init_params,
+createEmbeddingArpcService(int64_t model_rpc_port,
+                           int64_t arpc_thread_num,
+                           int64_t arpc_queue_num,
+                           int64_t arpc_io_thread_num,
                            py::object                                    py_render,
                            py::object                                    py_tokenizer,
                            std::shared_ptr<rtp_llm::MultimodalProcessor> mm_processor,
diff --git a/rtp_llm/cpp/embedding_engine/arpc/ArpcServiceCreator.h b/rtp_llm/cpp/embedding_engine/arpc/ArpcServiceCreator.h
index d8d16a877..95d3567c0 100644
--- a/rtp_llm/cpp/embedding_engine/arpc/ArpcServiceCreator.h
+++ b/rtp_llm/cpp/embedding_engine/arpc/ArpcServiceCreator.h
@@ -2,14 +2,17 @@
 
 #include <vector>
 #include <google/protobuf/service.h>
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "rtp_llm/cpp/embedding_engine/EmbeddingEngine.h"
 #include "rtp_llm/cpp/multimodal_processor/MultimodalProcessor.h"
 
 namespace rtp_llm {
 
 std::unique_ptr<::google::protobuf::Service>
-createEmbeddingArpcService(const rtp_llm::GptInitParameter&              gpt_init_params,
+createEmbeddingArpcService(int64_t model_rpc_port,
+                           int64_t arpc_thread_num,
+                           int64_t arpc_queue_num,
+                           int64_t arpc_io_thread_num,
                            py::object                                    py_render,
                            py::object                                    py_tokenizer,
                            std::shared_ptr<rtp_llm::MultimodalProcessor> mm_processor,
diff --git a/rtp_llm/cpp/engine_base/BUILD b/rtp_llm/cpp/engine_base/BUILD
index 161872460..72def1efe 100644
--- a/rtp_llm/cpp/engine_base/BUILD
+++ b/rtp_llm/cpp/engine_base/BUILD
@@ -82,6 +82,7 @@ cc_library(
         "//rtp_llm/cpp/cache:cache",
         "//rtp_llm/cpp/models:lora",
         "//rtp_llm/cpp/engine_base/system_prompt:system_prompt",
+        "//rtp_llm/cpp/config:eplb_config",
         "//rtp_llm/cpp/utils:hash_util",
         "//:rtp_compute_ops",
     ],
diff --git a/rtp_llm/cpp/engine_base/EngineBase.cc b/rtp_llm/cpp/engine_base/EngineBase.cc
index 477ccdf8b..25ccbfb0f 100644
--- a/rtp_llm/cpp/engine_base/EngineBase.cc
+++ b/rtp_llm/cpp/engine_base/EngineBase.cc
@@ -10,7 +10,7 @@ namespace rtp_llm {
 EngineBase::EngineBase(const EngineInitParams& params) {
     initDevices(params);
     lora_manager_ =
-        std::make_shared<lora::LoraManager>(params.gpt_init_parameter.model_specific_config.max_lora_model_size);
+        std::make_shared<lora::LoraManager>(params.model_specific_config.max_lora_model_size);
 }
 
 EngineBase::~EngineBase() {}
@@ -25,10 +25,23 @@ std::shared_ptr<GenerateStream> EngineBase::makeStream(const std::shared_ptr<Gen
 
 void EngineBase::initDevices(const EngineInitParams& params) {
     const auto rank =
-        params.gpt_init_parameter.dp_rank_ * params.gpt_init_parameter.tp_size_ + params.gpt_init_parameter.tp_rank_;
+        params.parallelism_config.dp_rank * params.parallelism_config.tp_size + params.parallelism_config.tp_rank;
     Logger::getEngineLogger().setRank(rank);
     Logger::getEngineLogger().flush();
-    rtp_llm::DeviceFactory::initDevices(params.gpt_init_parameter);
+    rtp_llm::DeviceFactory::initDevices(
+        params.parallelism_config,
+        params.model_config_,
+        params.eplb_config,
+        params.fmha_config,
+        params.device_resource_config,
+        params.moe_config,
+        params.sp_config,
+        params.misc_config,
+        params.profiling_debug_logging_config,
+        params.hw_kernel_config,
+        params.concurrency_config,
+        params.ffn_disaggregate_config,
+        params.runtime_config);
     device_ = rtp_llm::DeviceFactory::getDefaultDevice();
 }
 
diff --git a/rtp_llm/cpp/engine_base/EngineBase.h b/rtp_llm/cpp/engine_base/EngineBase.h
index 385b843b5..98db5a604 100644
--- a/rtp_llm/cpp/engine_base/EngineBase.h
+++ b/rtp_llm/cpp/engine_base/EngineBase.h
@@ -6,7 +6,8 @@
 #include "rtp_llm/cpp/engine_base/EngineInitParams.h"
 #include "rtp_llm/cpp/engine_base/ProposeModelEngineInitParams.h"
 #include "rtp_llm/cpp/cache/KvCacheInfo.h"
-#include "rtp_llm/cpp/models/eplb/EplbConfig.h"
+#include "rtp_llm/cpp/config/EplbConfig.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "rtp_llm/cpp/models/lora/LoraManager.h"
 #include "rtp_llm/cpp/devices/DeviceBase.h"
 #include "rtp_llm/cpp/disaggregate/cache_store/NormalCacheStore.h"
@@ -101,7 +102,7 @@ public:
         return false;
     }
 
-    virtual bool updateEplbConfig(const EplbConfig& config) {
+    virtual bool updateEplbConfig(const EPLBConfig& config) {
         return false;
     }
 
diff --git a/rtp_llm/cpp/engine_base/EngineInitParams.h b/rtp_llm/cpp/engine_base/EngineInitParams.h
index 237115842..0530d6629 100644
--- a/rtp_llm/cpp/engine_base/EngineInitParams.h
+++ b/rtp_llm/cpp/engine_base/EngineInitParams.h
@@ -7,7 +7,8 @@
 #include "rtp_llm/cpp/core/Buffer.h"
 #include "rtp_llm/cpp/devices/DeviceFactory.h"
 #include "rtp_llm/cpp/devices/Weights.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/EplbConfig.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "kmonitor/client/MetricsReporter.h"
 
 namespace th = torch;
@@ -23,40 +24,100 @@ struct EngineInitParams {
     EngineInitParams() {};
     // This class is the only one that holds gpt_weights object globally.
     EngineInitParams(size_t                           model_id,
-                     const rtp_llm::GptInitParameter& gpt_init_parameter,
+                     const ModelConfig&               model_config,
+                     const ParallelismConfig&         parallelism_config,
+                     const RuntimeConfig&              runtime_config,
+                     const PDSepConfig&               pd_sep_config,
+                     const ConcurrencyConfig&         concurrency_config,
+                     const FMHAConfig&                 fmha_config,
+                     const KVCacheConfig&              kv_cache_config,
+                     const ProfilingDebugLoggingConfig& profiling_debug_logging_config,
+                     const HWKernelConfig&             hw_kernel_config,
+                     const DeviceResourceConfig&       device_resource_config,
+                     const MoeConfig&                  moe_config,
+                     const ModelSpecificConfig&        model_specific_config,
+                     const SpeculativeExecutionConfig& sp_config,
+                     const CacheStoreConfig&           cache_store_config,
+                     const MiscellaneousConfig&         misc_config,
+                     const ArpcConfig&                 arpc_config,
+                     const FfnDisAggregateConfig&      ffn_disaggregate_config,
+                     const VitConfig&                  vit_config,
                      rtp_llm::Weights&&               gpt_weights,
-                     py::object                       py_model = py::none()):
+                     py::object                       py_model = py::none(),
+                     py::object                       py_eplb = py::none()):
         model_id(model_id),
-        gpt_init_parameter(gpt_init_parameter),
+        model_config_(model_config),
+        parallelism_config(parallelism_config),
+        runtime_config(runtime_config),
+        eplb_config(model_config.eplb_config),
+        pd_sep_config(pd_sep_config),
+        concurrency_config(concurrency_config),
+        fmha_config(fmha_config),
+        kv_cache_config(kv_cache_config),
+        profiling_debug_logging_config(profiling_debug_logging_config),
+        hw_kernel_config(hw_kernel_config),
+        device_resource_config(device_resource_config),
+        moe_config(moe_config),
+        model_specific_config(model_specific_config),
+        sp_config(sp_config),
+        cache_store_config(cache_store_config),
+        misc_config(misc_config),
+        arpc_config(arpc_config),
+        ffn_disaggregate_config(ffn_disaggregate_config),
+        vit_config(vit_config),
         gpt_weights(std::move(gpt_weights)),
-        py_model(py_model) {
+        py_model(py_model),
+        py_eplb(py_eplb) {
         StaticConfig::user_ft_core_dump_on_exception =
-            gpt_init_parameter.profiling_debug_logging_config.ft_core_dump_on_exception;
-        StaticConfig::user_disable_pdl = gpt_init_parameter.misc_config.disable_pdl;
+            profiling_debug_logging_config.ft_core_dump_on_exception;
+        StaticConfig::user_disable_pdl = misc_config.disable_pdl;
         // default 1 minute and 1000
         ParallelInfo& global_parallel_info    = ParallelInfo::globalParallelInfo();
-        global_parallel_info.setTpSize(gpt_init_parameter.parallelism_distributed_config.tp_size);
-        global_parallel_info.setPpSize(gpt_init_parameter.parallelism_distributed_config.pp_size);
-        global_parallel_info.setEpSize(gpt_init_parameter.parallelism_distributed_config.ep_size);
-        global_parallel_info.setDpSize(gpt_init_parameter.parallelism_distributed_config.dp_size);
-        global_parallel_info.setWorldSize(gpt_init_parameter.parallelism_distributed_config.world_size);
-        global_parallel_info.setWorldRank(gpt_init_parameter.parallelism_distributed_config.world_rank);
-        global_parallel_info.setLocalWorldSize(gpt_init_parameter.parallelism_distributed_config.local_world_size);
-        Logger::log_level_ = gpt_init_parameter.profiling_debug_logging_config.log_level;
-        gpt_init_parameter.showDebugInfo();
+        global_parallel_info.setTpSize(parallelism_config.tp_size);
+        global_parallel_info.setPpSize(parallelism_config.pp_size);
+        global_parallel_info.setEpSize(parallelism_config.ep_size);
+        global_parallel_info.setDpSize(parallelism_config.dp_size);
+        global_parallel_info.setWorldSize(parallelism_config.world_size);
+        global_parallel_info.setWorldRank(parallelism_config.world_rank);
+        global_parallel_info.setLocalWorldSize(parallelism_config.local_world_size);
+        Logger::log_level_ = profiling_debug_logging_config.log_level;
+        showDebugInfo();
     }
 
     size_t                    model_id;
-    rtp_llm::GptInitParameter gpt_init_parameter;
+    ModelConfig               model_config_;
+    ParallelismConfig         parallelism_config;
+    RuntimeConfig             runtime_config;
+    EPLBConfig                eplb_config;
+    PDSepConfig               pd_sep_config;
+    ConcurrencyConfig         concurrency_config;
+    FMHAConfig                fmha_config;
+    KVCacheConfig             kv_cache_config;
+    ProfilingDebugLoggingConfig profiling_debug_logging_config;
+    HWKernelConfig            hw_kernel_config;
+    DeviceResourceConfig      device_resource_config;
+    MoeConfig                 moe_config;
+    ModelSpecificConfig       model_specific_config;
+    SpeculativeExecutionConfig sp_config;
+    CacheStoreConfig          cache_store_config;
+    MiscellaneousConfig       misc_config;
+    ArpcConfig                arpc_config;
+    FfnDisAggregateConfig     ffn_disaggregate_config;
+    VitConfig                 vit_config;
     rtp_llm::Weights          gpt_weights;
     py::object                py_model;
+    py::object                py_eplb;
 
     kmonitor::MetricsReporterPtr metrics_reporter = nullptr;
 
 public:
-    void showGptInitParameter() {
-        gpt_init_parameter.showDebugInfo();
+    void showDebugInfo() const {
+        // Show debug info for all configs
+        RTP_LLM_LOG_INFO("ModelConfig: max_seq_len=%ld, vocab_size=%ld", model_config_.max_seq_len, model_config_.vocab_size);
+        RTP_LLM_LOG_INFO("ParallelismConfig: tp_size=%ld, ep_size=%ld, dp_size=%ld", parallelism_config.tp_size, parallelism_config.ep_size, parallelism_config.dp_size);
+        RTP_LLM_LOG_INFO("RuntimeConfig: max_generate_batch_size=%ld, max_context_batch_size=%ld", runtime_config.max_generate_batch_size, runtime_config.fifo_scheduler_config.max_context_batch_size);
     }
+    
 };
 
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/engine_base/Executor.h b/rtp_llm/cpp/engine_base/Executor.h
index 11041cdb1..8576a1741 100644
--- a/rtp_llm/cpp/engine_base/Executor.h
+++ b/rtp_llm/cpp/engine_base/Executor.h
@@ -4,7 +4,10 @@
 #include "rtp_llm/cpp/engine_base/stream/GenerateStream.h"
 #include "rtp_llm/cpp/models/GptModel.h"
 #include "rtp_llm/cpp/devices/DeviceBase.h"
+#include "rtp_llm/cpp/config/EplbConfig.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include <memory>
+#include <cstdlib>
 
 namespace rtp_llm {
 
@@ -13,64 +16,72 @@ public:
     Executor(rtp_llm::DeviceBase* device): device_(device) {};
     virtual absl::Status process(const std::list<GenerateStreamPtr>& streams) = 0;
 
-    static GptModelDescription genModelDescription(const rtp_llm::GptInitParameter& params) {
-        AttentionConfigs attention_config = params.getAttentionConfigs();
-        int              moe_tp_size      = params.tp_size_ * params.dp_size_ / params.ep_size_;
+    static GptModelDescription genModelDescription(const ModelConfig& model_config,
+                                                   const ParallelismConfig& parallelism_config,
+                                                   const EPLBConfig& eplb_config,
+                                                   const MoeConfig& moe_config) {
+        AttentionConfigs attention_config = model_config.getAttentionConfigs(
+            parallelism_config.tp_size);
+        int              moe_tp_size      = parallelism_config.tp_size * parallelism_config.dp_size / parallelism_config.ep_size;
+        // use_all_gather is now in moe_config, but we need to ensure it's not used
+        // when use_deepep_low_latency is True
+        bool use_all_gather = moe_config.use_all_gather 
+                                  && !moe_config.use_deepep_low_latency;
         // TP在init的时候处理，认为每个MOE Plugin只看到一个TP rank；EP在MOE Plugin中处理；
-        auto                moe_configs = params.moe_style_ ? (std::optional<rtp_llm::MoeConfigs>)rtp_llm::MoeConfigs(
-                                                   {(size_t)params.expert_num_,
-                                                                   (size_t)(params.phy_exp_num_ - params.expert_num_),
-                                                                   (size_t)params.moe_k_,
-                                                                   params.moe_normalize_expert_scale_,
-                                                                   params.moe_inter_padding_size_ / moe_tp_size,
-                                                                   params.has_moe_norm_,
-                                                                   params.use_all_gather_,
-                                                                   (size_t)params.ep_rank_,
-                                                                   (size_t)params.ep_size_,
-                                                                   (size_t)params.tp_rank_,
-                                                                   (size_t)params.tp_size_,
-                                                                   (size_t)params.dp_rank_,
-                                                                   (size_t)params.dp_size_,
-                                                                   (int)params.scoring_func_,
-                                                                   (int)params.moe_topk_group_,
-                                                                   (int)params.moe_n_group_,
-                                                                   params.routed_scaling_factor_,
-                                                                   params.enable_eplb_}) :
+        auto                moe_configs = model_config.moe_style ? (std::optional<rtp_llm::MoeConfigs>)rtp_llm::MoeConfigs(
+                                                   {(size_t)model_config.expert_num,
+                                                                   (size_t)(eplb_config.phy_exp_num(model_config.expert_num) - model_config.expert_num),
+                                                                   (size_t)model_config.moe_k,
+                                                                   model_config.moe_normalize_expert_scale,
+                                                                   model_config.moe_inter_padding_size / moe_tp_size,
+                                                                   model_config.has_moe_norm,
+                                                                   use_all_gather,
+                                                                   (size_t)parallelism_config.ep_rank,
+                                                                   (size_t)parallelism_config.ep_size,
+                                                                   (size_t)parallelism_config.tp_rank,
+                                                                   (size_t)parallelism_config.tp_size,
+                                                                   (size_t)parallelism_config.dp_rank,
+                                                                   (size_t)parallelism_config.dp_size,
+                                                                   (int)model_config.scoring_func,
+                                                                   (int)model_config.moe_topk_group,
+                                                                   (int)model_config.moe_n_group,
+                                                                   model_config.routed_scaling_factor,
+                                                                   eplb_config.enable_eplb()}) :
                                                               std::nullopt;
         rtp_llm::FfnConfigs ffn_config{
-            rtp_llm::getActivationType(params.activation_type_str_),
+            model_config.activation_type,
             std::move(moe_configs),
         };
         rtp_llm::QScheme act_qscheme = rtp_llm::QScheme::NoQuantize;
-        if (params.quant_algo_.isPerTensorQuant()) {
+        if (model_config.quant_algo.isPerTensorQuant()) {
             act_qscheme = rtp_llm::QScheme::Qint8PerTensor;
-        } else if (params.quant_algo_.isSmoothQuant() || params.quant_algo_.isOmniQuant()) {
+        } else if (model_config.quant_algo.isSmoothQuant() || model_config.quant_algo.isOmniQuant()) {
             act_qscheme = rtp_llm::QScheme::Qint8PerToken;
-        } else if (params.quant_algo_.isFp8() && !params.quant_algo_.isGroupwise()) {
+        } else if (model_config.quant_algo.isFp8() && !model_config.quant_algo.isGroupwise()) {
             act_qscheme = rtp_llm::QScheme::Qfp8PerTensor;
-        } else if (params.quant_algo_.isFp8() && params.quant_algo_.isGroupwise()) {
+        } else if (model_config.quant_algo.isFp8() && model_config.quant_algo.isGroupwise()) {
             act_qscheme = rtp_llm::QScheme::Qfp8PerTokenBlock;
-        } else if (params.quant_algo_.isFp8PTPC()) {
+        } else if (model_config.quant_algo.isFp8PTPC()) {
             act_qscheme = rtp_llm::QScheme::Qfp8PerToken;
         }
 
         return {attention_config,
                 ffn_config,
-                rtp_llm::getNormType(params.norm_type_str_),
-                params.data_type_,
+                model_config.norm_type,
+                model_config.data_type,
                 act_qscheme,
-                params.data_type_,
-                params.layernorm_eps_,
-                (size_t)params.vocab_size_,
-                params.layernorm_type_ == rtp_llm::LayerNormType::post_layernorm,
-                params.input_embedding_scalar_,
-                params.residual_scalar_,
-                params.reverse_e_h_norm_};
+                model_config.data_type,
+                model_config.layernorm_eps,
+                (size_t)model_config.vocab_size,
+                model_config.layernorm_type == rtp_llm::LayerNormType::post_layernorm,
+                model_config.input_embedding_scalar,
+                model_config.residual_scalar,
+                model_config.reverse_e_h_norm};
     }
 
     virtual ~Executor() {};
 
-    virtual bool updateEplbConfig(const EplbConfig& config) {
+    virtual bool updateEplbConfig(const EPLBConfig& config) {
         return false;
     }
 
diff --git a/rtp_llm/cpp/engine_base/ProposeModelEngineInitParams.h b/rtp_llm/cpp/engine_base/ProposeModelEngineInitParams.h
index 1ed956f56..a5406b6c1 100644
--- a/rtp_llm/cpp/engine_base/ProposeModelEngineInitParams.h
+++ b/rtp_llm/cpp/engine_base/ProposeModelEngineInitParams.h
@@ -16,11 +16,30 @@ struct ProposeModelEngineInitParams {
     ProposeModelEngineInitParams(size_t                           model_id,
                                  std::string                      sp_type,
                                  size_t                           gen_num_per_circle,
-                                 const rtp_llm::GptInitParameter& gpt_init_parameter,
-                                 rtp_llm::Weights&&               gpt_weights):
+                                 const ModelConfig&               model_config,
+                                 const ParallelismConfig&         parallelism_config,
+                                 const RuntimeConfig&              runtime_config,
+                                 const PDSepConfig&               pd_sep_config,
+                                 const ConcurrencyConfig&         concurrency_config,
+                                 const FMHAConfig&                 fmha_config,
+                                 const KVCacheConfig&              kv_cache_config,
+                                 const ProfilingDebugLoggingConfig& profiling_debug_logging_config,
+                                 const HWKernelConfig&             hw_kernel_config,
+                                 const DeviceResourceConfig&       device_resource_config,
+                                 const MoeConfig&                  moe_config,
+                                 const ModelSpecificConfig&        model_specific_config,
+                                 const SpeculativeExecutionConfig& sp_config,
+                                 const CacheStoreConfig&           cache_store_config,
+                                 const MiscellaneousConfig&         misc_config,
+                                 const ArpcConfig&                 arpc_config,
+                                 const FfnDisAggregateConfig&      ffn_disaggregate_config,
+                                 const VitConfig&                  vit_config,
+                                 rtp_llm::Weights&&               gpt_weights,
+                                 py::object                       py_model = py::none(),
+                                 py::object                       py_eplb = py::none()):
         sp_type(sp_type),
         gen_num_per_circle(gen_num_per_circle),
-        vanilla_model_params(new EngineInitParams(model_id, gpt_init_parameter, std::move(gpt_weights))) {}
+        vanilla_model_params(new EngineInitParams(model_id, model_config, parallelism_config, runtime_config, pd_sep_config, concurrency_config, fmha_config, kv_cache_config, profiling_debug_logging_config, hw_kernel_config, device_resource_config, moe_config, model_specific_config, sp_config, cache_store_config, misc_config, arpc_config, ffn_disaggregate_config, vit_config, std::move(gpt_weights), py_model, py_eplb)) {}
 
     // Consturctor for deterministic propose model
     ProposeModelEngineInitParams(std::string sp_type, size_t gen_num_per_circle):
@@ -39,15 +58,15 @@ struct ProposeModelEngineInitParams {
         return sp_type == "vanilla" || sp_type == "mtp" || sp_type == "eagle3" || sp_type == "eagle";
     }
 
-    const rtp_llm::GptInitParameter& getGptInitParameter() {
+    const EngineInitParams& getEngineInitParams() {
         if (sp_type == "vanilla") {
-            return vanilla_model_params->gpt_init_parameter;
+            return *vanilla_model_params;
         } else if (sp_type == "mtp" || sp_type == "eagle3" || sp_type == "eagle") {
             RTP_LLM_CHECK(!mtp_model_params_->empty());
             RTP_LLM_CHECK(mtp_model_params_->at(0) != nullptr);
-            return mtp_model_params_->at(0)->gpt_init_parameter;
+            return *mtp_model_params_->at(0);
         } else {
-            RTP_LLM_FAIL("error sp type[%s] do not have GptInitParameter", sp_type.c_str());
+            RTP_LLM_FAIL("error sp type[%s] do not have EngineInitParams", sp_type.c_str());
         }
     }
 
diff --git a/rtp_llm/cpp/engine_base/WeightsConverter.h b/rtp_llm/cpp/engine_base/WeightsConverter.h
index d42373043..79ea89ae4 100644
--- a/rtp_llm/cpp/engine_base/WeightsConverter.h
+++ b/rtp_llm/cpp/engine_base/WeightsConverter.h
@@ -10,7 +10,7 @@
 #include "rtp_llm/cpp/core/Buffer.h"
 #include "rtp_llm/cpp/devices/DeviceFactory.h"
 #include "rtp_llm/cpp/devices/Weights.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 
 namespace th = torch;
 
diff --git a/rtp_llm/cpp/engine_base/schedulers/BatchDecodeScheduler.h b/rtp_llm/cpp/engine_base/schedulers/BatchDecodeScheduler.h
index 2ba203f31..23eb11887 100644
--- a/rtp_llm/cpp/engine_base/schedulers/BatchDecodeScheduler.h
+++ b/rtp_llm/cpp/engine_base/schedulers/BatchDecodeScheduler.h
@@ -23,14 +23,14 @@ public:
         kBatchDecode  = 0,
         kBatchPrefill = 1
     };
-    BatchDecodeScheduler(const rtp_llm::GptInitParameter&     params,
+    BatchDecodeScheduler(const RuntimeConfig&                 runtime_config,
                          const std::shared_ptr<CacheManager>& cache_manager,
                          const kmonitor::MetricsReporterPtr   metrics_reporter,
                          rtp_llm::DeviceBase*                 device) {
         cache_manager_    = cache_manager;
         device_           = device;
         metrics_reporter_ = metrics_reporter;
-        batch_size_       = params.batch_decode_scheduler_config.batch_decode_scheduler_batch_size;
+        batch_size_       = runtime_config.batch_decode_scheduler_config.batch_decode_scheduler_batch_size;
         scheduler_type_   = SchedulerType::kBatchDecode;
     }
     virtual ~BatchDecodeScheduler() = default;
diff --git a/rtp_llm/cpp/engine_base/schedulers/FIFOScheduler.cc b/rtp_llm/cpp/engine_base/schedulers/FIFOScheduler.cc
index 1bbba192e..8012ee751 100644
--- a/rtp_llm/cpp/engine_base/schedulers/FIFOScheduler.cc
+++ b/rtp_llm/cpp/engine_base/schedulers/FIFOScheduler.cc
@@ -8,22 +8,27 @@
 using namespace std;
 namespace rtp_llm {
 
-FIFOScheduler::FIFOScheduler(const rtp_llm::GptInitParameter&     params,
+FIFOScheduler::FIFOScheduler(const RuntimeConfig&                 runtime_config,
+                             const ModelConfig&                   model_config,
+                             const PDSepConfig&                  pd_sep_config,
+                             const ParallelismConfig&            parallelism_config,
+                             const ModelSpecificConfig&          model_specific_config,
                              const std::shared_ptr<CacheManager>& cache_manager,
                              const kmonitor::MetricsReporterPtr   metrics_reporter,
                              const int                            max_score_len):
-    params_(params),
+    pd_sep_config_(pd_sep_config),
+    model_specific_config_(model_specific_config),
     cache_manager_(cache_manager),
-    max_seq_len_(params.max_seq_len_),
-    max_batch_tokens_size_(params.max_batch_tokens_size_),
-    max_generate_batch_size_(params.max_generate_batch_size_),
-    reserve_block_num_(params.scheduler_reserve_resource_ratio_ * cache_manager->availableBlockNums() / 100),
+    max_seq_len_(model_config.max_seq_len),
+    max_batch_tokens_size_(runtime_config.fifo_scheduler_config.max_batch_tokens_size),
+    max_generate_batch_size_(runtime_config.max_generate_batch_size),
+    reserve_block_num_(runtime_config.fifo_scheduler_config.scheduler_reserve_resource_ratio * cache_manager->availableBlockNums() / 100),
     // not support fallback when use pd_speration:use_cache_store
-    enable_partial_fallback_(params.enable_partial_fallback_ && params.role_type_ == RoleType::PDFUSION),
-    enable_whole_fallback_(params.role_type_ == RoleType::PDFUSION),
-    enable_fast_gen_(params.enable_fast_gen_),
-    need_fill_fake_stream_(params.dp_size_ > 1 && params.tp_rank_ == 0),
-    fast_gen_max_context_len_(params.fast_gen_max_context_len_),
+    enable_partial_fallback_(runtime_config.fifo_scheduler_config.enable_partial_fallback && pd_sep_config.role_type == RoleType::PDFUSION),
+    enable_whole_fallback_(pd_sep_config.role_type == RoleType::PDFUSION),
+    enable_fast_gen_(runtime_config.fifo_scheduler_config.enable_fast_gen),
+    need_fill_fake_stream_(parallelism_config.dp_size > 1 && parallelism_config.tp_rank == 0),
+    fast_gen_max_context_len_(runtime_config.fifo_scheduler_config.fast_gen_max_context_len),
     metrics_reporter_(metrics_reporter) {
     RTP_LLM_LOG_INFO("max_generate_batch_size %d", max_generate_batch_size_);
     RTP_LLM_LOG_INFO("max_batch_tokens_size %d", max_batch_tokens_size_);
@@ -192,12 +197,12 @@ tuple<int, int> FIFOScheduler::evaluateRunningNext(size_t reserve_step) {
 
 bool FIFOScheduler::evaluateRunningMemory(const list<GenerateStreamPtr>& streams,
                                           const GenerateStreamPtr&       new_stream) const {
-    if (params_.role_type_ == RoleType::DECODE) {
+    if (pd_sep_config_.role_type == RoleType::DECODE) {
         if (running_streams_.size() + streams.size() + 1 < max_generate_batch_size_) {
             return true;
         }
     }
-    if (params_.model_specific_config.load_python_model) {
+    if (model_specific_config_.load_python_model) {
         // new model py not support prefill and decode togather now
         if (!running_streams_.empty()) {
             return false;
diff --git a/rtp_llm/cpp/engine_base/schedulers/FIFOScheduler.h b/rtp_llm/cpp/engine_base/schedulers/FIFOScheduler.h
index ecaa2cc26..7d3d35d1d 100644
--- a/rtp_llm/cpp/engine_base/schedulers/FIFOScheduler.h
+++ b/rtp_llm/cpp/engine_base/schedulers/FIFOScheduler.h
@@ -8,13 +8,17 @@
 #include "rtp_llm/cpp/engine_base/stream/GenerateTypes.h"
 #include "rtp_llm/cpp/engine_base/schedulers/SchedulerBase.h"
 #include "kmonitor/client/MetricsReporter.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "rtp_llm/cpp/engine_base/schedulers/EngineScheduleInfo.h"
 namespace rtp_llm {
 
 class FIFOScheduler: public SchedulerBase {
 public:
-    explicit FIFOScheduler(const rtp_llm::GptInitParameter&     params,
+    explicit FIFOScheduler(const RuntimeConfig&                 runtime_config,
+                           const ModelConfig&                   model_config,
+                           const PDSepConfig&                  pd_sep_config,
+                           const ParallelismConfig&            parallelism_config,
+                           const ModelSpecificConfig&          model_specific_config,
                            const std::shared_ptr<CacheManager>& cache_manager,
                            const kmonitor::MetricsReporterPtr   metrics_reporter = nullptr,
                            const int                            max_score_len    = 1);
@@ -55,7 +59,8 @@ private:
     bool                         waitPredicate();
 
 protected:
-    rtp_llm::GptInitParameter     params_;
+    PDSepConfig                   pd_sep_config_;
+    ModelSpecificConfig           model_specific_config_;
     std::list<GenerateStreamPtr>  waiting_streams_;
     std::list<GenerateStreamPtr>  running_streams_;
     std::list<GenerateStreamPtr>  remote_running_streams_;
diff --git a/rtp_llm/cpp/engine_base/schedulers/GatherBatchScheduler.h b/rtp_llm/cpp/engine_base/schedulers/GatherBatchScheduler.h
index 834a0bb8a..fc5952522 100644
--- a/rtp_llm/cpp/engine_base/schedulers/GatherBatchScheduler.h
+++ b/rtp_llm/cpp/engine_base/schedulers/GatherBatchScheduler.h
@@ -14,11 +14,15 @@ struct GatherBatchSchedulerConfigLocal: public autil::legacy::Jsonizable {
 // Currently it is only used in CI with prompt_batch input, which may occur unstable result
 class GatherBatchScheduler: virtual public FIFOScheduler {
 public:
-    explicit GatherBatchScheduler(const rtp_llm::GptInitParameter&     params,
+    explicit GatherBatchScheduler(const RuntimeConfig&                 runtime_config,
+                                  const ModelConfig&                   model_config,
+                                  const PDSepConfig&                  pd_sep_config,
+                                  const ParallelismConfig&            parallelism_config,
+                                  const ModelSpecificConfig&          model_specific_config,
                                   const std::shared_ptr<CacheManager>& cache_manager,
                                   const kmonitor::MetricsReporterPtr   metrics_reporter,
                                   const int                            max_score_len = 1):
-        FIFOScheduler(params, cache_manager, metrics_reporter, max_score_len) {
+        FIFOScheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager, metrics_reporter, max_score_len) {
         RTP_LLM_LOG_INFO("GatherBatchScheduler init");
         gather_batch_size_ = 1;
     }
diff --git a/rtp_llm/cpp/engine_base/schedulers/test/BUILD b/rtp_llm/cpp/engine_base/schedulers/test/BUILD
index 45eac81c3..a35983395 100644
--- a/rtp_llm/cpp/engine_base/schedulers/test/BUILD
+++ b/rtp_llm/cpp/engine_base/schedulers/test/BUILD
@@ -7,7 +7,7 @@ test_deps = [
     "//rtp_llm/cpp/utils:core_utils",
     "//rtp_llm/cpp/normal_engine:normal_engine",
     "//rtp_llm/cpp/engine_base/schedulers:schedulers",
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
     "@com_google_googletest//:gtest",
     "@com_google_googletest//:gtest_main",
     "@local_config_cuda//cuda:cuda_headers",
diff --git a/rtp_llm/cpp/engine_base/schedulers/test/FIFOSchedulerTest.cc b/rtp_llm/cpp/engine_base/schedulers/test/FIFOSchedulerTest.cc
index 8febd5aa0..22a99b61d 100644
--- a/rtp_llm/cpp/engine_base/schedulers/test/FIFOSchedulerTest.cc
+++ b/rtp_llm/cpp/engine_base/schedulers/test/FIFOSchedulerTest.cc
@@ -7,6 +7,7 @@
 #include "rtp_llm/cpp/normal_engine/NormalGenerateStream.h"
 #include "rtp_llm/cpp/core/Types.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 using namespace std;
 
@@ -24,15 +25,19 @@ TEST_F(FIFOSchedulerTest, testSimple) {
     ResourceContext resource_context;
     resource_context.cache_manager = cache_manager;
 
-    GptInitParameter config;
-    config.max_seq_len_             = 8192;
-    config.max_generate_batch_size_ = 100;
-    config.max_batch_tokens_size_   = 8192;
-    FIFOScheduler                  scheduler(config, cache_manager);
+    ModelConfig model_config;
+    model_config.max_seq_len             = 8192;
+    RuntimeConfig runtime_config;
+    runtime_config.max_generate_batch_size = 100;
+    runtime_config.fifo_scheduler_config.max_batch_tokens_size   = 8192;
+    PDSepConfig pd_sep_config;
+    ParallelismConfig parallelism_config;
+    ModelSpecificConfig model_specific_config;
+    FIFOScheduler                  scheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager);
     std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
     query->input_ids                     = createBuffer<int32_t>({1}, {1}, AllocationType::HOST);
     query->generate_config               = make_shared<GenerateConfig>();
-    shared_ptr<GenerateStream> stream    = make_shared<NormalGenerateStream>(query, config, resource_context, nullptr);
+    shared_ptr<GenerateStream> stream    = make_shared<NormalGenerateStream>(query, model_config, runtime_config, resource_context, nullptr);
     ASSERT_TRUE(scheduler.enqueue(stream).ok());
     auto streams_status = scheduler.schedule();
     ASSERT_TRUE(streams_status.ok());
@@ -59,15 +64,19 @@ TEST_F(FIFOSchedulerTest, testInitKVCacheLackMem) {
     ASSERT_EQ(cache_manager->freeBlockNums(), 1);
     ResourceContext resource_context;
     resource_context.cache_manager = cache_manager;
-    GptInitParameter config;
-    config.max_seq_len_             = 8192;
-    config.max_generate_batch_size_ = 100;
-    config.max_batch_tokens_size_   = 8192;
-    FIFOScheduler                  scheduler(config, cache_manager);
+    ModelConfig model_config;
+    model_config.max_seq_len             = 8192;
+    RuntimeConfig runtime_config;
+    runtime_config.max_generate_batch_size = 100;
+    runtime_config.fifo_scheduler_config.max_batch_tokens_size   = 8192;
+    PDSepConfig pd_sep_config;
+    ParallelismConfig parallelism_config;
+    ModelSpecificConfig model_specific_config;
+    FIFOScheduler                  scheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager);
     std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
     query->input_ids                     = createBuffer<int32_t>({3}, {1, 2, 3}, AllocationType::HOST);
     query->generate_config               = make_shared<GenerateConfig>();
-    shared_ptr<GenerateStream> stream    = make_shared<NormalGenerateStream>(query, config, resource_context, nullptr);
+    shared_ptr<GenerateStream> stream    = make_shared<NormalGenerateStream>(query, model_config, runtime_config, resource_context, nullptr);
     ASSERT_TRUE(scheduler.enqueue(stream).ok());
     auto streams_status = scheduler.schedule();
     ASSERT_TRUE(streams_status.ok());
@@ -90,11 +99,15 @@ TEST_F(FIFOSchedulerTest, testIncrKVCacheLackMem) {
     ASSERT_EQ(cache_manager->freeBlockNums(), 2);
     ResourceContext resource_context;
     resource_context.cache_manager = cache_manager;
-    GptInitParameter config;
-    config.max_seq_len_             = 8192;
-    config.max_generate_batch_size_ = 100;
-    config.max_batch_tokens_size_   = 8192;
-    FIFOScheduler                  scheduler(config, cache_manager);
+    ModelConfig model_config;
+    model_config.max_seq_len             = 8192;
+    RuntimeConfig runtime_config;
+    runtime_config.max_generate_batch_size = 100;
+    runtime_config.fifo_scheduler_config.max_batch_tokens_size   = 8192;
+    PDSepConfig pd_sep_config;
+    ParallelismConfig parallelism_config;
+    ModelSpecificConfig model_specific_config;
+    FIFOScheduler                  scheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager);
     std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
     query->input_ids                     = createBuffer<int32_t>({4}, {1, 2, 3, 4}, AllocationType::HOST);
     query->generate_config               = make_shared<GenerateConfig>();
@@ -131,23 +144,27 @@ TEST_F(FIFOSchedulerTest, testIncrKVCacheFallBackReleaseAllBlocks) {
     ResourceContext resource_context;
     resource_context.cache_manager = cache_manager;
     resource_context.reuse_cache   = false;
-    GptInitParameter config;
-    config.max_seq_len_             = 8192;
-    config.max_generate_batch_size_ = 100;
-    config.max_batch_tokens_size_   = 8192;
-    FIFOScheduler scheduler(config, cache_manager);
+    ModelConfig model_config;
+    model_config.max_seq_len             = 8192;
+    RuntimeConfig runtime_config;
+    runtime_config.max_generate_batch_size = 100;
+    runtime_config.fifo_scheduler_config.max_batch_tokens_size   = 8192;
+    PDSepConfig pd_sep_config;
+    ParallelismConfig parallelism_config;
+    ModelSpecificConfig model_specific_config;
+    FIFOScheduler scheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager);
     scheduler.enable_partial_fallback_    = false;
     std::shared_ptr<GenerateInput> query1 = make_shared<GenerateInput>();
     query1->request_id                    = 1;
     query1->input_ids                     = createBuffer<int32_t>({4}, {1, 2, 3, 4}, AllocationType::HOST);
     query1->generate_config               = make_shared<GenerateConfig>();
-    shared_ptr<GenerateStream> stream1 = make_shared<NormalGenerateStream>(query1, config, resource_context, nullptr);
+    shared_ptr<GenerateStream> stream1 = make_shared<NormalGenerateStream>(query1, model_config, runtime_config, resource_context, nullptr);
 
     std::shared_ptr<GenerateInput> query2 = make_shared<GenerateInput>();
     query2->request_id                    = 2;
     query2->input_ids                     = createBuffer<int32_t>({4}, {1, 2, 3, 4}, AllocationType::HOST);
     query2->generate_config               = make_shared<GenerateConfig>();
-    shared_ptr<GenerateStream> stream2 = make_shared<NormalGenerateStream>(query2, config, resource_context, nullptr);
+    shared_ptr<GenerateStream> stream2 = make_shared<NormalGenerateStream>(query2, model_config, runtime_config, resource_context, nullptr);
 
     ASSERT_TRUE(scheduler.enqueue(stream1).ok());
     ASSERT_TRUE(scheduler.enqueue(stream2).ok());
@@ -198,11 +215,15 @@ TEST_F(FIFOSchedulerTest, testIncrKVCacheFallBackReleasePartBlocks) {
     ASSERT_EQ(cache_manager->freeBlockNums(), 5);
     ResourceContext resource_context;
     resource_context.cache_manager = cache_manager;
-    GptInitParameter config;
-    config.max_seq_len_             = 8192;
-    config.max_generate_batch_size_ = 100;
-    config.max_batch_tokens_size_   = 8192;
-    FIFOScheduler scheduler(config, cache_manager);
+    ModelConfig model_config;
+    model_config.max_seq_len             = 8192;
+    RuntimeConfig runtime_config;
+    runtime_config.max_generate_batch_size = 100;
+    runtime_config.fifo_scheduler_config.max_batch_tokens_size   = 8192;
+    PDSepConfig pd_sep_config;
+    ParallelismConfig parallelism_config;
+    ModelSpecificConfig model_specific_config;
+    FIFOScheduler scheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager);
     scheduler.enable_partial_fallback_   = true;
     std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
     query->input_ids                     = createBuffer<int32_t>({4}, {1, 2, 3, 4}, AllocationType::HOST);
@@ -270,11 +291,15 @@ TEST_F(FIFOSchedulerTest, testReuseCache) {
     std::shared_ptr<CacheManager> cache_manager = make_shared<CacheManager>(cache_config, device_);
     ASSERT_EQ(cache_manager->freeBlockNums(), 10);
     ResourceContext  resource_context = {cache_manager, nullptr, nullptr, true};
-    GptInitParameter config;
-    config.max_seq_len_             = 8192;
-    config.max_generate_batch_size_ = 100;
-    config.max_batch_tokens_size_   = 8192;
-    FIFOScheduler scheduler(config, cache_manager);
+    ModelConfig model_config;
+    model_config.max_seq_len             = 8192;
+    RuntimeConfig runtime_config;
+    runtime_config.max_generate_batch_size = 100;
+    runtime_config.fifo_scheduler_config.max_batch_tokens_size   = 8192;
+    PDSepConfig pd_sep_config;
+    ParallelismConfig parallelism_config;
+    ModelSpecificConfig model_specific_config;
+    FIFOScheduler scheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager);
 
     std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
     query->input_ids                     = createBuffer<int32_t>({5}, {1, 2, 3, 4, 5}, AllocationType::HOST);
@@ -297,7 +322,7 @@ TEST_F(FIFOSchedulerTest, testReuseCache) {
     std::shared_ptr<GenerateInput> query2 = make_shared<GenerateInput>();
     query2->input_ids                     = createBuffer<int32_t>({7}, {1, 2, 3, 4, 5, 6, 7}, AllocationType::HOST);
     query2->generate_config               = make_shared<GenerateConfig>();
-    shared_ptr<GenerateStream> stream2 = make_shared<NormalGenerateStream>(query2, config, resource_context, nullptr);
+    shared_ptr<GenerateStream> stream2 = make_shared<NormalGenerateStream>(query2, model_config, runtime_config, resource_context, nullptr);
     ASSERT_TRUE(scheduler.enqueue(stream2).ok());
 
     auto streams_status3 = scheduler.schedule();
@@ -318,11 +343,15 @@ TEST_F(FIFOSchedulerTest, testMaxContextBatchSize) {
     std::shared_ptr<CacheManager> cache_manager = make_shared<CacheManager>(cache_config, device_);
     ASSERT_EQ(cache_manager->freeBlockNums(), 20);
     ResourceContext  resource_context = {cache_manager, nullptr, nullptr, true};
-    GptInitParameter config;
-    config.max_seq_len_            = 100;
-    config.max_context_batch_size_ = 1;
-    config.max_batch_tokens_size_  = 100;
-    FIFOScheduler scheduler(config, cache_manager);
+    ModelConfig model_config;
+    model_config.max_seq_len            = 100;
+    RuntimeConfig runtime_config;
+    runtime_config.fifo_scheduler_config.max_context_batch_size = 1;
+    runtime_config.fifo_scheduler_config.max_batch_tokens_size  = 100;
+    PDSepConfig pd_sep_config;
+    ParallelismConfig parallelism_config;
+    ModelSpecificConfig model_specific_config;
+    FIFOScheduler scheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager);
 
     {
         // test normalcase
@@ -330,7 +359,7 @@ TEST_F(FIFOSchedulerTest, testMaxContextBatchSize) {
         query->input_ids                     = createBuffer<int32_t>({5}, {1, 2, 3, 4, 5}, AllocationType::HOST);
         query->generate_config               = make_shared<GenerateConfig>();
         shared_ptr<GenerateStream> stream1 =
-            make_shared<NormalGenerateStream>(query, config, resource_context, nullptr);
+            make_shared<NormalGenerateStream>(query, model_config, runtime_config, resource_context, nullptr);
         ASSERT_TRUE(scheduler.enqueue(stream1).ok());
 
         auto streams_status = scheduler.schedule();
@@ -352,7 +381,7 @@ TEST_F(FIFOSchedulerTest, testMaxContextBatchSize) {
         query->generate_config               = make_shared<GenerateConfig>();
         query->generate_config->num_beams    = 2;
         shared_ptr<GenerateStream> stream1 =
-            make_shared<NormalGenerateStream>(query, config, resource_context, nullptr);
+            make_shared<NormalGenerateStream>(query, model_config, runtime_config, resource_context, nullptr);
         ASSERT_TRUE(scheduler.enqueue(stream1).ok());
 
         auto streams_status = scheduler.schedule();
@@ -401,24 +430,28 @@ TEST_F(FIFOSchedulerTest, testBatchEnqueue) {
     ResourceContext resource_context;
     resource_context.cache_manager = cache_manager;
 
-    GptInitParameter config;
-    config.max_seq_len_             = 8192;
-    config.max_generate_batch_size_ = 100;
-    config.max_batch_tokens_size_   = 8192;
-    FIFOScheduler             scheduler(config, cache_manager);
+    ModelConfig model_config;
+    model_config.max_seq_len             = 8192;
+    RuntimeConfig runtime_config;
+    runtime_config.max_generate_batch_size = 100;
+    runtime_config.fifo_scheduler_config.max_batch_tokens_size   = 8192;
+    PDSepConfig pd_sep_config;
+    ParallelismConfig parallelism_config;
+    ModelSpecificConfig model_specific_config;
+    FIFOScheduler             scheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager);
     vector<GenerateStreamPtr> streams;
     {
         std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
         query->input_ids                     = createBuffer<int32_t>({1}, {1}, AllocationType::HOST);
         query->generate_config               = make_shared<GenerateConfig>();
-        shared_ptr<GenerateStream> stream = make_shared<NormalGenerateStream>(query, config, resource_context, nullptr);
+        shared_ptr<GenerateStream> stream = make_shared<NormalGenerateStream>(query, model_config, runtime_config, resource_context, nullptr);
         streams.push_back(stream);
     }
     {
         std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
         query->input_ids                     = createBuffer<int32_t>({1}, {1}, AllocationType::HOST);
         query->generate_config               = make_shared<GenerateConfig>();
-        shared_ptr<GenerateStream> stream = make_shared<NormalGenerateStream>(query, config, resource_context, nullptr);
+        shared_ptr<GenerateStream> stream = make_shared<NormalGenerateStream>(query, model_config, runtime_config, resource_context, nullptr);
         streams.push_back(stream);
     }
     ASSERT_TRUE(scheduler.batchEnqueue(streams).ok());
diff --git a/rtp_llm/cpp/engine_base/stream/BUILD b/rtp_llm/cpp/engine_base/stream/BUILD
index 6c9847ac3..c7ad3d031 100644
--- a/rtp_llm/cpp/engine_base/stream/BUILD
+++ b/rtp_llm/cpp/engine_base/stream/BUILD
@@ -6,6 +6,8 @@ cc_library(
         "GenerateConfig.h",
     ],
     deps = [
+        "//rtp_llm/cpp/config:role_types",
+        "//rtp_llm/cpp/config:special_tokens",
         "//:rtp_compute_ops",
     ],
     visibility = ["//visibility:public"],
diff --git a/rtp_llm/cpp/engine_base/stream/GenerateConfig.h b/rtp_llm/cpp/engine_base/stream/GenerateConfig.h
index 39771e66a..b2e1386c6 100644
--- a/rtp_llm/cpp/engine_base/stream/GenerateConfig.h
+++ b/rtp_llm/cpp/engine_base/stream/GenerateConfig.h
@@ -6,8 +6,9 @@
 #include <vector>
 
 #include "rtp_llm/cpp/utils/StringUtil.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
 #include "autil/legacy/jsonizable.h"
+#include "rtp_llm/cpp/config/RoleTypes.h"
+#include "rtp_llm/cpp/config/SpecialTokens.h"
 
 namespace rtp_llm {
 
@@ -104,14 +105,14 @@ public:
     }
 
     void addSpecialTokens(const rtp_llm::SpecialTokens& special_tokens) {
-        for (const auto& vec : special_tokens.stop_words_id_list_) {
+        for (const auto& vec : special_tokens.stop_words_id_list) {
             std::vector<int> tmpVec;
             for (int64_t val : vec) {
                 tmpVec.push_back(static_cast<int>(val));
             }
             stop_words_list.push_back(tmpVec);
         }
-        const auto& vec = special_tokens.stop_words_str_list_;
+        const auto& vec = special_tokens.stop_words_str_list;
         stop_words_str.insert(stop_words_str.begin(), vec.begin(), vec.end());
     }
 
diff --git a/rtp_llm/cpp/engine_base/stream/GenerateStream.cc b/rtp_llm/cpp/engine_base/stream/GenerateStream.cc
index 79e16f74b..19a565279 100644
--- a/rtp_llm/cpp/engine_base/stream/GenerateStream.cc
+++ b/rtp_llm/cpp/engine_base/stream/GenerateStream.cc
@@ -14,33 +14,34 @@
 #include "rtp_llm/cpp/core/Types.h"
 #include "rtp_llm/cpp/core/torch_utils/BufferTorchUtils.h"
 #include "rtp_llm/cpp/devices/DeviceFactory.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 
 using namespace std;
 
 namespace rtp_llm {
 
 GenerateStream::GenerateStream(const shared_ptr<GenerateInput>& input,
-                               const rtp_llm::GptInitParameter& params,
-                               const ResourceContext&           resource_context,
-                               kmonitor::MetricsReporterPtr     metrics_reporter,
-                               size_t                           extra_reserve_token_num,
-                               bool                             perf_test):
+                               const ModelConfig&                model_config,
+                               const RuntimeConfig&               runtime_config,
+                               const ResourceContext&            resource_context,
+                               kmonitor::MetricsReporterPtr      metrics_reporter,
+                               size_t                             extra_reserve_token_num,
+                               bool                               perf_test):
     generate_input_(input),
-    max_seq_len_(params.max_seq_len_),
-    vocab_size_(params.vocab_size_),
+    max_seq_len_(model_config.max_seq_len),
+    vocab_size_(model_config.vocab_size),
     stream_cache_resource_(std::make_shared<StreamCacheResource>(
         this, resource_context, input->need_release_resource, input->generate_config->adapter_name)),
     need_release_resource_(input->need_release_resource),
-    enable_fast_gen_(params.enable_fast_gen_),
+    enable_fast_gen_(runtime_config.fifo_scheduler_config.enable_fast_gen),
     gen_timeline_(input->generate_config->gen_timeline),
     metrics_reporter_(metrics_reporter),
-    special_tokens_(params.special_tokens_),
+    special_tokens_(model_config.special_tokens),
     output_mutex_(std::make_shared<std::mutex>()),
     cv_(std::make_shared<std::condition_variable>()),
-    mm_position_ids_style_(PositionIdsStyle(params.mm_position_ids_style_)),
-    dtype_(params.data_type_),
-    hidden_size_(params.hidden_size_) {
+    mm_position_ids_style_(PositionIdsStyle(model_config.position_ids_style)),
+    dtype_(model_config.data_type),
+    hidden_size_(model_config.hidden_size) {
     if (!updatePrefix(resource_context.system_prompt)) {
         return;
     }
@@ -71,7 +72,7 @@ GenerateStream::GenerateStream(const shared_ptr<GenerateInput>& input,
         setReturnLastHiddenStates(true);
     }
     complete_token_ids_ = std::make_shared<CompleteTokenIds>(
-        device_, init_batch_size, maxBatchSize(), max_seq_len_, params.seq_size_per_block_);
+        device_, init_batch_size, maxBatchSize(), max_seq_len_, model_config.attn_config.tokens_per_block);
     complete_token_ids_->init(input, extra_reserve_token_num);
 
     last_output_pos_ = seqLength();
@@ -95,7 +96,7 @@ GenerateStream::GenerateStream(const shared_ptr<GenerateInput>& input,
     think_logits_processor_ptr_ = ThinkModeLogitsProcessor::fromGenerateInput(device_, generate_input_, maxBatchSize());
     tree_logits_processor_ptr_  = TreeLogitsProcessor::fromGenerateInput(device_, generate_input_, init_batch_size);
     multi_seq_logits_processor_ptr_ =
-        MultiSeqLogitsProcessor::fromGenerateInput(device_, generate_input_, special_tokens_.eos_token_id_);
+        MultiSeqLogitsProcessor::fromGenerateInput(device_, generate_input_, special_tokens_.eos_token_id);
 
     initializeLogitsProcessorList();
     if (generateConfig()->random_seed.has_value()) {
@@ -765,7 +766,7 @@ void GenerateStream::matchEosToken() {
 
 void GenerateStream::matchEosToken(int batch_id) {
     if ((!generate_input_->generate_config->ignore_eos)
-        && complete_token_ids_->matchEosToken(batch_id, special_tokens_.eos_token_id_)) {
+        && complete_token_ids_->matchEosToken(batch_id, special_tokens_.eos_token_id)) {
         sub_generate_status_[batch_id].status = StreamState::FINISHED;
     }
 }
@@ -808,7 +809,7 @@ void GenerateStream::matchStopWordsList(int batch_id) {
     bool match = false;
     for (auto& stop_words : generate_input_->generate_config->stop_words_list) {
         if (generate_input_->generate_config->ignore_eos && stop_words.size() == 1
-            && stop_words[0] == special_tokens_.eos_token_id_) {
+            && stop_words[0] == special_tokens_.eos_token_id) {
             continue;
         }
         if (complete_token_ids_->matchStopWordsList(batch_id, stop_words)) {
diff --git a/rtp_llm/cpp/engine_base/stream/GenerateStream.h b/rtp_llm/cpp/engine_base/stream/GenerateStream.h
index b6c686649..df5b56662 100644
--- a/rtp_llm/cpp/engine_base/stream/GenerateStream.h
+++ b/rtp_llm/cpp/engine_base/stream/GenerateStream.h
@@ -77,7 +77,8 @@ using GenerateStreamPtr = std::shared_ptr<GenerateStream>;
 class GenerateStream {
 public:
     GenerateStream(const std::shared_ptr<GenerateInput>& query,
-                   const rtp_llm::GptInitParameter&      params,
+                   const ModelConfig&                    model_config,
+                   const RuntimeConfig&                  runtime_config,
                    const ResourceContext&                resource_context,
                    kmonitor::MetricsReporterPtr          metrics_reporter,
                    size_t                                extra_reserve_token_num = 0,
diff --git a/rtp_llm/cpp/engine_base/stream/test/BUILD b/rtp_llm/cpp/engine_base/stream/test/BUILD
index d67c2fd01..5906b6176 100644
--- a/rtp_llm/cpp/engine_base/stream/test/BUILD
+++ b/rtp_llm/cpp/engine_base/stream/test/BUILD
@@ -11,7 +11,7 @@ test_deps = [
     "//rtp_llm/cpp/engine_base/stream:stream",
     "//rtp_llm/cpp/devices/testing:device_test_utils",
     "//rtp_llm/cpp/devices/cuda_impl:cuda_impl",
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
 ] + torch_deps()
 
 cc_test(
diff --git a/rtp_llm/cpp/engine_base/stream/test/GenerateStreamTest.cc b/rtp_llm/cpp/engine_base/stream/test/GenerateStreamTest.cc
index 903d4df26..308fe75eb 100644
--- a/rtp_llm/cpp/engine_base/stream/test/GenerateStreamTest.cc
+++ b/rtp_llm/cpp/engine_base/stream/test/GenerateStreamTest.cc
@@ -5,6 +5,7 @@
 #include "rtp_llm/cpp/engine_base/stream/GenerateStream.h"
 #include "rtp_llm/cpp/normal_engine/NormalGenerateStream.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 using namespace std;
 
@@ -12,9 +13,9 @@ namespace rtp_llm {
 
 class GenerateStreamBuilder {
 public:
-    GenerateStreamBuilder(rtp_llm::GptInitParameter params):
-        params_(params), device_(rtp_llm::DeviceFactory::getDefaultDevice()) {
-        params_.max_seq_len_ = 2048;
+    GenerateStreamBuilder():
+        device_(rtp_llm::DeviceFactory::getDefaultDevice()) {
+        model_config_.max_seq_len = 2048;
     }
 
     CacheConfig init_config() {
@@ -28,7 +29,7 @@ public:
         ResourceContext                 resource_context;
         generate_input->generate_config = generate_config;
         generate_input->input_ids       = rtp_llm::vector2Buffer(input_ids);
-        return std::make_shared<NormalGenerateStream>(generate_input, params_, resource_context, nullptr);
+        return std::make_shared<NormalGenerateStream>(generate_input, model_config_, runtime_config_, resource_context, nullptr);
     };
 
     GenerateStreamPtr createComplexContextStream(std::vector<int> input_ids) {
@@ -45,9 +46,10 @@ public:
         generate_config->num_return_sequences = 2;
         generate_input->input_ids             = rtp_llm::vector2Buffer(input_ids);
         generate_input->generate_config       = generate_config;
-        rtp_llm::GptInitParameter params;
-        params.max_seq_len_ = 2048;
-        auto stream         = std::make_shared<NormalGenerateStream>(generate_input, params, resource_context, nullptr);
+        ModelConfig model_config;
+        RuntimeConfig runtime_config;
+        model_config.max_seq_len = 2048;
+        auto stream         = std::make_shared<NormalGenerateStream>(generate_input, model_config, runtime_config, resource_context, nullptr);
 
         return stream;
     }
@@ -58,7 +60,7 @@ public:
         ResourceContext                 resource_context;
         generate_input->generate_config = generate_config;
         generate_input->input_ids       = rtp_llm::vector2Buffer(input_ids);
-        auto stream_ptr = std::make_shared<NormalGenerateStream>(generate_input, params_, resource_context, nullptr);
+        auto stream_ptr = std::make_shared<NormalGenerateStream>(generate_input, model_config_, runtime_config_, resource_context, nullptr);
         stream_ptr->setIsContextStream(false);
         auto new_tokens_ptr = rtp_llm::vector2Buffer(new_token_ids);
         device_->copy(
@@ -69,7 +71,8 @@ public:
     };
 
 private:
-    rtp_llm::GptInitParameter params_;
+    ModelConfig model_config_;
+    RuntimeConfig runtime_config_;
     rtp_llm::DeviceBase*      device_;
 };
 
@@ -78,15 +81,13 @@ protected:
 };
 
 TEST_F(GenerateStreamTest, testConstruct) {
-    rtp_llm::GptInitParameter params;
-    auto                      builder = GenerateStreamBuilder(params);
+    auto                      builder = GenerateStreamBuilder();
     auto                      stream1 = builder.createContextStream({{1, 2, 3, 4, 5}, {}});
     auto                      stream2 = builder.createDecoderStream({1, 2, 3, 4, 5}, {1, 2, 3});
 }
 
 TEST_F(GenerateStreamTest, testConstructCacheKey) {
-    rtp_llm::GptInitParameter params;
-    auto                      builder    = GenerateStreamBuilder(params);
+    auto                      builder    = GenerateStreamBuilder();
     auto                      stream1    = builder.createComplexContextStream({{1, 2, 3, 4, 5}, {}});
     auto&                     cache_key1 = stream1->cacheKeys(0);
     auto&                     cache_key2 = stream1->cacheKeys(1);
@@ -117,8 +118,7 @@ TEST_F(GenerateStreamTest, testConstructCacheKey) {
 }
 
 TEST_F(GenerateStreamTest, testGenerateStreamReuseCacheMethod) {
-    rtp_llm::GptInitParameter params;
-    auto                      builder = GenerateStreamBuilder(params);
+    auto                      builder = GenerateStreamBuilder();
     auto                      stream  = builder.createContextStream({1, 2, 3, 4, 5, 6});
 
     // default true
@@ -134,8 +134,7 @@ TEST_F(GenerateStreamTest, testGenerateStreamReuseCacheMethod) {
 }
 
 TEST_F(GenerateStreamTest, testGenerateStreamEnable3FSMethod) {
-    rtp_llm::GptInitParameter params;
-    auto                      builder = GenerateStreamBuilder(params);
+    auto                      builder = GenerateStreamBuilder();
     auto                      stream  = builder.createContextStream({1, 2, 3, 4, 5, 6});
 
     // default true
diff --git a/rtp_llm/cpp/engine_base/stream/test/StreamCacheResourceTest.cc b/rtp_llm/cpp/engine_base/stream/test/StreamCacheResourceTest.cc
index b00802066..b2edebe66 100644
--- a/rtp_llm/cpp/engine_base/stream/test/StreamCacheResourceTest.cc
+++ b/rtp_llm/cpp/engine_base/stream/test/StreamCacheResourceTest.cc
@@ -10,6 +10,7 @@
 #include "rtp_llm/cpp/normal_engine/NormalGenerateStream.h"
 #include "rtp_llm/cpp/core/Types.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 #include <chrono>
 #include <memory>
@@ -45,9 +46,10 @@ protected:
         generate_input->input_ids =
             std::make_unique<rtp_llm::Buffer>(rtp_llm::MEMORY_CPU, rtp_llm::TYPE_INT32, shape, (void*)(vec.data()));
         generate_input->generate_config = generate_config;
-        rtp_llm::GptInitParameter params;
-        params.max_seq_len_ = 2048;
-        stream_             = std::make_shared<NormalGenerateStream>(generate_input, params, resource_context, nullptr);
+        ModelConfig model_config;
+        RuntimeConfig runtime_config;
+        model_config.max_seq_len = 2048;
+        stream_             = std::make_shared<NormalGenerateStream>(generate_input, model_config, runtime_config, resource_context, nullptr);
         stream_->setRunning();
     }
 
@@ -335,9 +337,10 @@ TEST_F(StreamCacheResourceTest, testReuseCache) {
     ResourceContext resource_context;
     resource_context.reuse_cache   = true;
     resource_context.cache_manager = cache_manager_;
-    rtp_llm::GptInitParameter params;
-    params.max_seq_len_ = 2048;
-    stream_             = std::make_shared<NormalGenerateStream>(generate_input, params, resource_context, nullptr);
+    ModelConfig model_config;
+    RuntimeConfig runtime_config;
+    model_config.max_seq_len = 2048;
+    stream_             = std::make_shared<NormalGenerateStream>(generate_input, model_config, runtime_config, resource_context, nullptr);
     stream_->setRunning();
 
     auto& resource2 = stream_->streamCacheResource();
@@ -396,9 +399,10 @@ TEST_F(StreamCacheResourceTest, testReuseCacheWithFastGen) {
     ResourceContext resource_context;
     resource_context.reuse_cache   = true;
     resource_context.cache_manager = cache_manager_;
-    rtp_llm::GptInitParameter params;
-    params.max_seq_len_ = 2048;
-    stream_             = std::make_shared<NormalGenerateStream>(generate_input, params, resource_context, nullptr);
+    ModelConfig model_config;
+    RuntimeConfig runtime_config;
+    model_config.max_seq_len = 2048;
+    stream_             = std::make_shared<NormalGenerateStream>(generate_input, model_config, runtime_config, resource_context, nullptr);
     stream_->setRunning();
     stream_->enable_fast_gen_ = true;
 
diff --git a/rtp_llm/cpp/engine_base/system_prompt/SystemPromptConstructor.cc b/rtp_llm/cpp/engine_base/system_prompt/SystemPromptConstructor.cc
index 1e606b673..4bc9dd3e7 100644
--- a/rtp_llm/cpp/engine_base/system_prompt/SystemPromptConstructor.cc
+++ b/rtp_llm/cpp/engine_base/system_prompt/SystemPromptConstructor.cc
@@ -12,9 +12,9 @@
 namespace rtp_llm {
 
 absl::StatusOr<std::unordered_map<std::string, SystemPromptParams>> SystemPromptConstructor::construct(
-    const rtp_llm::GptInitParameter& params, EngineBase* engine, CacheManager* cache_manager, bool insert_kv_cache) {
+    const KVCacheConfig& kv_cache_config, EngineBase* engine, CacheManager* cache_manager, bool insert_kv_cache) {
     std::unordered_map<std::string, SystemPromptParams> multi_task_prompt_args;
-    for (const auto& item : params.multi_task_prompt_tokens_) {
+    for (const auto& item : kv_cache_config.multi_task_prompt_tokens) {
         const auto& task_id   = item.first;
         const auto& tokens_id = item.second;
 
diff --git a/rtp_llm/cpp/engine_base/system_prompt/SystemPromptConstructor.h b/rtp_llm/cpp/engine_base/system_prompt/SystemPromptConstructor.h
index cc07bc771..9da1c965f 100644
--- a/rtp_llm/cpp/engine_base/system_prompt/SystemPromptConstructor.h
+++ b/rtp_llm/cpp/engine_base/system_prompt/SystemPromptConstructor.h
@@ -9,14 +9,14 @@
 #include "rtp_llm/cpp/cache/CacheManager.h"
 #include "rtp_llm/cpp/engine_base/system_prompt/SystemPrompt.h"
 #include "rtp_llm/cpp/engine_base/EngineBase.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 namespace rtp_llm {
 
 class SystemPromptConstructor {
 public:
     static absl::StatusOr<std::unordered_map<std::string, SystemPromptParams>> construct(
-        const rtp_llm::GptInitParameter& params, EngineBase* engine, CacheManager* cache_manager, bool insert_kv_cache);
+        const KVCacheConfig& kv_cache_config, EngineBase* engine, CacheManager* cache_manager, bool insert_kv_cache);
 };
 
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/engine_base/system_prompt/test/SystemPromptConstructorTest.cc b/rtp_llm/cpp/engine_base/system_prompt/test/SystemPromptConstructorTest.cc
index c547f644b..6aae5c6ec 100644
--- a/rtp_llm/cpp/engine_base/system_prompt/test/SystemPromptConstructorTest.cc
+++ b/rtp_llm/cpp/engine_base/system_prompt/test/SystemPromptConstructorTest.cc
@@ -6,6 +6,7 @@
 #include "rtp_llm/cpp/engine_base/system_prompt/SystemPromptConstructor.h"
 #include "rtp_llm/cpp/normal_engine/test/MockEngine.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include <cuda_runtime.h>
 
 #include <cstdlib>
@@ -21,17 +22,16 @@ class SystemPromptConstructorTest: public DeviceTestBase {};
 
 TEST_F(SystemPromptConstructorTest, testMultiTaskPromptConstruct) {
     SystemPromptConstructor constructor;
-    GptInitParameter        params;
+    KVCacheConfig kv_cache_config;
     vector<int>             prompt_1 = {1, 2, 3};
     vector<int>             prompt_2 = {4, 5, 6, 7};
-    params.multi_task_prompt_tokens_ = {{"1", prompt_1}, {"2", prompt_2}};
+    kv_cache_config.multi_task_prompt_tokens = {{"1", prompt_1}, {"2", prompt_2}};
     CustomConfig config;
-    auto         gpt_init_params = GptInitParameter();
-    auto         engine          = createMockEngine(device_, config, gpt_init_params);
+    auto         engine          = createMockEngine(device_, config);
     ASSERT_EQ(engine->resourceContext().cache_manager->freeBlockNums(), 99);
     const_cast<ResourceContext*>(&engine->resourceContext())->reuse_cache = true;
     auto result_status =
-        constructor.construct(params, engine.get(), engine->resourceContext().cache_manager.get(), true);
+        constructor.construct(kv_cache_config, engine.get(), engine->resourceContext().cache_manager.get(), true);
     ASSERT_EQ(result_status.ok(), true);
     auto result = result_status.value();
     ASSERT_EQ(result.size(), 2);
diff --git a/rtp_llm/cpp/model_rpc/DecodeRpcServer.cc b/rtp_llm/cpp/model_rpc/DecodeRpcServer.cc
index 5b3689252..61cd276d3 100644
--- a/rtp_llm/cpp/model_rpc/DecodeRpcServer.cc
+++ b/rtp_llm/cpp/model_rpc/DecodeRpcServer.cc
@@ -197,7 +197,7 @@ BroadcastLoadRequestPB DecodeRpcServer::constructRemoteLoadRequestForMla(
     BroadcastLoadRequestPB request;
     request.set_request_id(load_context.request_id);
     request.set_request_key(load_context.request_key);
-    request.set_dp_rank(maga_init_params_.gpt_init_parameter.dp_rank_);
+    request.set_dp_rank(maga_init_params_.parallelism_config.dp_rank);
     request.set_partition_count(1);
     request.set_partition_id(0);
 
@@ -226,7 +226,7 @@ BroadcastLoadRequestPB DecodeRpcServer::constructRemoteLoadRequest(const LoadKVC
     BroadcastLoadRequestPB request;
     request.set_request_id(load_context.request_id);
     request.set_request_key(load_context.request_key);
-    request.set_dp_rank(maga_init_params_.gpt_init_parameter.dp_rank_);
+    request.set_dp_rank(maga_init_params_.parallelism_config.dp_rank);
 
     if (resource_.workers.size() % peer_addrs.size() == 0) {
         // D >= P, load part block of prefill
@@ -274,9 +274,9 @@ ErrorInfo DecodeRpcServer::loadCacheForAllRank(DecodeGenerateContext& decode_con
         return ErrorInfo(ErrorCode::LOAD_KV_CACHE_FAILED, "peer ips size not equal to worker size");
     }
 
-    auto load_cache_timeout_ms = maga_init_params_.gpt_init_parameter.load_cache_timeout_ms_;
+    auto load_cache_timeout_ms = maga_init_params_.pd_sep_config.load_cache_timeout_ms;
     load_cache_timeout_ms      = load_cache_timeout_ms > 0 ? load_cache_timeout_ms : LOAD_TIMEOUT_MS;
-    auto max_rpc_timeout_ms    = maga_init_params_.gpt_init_parameter.max_rpc_timeout_ms_;
+    auto max_rpc_timeout_ms    = maga_init_params_.pd_sep_config.max_rpc_timeout_ms;
     auto rpc_timeout           = max_rpc_timeout_ms > 0 ? max_rpc_timeout_ms : MAX_GRPC_TIMEOUT_MS;
     auto min_timeout_ms        = std::min(load_cache_timeout_ms, rpc_timeout);
     auto request_timeout_ms    = decode_context.request_timeout_ms;
@@ -295,7 +295,7 @@ ErrorInfo DecodeRpcServer::loadCacheForAllRank(DecodeGenerateContext& decode_con
 
     // Prefill: TP = 1 && Decode: TP = 1
     if (resource_.workers.size() == 1 && decode_context.peer_addrs.size() == 1) {
-        for (size_t i = 0; i < maga_init_params_.gpt_init_parameter.rdma_connect_retry_times_ + 1; i++) {
+        for (size_t i = 0; i < maga_init_params_.pd_sep_config.rdma_connect_retry_times + 1; i++) {
             auto error_info = loadCache(load_context);
             if (error_info.code() != ErrorCode::CACHE_STORE_LOAD_CONNECT_FAILED
                 && error_info.code() != ErrorCode::CACHE_STORE_LOAD_RDMA_CONNECT_FAILED) {
@@ -379,7 +379,7 @@ ErrorInfo DecodeRpcServer::loadCacheAsyncForTp(DecodeGenerateContext& decode_con
         }
         auto once_deadline =
             std::chrono::system_clock::now()
-            + std::chrono::milliseconds(maga_init_params_.gpt_init_parameter.decode_polling_kv_cache_step_ms_);
+            + std::chrono::milliseconds(maga_init_params_.pd_sep_config.decode_polling_kv_cache_step_ms);
         RTP_LLM_LOG_DEBUG("request [%s] start to execute async next", decode_context.request_key.c_str());
         // TODO(xinfei.sxf) There is a problem with complete queue next call delay here, the reason is yet to be
         // investigated
@@ -525,7 +525,7 @@ ErrorInfo DecodeRpcServer::loadCache(const LoadKVCacheContext& load_context) {
     auto        k_block_size     = cache_config.k_block_stride;
     auto        v_block_size     = cache_config.v_block_stride;
     auto        scale_block_size = cache_config.kv_scale_block_stride;
-    auto        layer_num        = maga_init_params_.gpt_init_parameter.num_layers_;
+    auto        layer_num        = maga_init_params_.model_config_.num_layers;
 
     if (v_block_size % load_context.peer_addrs.size() != 0 || k_block_size % load_context.peer_addrs.size() != 0
         || scale_block_size % load_context.peer_addrs.size() != 0) {
@@ -595,7 +595,7 @@ ErrorInfo DecodeRpcServer::loadCache(const LoadKVCacheContext& load_context) {
                 const auto& cache_config     = sp_cache_manager->cacheConfig();
                 const auto  sp_k_block_size  = cache_config.k_block_stride / load_context.peer_addrs.size();
                 const auto  sp_v_block_size  = cache_config.v_block_stride / load_context.peer_addrs.size();
-                size_t      layer_num        = mtp_engine_init_params->gpt_init_parameter.num_layers_;
+                size_t      layer_num        = mtp_engine_init_params->model_config_.num_layers;
                 for (size_t layer_id = 0; layer_id < layer_num; layer_id++) {
                     auto request_key = std::to_string(load_context.request_id) + "-" + std::to_string(layer_id);
                     auto load_layer_cache =
@@ -680,7 +680,7 @@ ErrorInfo DecodeRpcServer::loadCache(const LoadKVCacheContext& load_context) {
 grpc::Status DecodeRpcServer::RemoteLoad(grpc::ServerContext*          server_context,
                                          const BroadcastLoadRequestPB* request,
                                          BroadcastLoadResponsePB*      response) {
-    if (request->dp_rank() != maga_init_params_.gpt_init_parameter.dp_rank_) {
+    if (request->dp_rank() != maga_init_params_.parallelism_config.dp_rank) {
         RTP_LLM_LOG_WARNING("only load when in dp group, skip load for dp rank %d", request->dp_rank());
         return grpc::Status::OK;
     }
@@ -720,8 +720,8 @@ grpc::Status DecodeRpcServer::RemoteGenerate(grpc::ServerContext* server_context
     decode_context.onflight_requests = onflight_requests_;
     decode_context.loading_cache_requests = loading_cache_requests_;
 
-    auto max_retry_times      = maga_init_params_.gpt_init_parameter.decode_retry_times_;
-    auto max_retry_timeout_ms = maga_init_params_.gpt_init_parameter.decode_retry_timeout_ms_;
+    auto max_retry_times      = maga_init_params_.pd_sep_config.decode_retry_times;
+    auto max_retry_timeout_ms = maga_init_params_.pd_sep_config.decode_retry_timeout_ms;
 
     try {
         EXECUTE_STAGE_FUNC(prepareGenerateContext, decode_context);
diff --git a/rtp_llm/cpp/model_rpc/DecodeRpcServerNew.cc b/rtp_llm/cpp/model_rpc/DecodeRpcServerNew.cc
index 395cdcf02..9ad932dc9 100644
--- a/rtp_llm/cpp/model_rpc/DecodeRpcServerNew.cc
+++ b/rtp_llm/cpp/model_rpc/DecodeRpcServerNew.cc
@@ -1,6 +1,7 @@
 #include "rtp_llm/cpp/model_rpc/DecodeRpcServerNew.h"
 #include "rtp_llm/cpp/devices/utils/DebugUtils.h"
 #include "rtp_llm/cpp/engine_base/Host.h"
+#include <cstring>
 
 namespace rtp_llm {
 
@@ -91,7 +92,7 @@ void DecodeRpcServerNew::makeRemoteGenerateRequest(DecodeGenerateContextNew& dec
     request.set_reuse_block_size(generate_stream->reuseBlockSize());
 
     request.set_use_mla(engine_->resourceContext().cache_manager->cacheConfig().use_mla);
-    request.set_layer_num(maga_init_params_.gpt_init_parameter.num_layers_);
+    request.set_layer_num(maga_init_params_.model_config_.num_layers);
     request.set_deadline_us(currentTimeUs() + decode_context.request_timeout_ms * 1000);
 }
 
@@ -111,10 +112,10 @@ ErrorInfo DecodeRpcServerNew::callPrefill(DecodeGenerateContextNew& decode_conte
 
     // If no host specified in request, check if there's a master role
     char* decode_cm2_config_env = std::getenv("RTP_LLM_DECODE_CM2_CONFIG");
+    char* remote_rpc_server_ip_env = std::getenv("REMOTE_RPC_SERVER_IP");
     bool  has_master_role =
-        !maga_init_params_.gpt_init_parameter.service_discovery_config.use_local
-        && (decode_cm2_config_env != nullptr
-            || !maga_init_params_.gpt_init_parameter.service_discovery_config.remote_rpc_server_ip.empty());
+        (decode_cm2_config_env != nullptr
+            || (remote_rpc_server_ip_env != nullptr && strlen(remote_rpc_server_ip_env) > 0));
 
     // For PD inversion where request directly reaches decode, we need to select prefill machines
     if (!host && has_master_role) {
@@ -157,7 +158,7 @@ ErrorInfo DecodeRpcServerNew::callPrefill(DecodeGenerateContextNew& decode_conte
                 &got_tag,
                 &ok,
                 std::chrono::system_clock::now()
-                    + std::chrono::milliseconds(maga_init_params_.gpt_init_parameter.decode_polling_call_prefill_ms_))
+                    + std::chrono::milliseconds(maga_init_params_.pd_sep_config.decode_polling_call_prefill_ms))
             == grpc::CompletionQueue::NextStatus::TIMEOUT) {
             if (decode_context.server_context->IsCancelled()) {
                 RTP_LLM_LOG_WARNING("request [%s] is cancelled", decode_context.request_key.c_str());
diff --git a/rtp_llm/cpp/model_rpc/LocalRpcServer.cc b/rtp_llm/cpp/model_rpc/LocalRpcServer.cc
index 7f8e9cbe0..05cce5cce 100644
--- a/rtp_llm/cpp/model_rpc/LocalRpcServer.cc
+++ b/rtp_llm/cpp/model_rpc/LocalRpcServer.cc
@@ -19,7 +19,7 @@ grpc::Status LocalRpcServer::init(const EngineInitParams&
     maga_init_params_ = maga_init_params;
     metrics_reporter_ = maga_init_params.metrics_reporter;
     RTP_LLM_LOG_WARNING("LocalRpcServer aux_string %s",
-                        maga_init_params_.gpt_init_parameter.misc_config.aux_string.c_str());
+                        maga_init_params_.misc_config.aux_string.c_str());
     if (propose_params) {
         propose_maga_init_params_ = propose_params.get();
         if (!mm_process_engine.is_none()) {
@@ -44,13 +44,13 @@ grpc::Status LocalRpcServer::init(const EngineInitParams&
             engine_.reset(new NormalEngine(maga_init_params));
         }
         if (!mm_process_engine.is_none()) {
-            auto vit_separation = maga_init_params.gpt_init_parameter.vit_separation_;
-            if (vit_separation == 2) {
+            auto vit_separation = maga_init_params.vit_config.vit_separation;
+            if (vit_separation == VitSeparation::VIT_SEPARATION_REMOTE) {
                 mm_processor_.reset(
-                    new RemoteMultimodalProcessor(mm_process_engine, maga_init_params.gpt_init_parameter));
-            } else if (vit_separation == 0) {
+                    new RemoteMultimodalProcessor(mm_process_engine, maga_init_params.model_config_.mm_model_config, maga_init_params.model_config_.max_seq_len));
+            } else if (vit_separation == VitSeparation::VIT_SEPARATION_LOCAL) {
                 mm_processor_.reset(
-                    new LocalMultimodalProcessor(mm_process_engine, maga_init_params.gpt_init_parameter));
+                    new LocalMultimodalProcessor(mm_process_engine, maga_init_params.model_config_.mm_model_config, maga_init_params.model_config_.max_seq_len));
             } else {
                 return grpc::Status(grpc::StatusCode::INTERNAL, "invalid vit separation value in config");
             }
@@ -95,7 +95,7 @@ grpc::Status LocalRpcServer::pollStreamOutput(grpc::ServerContext*             c
         RTP_LLM_LOG_DEBUG("request [%s] generate next output success", request_key.c_str());
         GenerateOutputsPB outputs_pb;
         QueryConverter::transResponse(
-            &outputs_pb, &(result.value()), maga_init_params_.gpt_init_parameter.misc_config.aux_string);
+            &outputs_pb, &(result.value()), maga_init_params_.misc_config.aux_string);
         if (context->IsCancelled()) {
             stream->cancel();
             RTP_LLM_LOG_WARNING("request [%s] cancelled by user", request_key.c_str());
@@ -177,7 +177,7 @@ grpc::Status LocalRpcServer::GetWorkerStatus(grpc::ServerContext*   context,
         "receive workerStatus rpc request from client: %s, latest_finished_version: %ld, config role_type: %d",
         context->peer().c_str(),
         latest_finished_version,
-        maga_init_params_.gpt_init_parameter.role_type_);
+        maga_init_params_.pd_sep_config.role_type);
 
     WorkerStatusInfo status_info              = getWorkerStatusInfo(latest_finished_version);
     int64_t          request_after_ws_time_us = currentTimeUs();
@@ -223,7 +223,7 @@ grpc::Status LocalRpcServer::GetWorkerStatus(grpc::ServerContext*   context,
 WorkerStatusInfo LocalRpcServer::getWorkerStatusInfo(int64_t latest_finished_version) {
     WorkerStatusInfo status_info;
     status_info.engine_schedule_info = getEngineScheduleInfo(latest_finished_version);
-    switch (maga_init_params_.gpt_init_parameter.role_type_) {
+    switch (maga_init_params_.pd_sep_config.role_type) {
         case RoleType::PDFUSION:
             status_info.role = "RoleType.PDFUSION";
             break;
@@ -242,12 +242,12 @@ WorkerStatusInfo LocalRpcServer::getWorkerStatusInfo(int64_t latest_finished_ver
         default:
             status_info.role = "RoleType.UNKNOWN";
     }
-    status_info.dp_size        = maga_init_params_.gpt_init_parameter.dp_size_;
-    status_info.tp_size        = maga_init_params_.gpt_init_parameter.tp_size_;
-    status_info.dp_rank        = maga_init_params_.gpt_init_parameter.dp_rank_;
+    status_info.dp_size        = maga_init_params_.parallelism_config.dp_size;
+    status_info.tp_size        = maga_init_params_.parallelism_config.tp_size;
+    status_info.dp_rank        = maga_init_params_.parallelism_config.dp_rank;
     status_info.status_version = currentTimeUs();
     status_info.alive          = true;
-    auto quant_method          = maga_init_params_.gpt_init_parameter.quant_algo_.getQuantMethod();
+    auto quant_method          = maga_init_params_.model_config_.quant_algo.getQuantMethod();
 
     switch (quant_method) {
         case QuantMethod::WeightOnlyPerCol:
diff --git a/rtp_llm/cpp/model_rpc/LocalRpcServer.h b/rtp_llm/cpp/model_rpc/LocalRpcServer.h
index b8c2d470f..b707cb82d 100644
--- a/rtp_llm/cpp/model_rpc/LocalRpcServer.h
+++ b/rtp_llm/cpp/model_rpc/LocalRpcServer.h
@@ -62,7 +62,7 @@ public:
     }
 
     int64_t tpSize() const {
-        return maga_init_params_.gpt_init_parameter.tp_size_;
+        return maga_init_params_.parallelism_config.tp_size;
     }
 
     virtual size_t onflightRequestNum();
diff --git a/rtp_llm/cpp/model_rpc/PrefillRpcServer.cc b/rtp_llm/cpp/model_rpc/PrefillRpcServer.cc
index 57baf09e3..137d893fe 100644
--- a/rtp_llm/cpp/model_rpc/PrefillRpcServer.cc
+++ b/rtp_llm/cpp/model_rpc/PrefillRpcServer.cc
@@ -77,7 +77,7 @@ namespace rtp_llm {
 grpc::Status PrefillRpcServer::init(const EngineInitParams&                                maga_init_params,
                                     py::object                                             mm_process_engine,
                                     std::unique_ptr<rtp_llm::ProposeModelEngineInitParams> propose_params) {
-    RTP_LLM_CHECK_WITH_INFO(maga_init_params.gpt_init_parameter.role_type_ == RoleType::PREFILL,
+    RTP_LLM_CHECK_WITH_INFO(maga_init_params.pd_sep_config.role_type == RoleType::PREFILL,
                             "prefill's role_type must be PREFILL");
     auto ret = RemoteRpcServer::init(maga_init_params, mm_process_engine, std::move(propose_params));
     if (!ret.ok()) {
@@ -87,7 +87,7 @@ grpc::Status PrefillRpcServer::init(const EngineInitParams&
 }
 
 ErrorInfo PrefillRpcServer::waitStreamBeforeRun(std::shared_ptr<GenerateStream> stream) {
-    static int max_wait_timeout_us = maga_init_params_.gpt_init_parameter.prefill_max_wait_timeout_ms_ * 1000;
+    static int max_wait_timeout_us = maga_init_params_.pd_sep_config.prefill_max_wait_timeout_ms * 1000;
     auto       begin_time_us       = currentTimeUs();
     while (stream->waiting()) {
         usleep(100);
@@ -130,9 +130,9 @@ void PrefillRpcServer::getRpcConnection(PrefillGenerateContext& prefill_context)
     }
 
     // If no host specified in request, check if there's a master role
+    char* remote_rpc_server_ip_env = std::getenv("REMOTE_RPC_SERVER_IP");
     bool has_master_role =
-        !maga_init_params_.gpt_init_parameter.service_discovery_config.use_local
-        && !maga_init_params_.gpt_init_parameter.service_discovery_config.remote_rpc_server_ip.empty();
+        (remote_rpc_server_ip_env != nullptr && strlen(remote_rpc_server_ip_env) > 0);
 
     // If no host specified in request and no master role, this is a direct prefill request
     // In this case, we still need to select decode machines as specified in the requirements
@@ -183,7 +183,7 @@ void PrefillRpcServer::remoteAllocateResource(PrefillGenerateContext& prefill_co
     RTP_LLM_LOG_DEBUG("request [%ld] start to remote allocate resource", prefill_context.request_id);
     prefill_context.client_context.reset(new ClientContext());
     auto request_timeout_ms = prefill_context.request_timeout_ms;
-    auto max_rpc_timeout_ms = maga_init_params_.gpt_init_parameter.max_rpc_timeout_ms_;
+    auto max_rpc_timeout_ms = maga_init_params_.pd_sep_config.max_rpc_timeout_ms;
     auto final_timeout_ms   = max_rpc_timeout_ms > 0 ? max_rpc_timeout_ms : MAX_GRPC_TIMEOUT_MS;
     final_timeout_ms        = request_timeout_ms > 0 ? request_timeout_ms : final_timeout_ms;
 
@@ -383,8 +383,8 @@ grpc::Status PrefillRpcServer::GenerateStreamCall(grpc::ServerContext*
                                                   meta_);
     prefill_context.onflight_requests      = onflight_requests_;
     prefill_context.loading_cache_requests = loading_cache_requests_;
-    auto max_retry_times                   = maga_init_params_.gpt_init_parameter.prefill_retry_times_;
-    auto max_retry_timeout_ms              = maga_init_params_.gpt_init_parameter.prefill_retry_timeout_ms_;
+    auto max_retry_times                   = maga_init_params_.pd_sep_config.prefill_retry_times;
+    auto max_retry_timeout_ms              = maga_init_params_.pd_sep_config.prefill_retry_timeout_ms;
 
     try {
         EXECUTE_WITH_RETRY(prepareAllocateResource, prefill_context, max_retry_times, max_retry_timeout_ms);
diff --git a/rtp_llm/cpp/model_rpc/PrefillRpcServerNew.cc b/rtp_llm/cpp/model_rpc/PrefillRpcServerNew.cc
index 5e474ebdd..67f6d9e12 100644
--- a/rtp_llm/cpp/model_rpc/PrefillRpcServerNew.cc
+++ b/rtp_llm/cpp/model_rpc/PrefillRpcServerNew.cc
@@ -114,17 +114,17 @@ bool PrefillRpcServerNew::validRequest(PrefillGenerateContextNew& prefill_contex
         return false;
     }
 
-    if (request->use_mla() != maga_init_params_.gpt_init_parameter.use_mla_) {
+    if (request->use_mla() != maga_init_params_.model_config_.attn_config.use_mla) {
         RTP_LLM_LOG_WARNING("request [%s] request is invalid, mla config not match",
                             prefill_context.request_key.c_str());
         return false;
     }
 
-    if (request->layer_num() != maga_init_params_.gpt_init_parameter.num_layers_) {
+    if (request->layer_num() != maga_init_params_.model_config_.num_layers) {
         RTP_LLM_LOG_WARNING("request [%s] request is invalid, layer_num %d vs %d not match",
                             prefill_context.request_key.c_str(),
                             request->layer_num(),
-                            maga_init_params_.gpt_init_parameter.num_layers_);
+                            maga_init_params_.model_config_.num_layers);
         return false;
     }
     return true;
@@ -175,7 +175,7 @@ ErrorInfo PrefillRpcServerNew::notifyStoreCache(PrefillGenerateContextNew& prefi
 
 void PrefillRpcServerNew::constructRemoteLoadRequest(PrefillGenerateContextNew& prefill_context, int index) {
     auto& request = prefill_context.rpc_contexts[index]->request;
-    request.set_dp_rank(maga_init_params_.gpt_init_parameter.dp_rank_);
+    request.set_dp_rank(maga_init_params_.parallelism_config.dp_rank);
     request.set_request_id(prefill_context.request_id);
     request.set_request_key(prefill_context.request_key);
     request.set_deadline_us(prefill_context.request->deadline_us());
@@ -253,7 +253,7 @@ ErrorInfo PrefillRpcServerNew::generateFirstToken(PrefillGenerateContextNew& pre
         RTP_LLM_LOG_DEBUG("request [%s] generate next output success", prefill_context.request_key.c_str());
         auto response_output = prefill_context.response->mutable_output();
         QueryConverter::transResponse(
-            response_output, &(result.value()), maga_init_params_.gpt_init_parameter.misc_config.aux_string);
+            response_output, &(result.value()), maga_init_params_.misc_config.aux_string);
         // should only generate one token
         break;
     }
@@ -288,7 +288,7 @@ ErrorInfo PrefillRpcServerNew::waitStoreCacheForAllRankDone(PrefillGenerateConte
 
         auto once_deadline =
             std::chrono::system_clock::now()
-            + std::chrono::milliseconds(maga_init_params_.gpt_init_parameter.decode_polling_kv_cache_step_ms_);
+            + std::chrono::milliseconds(maga_init_params_.pd_sep_config.decode_polling_kv_cache_step_ms);
         void* got_tag;
         bool  ok = false;
 
@@ -352,7 +352,7 @@ grpc::Status PrefillRpcServerNew::RemoteStore(grpc::ServerContext*        server
                                               const RemoteStoreRequestPB* request,
                                               RemoteStoreResponsePB*      response) {
     RTP_LLM_LOG_DEBUG("request [%s] remote store", request->request_key().c_str());
-    if (request->dp_rank() != maga_init_params_.gpt_init_parameter.dp_rank_) {
+    if (request->dp_rank() != maga_init_params_.parallelism_config.dp_rank) {
         RTP_LLM_LOG_WARNING("only load when in dp group, skip load for dp rank %d", request->dp_rank());
         return grpc::Status(grpc::StatusCode::INVALID_ARGUMENT, "error dp rank");
     }
@@ -367,7 +367,7 @@ grpc::Status PrefillRpcServerNew::RemoteStore(grpc::ServerContext*        server
     auto        k_block_size     = cache_config.k_block_stride;
     auto        v_block_size     = cache_config.v_block_stride;
     auto        scale_block_size = cache_config.kv_scale_block_stride;
-    auto        layer_num        = maga_init_params_.gpt_init_parameter.num_layers_;
+    auto        layer_num        = maga_init_params_.model_config_.num_layers;
 
     auto remote_addr_size = request->partition_infos_size();
     if (remote_addr_size == 0) {
diff --git a/rtp_llm/cpp/model_rpc/RemoteRpcServer.cc b/rtp_llm/cpp/model_rpc/RemoteRpcServer.cc
index 53bcd052f..ffd51eba8 100644
--- a/rtp_llm/cpp/model_rpc/RemoteRpcServer.cc
+++ b/rtp_llm/cpp/model_rpc/RemoteRpcServer.cc
@@ -15,7 +15,7 @@ grpc::Status RemoteRpcServer::init(const EngineInitParams&
     }
     initLocalHostInfo();
     initLocalPeerInfo();
-    initCacheStore(maga_init_params.gpt_init_parameter, propose_params_ptr);
+    initCacheStore(maga_init_params, propose_params_ptr);
     return grpc::Status::OK;
 }
 
@@ -36,16 +36,13 @@ void RemoteRpcServer::initLocalHostInfo() {
 
 void RemoteRpcServer::initLocalPeerInfo() {
     // not init when tp rank != 0
-    if (maga_init_params_.gpt_init_parameter.tp_rank_ > 0) {
+    if (maga_init_params_.parallelism_config.tp_rank > 0) {
         return;
     }
     // worker 0 is master (rank 0)
-    for (auto& worker_addr : maga_init_params_.gpt_init_parameter.worker_addrs_) {
-        RTP_LLM_LOG_INFO("In gpt init params: worker address is %s", worker_addr.c_str());
-        resource_.workers.push_back(worker_addr);
-    }
-    for (auto& worker_grpc_addr : maga_init_params_.gpt_init_parameter.worker_grpc_addrs_) {
+    for (auto& worker_grpc_addr : maga_init_params_.runtime_config.worker_grpc_addrs) {
         RTP_LLM_LOG_INFO("In gpt init params: worker grpc address is %s", worker_grpc_addr.c_str());
+        resource_.workers.push_back(worker_grpc_addr);
         resource_.grpc_workers.push_back(worker_grpc_addr);
     }
     string worker_info = "worker address is ";
@@ -61,21 +58,21 @@ void RemoteRpcServer::initLocalPeerInfo() {
     RTP_LLM_LOG_INFO(worker_grpc_info);
 }
 
-void RemoteRpcServer::initCacheStore(const GptInitParameter&                init_params,
+void RemoteRpcServer::initCacheStore(const EngineInitParams&                init_params,
                                      rtp_llm::ProposeModelEngineInitParams* propose_params) {
-    RTP_LLM_LOG_INFO("init_params.role_type : %d", init_params.role_type_);
+    RTP_LLM_LOG_INFO("init_params.role_type : %d", init_params.pd_sep_config.role_type);
 
-    if (init_params.role_type_ != RoleType::PREFILL && init_params.role_type_ != RoleType::DECODE) {
-        RTP_LLM_FAIL("role_type must be prefill or decode, but it is %d", init_params.role_type_);
+    if (init_params.pd_sep_config.role_type != RoleType::PREFILL && init_params.pd_sep_config.role_type != RoleType::DECODE) {
+        RTP_LLM_FAIL("role_type must be prefill or decode, but it is %d", init_params.pd_sep_config.role_type);
     }
     const_cast<ResourceContext*>(&engine_->resourceContext())->use_cache_store = true;
     auto device                                                                = engine_->getDevice();
     auto cache_manager = engine_->resourceContext().cache_manager;
 
     CacheStoreInitParams params;
-    params.listen_port                  = init_params.cache_store_listen_port_;
-    params.rdma_listen_port             = init_params.cache_store_rdma_listen_port_;
-    params.rdma_mode                    = init_params.cache_store_rdma_mode_;
+    params.listen_port                  = init_params.pd_sep_config.cache_store_listen_port;
+    params.rdma_listen_port             = init_params.pd_sep_config.cache_store_rdma_listen_port;
+    params.rdma_mode                    = init_params.pd_sep_config.cache_store_rdma_mode;
     params.thread_count                 = init_params.cache_store_config.thread_count;
     params.queue_size                   = 500;
     params.rdma_connect_timeout_ms      = init_params.cache_store_config.rdma_connect_timeout_ms;
diff --git a/rtp_llm/cpp/model_rpc/RemoteRpcServer.h b/rtp_llm/cpp/model_rpc/RemoteRpcServer.h
index fd975bd3d..a93aa5b15 100644
--- a/rtp_llm/cpp/model_rpc/RemoteRpcServer.h
+++ b/rtp_llm/cpp/model_rpc/RemoteRpcServer.h
@@ -21,7 +21,7 @@ public:
 private:
     void initLocalHostInfo();
     void initLocalPeerInfo();
-    void initCacheStore(const rtp_llm::GptInitParameter& params, rtp_llm::ProposeModelEngineInitParams* propose_params);
+    void initCacheStore(const EngineInitParams& params, rtp_llm::ProposeModelEngineInitParams* propose_params);
 
 protected:
     std::string                 process_id_;
diff --git a/rtp_llm/cpp/model_rpc/RemoteRpcServiceImpl.cc b/rtp_llm/cpp/model_rpc/RemoteRpcServiceImpl.cc
index 1287801df..9405a558a 100644
--- a/rtp_llm/cpp/model_rpc/RemoteRpcServiceImpl.cc
+++ b/rtp_llm/cpp/model_rpc/RemoteRpcServiceImpl.cc
@@ -8,10 +8,10 @@ namespace rtp_llm {
 grpc::Status RemoteRpcServiceImpl::init(const EngineInitParams&                                maga_init_params,
                                         py::object                                             mm_process_engine,
                                         std::unique_ptr<rtp_llm::ProposeModelEngineInitParams> propose_params) {
-    decode_entrance_ = maga_init_params.gpt_init_parameter.decode_entrance_;
+    decode_entrance_ = maga_init_params.pd_sep_config.decode_entrance;
     RTP_LLM_LOG_INFO("remote rpc service init, decode_entrance is %d", decode_entrance_);
     if (decode_entrance_) {
-        if (maga_init_params.gpt_init_parameter.role_type_ == RoleType::PREFILL) {
+        if (maga_init_params.pd_sep_config.role_type == RoleType::PREFILL) {
             prefill_server_new_ = std::make_shared<PrefillRpcServerNew>();
             local_server_ = prefill_server_new_;
             return prefill_server_new_->init(maga_init_params, mm_process_engine, std::move(propose_params));
@@ -21,7 +21,7 @@ grpc::Status RemoteRpcServiceImpl::init(const EngineInitParams&
             return decode_server_new_->init(maga_init_params, mm_process_engine, std::move(propose_params));
         }
     } else {
-        if (maga_init_params.gpt_init_parameter.role_type_ == RoleType::PREFILL) {
+        if (maga_init_params.pd_sep_config.role_type == RoleType::PREFILL) {
             prefill_server_ = std::make_shared<PrefillRpcServer>();
             local_server_ = prefill_server_;
             return prefill_server_->init(maga_init_params, mm_process_engine, std::move(propose_params));
diff --git a/rtp_llm/cpp/model_rpc/model_rpc_client.py b/rtp_llm/cpp/model_rpc/model_rpc_client.py
index 775707753..194eadbe1 100644
--- a/rtp_llm/cpp/model_rpc/model_rpc_client.py
+++ b/rtp_llm/cpp/model_rpc/model_rpc_client.py
@@ -7,7 +7,7 @@ from grpc import StatusCode
 
 from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
 from rtp_llm.config.generate_config import RoleType
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.ops import EPLBConfig, FfnDisAggregateConfig
 from rtp_llm.cpp.model_rpc.proto.model_rpc_service_pb2 import (
     ErrorDetailsPB,
     GenerateInputPB,
@@ -16,7 +16,6 @@ from rtp_llm.cpp.model_rpc.proto.model_rpc_service_pb2 import (
     RoleAddrPB,
 )
 from rtp_llm.cpp.model_rpc.proto.model_rpc_service_pb2_grpc import RpcServiceStub
-from rtp_llm.distribute.gang_info import get_gang_info
 from rtp_llm.distribute.worker_info import g_parallel_info, g_worker_info
 from rtp_llm.utils.base_model_datatypes import (
     AuxInfo,
@@ -265,21 +264,28 @@ def trans_output(
 
 class ModelRpcClient(object):
 
-    def __init__(self, config: GptInitModelParameters, address: Optional[str] = None):
+    def __init__(
+        self,
+        ffn_disaggregate_config: FfnDisAggregateConfig,
+        max_rpc_timeout_ms: int = 0,
+        decode_entrance: bool = False,
+        address: Optional[str] = None,
+        gang_info=None,
+    ):
         # 创建到服务器的连接
         if not address:
             address = f"localhost:{g_worker_info.rpc_server_port}"
         self._addresses = []
-        # for test usage
-        hack_ep_single_entry = config.py_env_configs.py_eplb_config.hack_ep_single_entry
-        logging.info(f"hack ep single entry: {hack_ep_single_entry}")
-        if (g_parallel_info.dp_size > 1) and (not hack_ep_single_entry):
+        self.ffn_disaggregate_config = ffn_disaggregate_config
+        self.max_rpc_timeout_ms = max_rpc_timeout_ms
+        self.decode_entrance = decode_entrance
+        if g_parallel_info.dp_size > 1:
             members_info_str = (
                 f"[world_rank: {g_parallel_info.world_rank}]"
                 + f"[tp_size: {g_parallel_info.tp_size}] all members: "
                 + "{"
             )
-            members = get_gang_info().members
+            members = gang_info.members
             for member in members:
                 members_info_str += f"{member}\n"
                 if member.local_rank % g_parallel_info.tp_size == 0:
@@ -289,22 +295,21 @@ class ModelRpcClient(object):
         else:
             self._addresses = [address]
         # last rank as ffn service, no be entry
-        if config.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate:
+        if ffn_disaggregate_config.enable_ffn_disaggregate:
             serving_ranks = (
-                config.gpt_init_params.ffn_disaggregate_config.attention_tp_size
-                * config.gpt_init_params.ffn_disaggregate_config.attention_dp_size
+                ffn_disaggregate_config.attention_tp_size
+                * ffn_disaggregate_config.attention_dp_size
             )
             self._addresses = self._addresses[:serving_ranks]
         logging.info(f"client connect to rpc addresses: {self._addresses}")
-        self.model_config = config
 
     async def enqueue(
         self, input_py: GenerateInput
     ) -> AsyncGenerator[GenerateOutputs, None]:
         request_timeout_ms = input_py.generate_config.timeout_ms
         rpc_timeout_ms = (
-            self.model_config.max_rpc_timeout_ms
-            if self.model_config.max_rpc_timeout_ms > 0
+            self.max_rpc_timeout_ms
+            if self.max_rpc_timeout_ms > 0
             else MAX_GRPC_TIMEOUT_SECONDS * 1000
         )
         if request_timeout_ms == None or request_timeout_ms <= 0:
@@ -321,12 +326,12 @@ class ModelRpcClient(object):
         for role_addr in input_py.generate_config.role_addrs:
             if (
                 (
-                    self.model_config.decode_entrance
+                    self.decode_entrance
                     and role_addr.role == RoleType.DECODE
                 )
                 or role_addr.role == RoleType.PDFUSION
                 or (
-                    not self.model_config.decode_entrance
+                    not self.decode_entrance
                     and role_addr.role == RoleType.PREFILL
                 )
             ):
diff --git a/rtp_llm/cpp/model_rpc/test/BUILD b/rtp_llm/cpp/model_rpc/test/BUILD
index 2a3ac9969..f5d680de3 100644
--- a/rtp_llm/cpp/model_rpc/test/BUILD
+++ b/rtp_llm/cpp/model_rpc/test/BUILD
@@ -24,7 +24,7 @@ py_test (
 test_deps = [
     "//rtp_llm/cpp/devices/testing:device_test_utils",
     "//rtp_llm/cpp/models:models",
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
     "//rtp_llm/cpp/model_rpc/proto:model_rpc_service_cc_proto",
     "//rtp_llm/cpp/model_rpc:model_rpc_server",
     "@com_google_googletest//:gtest",
diff --git a/rtp_llm/cpp/model_rpc/test/MockModelRpcServer.h b/rtp_llm/cpp/model_rpc/test/MockModelRpcServer.h
index 5cc37bcb9..d368ad26b 100644
--- a/rtp_llm/cpp/model_rpc/test/MockModelRpcServer.h
+++ b/rtp_llm/cpp/model_rpc/test/MockModelRpcServer.h
@@ -8,6 +8,7 @@
 #include "rtp_llm/cpp/core/Buffer.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
 #include "rtp_llm/cpp/models/models_weight/W.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include <memory>
 
 using namespace std;
@@ -16,24 +17,44 @@ namespace W = rtp_llm::W;
 namespace rtp_llm {
 
 EngineInitParams createMockEngineInitParams(DeviceBase* device) {
-    auto params                     = GptInitParameter();
-    params.head_num_                = 2;
-    params.size_per_head_           = 64;
-    params.num_layers_              = 2;
-    params.max_seq_len_             = 20;
-    params.vocab_size_              = 20;
-    params.hidden_size_             = 128;
-    params.head_num_kv_             = 2;
-    params.block_nums_              = 100;
-    params.reuse_cache_             = false;
-    params.max_generate_batch_size_ = 128;
-    params.max_context_batch_size_  = 128;
-    params.kv_cache_data_type_      = DataType::TYPE_FP16;
+    ModelConfig model_config;
+    model_config.attn_config.head_num = 2;
+    model_config.attn_config.size_per_head = 64;
+    model_config.num_layers              = 2;
+    model_config.max_seq_len             = 20;
+    model_config.vocab_size              = 20;
+    model_config.hidden_size             = 128;
+    model_config.attn_config.kv_head_num = 2;
+    model_config.attn_config.kv_cache_dtype = KvCacheDataType::BASE;
     const size_t inter_size         = 512;
-    params.inter_size_              = inter_size;
-    params.inter_padding_size_      = inter_size;
-    params.seq_size_per_block_      = 2;
-    params.reserve_runtime_mem_mb_  = 1024;
+    model_config.inter_size              = inter_size;
+    model_config.inter_padding_size_      = inter_size;
+    model_config.attn_config.tokens_per_block = 2;
+    
+    RuntimeConfig runtime_config;
+    runtime_config.max_generate_batch_size = 128;
+    runtime_config.fifo_scheduler_config.max_context_batch_size  = 128;
+    runtime_config.reserve_runtime_mem_mb  = 1024;
+    
+    MMModelConfig mm_model_config;
+    ParallelismConfig parallelism_config;
+    EPLBConfig eplb_config;
+    PDSepConfig pd_sep_config;
+    ConcurrencyConfig concurrency_config;
+    FMHAConfig fmha_config;
+    KVCacheConfig kv_cache_config;
+    kv_cache_config.test_block_num              = 100;
+    kv_cache_config.reuse_cache             = false;
+    ProfilingDebugLoggingConfig profiling_debug_logging_config;
+    HWKernelConfig hw_kernel_config;
+    DeviceResourceConfig device_resource_config;
+    MoeConfig moe_config;
+    ModelSpecificConfig model_specific_config;
+    SpeculativeExecutionConfig sp_config;
+    CacheStoreConfig cache_store_config;
+    MiscellaneousConfig misc_config;
+    ArpcConfig arpc_config;
+    FfnDisAggregateConfig ffn_disaggregate_config;
     typedef half            T;
     const at::ScalarType    scalar_type  = at::ScalarType::Half;
     const rtp_llm::DataType data_type    = getTensorType<T>();
@@ -50,7 +71,7 @@ EngineInitParams createMockEngineInitParams(DeviceBase* device) {
     global_weights.emplace(W::lm_head, std::move(lm_head));
 
     std::vector<std::unordered_map<std::string, rtp_llm::ConstBufferPtr>> layer_weights;
-    for (int i = 0; i < params.num_layers_; ++i) {
+    for (int i = 0; i < model_config.num_layers; ++i) {
         auto pre_layernorm_weights =
             make_unique<const rtp_llm::Buffer>(mem_type, data_type, vector<size_t>{hidden_units}, data->data());
         auto pre_layernorm_beta =
@@ -105,7 +126,25 @@ EngineInitParams createMockEngineInitParams(DeviceBase* device) {
     auto                      convert = rtp_llm::WeightsConverter(false);
     rtp_llm::EngineInitParams rtp_llm_params(
         0,
-        params,
+        model_config,
+        mm_model_config,
+        parallelism_config,
+        runtime_config,
+        eplb_config,
+        pd_sep_config,
+        concurrency_config,
+        fmha_config,
+        kv_cache_config,
+        profiling_debug_logging_config,
+        hw_kernel_config,
+        device_resource_config,
+        moe_config,
+        model_specific_config,
+        sp_config,
+        cache_store_config,
+        misc_config,
+        arpc_config,
+        ffn_disaggregate_config,
         std::move(*convert.createGptWeights(std::make_unique<ConstBufferPtrMaps>(layer_weights),
                                             std::make_unique<ConstBufferPtrMap>(global_weights))));
     return rtp_llm_params;
diff --git a/rtp_llm/cpp/model_rpc/test/model_rpc_client_test.py b/rtp_llm/cpp/model_rpc/test/model_rpc_client_test.py
index f3f40bfe2..b22d382b5 100644
--- a/rtp_llm/cpp/model_rpc/test/model_rpc_client_test.py
+++ b/rtp_llm/cpp/model_rpc/test/model_rpc_client_test.py
@@ -36,7 +36,7 @@ from rtp_llm.cpp.model_rpc.proto.model_rpc_service_pb2 import (
     GenerateOutputsPB,
     TensorPB,
 )
-from rtp_llm.models.base_model import GenerateInput, GenerateOutputs
+from rtp_llm.utils.base_model_datatypes import GenerateInput, GenerateOutputs
 
 
 class FakeStub:
@@ -79,6 +79,14 @@ class FakeStub:
 class FakeModelRpcClient(ModelRpcClient):
 
     def __init__(self):
+        # Call parent __init__ with minimal required parameters
+        from rtp_llm.ops import FfnDisAggregateConfig
+        super().__init__(
+            FfnDisAggregateConfig(),
+            None,  # py_env_configs
+            0,     # max_rpc_timeout_ms
+            False, # decode_entrance
+        )
         self.stub = FakeStub()
 
     async def enqueue(
diff --git a/rtp_llm/cpp/model_utils/AttentionConfig.cc b/rtp_llm/cpp/model_utils/AttentionConfig.cc
index 734225667..bc496de12 100644
--- a/rtp_llm/cpp/model_utils/AttentionConfig.cc
+++ b/rtp_llm/cpp/model_utils/AttentionConfig.cc
@@ -1,16 +1,30 @@
 #include "rtp_llm/cpp/model_utils/AttentionConfig.h"
+#include <sstream>
 
 namespace rtp_llm {
 
+// Helper function to convert KvCacheDataType enum to string
+static std::string kvCacheDataTypeToString(KvCacheDataType kv_cache_dtype) {
+    switch (kv_cache_dtype) {
+        case KvCacheDataType::BASE:
+            return "BASE";
+        case KvCacheDataType::INT8:
+            return "INT8";
+        case KvCacheDataType::FP8:
+            return "FP8";
+        default:
+            return "UNKNOWN(" + std::to_string(static_cast<int>(kv_cache_dtype)) + ")";
+    }
+}
+
 std::string AttentionConfigs::DebugAttentionConfigStr() const {
     std::ostringstream oss;
     oss << "AttentionConfigs Debug Info:" << std::endl;
     oss << "  head_num: " << head_num << std::endl;
     oss << "  kv_head_num: " << kv_head_num << std::endl;
     oss << "  size_per_head: " << size_per_head << std::endl;
-    oss << "  hidden_size: " << hidden_size << std::endl;
     oss << "  tokens_per_block: " << tokens_per_block << std::endl;
-    oss << "  mask_type: " << mask_type << std::endl;
+    oss << "  is_causal: " << is_causal << std::endl;
     oss << "  q_scaling: " << q_scaling << std::endl;
     oss << "  fuse_qkv_add_bias: " << fuse_qkv_add_bias << std::endl;
     oss << "  use_logn_attn: " << use_logn_attn << std::endl;
@@ -21,7 +35,7 @@ std::string AttentionConfigs::DebugAttentionConfigStr() const {
     oss << "  rope_head_dim: " << rope_head_dim << std::endl;
     oss << "  v_head_dim: " << v_head_dim << std::endl;
     oss << "  softmax_extra_scale: " << softmax_extra_scale << std::endl;
-    oss << "  kv_cache_dtype: " << static_cast<int>(kv_cache_dtype) << std::endl;
+    oss << "  kv_cache_dtype: " << kvCacheDataTypeToString(kv_cache_dtype) << std::endl;
     oss << "  skip_append_kv_cache: " << skip_append_kv_cache << std::endl;
     oss << rope_config.DebugRopeConfigStr();
     return oss.str();
diff --git a/rtp_llm/cpp/model_utils/AttentionConfig.h b/rtp_llm/cpp/model_utils/AttentionConfig.h
index 960f95026..220e279d7 100644
--- a/rtp_llm/cpp/model_utils/AttentionConfig.h
+++ b/rtp_llm/cpp/model_utils/AttentionConfig.h
@@ -17,12 +17,6 @@ enum class FMHAType {
     AITER_DECODE
 };
 
-enum AttentionMaskType {
-    // ones matrix, for bert model.
-    noMask,
-    causalMask,
-};
-
 enum class KvCacheDataType : int8_t {
     BASE = 0,
     INT8 = 1,
@@ -45,18 +39,17 @@ struct AttentionConfigs {
     size_t head_num;
     size_t kv_head_num;
     size_t size_per_head;
-    size_t hidden_size;
 
     // rotary embending config
     RopeConfig rope_config;
 
     // kv cache block
-    size_t tokens_per_block;
+    size_t tokens_per_block = 8;
 
-    AttentionMaskType mask_type         = noMask;
-    float             q_scaling         = 1.0f;
-    bool              fuse_qkv_add_bias = true;
-    bool              use_logn_attn     = false;
+    float  q_scaling         = 1.0f;
+    bool   fuse_qkv_add_bias = true;
+    bool   use_logn_attn     = false;
+    bool   is_causal         = true;
 
     // mla config
     bool   use_mla = false;
diff --git a/rtp_llm/cpp/model_utils/MlaConfig.h b/rtp_llm/cpp/model_utils/MlaConfig.h
index 4c4a89cd0..24303a478 100644
--- a/rtp_llm/cpp/model_utils/MlaConfig.h
+++ b/rtp_llm/cpp/model_utils/MlaConfig.h
@@ -1,6 +1,9 @@
 #pragma once
 
 #include <cstdint>
+#include <string>
+#include <algorithm>
+#include "rtp_llm/cpp/utils/AssertUtils.h"
 
 namespace rtp_llm {
 
@@ -11,4 +14,23 @@ enum MlaOpsType : int8_t {
     FLASH_MLA   = 3,
 };
 
+inline MlaOpsType getMlaOpsType(std::string mla_ops_type_str) {
+    // Convert to uppercase for case-insensitive comparison
+    std::string upper_str = mla_ops_type_str;
+    std::transform(upper_str.begin(), upper_str.end(), upper_str.begin(), ::toupper);
+    
+    if (upper_str == "AUTO" || mla_ops_type_str == "auto") {
+        return MlaOpsType::AUTO;
+    } else if (upper_str == "MHA" || mla_ops_type_str == "mha") {
+        return MlaOpsType::MHA;
+    } else if (upper_str == "FLASH_INFER" || mla_ops_type_str == "flash_infer" || mla_ops_type_str == "flash-infer") {
+        return MlaOpsType::FLASH_INFER;
+    } else if (upper_str == "FLASH_MLA" || mla_ops_type_str == "flash_mla" || mla_ops_type_str == "flash-mla") {
+        return MlaOpsType::FLASH_MLA;
+    } else {
+        RTP_LLM_FAIL("MlaOpsType: " + mla_ops_type_str + " not supported !");
+    }
+    return MlaOpsType::AUTO;
+}
+
 }  // namespace rtp_llm
\ No newline at end of file
diff --git a/rtp_llm/cpp/model_utils/QuantInfo.cc b/rtp_llm/cpp/model_utils/QuantInfo.cc
new file mode 100644
index 000000000..7c24c1c97
--- /dev/null
+++ b/rtp_llm/cpp/model_utils/QuantInfo.cc
@@ -0,0 +1,61 @@
+#include "rtp_llm/cpp/model_utils/QuantInfo.h"
+#include <stdexcept>
+#include <algorithm>
+#include <cctype>
+
+namespace rtp_llm {
+
+void QuantAlgo::setQuantAlgo(const std::string& method, int64_t bits, int64_t group_size) {
+    std::string method_lower = method;
+    std::transform(method_lower.begin(), method_lower.end(), method_lower.begin(),
+                   [](unsigned char c) { return std::tolower(c); });
+
+    if (method_lower == "gptq") {
+        quant_method_ = GptQ;
+        weight_bits_  = bits;
+        group_size_   = group_size;
+    } else if (method_lower == "awq") {
+        quant_method_ = Awq;
+        weight_bits_  = bits;
+        group_size_   = group_size;
+    } else if (method_lower == "weight_only_per_col") {
+        quant_method_ = WeightOnlyPerCol;
+        weight_bits_  = bits;
+        if (weight_bits_ != 8) {
+            throw std::invalid_argument("invalid weight_bits: " + std::to_string(weight_bits_));
+        }
+    } else if (method_lower == "smooth_quant") {
+        quant_method_ = SmoothQuant;
+        weight_bits_  = 8;
+    } else if (method_lower == "omni_quant") {
+        quant_method_ = OmniQuant;
+        weight_bits_  = 8;
+    } else if (method_lower == "per_tensor_quant") {
+        quant_method_ = PerTensorQuant;
+        weight_bits_  = bits;
+    } else if (method_lower == "fp8" || method_lower == "fp8_quant") {
+        quant_method_ = FP8Quant;
+        weight_bits_  = 8;
+    } else if (method_lower == "fp8ptpc") {
+        quant_method_ = FP8PTPC;
+        weight_bits_  = 8;
+    } else if (method_lower == "none" || method_lower == "") {
+        quant_method_ = None;
+        weight_bits_  = 16;
+        group_size_   = 0;
+    } else {
+        throw std::invalid_argument("unknown quant method: " + method);
+    }
+}
+
+}  // namespace rtp_llm
+
+
+
+
+
+
+
+
+
+
diff --git a/rtp_llm/cpp/model_utils/activation_types.h b/rtp_llm/cpp/model_utils/activation_types.h
index 7c964acdb..d22ba635a 100644
--- a/rtp_llm/cpp/model_utils/activation_types.h
+++ b/rtp_llm/cpp/model_utils/activation_types.h
@@ -60,4 +60,29 @@ inline bool isGatedActivation(ActivationType activaiton_type) {
            || activaiton_type == ActivationType::GeGluNoneApproximate;
 }
 
+inline std::string getActivationTypeStr(ActivationType activation_type) {
+    switch (activation_type) {
+        case ActivationType::Gelu:
+            return "gelu";
+        case ActivationType::Relu:
+            return "relu";
+        case ActivationType::Silu:
+            return "silu";
+        case ActivationType::Swiglu:
+            return "gated-silu";
+        case ActivationType::Geglu:
+            return "geglu";
+        case ActivationType::Identity:
+            return "identity";
+        case ActivationType::GeluNoneApproximate:
+            return "gelu-none-approximate";
+        case ActivationType::GeGluNoneApproximate:
+            return "geglu-none-approximate";
+        case ActivationType::Sigmoid:
+            return "sigmoid";
+        default:
+            throw std::runtime_error("Invalid ActivationType: " + std::to_string(static_cast<int>(activation_type)));
+    }
+}
+
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/model_utils/layernorm_types.h b/rtp_llm/cpp/model_utils/layernorm_types.h
index a907665cf..09ce1c547 100644
--- a/rtp_llm/cpp/model_utils/layernorm_types.h
+++ b/rtp_llm/cpp/model_utils/layernorm_types.h
@@ -43,4 +43,30 @@ inline NormType getNormType(std::string norm_type_str) {
     return NormType::invalid_type;
 }
 
+inline std::string getLayerNormTypeStr(LayerNormType layernorm_type) {
+    switch (layernorm_type) {
+        case LayerNormType::pre_layernorm:
+            return "pre_layernorm";
+        case LayerNormType::post_layernorm:
+            return "post_layernorm";
+        default:
+            throw std::runtime_error("Invalid LayerNormType: " + std::to_string(static_cast<int>(layernorm_type)));
+    }
+}
+
+inline std::string getNormTypeStr(NormType norm_type) {
+    switch (norm_type) {
+        case NormType::layernorm:
+            return "layernorm";
+        case NormType::rmsnorm:
+            return "rmsnorm";
+        case NormType::alphanorm:
+            return "alphanorm";
+        case NormType::add_bias:
+            return "add_bias";
+        default:
+            throw std::runtime_error("Invalid NormType: " + std::to_string(static_cast<int>(norm_type)));
+    }
+}
+
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/models/BUILD b/rtp_llm/cpp/models/BUILD
index e2c3f9ac1..32d54181b 100644
--- a/rtp_llm/cpp/models/BUILD
+++ b/rtp_llm/cpp/models/BUILD
@@ -134,6 +134,7 @@ cc_library(
     deps = [
         ":models",
         ":dfa_utils",
+        "//rtp_llm/cpp/config:eplb_config",
         "//rtp_llm/cpp/pybind:py_utils",
     ],
     visibility = ["//visibility:public"],
diff --git a/rtp_llm/cpp/models/GptModel.cc b/rtp_llm/cpp/models/GptModel.cc
index e1b0b8299..0dda5cd16 100644
--- a/rtp_llm/cpp/models/GptModel.cc
+++ b/rtp_llm/cpp/models/GptModel.cc
@@ -258,7 +258,7 @@ rtp_llm::AttentionCommonInputs GptModel::prepareAttentionInputs(const GptModelIn
             device_->attentionMask({inputs.input_lengths->view(decoder_batch_size, context_batch_size),
                                     *inputs.prefix_lengths,
                                     attn_dtype,
-                                    description_.attention_conf.mask_type == rtp_llm::AttentionMaskType::causalMask});
+                                    description_.attention_conf.is_causal});
     }
 
     return attention_inputs;
@@ -364,7 +364,7 @@ GptModel::splitInputsIntoMicroBatches(const GptModelInputs& inputs, const MicroB
         fake_inputs.prefix_lengths   = device_->allocateBuffer({DataType::TYPE_INT32, {1}, AllocationType::HOST});
         fake_inputs.prefix_lengths->data<int32_t>()[0] = 0;
         auto fake_hidden =
-            device_->allocateBuffer({description_.data_type, {1, description_.attention_conf.hidden_size}});
+            device_->allocateBuffer({description_.data_type, {1, description_.attention_conf.head_num * description_.attention_conf.size_per_head}});
         micro_batch_inputs.push_back(fake_inputs);
     } else {
         // TODO(wangyin.yx): refact this splitting method, extract common code
@@ -706,7 +706,7 @@ GptLayerInputs GptModel::forwardPreLayers(const GptModelInputs& inputs) {
                                     || description_.act_qscheme == Qfp8PerTensor,
                                 "ring p2p overlap only supports bf16/fp16 or w8a8 or fp8 per block");
         const size_t max_batch_seq_len =
-            device_->initParams().fifo_scheduler_config.max_context_batch_size * device_->initParams().max_seq_len;
+            device_->initParams().runtime_config.fifo_scheduler_config.max_context_batch_size * device_->initParams().max_seq_len;
         const size_t attn_rs_hidden         = layer0.self_attention_weights.output_weight->kernel->shape()[1];
         const size_t ffn_rs_hidden          = layer0.ffn_weights.down_weight->kernel->shape()[1];
         const size_t attn_ag_hidden         = layer0.self_attention_weights.qkv_weight->kernel->shape()[0];
diff --git a/rtp_llm/cpp/models/PyWrappedModel.cc b/rtp_llm/cpp/models/PyWrappedModel.cc
index 2a4d117eb..502807cb1 100644
--- a/rtp_llm/cpp/models/PyWrappedModel.cc
+++ b/rtp_llm/cpp/models/PyWrappedModel.cc
@@ -184,7 +184,7 @@ GptModelOutputs PyWrappedModel::forwardMicroBatched(const GptModelInputs& inputs
     } else {
         hidden_states =
             device_->allocateBuffer({description_.data_type,
-                                     {inputs.combo_tokens->shape()[0], description_.attention_conf.hidden_size},
+                                     {inputs.combo_tokens->shape()[0], description_.attention_conf.head_num * description_.attention_conf.size_per_head},
                                      AllocationType::DEVICE});
         int offset = 0;
         for (int i = 0; i < py_model_outputs.size(); i++) {
diff --git a/rtp_llm/cpp/models/eplb/EplbConfig.h b/rtp_llm/cpp/models/eplb/EplbConfig.h
deleted file mode 100644
index 5e4ecb6f7..000000000
--- a/rtp_llm/cpp/models/eplb/EplbConfig.h
+++ /dev/null
@@ -1,51 +0,0 @@
-#pragma once
-#include <string>
-#include <vector>
-
-namespace rtp_llm {
-
-enum class EplbMode {
-    NONE,
-    STATS,  // stats, only
-    EPLB,   // load balance, only
-    ALL     // stats + load balance
-};
-
-template<typename... CheckModes>
-bool checkEplbMode(EplbMode mode, CheckModes... modes) {
-    return ((mode == modes) || ...);
-}
-
-struct EplbConfig {
-    EplbMode mode;
-    int      update_time;
-
-    std::vector<int> toList() const {
-        std::vector<int> list;
-        list.push_back((int)mode);
-        list.push_back(update_time);
-        return list;
-    }
-
-    static EplbConfig fromList(const int* list) {
-        EplbConfig data;
-        data.mode        = (EplbMode)list[0];
-        data.update_time = list[1];
-        return data;
-    }
-
-    bool operator==(const EplbConfig& other) const {
-        return mode == other.mode && update_time == other.update_time;
-    }
-
-    bool operator!=(const EplbConfig& other) const {
-        return !(*this == other);
-    }
-
-    std::string toString() const {
-        return "EplbControlData{mode=" + std::to_string((int)mode) + ", update_time=" + std::to_string(update_time)
-               + "}";
-    }
-};
-
-}  // namespace rtp_llm
diff --git a/rtp_llm/cpp/models/eplb/ExpertBalancer.cc b/rtp_llm/cpp/models/eplb/ExpertBalancer.cc
index 5aca75807..e6d967cd4 100644
--- a/rtp_llm/cpp/models/eplb/ExpertBalancer.cc
+++ b/rtp_llm/cpp/models/eplb/ExpertBalancer.cc
@@ -97,10 +97,10 @@ bool LoadFlags::isReady(DeviceBase* device) {
     return *flag_host->data<int>() == 0;
 }
 
-void EplbController::init(const EplbConfig& eplb_control_data, DeviceBase* device) {
+void EplbController::init(const EPLBConfig& eplb_control_data, DeviceBase* device, const EPLBConfig& eplb_config) {
     this->eplb_control_data = eplb_control_data;
 
-    control_step = device->initParams().moe_config.eplb_control_step;
+    control_step = eplb_config.eplb_control_step;
     RTP_LLM_LOG_INFO("EPLB control step: %d", control_step);
 
     auto eplb_control_data_list = eplb_control_data.toList();
@@ -110,7 +110,7 @@ void EplbController::init(const EplbConfig& eplb_control_data, DeviceBase* devic
         {DataType::TYPE_INT32, {eplb_control_data_list.size()}, AllocationType::DEVICE}, {"eplb_control_data_device"});
 }
 
-void EplbController::setData(const EplbConfig& updated_control_data) {
+void EplbController::setData(const EPLBConfig& updated_control_data) {
     // lock mutex
     lock_guard<mutex> lock(eplb_control_mutex);
     eplb_control_data = updated_control_data;
@@ -126,9 +126,9 @@ bool EplbController::stepAndCheckSyncStep() {
     return false;
 }
 
-EplbConfig EplbController::getAndSyncData(DeviceBase* device) {
+EPLBConfig EplbController::getAndSyncData(DeviceBase* device) {
     // copy control data to host buffer
-    EplbConfig cur_data;
+    EPLBConfig cur_data;
     {
         lock_guard<mutex> lock(eplb_control_mutex);
         cur_data = eplb_control_data;
@@ -148,8 +148,8 @@ EplbConfig EplbController::getAndSyncData(DeviceBase* device) {
     // copy to host
     device->copy({*eplb_control_data_buf_host, *eplb_control_data_buf_device});
 
-    // convert to EplbControlData
-    auto eplb_control_data = EplbConfig::fromList(eplb_control_data_host_ptr);
+    // convert to EPLBConfig
+    auto eplb_control_data = EPLBConfig::fromList(eplb_control_data_host_ptr);
 
     return eplb_control_data;
 }
@@ -159,15 +159,14 @@ ExpertBalancer::ExpertBalancer(size_t                       log_exp_num,
                                size_t                       num_layers,
                                size_t                       moe_size,
                                size_t                       hidden_size,
-                               size_t                       update_time,
                                size_t                       ep_rank,
                                size_t                       ep_size,
                                py::object                   py_eplb,
                                DataType                     dtype,
                                DeviceBase*                  device,
-                               EplbMode                     eplb_mode,
                                QuantAlgo                    quant_algo,
-                               kmonitor::MetricsReporterPtr metrics_reporter):
+                               kmonitor::MetricsReporterPtr metrics_reporter,
+                               const EPLBConfig&             eplb_config):
 
         
     device_(device),
@@ -179,8 +178,7 @@ ExpertBalancer::ExpertBalancer(size_t                       log_exp_num,
     eplb_python_wrapper_(py_eplb) {
     cout << "ExpertBalancer constructed with " << log_exp_num << " logical experts" << endl;
     printf("DEBUG: ExpertBalancer constructor called for linker debug\n");
-    eplb_control_data_.mode        = eplb_mode;
-    eplb_control_data_.update_time = update_time;
+    eplb_control_data_ = eplb_config;
 
     // init memory
     stats_.init(num_layers, log_exp_num, ep_size_, device_);
@@ -188,11 +186,11 @@ ExpertBalancer::ExpertBalancer(size_t                       log_exp_num,
     eplb_plan_tensors_.init(log_exp_num, phy_exp_num);
     load_flags_.init(device_);
     load_flags_.setReady(false, device_);
-    eplb_controller_.init(eplb_control_data_, device);
+    eplb_controller_.init(eplb_control_data_, device, eplb_config);
 
-    test_mode_ = device_->initParams().moe_config.eplb_test_mode;
+    test_mode_ = eplb_config.eplb_test_mode;
 
-    balance_layer_per_step_ = device_->initParams().moe_config.eplb_balance_layer_per_step;
+    balance_layer_per_step_ = eplb_config.eplb_balance_layer_per_step;
 
     resetPlan(true);
 }
@@ -202,7 +200,7 @@ ExpertBalancer::~ExpertBalancer() {}
 void ExpertBalancer::stepForward(GptModel& model, RtpLLMExecutorMetricsCollector& executor_collector) {
     syncController();
 
-    if (checkEplbMode(eplb_control_data_.mode, EplbMode::NONE)) {
+    if (eplb_control_data_.checkEplbMode(eplb_control_data_.eplb_mode, EplbMode::NONE)) {
         return;
     }
 
@@ -215,7 +213,7 @@ void ExpertBalancer::stepForward(GptModel& model, RtpLLMExecutorMetricsCollector
     excuteEplbPlan(stats, model);
 }
 
-bool ExpertBalancer::updateEplbConfig(const EplbConfig& config) {
+bool ExpertBalancer::updateEplbConfig(const EPLBConfig& config) {
     eplb_controller_.setData(config);
     return true;
 }
@@ -224,15 +222,16 @@ void ExpertBalancer::syncController() {
     // sync control data
     if (eplb_controller_.stepAndCheckSyncStep()) {
         auto eplb_control_data = eplb_controller_.getAndSyncData(device_);
-        if (eplb_control_data != eplb_control_data_) {
+        if (eplb_control_data.eplb_mode != eplb_control_data_.eplb_mode || 
+            eplb_control_data.eplb_update_time != eplb_control_data_.eplb_update_time) {
             eplb_control_data_ = eplb_control_data;
-            RTP_LLM_LOG_INFO("EPLB coinfig changed to %s", eplb_control_data_.toString().c_str());
+            RTP_LLM_LOG_INFO("EPLB config changed to %s", eplb_control_data_.to_string().c_str());
         }
     }
 }
 
 void ExpertBalancer::reportStats(OverallExpertStats& stats) {
-    if (metrics_reporter_ && checkEplbMode(eplb_control_data_.mode, EplbMode::STATS, EplbMode::ALL)) {
+    if (metrics_reporter_ && eplb_control_data_.checkEplbMode(eplb_control_data_.eplb_mode, EplbMode::STATS, EplbMode::ALL)) {
         int layer_num = stats.layer_num;
         executor_collector_.gpu_loads.resize(layer_num);
         executor_collector_.ep_rank = ep_rank_;
@@ -260,13 +259,13 @@ EplbPlanStatus ExpertBalancer::getPlanStatus() const {
 }
 
 void ExpertBalancer::excuteEplbPlan(OverallExpertStats& stats, GptModel& model) {
-    if (checkEplbMode(eplb_control_data_.mode, EplbMode::EPLB, EplbMode::ALL)) {
+    if (eplb_control_data_.checkEplbMode(eplb_control_data_.eplb_mode, EplbMode::EPLB, EplbMode::ALL)) {
         EplbPlanStatus status = getPlanStatus();
         switch (status) {
             case EplbPlanStatus::INIT:
                 update_cnt_++;
                 updateStats(stats);
-                if (update_cnt_ >= eplb_control_data_.update_time) {
+                if (update_cnt_ >= eplb_control_data_.eplb_update_time) {
                     setPlanStatus(EplbPlanStatus::PREPARING);
                 }
                 break;
@@ -299,7 +298,7 @@ void ExpertBalancer::excuteEplbPlan(OverallExpertStats& stats, GptModel& model)
                         update_cnt_        = 0;
                         balance_layer_cnt_ = 0;
                     } else {
-                        update_cnt_ = eplb_control_data_.update_time;  // quick update
+                        update_cnt_ = eplb_control_data_.eplb_update_time;  // quick update
                     }
 
                     setPlanStatus(EplbPlanStatus::INIT);
diff --git a/rtp_llm/cpp/models/eplb/ExpertBalancer.h b/rtp_llm/cpp/models/eplb/ExpertBalancer.h
index 759a6705d..cd92b755c 100644
--- a/rtp_llm/cpp/models/eplb/ExpertBalancer.h
+++ b/rtp_llm/cpp/models/eplb/ExpertBalancer.h
@@ -5,7 +5,7 @@
 #include "rtp_llm/cpp/metrics/RtpLLMMetrics.h"
 #include "rtp_llm/cpp/devices/DeviceBase.h"
 #include "kmonitor/client/MetricsReporter.h"
-#include "rtp_llm/cpp/models/eplb/EplbConfig.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 namespace rtp_llm {
 
@@ -69,7 +69,7 @@ enum class EplbPlanStatus {
 class EplbController {
 private:
     std::mutex eplb_control_mutex;
-    EplbConfig eplb_control_data;
+    EPLBConfig eplb_control_data;
 
     BufferPtr eplb_control_data_buf_host;
     BufferPtr eplb_control_data_buf_device;
@@ -78,10 +78,10 @@ private:
     int cur_step     = 0;
 
 public:
-    void       init(const EplbConfig& eplb_control_data, DeviceBase* device);
-    void       setData(const EplbConfig& updated_control_data);
+    void       init(const EPLBConfig& eplb_control_data, DeviceBase* device, const EPLBConfig& eplb_config);
+    void       setData(const EPLBConfig& updated_control_data);
     bool       stepAndCheckSyncStep();
-    EplbConfig getAndSyncData(DeviceBase* device);
+    EPLBConfig getAndSyncData(DeviceBase* device);
 };
 
 class ExpertBalancer {
@@ -92,20 +92,19 @@ public:
                    size_t                       num_layers,
                    size_t                       moe_size,
                    size_t                       hidden_size,
-                   size_t                       update_time,
                    size_t                       ep_rank,
                    size_t                       ep_size,
                    py::object                   py_eplb,
                    DataType                     dtype,
                    DeviceBase*                  device,
-                   EplbMode                     eplb_mode,
                    QuantAlgo                    quant_algo,
-                   kmonitor::MetricsReporterPtr metrics_reporter);
+                   kmonitor::MetricsReporterPtr metrics_reporter,
+                   const EPLBConfig&             eplb_config);
     ~ExpertBalancer();
 
     void stepForward(GptModel& model, RtpLLMExecutorMetricsCollector& executor_collector);
 
-    bool updateEplbConfig(const EplbConfig& config);
+    bool updateEplbConfig(const EPLBConfig& config);
 
 private:
     void syncController();
@@ -151,7 +150,7 @@ private:
     LoadFlags           load_flags_;
 
     EplbController eplb_controller_;
-    EplbConfig     eplb_control_data_;
+    EPLBConfig     eplb_control_data_;
 
     RtpLLmEplbMetricsCollector   executor_collector_;
     kmonitor::MetricsReporterPtr metrics_reporter_;
diff --git a/rtp_llm/cpp/models/logits_processor/test/BUILD b/rtp_llm/cpp/models/logits_processor/test/BUILD
index 41ed546c7..2de10e8f3 100644
--- a/rtp_llm/cpp/models/logits_processor/test/BUILD
+++ b/rtp_llm/cpp/models/logits_processor/test/BUILD
@@ -9,7 +9,7 @@ test_deps = [
     "//rtp_llm/cpp/devices/testing:device_test_utils",
     "//rtp_llm/cpp/devices/cuda_impl:cuda_impl",
     "//rtp_llm/cpp/models:logits_processor",
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
     "@com_google_googletest//:gtest",
     "@com_google_googletest//:gtest_main",
     "@local_config_cuda//cuda:cuda_headers",
diff --git a/rtp_llm/cpp/models/lora/test/BUILD b/rtp_llm/cpp/models/lora/test/BUILD
index 284f2e85c..aab417343 100644
--- a/rtp_llm/cpp/models/lora/test/BUILD
+++ b/rtp_llm/cpp/models/lora/test/BUILD
@@ -9,7 +9,7 @@ test_deps = [
     "//rtp_llm/cpp/devices/testing:device_test_utils",
     "//rtp_llm/cpp/devices/cuda_impl:cuda_impl",
     "//rtp_llm/cpp/models:lora",
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
     "//rtp_llm/cpp/normal_engine:normal_engine",
     "//rtp_llm/cpp/engine_base:weights_converter",
     "@com_google_googletest//:gtest",
diff --git a/rtp_llm/cpp/models/lora/test/LoraNormEngineTest.cc b/rtp_llm/cpp/models/lora/test/LoraNormEngineTest.cc
index 2935e01b5..643744a77 100644
--- a/rtp_llm/cpp/models/lora/test/LoraNormEngineTest.cc
+++ b/rtp_llm/cpp/models/lora/test/LoraNormEngineTest.cc
@@ -7,6 +7,7 @@
 #include "rtp_llm/cpp/normal_engine/NormalEngine.h"
 #include "rtp_llm/cpp/engine_base/WeightsConverter.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 using namespace std;
 
@@ -15,24 +16,44 @@ namespace rtp_llm {
 class LoraNormalEngineTest: public DeviceTestBase {
 protected:
     EngineInitParams createMockEngineInitParams(DeviceBase* device) {
-        auto params                     = GptInitParameter();
-        params.head_num_                = 2;
-        params.size_per_head_           = 64;
-        params.num_layers_              = 2;
-        params.max_seq_len_             = 20;
-        params.vocab_size_              = 20;
-        params.hidden_size_             = 128;
-        params.head_num_kv_             = 2;
-        params.block_nums_              = 100;
-        params.reuse_cache_             = false;
-        params.max_generate_batch_size_ = 128;
-        params.max_context_batch_size_  = 128;
-        params.kv_cache_data_type_      = DataType::TYPE_FP16;
+        ModelConfig model_config;
+        model_config.attn_config.head_num = 2;
+        model_config.attn_config.size_per_head = 64;
+        model_config.num_layers              = 2;
+        model_config.max_seq_len             = 20;
+        model_config.vocab_size              = 20;
+        model_config.hidden_size             = 128;
+        model_config.attn_config.kv_head_num = 2;
+        model_config.attn_config.kv_cache_dtype = KvCacheDataType::BASE;
         const size_t inter_size         = 512;
-        params.inter_size_              = inter_size;
-        params.inter_padding_size_      = inter_size;
-        params.seq_size_per_block_      = 2;
-        params.reserve_runtime_mem_mb_  = 1024;
+        model_config.inter_size              = inter_size;
+        model_config.inter_padding_size_      = inter_size;
+        model_config.attn_config.tokens_per_block = 2;
+        
+        RuntimeConfig runtime_config;
+        runtime_config.max_generate_batch_size = 128;
+        runtime_config.fifo_scheduler_config.max_context_batch_size  = 128;
+        runtime_config.reserve_runtime_mem_mb  = 1024;
+        
+        MMModelConfig mm_model_config;
+        ParallelismConfig parallelism_config;
+        EPLBConfig eplb_config;
+        PDSepConfig pd_sep_config;
+        ConcurrencyConfig concurrency_config;
+        FMHAConfig fmha_config;
+        KVCacheConfig kv_cache_config;
+        kv_cache_config.test_block_num              = 100;
+        kv_cache_config.reuse_cache             = false;
+        ProfilingDebugLoggingConfig profiling_debug_logging_config;
+        HWKernelConfig hw_kernel_config;
+        DeviceResourceConfig device_resource_config;
+        MoeConfig moe_config;
+        ModelSpecificConfig model_specific_config;
+        SpeculativeExecutionConfig sp_config;
+        CacheStoreConfig cache_store_config;
+        MiscellaneousConfig misc_config;
+        ArpcConfig arpc_config;
+        FfnDisAggregateConfig ffn_disaggregate_config;
         typedef half            T;
         const rtp_llm::DataType data_type    = getTensorType<T>();
         auto                    mem_type     = rtp_llm::MemoryType::MEMORY_GPU;
@@ -46,7 +67,7 @@ protected:
         global_weights.emplace(W::embedding, std::move(word_embeddings));
         global_weights.emplace(W::lm_head, std::move(lm_head));
         std::vector<std::unordered_map<std::string, rtp_llm::ConstBufferPtr>> layer_weights;
-        for (int i = 0; i < params.num_layers_; ++i) {
+        for (int i = 0; i < model_config.num_layers; ++i) {
             auto pre_layernorm_weights =
                 make_unique<const rtp_llm::Buffer>(mem_type, data_type, vector<size_t>{hidden_units}, data->data());
             auto pre_layernorm_beta =
@@ -101,7 +122,25 @@ protected:
         auto                      convert = rtp_llm::WeightsConverter(false);
         rtp_llm::EngineInitParams rtp_llm_params(
             0,
-            params,
+            model_config,
+            mm_model_config,
+            parallelism_config,
+            runtime_config,
+            eplb_config,
+            pd_sep_config,
+            concurrency_config,
+            fmha_config,
+            kv_cache_config,
+            profiling_debug_logging_config,
+            hw_kernel_config,
+            device_resource_config,
+            moe_config,
+            model_specific_config,
+            sp_config,
+            cache_store_config,
+            misc_config,
+            arpc_config,
+            ffn_disaggregate_config,
             std::move(*convert.createGptWeights(std::make_unique<ConstBufferPtrMaps>(layer_weights),
                                                 std::make_unique<ConstBufferPtrMap>(global_weights))));
         return rtp_llm_params;
diff --git a/rtp_llm/cpp/models/position_ids/test/BUILD b/rtp_llm/cpp/models/position_ids/test/BUILD
index cea07ef98..43449990d 100644
--- a/rtp_llm/cpp/models/position_ids/test/BUILD
+++ b/rtp_llm/cpp/models/position_ids/test/BUILD
@@ -19,7 +19,7 @@ cc_test(
         "//rtp_llm/cpp/models:position_ids_generator",
         "//rtp_llm/cpp/models:models",
         "//rtp_llm/cpp/engine_base/stream:stream",
-        "//rtp_llm/cpp/config:gpt_init_params",
+        "//rtp_llm/cpp/config:config_modules",
     ],
     env = {
         "TEST_USING_DEVICE": "CUDA",
diff --git a/rtp_llm/cpp/models/test/BUILD b/rtp_llm/cpp/models/test/BUILD
index 6e9819a85..7a71f4e43 100644
--- a/rtp_llm/cpp/models/test/BUILD
+++ b/rtp_llm/cpp/models/test/BUILD
@@ -10,7 +10,7 @@ test_deps = [
     "//rtp_llm/cpp/devices/cuda_impl:cuda_impl",
     "//rtp_llm/cpp/normal_engine:normal_engine",
     "//rtp_llm/cpp/models:models",
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
     "@com_google_googletest//:gtest",
     "@com_google_googletest//:gtest_main",
     "@local_config_cuda//cuda:cuda_headers",
diff --git a/rtp_llm/cpp/multimodal_processor/MultimodalProcessor.h b/rtp_llm/cpp/multimodal_processor/MultimodalProcessor.h
index ab0962202..7ee6e4f97 100644
--- a/rtp_llm/cpp/multimodal_processor/MultimodalProcessor.h
+++ b/rtp_llm/cpp/multimodal_processor/MultimodalProcessor.h
@@ -8,7 +8,7 @@
 #include "rtp_llm/cpp/utils/StatusUtil.h"
 #include "rtp_llm/cpp/pybind/PyUtils.h"
 #include "rtp_llm/cpp/core/Buffer.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 namespace py = pybind11;
 
@@ -28,16 +28,16 @@ struct ExpandedOutput {
 
 class MultimodalProcessor {
 public:
-    MultimodalProcessor(py::object mm_process_engine, rtp_llm::GptInitParameter params):
+    MultimodalProcessor(py::object mm_process_engine,
+                       const MMModelConfig& mm_model_config,
+                       int64_t max_seq_len):
         mm_process_engine_(mm_process_engine),
-        gpt_init_parameter_(params),
-        sep_token_ids_(params.mm_sep_tokens_),
-        include_sep_tokens_(params.include_sep_tokens_),
-        max_seq_len_(params.max_seq_len_) {}
+        sep_token_ids_(mm_model_config.mm_sep_tokens),
+        include_sep_tokens_(mm_model_config.include_sep_tokens),
+        max_seq_len_(max_seq_len) {}
 
 protected:
     py::object                mm_process_engine_;
-    rtp_llm::GptInitParameter gpt_init_parameter_;
 
 private:
     std::vector<std::vector<int64_t>> sep_token_ids_;
diff --git a/rtp_llm/cpp/multimodal_processor/RemoteMultimodalProcessor.h b/rtp_llm/cpp/multimodal_processor/RemoteMultimodalProcessor.h
index 4bf14d878..2c8ac4192 100644
--- a/rtp_llm/cpp/multimodal_processor/RemoteMultimodalProcessor.h
+++ b/rtp_llm/cpp/multimodal_processor/RemoteMultimodalProcessor.h
@@ -24,8 +24,10 @@ namespace rtp_llm {
 
 class RemoteMultimodalProcessor: public MultimodalProcessor {
 public:
-    RemoteMultimodalProcessor(py::object mm_process_engine, rtp_llm::GptInitParameter params):
-        MultimodalProcessor(mm_process_engine, params) {
+    RemoteMultimodalProcessor(py::object mm_process_engine,
+                             const MMModelConfig& mm_model_config,
+                             int64_t max_seq_len):
+        MultimodalProcessor(mm_process_engine, mm_model_config, max_seq_len) {
     }
 
 private:
diff --git a/rtp_llm/cpp/multimodal_processor/test/BUILD b/rtp_llm/cpp/multimodal_processor/test/BUILD
index dec4c0f82..b29e96c9c 100644
--- a/rtp_llm/cpp/multimodal_processor/test/BUILD
+++ b/rtp_llm/cpp/multimodal_processor/test/BUILD
@@ -16,7 +16,7 @@ cc_test(
         "//rtp_llm/cpp/devices/testing:device_test_utils",
         "//rtp_llm/cpp/core:buffer",
         "//rtp_llm/cpp/devices/cuda_impl:cuda_impl",
-        "//rtp_llm/cpp/config:gpt_init_params",
+        "//rtp_llm/cpp/config:config_modules",
         "@com_google_googletest//:gtest",
         "@com_google_googletest//:gtest_main",
         "//rtp_llm/cpp/multimodal_processor:multimodal_processor",
diff --git a/rtp_llm/cpp/multimodal_processor/test/MultimodalProcessorTest.cc b/rtp_llm/cpp/multimodal_processor/test/MultimodalProcessorTest.cc
index 087a9d54a..6297ec93c 100644
--- a/rtp_llm/cpp/multimodal_processor/test/MultimodalProcessorTest.cc
+++ b/rtp_llm/cpp/multimodal_processor/test/MultimodalProcessorTest.cc
@@ -8,7 +8,8 @@
 #include "rtp_llm/cpp/multimodal_processor/MultimodalProcessor.h"
 #include "rtp_llm/cpp/multimodal_processor/MultimodalTypes.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 using namespace std;
 
@@ -21,11 +22,10 @@ public:
     static FakeMultimodalProcessor createFakeMultimodalProcessor(std::vector<std::vector<int64_t>> sep_token_ids,
                                                                  bool                              include_sep_tokens,
                                                                  int                               max_seq_len) {
-        GptInitParameter params;
-        params.mm_sep_tokens_      = sep_token_ids;
-        params.include_sep_tokens_ = include_sep_tokens;
-        params.max_seq_len_        = max_seq_len;
-        return FakeMultimodalProcessor(py::none(), params);
+        MMModelConfig mm_model_config;
+        mm_model_config.mm_sep_tokens_      = sep_token_ids;
+        mm_model_config.include_sep_tokens_ = include_sep_tokens;
+        return FakeMultimodalProcessor(py::none(), mm_model_config, max_seq_len);
     }
 
     ErrorResult<MultimodalOutput> MultimodalEmbedding(const std::vector<rtp_llm::MultimodalInput> mm_inputs,
diff --git a/rtp_llm/cpp/normal_engine/NormalBatchStreamProcessor.h b/rtp_llm/cpp/normal_engine/NormalBatchStreamProcessor.h
index fd7609005..676aac8d3 100644
--- a/rtp_llm/cpp/normal_engine/NormalBatchStreamProcessor.h
+++ b/rtp_llm/cpp/normal_engine/NormalBatchStreamProcessor.h
@@ -1,7 +1,7 @@
 #pragma once
 
 #include "rtp_llm/cpp/devices/DeviceBase.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "rtp_llm/cpp/models/SampleInfos.h"
 #include "rtp_llm/cpp/engine_base/stream/StreamGroups.h"
 #include "absl/status/statusor.h"
@@ -11,23 +11,27 @@ namespace rtp_llm {
 
 class NormalBatchStreamProcessor {
 public:
-    NormalBatchStreamProcessor(const rtp_llm::GptInitParameter& params, const CacheConfig& cache_config, bool warm_up):
-        num_layers_(params.num_layers_),
-        vocab_size_(params.vocab_size_),
-        input_vocab_size_(params.input_vocab_size_),
-        use_int8_kv_cache_(params.kv_cache_data_type_ == rtp_llm::DataType::TYPE_INT8),
-        has_positional_encoding_(params.has_positional_encoding_),
-        is_multimodal_(params.is_multimodal_),
-        mm_position_ids_style_((PositionIdsStyle)params.mm_position_ids_style_),
-        position_id_len_factor_(params.position_id_len_factor_),
-        role_type_(params.role_type_),
-        decode_entrance_(params.decode_entrance_),
+    NormalBatchStreamProcessor(const ModelConfig& model_config,
+                               const PDSepConfig& pd_sep_config,
+                               const ProfilingDebugLoggingConfig& profiling_debug_logging_config,
+                               const CacheConfig& cache_config,
+                               bool warm_up):
+        num_layers_(model_config.num_layers),
+        vocab_size_(model_config.vocab_size),
+        input_vocab_size_(model_config.input_vocab_size),
+        use_int8_kv_cache_(model_config.attn_config.kv_cache_dtype == rtp_llm::KvCacheDataType::INT8),
+        has_positional_encoding_(model_config.has_positional_encoding),
+        is_multimodal_(model_config.mm_model_config.is_multimodal),
+        mm_position_ids_style_((PositionIdsStyle)model_config.mm_model_config.mm_position_ids_style),
+        position_id_len_factor_(model_config.attn_config.rope_config.index_factor),
+        role_type_(pd_sep_config.role_type),
+        decode_entrance_(pd_sep_config.decode_entrance),
         k_block_size_(cache_config.k_block_stride),
         v_block_size_(cache_config.v_block_stride),
         scale_block_size_(cache_config.kv_scale_block_stride),
         seq_size_per_block_(cache_config.seq_size_per_block),
         warm_up_(warm_up),
-        enable_detail_log_(params.profiling_debug_logging_config.enable_detail_log),
+        enable_detail_log_(profiling_debug_logging_config.enable_detail_log),
         device_(rtp_llm::DeviceFactory::getDefaultDevice()) {}
     virtual absl::Status dispatch(const StreamGroups& stream_groups, const MergedOutput& merge_outputs) const;
     virtual absl::StatusOr<GptModelInputs> gatherModelInput(const StreamGroups& stream_groups) const;
diff --git a/rtp_llm/cpp/normal_engine/NormalEngine.cc b/rtp_llm/cpp/normal_engine/NormalEngine.cc
index 1ce3a5ab1..ecbbb3174 100644
--- a/rtp_llm/cpp/normal_engine/NormalEngine.cc
+++ b/rtp_llm/cpp/normal_engine/NormalEngine.cc
@@ -22,18 +22,27 @@ namespace rtp_llm {
 
 NormalEngine::NormalEngine(const EngineInitParams& params):
     EngineBase(params),
-    params_(params.gpt_init_parameter),
+    model_config_(params.model_config_),
+    parallelism_config(params.parallelism_config),
+    runtime_config(params.runtime_config),
+    eplb_config(params.eplb_config),
+    pd_sep_config(params.pd_sep_config),
+    profiling_debug_logging_config(params.profiling_debug_logging_config),
+    kv_cache_config(params.kv_cache_config),
+    ffn_disaggregate_config(params.ffn_disaggregate_config),
+    model_specific_config(params.model_specific_config),
+    sp_config(params.sp_config),
     metrics_reporter_(params.metrics_reporter),
     profiler_step_(0),
-    gen_timeline_sync_(params.gpt_init_parameter.profiling_debug_logging_config.gen_timeline_sync) {
+    gen_timeline_sync_(params.profiling_debug_logging_config.gen_timeline_sync) {
     RTP_LLM_LOG_INFO(__PRETTY_FUNCTION__);
     std::optional<WarmUpResult> warm_up_result = std::nullopt;
-    if (params_.warm_up_ && (!params_.is_multimodal_) && !params_.ffn_disaggregate_config.enable_ffn_disaggregate) {
+    if (runtime_config.warm_up && (!model_config_.mm_model_config.is_multimodal) && !ffn_disaggregate_config.enable_ffn_disaggregate) {
         // warm up
         RTP_LLM_LOG_INFO("warm up (max_context_batch_size %d, max_seq_len %d calculate_loss %d) query begin",
-                         params_.max_context_batch_size_,
-                         params_.max_seq_len_,
-                         int(params_.warm_up_with_loss_));
+                         runtime_config.fifo_scheduler_config.max_context_batch_size,
+                         model_config_.max_seq_len,
+                         int(runtime_config.warm_up_with_loss));
         warm_up_result = warmUp(params);
         RTP_LLM_LOG_INFO(
             "warm up done, max runtime used memory: %ld bytes (%ld MiB), device reserved memory: %ld bytes (%ld MiB)",
@@ -53,15 +62,19 @@ NormalEngine::NormalEngine(const EngineInitParams& params):
 }
 
 void NormalEngine::initScheduler() {
-    if (params_.scheduler_config.use_batch_decode_scheduler) {
+    if (runtime_config.use_batch_decode_scheduler) {
         scheduler_.reset(
-            new BatchDecodeScheduler(params_, resource_context_.cache_manager, metrics_reporter_, device_));
+            new BatchDecodeScheduler(runtime_config, resource_context_.cache_manager, metrics_reporter_, device_));
         RTP_LLM_LOG_INFO("create batch decode scheduler done");
-    } else if (params_.scheduler_config.use_gather_batch_scheduler) {
-        scheduler_.reset(new GatherBatchScheduler(params_, resource_context_.cache_manager, metrics_reporter_));
+    } else if (runtime_config.use_gather_batch_scheduler) {
+        scheduler_.reset(new GatherBatchScheduler(
+            runtime_config, model_config_, pd_sep_config, parallelism_config, model_specific_config,
+            resource_context_.cache_manager, metrics_reporter_));
         RTP_LLM_LOG_INFO("create gather batch scheduler done");
     } else {
-        scheduler_.reset(new FIFOScheduler(params_, resource_context_.cache_manager, metrics_reporter_));
+        scheduler_.reset(new FIFOScheduler(
+            runtime_config, model_config_, pd_sep_config, parallelism_config, model_specific_config,
+            resource_context_.cache_manager, metrics_reporter_));
         RTP_LLM_LOG_INFO("create fifo scheduler done");
     }
 }
@@ -74,7 +87,7 @@ NormalEngine::~NormalEngine() {
 absl::StatusOr<GenerateStreamPtr> NormalEngine::preRun(const std::shared_ptr<GenerateInput>& generate_input,
                                                        preRunMode                            mode) {
     auto stream = std::make_shared<NormalGenerateStream>(
-        generate_input, params_, resource_context_, nullptr, 0, mode == preRunMode::prefill_warm_up);
+        generate_input, model_config_, runtime_config, resource_context_, nullptr, 0, mode == preRunMode::prefill_warm_up);
     if (mode == preRunMode::decode_warm_up) {
         stream->setIsContextStream(false);
         stream->fakeInitKVBlock();
@@ -91,16 +104,16 @@ int64_t NormalEngine::getLastScheduleTime() {
 }
 
 WarmUpResult NormalEngine::warmUp(const EngineInitParams& params) {
-    if (params_.scheduler_config.use_batch_decode_scheduler) {
-        if (params_.batch_decode_scheduler_config.batch_decode_scheduler_warmup_type == 0) {
+    if (runtime_config.use_batch_decode_scheduler) {
+        if (runtime_config.batch_decode_scheduler_config.batch_decode_scheduler_warmup_type == 0) {
             return decodeWarmUp(params);
         } else {
             return prefillWarmUp(params);
         }
     }
-    if (params_.role_type_ == RoleType::PDFUSION || params_.role_type_ == RoleType::PREFILL) {
+    if (pd_sep_config.role_type == RoleType::PDFUSION || pd_sep_config.role_type == RoleType::PREFILL) {
         return prefillWarmUp(params);
-    } else if (params_.role_type_ == RoleType::DECODE) {
+    } else if (pd_sep_config.role_type == RoleType::DECODE) {
         return decodeWarmUp(params);
     } else {
         RTP_LLM_CHECK_WITH_INFO(false, "invalid role type");
@@ -117,7 +130,7 @@ std::shared_ptr<GenerateInput> NormalEngine::makeFakeInput(size_t seq_len) {
     fake_input->generate_config->top_k = 1;
     std::default_random_engine generator;
     size_t                     token_size =
-        params_.embedding_size_ ? std::min(params_.embedding_size_, params_.vocab_size_) : params_.vocab_size_;
+        model_config_.embedding_size ? std::min(model_config_.embedding_size, model_config_.vocab_size) : model_config_.vocab_size;
     std::uniform_int_distribution<int> distribution(0, token_size - 1);
     for (size_t i = 0; i < fake_input->input_ids->size(); ++i) {
         *fake_input->input_ids->dataWithOffset<int32_t>(i) = distribution(generator);
@@ -127,9 +140,9 @@ std::shared_ptr<GenerateInput> NormalEngine::makeFakeInput(size_t seq_len) {
 }
 
 WarmUpResult NormalEngine::prefillWarmUp(const EngineInitParams& params) {
-    auto fake_input                                   = makeFakeInput((size_t)params_.max_seq_len_ - 1);
-    fake_input->generate_config->num_return_sequences = params_.max_context_batch_size_;
-    fake_input->generate_config->calculate_loss       = int(params_.warm_up_with_loss_);
+    auto fake_input                                   = makeFakeInput((size_t)model_config_.max_seq_len - 1);
+        fake_input->generate_config->num_return_sequences = runtime_config.fifo_scheduler_config.max_context_batch_size;
+    fake_input->generate_config->calculate_loss       = int(runtime_config.warm_up_with_loss);
     device_->setTraceMemory(true);
     executor_.reset(new NormalExecutor(params, nullptr, device_, nullptr, true));
     THROW_IF_STATUSOR_ERROR(preRun(fake_input, preRunMode::prefill_warm_up));
@@ -141,15 +154,17 @@ WarmUpResult NormalEngine::prefillWarmUp(const EngineInitParams& params) {
 }
 
 WarmUpResult NormalEngine::decodeWarmUp(const EngineInitParams& params) {
-    auto fake_input                                   = makeFakeInput((size_t)params_.max_seq_len_ - 1);
-    fake_input->generate_config->num_return_sequences = params_.max_generate_batch_size_;
-    fake_input->generate_config->calculate_loss       = int(params_.warm_up_with_loss_);
+    auto fake_input                                   = makeFakeInput((size_t)model_config_.max_seq_len - 1);
+    fake_input->generate_config->num_return_sequences = runtime_config.max_generate_batch_size;
+    fake_input->generate_config->calculate_loss       = int(runtime_config.warm_up_with_loss);
     device_->setTraceMemory(true);
 
-    auto cache_config               = CacheConfigCreator::createBasicConfig(params_);
-    cache_config.seq_size_per_block = params_.seq_size_per_block_;
+    auto cache_config               = CacheConfigCreator::createBasicConfig(model_config_, parallelism_config);
+    cache_config.seq_size_per_block = model_config_.attn_config.tokens_per_block;
     cache_config.block_nums         = 5;
-    auto cache_manager              = make_shared<CacheManager>(cache_config, device_, true);
+    ParallelismConfig temp_parallelism_config;
+    RuntimeConfig temp_runtime_config;
+    auto cache_manager              = make_shared<CacheManager>(cache_config, device_, true, nullptr, KVCacheConfig{}, temp_parallelism_config, temp_runtime_config);
     executor_.reset(new NormalExecutor(params, cache_manager, device_, nullptr, true));
     THROW_IF_STATUSOR_ERROR(preRun(fake_input, preRunMode::decode_warm_up));
     const auto device_status = device_->getDeviceStatus();
@@ -172,23 +187,23 @@ std::shared_ptr<GenerateStream> NormalEngine::createMinFakeStream(int32_t max_ne
 }
 
 void NormalEngine::initCacheManager(std::optional<WarmUpResult> warm_up_result) {
-    auto result = CacheConfigCreator::createConfig(params_, warm_up_result);
+    auto result = CacheConfigCreator::createConfig(model_config_, parallelism_config, runtime_config, kv_cache_config, warm_up_result);
     RTP_LLM_LOG_INFO(
         "create cache manager with block nums %d, block size %ld KB", result.block_nums, result.block_size / 1024);
-    resource_context_.cache_manager = make_shared<CacheManager>(result, device_, false, metrics_reporter_, params_);
+    resource_context_.cache_manager = make_shared<CacheManager>(result, device_, false, metrics_reporter_, kv_cache_config, parallelism_config, runtime_config);
 }
 
 absl::Status NormalEngine::initSystemPrompt() {
-    resource_context_.reuse_cache               = params_.reuse_cache_;
-    resource_context_.enable_3fs                = params_.kv_cache_config.enable_3fs;
-    resource_context_.enable_memory_block_cache = params_.kv_cache_config.memory_block_cache_size_mb > 0;
+    resource_context_.reuse_cache               = kv_cache_config.reuse_cache;
+    resource_context_.enable_3fs                = kv_cache_config.enable_3fs;
+    resource_context_.enable_memory_block_cache = kv_cache_config.memory_block_cache_size_mb > 0;
 
-    if (!params_.multi_task_prompt_tokens_.empty()) {
+    if (!kv_cache_config.multi_task_prompt_tokens.empty()) {
         resource_context_.reuse_cache = true;
         CHECK_AND_RETURN_REF(
             system_prompt_param,
             SystemPromptConstructor::construct(
-                params_, this, resource_context_.cache_manager.get(), device_->getDeviceProperties().tp_rank == 0));
+                kv_cache_config, this, resource_context_.cache_manager.get(), device_->getDeviceProperties().tp_rank == 0));
         resource_context_.system_prompt.reset(new SystemPrompt(system_prompt_param));
     }
 
@@ -235,7 +250,7 @@ absl::Status NormalEngine::trySaveStepError() const {
 
 std::shared_ptr<GenerateStream> NormalEngine::makeStream(const std::shared_ptr<GenerateInput>& input) {
     std::shared_ptr<GenerateStream> stream =
-        std::make_shared<NormalGenerateStream>(input, params_, resource_context_, metrics_reporter_);
+        std::make_shared<NormalGenerateStream>(input, model_config_, runtime_config, resource_context_, metrics_reporter_);
     return stream;
 }
 
@@ -245,7 +260,7 @@ void NormalEngine::enqueue(std::shared_ptr<GenerateStream>& stream) {
 
 std::shared_ptr<GenerateStream> NormalEngine::enqueue(const std::shared_ptr<GenerateInput>& input) {
     std::shared_ptr<GenerateStream> stream =
-        std::make_shared<NormalGenerateStream>(input, params_, resource_context_, metrics_reporter_);
+        std::make_shared<NormalGenerateStream>(input, model_config_, runtime_config, resource_context_, metrics_reporter_);
     (void)scheduler_->enqueue(stream);
     return stream;
 }
@@ -255,7 +270,7 @@ NormalEngine::batchEnqueue(const std::vector<std::shared_ptr<GenerateInput>>& in
     std::vector<std::shared_ptr<GenerateStream>> streams;
     streams.reserve(inputs.size());
     for (auto& inp : inputs) {
-        auto stream = std::make_shared<NormalGenerateStream>(inp, params_, resource_context_, metrics_reporter_);
+        auto stream = std::make_shared<NormalGenerateStream>(inp, model_config_, runtime_config, resource_context_, metrics_reporter_);
         streams.push_back(stream);
     }
     (void)scheduler_->batchEnqueue(streams);
@@ -269,10 +284,10 @@ absl::Status NormalEngine::step() {
     }
 
     list<GenerateStreamPtr> streams;
-    if (device_->getDeviceProperties().tp_rank == 0 && !params_.ffn_disaggregate_config.is_ffn_service()) {
+    if (device_->getDeviceProperties().tp_rank == 0 && !ffn_disaggregate_config.is_ffn_service()) {
         CHECK_AND_ASSIGN(streams, scheduler_->schedule());
         if (streams.empty()) {
-            if (params_.dp_size_ > 1) {
+            if (parallelism_config.dp_size > 1) {
                 CHECK_AND_ASSIGN(streams, scheduler_->schedule());
                 if (streams.empty()) {
                     streams.emplace_back(createMinFakeStream(1));
@@ -316,7 +331,7 @@ absl::Status NormalEngine::step() {
                                                                stream_group.maxSeqLen(),
                                                                int(stream_group.totalContextBatchSize() > 0));
         profiler_            = std::make_shared<CudaProfiler>(profiler_prefix,
-                                                   params_.profiling_debug_logging_config.torch_cuda_profiler_dir);
+                                                   profiling_debug_logging_config.torch_cuda_profiler_dir);
         profiler_->start();
     }
     int64_t      step_begin_time_us = autil::TimeUtility::currentTimeInMicroSeconds();
@@ -339,11 +354,8 @@ absl::Status NormalEngine::step() {
     return status;
 }
 
-const rtp_llm::GptInitParameter NormalEngine::gptInitParameter() const {
-    return params_;
-}
 
-bool NormalEngine::updateEplbConfig(const EplbConfig& config) {
+bool NormalEngine::updateEplbConfig(const EPLBConfig& config) {
     if (executor_) {
         return executor_->updateEplbConfig(config);
     }
diff --git a/rtp_llm/cpp/normal_engine/NormalEngine.h b/rtp_llm/cpp/normal_engine/NormalEngine.h
index 3d87f9ecf..a6d83d2c4 100644
--- a/rtp_llm/cpp/normal_engine/NormalEngine.h
+++ b/rtp_llm/cpp/normal_engine/NormalEngine.h
@@ -38,13 +38,12 @@ public:
     absl::Status                    step();
     absl::Status                    startLoop();
     int64_t                         getLastScheduleTime() override;
-    const rtp_llm::GptInitParameter gptInitParameter() const;
     void                            reportMetrics(RtpLLMEngineMetricsCollector collector) {
         if (metrics_reporter_) {
             metrics_reporter_->report<RtpLLMEngineMetrics, RtpLLMEngineMetricsCollector>(nullptr, &collector);
         }
     }
-    bool updateEplbConfig(const EplbConfig& config) override;
+    bool updateEplbConfig(const EPLBConfig& config) override;
 
 private:
     void                            initScheduler();
@@ -63,7 +62,16 @@ private:
     autil::ThreadPtr                loop_thread_;
     std::atomic<bool>               running_{false};
     std::unique_ptr<Executor>       executor_;
-    const rtp_llm::GptInitParameter params_;
+    ModelConfig                     model_config_;
+    ParallelismConfig               parallelism_config;
+    RuntimeConfig                   runtime_config;
+    EPLBConfig                      eplb_config;
+    PDSepConfig                     pd_sep_config;
+    ProfilingDebugLoggingConfig     profiling_debug_logging_config;
+    KVCacheConfig                   kv_cache_config;
+    FfnDisAggregateConfig           ffn_disaggregate_config;
+    ModelSpecificConfig             model_specific_config;
+    SpeculativeExecutionConfig      sp_config;
     kmonitor::MetricsReporterPtr    metrics_reporter_;
     std::shared_ptr<CudaProfiler>   profiler_;
     int                             profiler_step_     = 0;
diff --git a/rtp_llm/cpp/normal_engine/NormalExecutor.cc b/rtp_llm/cpp/normal_engine/NormalExecutor.cc
index 347763132..ed0fb3d79 100644
--- a/rtp_llm/cpp/normal_engine/NormalExecutor.cc
+++ b/rtp_llm/cpp/normal_engine/NormalExecutor.cc
@@ -6,7 +6,7 @@
 #include "rtp_llm/cpp/models/PyWrappedModel.h"
 #include "rtp_llm/cpp/models/NativeDeviceGraphModel.h"
 #include "rtp_llm/cpp/models/Sampler.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 
 using namespace std;
 
@@ -21,32 +21,30 @@ NormalExecutor::NormalExecutor(const EngineInitParams&                   params,
     cache_manager_(cache_manager),
     lora_manager_(lora_manager),
     warm_up_(warm_up),
-    use_all_gather_(params.gpt_init_parameter.use_all_gather_),
+    use_all_gather_(params.moe_config.use_all_gather && !params.moe_config.use_deepep_low_latency),
     metrics_reporter_(params.metrics_reporter),
     tps_reporter_(MetricsLoopReporter<RtpLLMTokenPSMetrics, RtpLLMTokenPSMetricsCollector>(metrics_reporter_)) {
-    auto& gpt_param    = params.gpt_init_parameter;
-    enable_detail_log_ = gpt_param.profiling_debug_logging_config.enable_detail_log;
+    enable_detail_log_ = params.profiling_debug_logging_config.enable_detail_log;
     RTP_LLM_LOG_INFO("enable_detail_log_ = %d", enable_detail_log_);
 
-    if (gpt_param.enable_eplb_ && gpt_param.moe_style_ != 0) {
+    if (params.eplb_config.enable_eplb() && params.model_config_.moe_style != 0) {
         // use first moe layer weight as moe weight type
-        int  first_moe_layer = gpt_param.moe_layer_index_.front();
+        int  first_moe_layer = params.model_config_.moe_layer_index.front();
         auto moe_weight_type = params.gpt_weights.layers[first_moe_layer].ffn_weights.moe_gate_weight->kernel->type();
 
-        expert_balancer_ = make_shared<ExpertBalancer>(gpt_param.expert_num_,
-                                                       gpt_param.phy_exp_num_,
-                                                       gpt_param.num_layers_,
-                                                       gpt_param.moe_inter_padding_size_,
-                                                       gpt_param.hidden_size_,
-                                                       gpt_param.eplb_update_time_,
-                                                       gpt_param.ep_rank_,
-                                                       gpt_param.ep_size_,
-                                                       gpt_param.py_eplb_,
+        expert_balancer_ = make_shared<ExpertBalancer>(params.model_config_.expert_num,
+                                                       params.eplb_config.phy_exp_num(params.model_config_.expert_num),
+                                                       params.model_config_.num_layers,
+                                                       params.model_config_.moe_inter_padding_size,
+                                                       params.model_config_.hidden_size,
+                                                       params.parallelism_config.ep_rank,
+                                                       params.parallelism_config.ep_size,
+                                                       params.py_eplb,
                                                        moe_weight_type,
                                                        device_,
-                                                       gpt_param.eplb_mode_,
-                                                       gpt_param.quant_algo_,
-                                                       metrics_reporter_);
+                                                       params.model_config_.quant_algo,
+                                                       metrics_reporter_,
+                                                       params.eplb_config);
     }
 
     sampler_.reset(new Sampler(SamplerInitParams{device_}));
@@ -54,11 +52,11 @@ NormalExecutor::NormalExecutor(const EngineInitParams&                   params,
     GptModelInitParams model_init_params(
         {device_,
          params.gpt_weights,
-         genModelDescription(params.gpt_init_parameter),
+         genModelDescription(params.model_config_, params.parallelism_config, params.eplb_config, params.moe_config),
          cache_manager ? ((optional<KVCacheAllocator::KVCacheBuffer>)cache_manager->kvCacheBuffer()) : nullopt,
          params.model_id});
 
-    if (params.gpt_init_parameter.ffn_disaggregate_config.enable_ffn_disaggregate) {
+    if (params.ffn_disaggregate_config.enable_ffn_disaggregate) {
         RTP_LLM_LOG_INFO("using ffn as service");
         enable_ffn_disaggregate_ = true;
     }
@@ -75,9 +73,14 @@ NormalExecutor::NormalExecutor(const EngineInitParams&                   params,
 
     // when warmup, cache manager maybe nullptr
     const auto& cache_config = cache_manager ? cache_manager->cacheConfig() : CacheConfig();
-    batch_stream_processor_.reset(new NormalBatchStreamProcessor(params.gpt_init_parameter, cache_config, warm_up_));
+    batch_stream_processor_.reset(new NormalBatchStreamProcessor(
+        params.model_config_,
+        params.pd_sep_config,
+        params.profiling_debug_logging_config,
+        cache_config,
+        warm_up_));
     PrefixToCandidateTokens::instance()->reloadPrefixDictWithPrefix(
-        params.gpt_init_parameter.ckpt_path_, params.gpt_init_parameter.sp_config.tree_decode_config);
+        params.model_config_.ckpt_path, params.sp_config.tree_decode_config);
     device_->profileStart();
 }
 
@@ -179,7 +182,7 @@ void NormalExecutor::reportMetrics(const StreamGroups&             stream_groups
     }
 }
 
-bool NormalExecutor::updateEplbConfig(const EplbConfig& config) {
+bool NormalExecutor::updateEplbConfig(const EPLBConfig& config) {
     if (expert_balancer_) {
         return expert_balancer_->updateEplbConfig(config);
     }
diff --git a/rtp_llm/cpp/normal_engine/NormalExecutor.h b/rtp_llm/cpp/normal_engine/NormalExecutor.h
index de0d17879..8f03812bf 100644
--- a/rtp_llm/cpp/normal_engine/NormalExecutor.h
+++ b/rtp_llm/cpp/normal_engine/NormalExecutor.h
@@ -36,7 +36,7 @@ public:
         model_ = std::move(model);
     }
 
-    bool updateEplbConfig(const EplbConfig& config) override;
+    bool updateEplbConfig(const EPLBConfig& config) override;
 
 private:
     std::unique_ptr<GptModel>                                                model_;
diff --git a/rtp_llm/cpp/normal_engine/NormalGenerateStream.h b/rtp_llm/cpp/normal_engine/NormalGenerateStream.h
index c42944f9c..e1ac937a8 100644
--- a/rtp_llm/cpp/normal_engine/NormalGenerateStream.h
+++ b/rtp_llm/cpp/normal_engine/NormalGenerateStream.h
@@ -12,12 +12,13 @@ public:
     }
 
     NormalGenerateStream(const std::shared_ptr<GenerateInput>& query,
-                         const rtp_llm::GptInitParameter&      params,
+                         const ModelConfig&                    model_config,
+                         const RuntimeConfig&                  runtime_config,
                          const ResourceContext&                resource_context,
                          kmonitor::MetricsReporterPtr          metrics_reporter,
                          size_t                                extra_reserve_token_num = 0,
                          bool                                  perf_test               = false):
-        GenerateStream(query, params, resource_context, metrics_reporter, extra_reserve_token_num, perf_test),
+        GenerateStream(query, model_config, runtime_config, resource_context, metrics_reporter, extra_reserve_token_num, perf_test),
         request_id_(query->request_id) {
         generate_outputs_queue_.setCapacity(1000);
     }
diff --git a/rtp_llm/cpp/normal_engine/test/BUILD b/rtp_llm/cpp/normal_engine/test/BUILD
index c1ea9539d..e1d3c4e85 100644
--- a/rtp_llm/cpp/normal_engine/test/BUILD
+++ b/rtp_llm/cpp/normal_engine/test/BUILD
@@ -19,7 +19,7 @@ cc_library(
         "//rtp_llm/cpp/devices/cuda_impl:cuda_impl",
         "//rtp_llm/cpp/normal_engine:normal_engine",
         "//rtp_llm/cpp/models:models",
-        "//rtp_llm/cpp/config:gpt_init_params",
+        "//rtp_llm/cpp/config:config_modules",
         "//rtp_llm/cpp/engine_base:weights_converter",
         "@com_google_googletest//:gtest",
         "@com_google_googletest//:gtest_main",
diff --git a/rtp_llm/cpp/normal_engine/test/EngineTest.cc b/rtp_llm/cpp/normal_engine/test/EngineTest.cc
index 31d1cb4ca..e0746d945 100644
--- a/rtp_llm/cpp/normal_engine/test/EngineTest.cc
+++ b/rtp_llm/cpp/normal_engine/test/EngineTest.cc
@@ -26,8 +26,7 @@ public:
 TEST_F(NormalEngineTest, testInt8KVCache) {
     CustomConfig config;
     config.kv_cache_data_type = DataType::TYPE_INT8;
-    auto gpt_init_params      = rtp_llm::GptInitParameter();
-    auto engine               = createMockEngine(device_, config, gpt_init_params);
+    auto engine               = createMockEngine(device_, config);
 
     std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
     query->input_ids       = createBuffer<int32_t>({7}, {1, 2, 3, 4, 5, 6, 7}, rtp_llm::AllocationType::HOST);
@@ -55,8 +54,7 @@ TEST_F(NormalEngineTest, testInt8KVCache) {
 
 TEST_F(NormalEngineTest, testSimple) {
     CustomConfig config;
-    auto         gpt_init_params = rtp_llm::GptInitParameter();
-    auto         engine          = createMockEngine(device_, config, gpt_init_params);
+    auto         engine          = createMockEngine(device_, config);
 
     ASSERT_TRUE(engine->resourceContext().cache_manager);
     ASSERT_FALSE(engine->resourceContext().system_prompt);
@@ -124,8 +122,7 @@ TEST_F(NormalEngineTest, testSystemPrompt) {
     vector<int>  prompt_1           = {1, 2, 3};
     vector<int>  prompt_2           = {4, 5, 6, 7, 8, 9};
     config.multi_task_prompt_tokens = {{"1", prompt_1}, {"2", prompt_2}};
-    auto gpt_init_params            = rtp_llm::GptInitParameter();
-    auto engine                     = createMockEngine(device_, config, gpt_init_params);
+    auto engine                     = createMockEngine(device_, config);
     ASSERT_TRUE(engine->resourceContext().cache_manager);
     ASSERT_TRUE(engine->resourceContext().system_prompt);
     ASSERT_TRUE(engine->resourceContext().reuse_cache);
@@ -193,21 +190,18 @@ TEST_F(NormalEngineTest, testSystemPrompt) {
 TEST_F(NormalEngineTest, testReuseCacheOption) {
     CustomConfig config;
     config.reuse_cache   = true;
-    auto gpt_init_params = rtp_llm::GptInitParameter();
-    auto engine          = createMockEngine(device_, config, gpt_init_params);
+    auto engine          = createMockEngine(device_, config);
     ASSERT_TRUE(engine->resourceContext().reuse_cache);
 
     config.reuse_cache    = false;
-    auto gpt_init_params2 = rtp_llm::GptInitParameter();
-    auto engine2          = createMockEngine(device_, config, gpt_init_params2);
+    auto engine2          = createMockEngine(device_, config);
     ASSERT_FALSE(engine2->resourceContext().reuse_cache);
 }
 
 TEST_F(NormalEngineTest, testReuseCache) {
     CustomConfig config;
     config.reuse_cache   = true;
-    auto gpt_init_params = rtp_llm::GptInitParameter();
-    auto engine          = createMockEngine(device_, config, gpt_init_params);
+    auto engine          = createMockEngine(device_, config);
     ASSERT_TRUE(engine->resourceContext().reuse_cache);
     {
         std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
@@ -253,8 +247,7 @@ TEST_F(NormalEngineTest, testReuseCache) {
 TEST_F(NormalEngineTest, testQueryReuseCacheWhenSwitchIsOn) {
     CustomConfig config;
     config.reuse_cache   = true;
-    auto gpt_init_params = rtp_llm::GptInitParameter();
-    auto engine          = createMockEngine(device_, config, gpt_init_params);
+    auto engine          = createMockEngine(device_, config);
     ASSERT_TRUE(engine->resourceContext().reuse_cache);
 
     // First query with reuse_cache = true
@@ -329,8 +322,7 @@ TEST_F(NormalEngineTest, testQueryReuseCacheWhenSwitchIsOff) {
     // Test with engine-level reuse_cache = false (master switch off)
     CustomConfig config;
     config.reuse_cache   = false;
-    auto gpt_init_params = rtp_llm::GptInitParameter();
-    auto engine          = createMockEngine(device_, config, gpt_init_params);
+    auto engine          = createMockEngine(device_, config);
     ASSERT_FALSE(engine->resourceContext().reuse_cache);
 
     // Query with reuse_cache = true, but should be ignored because engine-level is false
diff --git a/rtp_llm/cpp/normal_engine/test/MockEngine.h b/rtp_llm/cpp/normal_engine/test/MockEngine.h
index 8a7105923..771dc560a 100644
--- a/rtp_llm/cpp/normal_engine/test/MockEngine.h
+++ b/rtp_llm/cpp/normal_engine/test/MockEngine.h
@@ -15,6 +15,7 @@
 #include "rtp_llm/cpp/core/Buffer.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
 #include "rtp_llm/cpp/models/models_weight/W.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 using namespace std;
 namespace W = rtp_llm::W;
@@ -28,27 +29,30 @@ struct CustomConfig {
 };
 
 rtp_llm::EngineInitParams
-createEngineInitParams(DeviceBase* device, const CustomConfig& config, GptInitParameter& params) {
-    params.head_num_                     = 2;
-    params.size_per_head_                = 64;
-    params.num_layers_                   = 2;
-    params.max_seq_len_                  = 20;
-    params.vocab_size_                   = 100;
-    params.hidden_size_                  = 128;
-    params.head_num_kv_                  = 2;
-    params.block_nums_                   = 100;
-    params.reuse_cache_                  = config.reuse_cache;
-    params.multi_task_prompt_tokens_     = config.multi_task_prompt_tokens;
-    params.max_generate_batch_size_      = 128;
-    params.max_context_batch_size_       = 128;
-    params.kv_cache_data_type_           = config.kv_cache_data_type;
-    params.special_tokens_.eos_token_id_ = -1;  // never eos
+createEngineInitParams(DeviceBase* device, const CustomConfig& config,
+                      rtp_llm::ModelConfig& model_config,
+                      rtp_llm::RuntimeConfig& runtime_config,
+                      rtp_llm::KVCacheConfig& kv_cache_config) {
+    model_config.attn_config.head_num = 2;
+    model_config.attn_config.size_per_head = 64;
+    model_config.num_layers                   = 2;
+    model_config.max_seq_len                  = 20;
+    model_config.vocab_size                   = 100;
+    model_config.hidden_size                  = 128;
+    model_config.attn_config.kv_head_num = 2;
+    kv_cache_config.test_block_num                   = 100;
+    kv_cache_config.reuse_cache                  = config.reuse_cache;
+    kv_cache_config.multi_task_prompt_tokens     = config.multi_task_prompt_tokens;
+    runtime_config.max_generate_batch_size      = 128;
+    runtime_config.fifo_scheduler_config.max_context_batch_size       = 128;
+    model_config.attn_config.kv_cache_dtype = config.kv_cache_data_type == DataType::TYPE_INT8 ? KvCacheDataType::INT8 : (config.kv_cache_data_type == DataType::TYPE_FP8_E4M3 ? KvCacheDataType::FP8 : KvCacheDataType::BASE);
+    model_config.special_tokens.eos_token_id = -1;  // never eos
 
     const size_t inter_size        = 512;
-    params.inter_size_             = inter_size;
-    params.inter_padding_size_     = inter_size;
-    params.seq_size_per_block_     = 2;
-    params.reserve_runtime_mem_mb_ = 1024;
+    model_config.inter_size             = inter_size;
+    model_config.inter_padding_size_     = inter_size;
+    model_config.attn_config.tokens_per_block = 2;
+    runtime_config.reserve_runtime_mem_mb = 1024;
     typedef half            T;
     const rtp_llm::DataType data_type    = getTensorType<T>();
     auto                    mem_type     = rtp_llm::MemoryType::MEMORY_GPU;
@@ -60,16 +64,16 @@ createEngineInitParams(DeviceBase* device, const CustomConfig& config, GptInitPa
     device->copy({*data, *buf_host});
 
     auto word_embeddings = make_unique<const Buffer>(
-        mem_type, data_type, vector<size_t>{(size_t)params.vocab_size_, hidden_units}, data->data(), [data](Buffer*) {
+        mem_type, data_type, vector<size_t>{(size_t)model_config.vocab_size, hidden_units}, data->data(), [data](Buffer*) {
         });
     auto lm_head = make_unique<const rtp_llm::Buffer>(
-        mem_type, data_type, vector<size_t>{(size_t)params.vocab_size_, hidden_units}, data->data());
+        mem_type, data_type, vector<size_t>{(size_t)model_config.vocab_size, hidden_units}, data->data());
     std::unordered_map<std::string, rtp_llm::ConstBufferPtr> global_weights;
     global_weights.emplace(W::embedding, std::move(word_embeddings));
     global_weights.emplace(W::lm_head, std::move(lm_head));
 
     std::vector<std::unordered_map<std::string, rtp_llm::ConstBufferPtr>> layer_weights;
-    for (int i = 0; i < params.num_layers_; ++i) {
+    for (int i = 0; i < model_config.num_layers; ++i) {
         auto pre_layernorm_weights =
             make_unique<const rtp_llm::Buffer>(mem_type, data_type, vector<size_t>{hidden_units}, data->data());
         auto pre_layernorm_beta =
@@ -124,13 +128,55 @@ createEngineInitParams(DeviceBase* device, const CustomConfig& config, GptInitPa
     auto                      convert = rtp_llm::WeightsConverter(false);
     auto                      weights = convert.createGptWeights(std::make_unique<ConstBufferPtrMaps>(layer_weights),
                                             std::make_unique<ConstBufferPtrMap>(global_weights));
-    rtp_llm::EngineInitParams rtp_llm_params(0, params, std::move(*weights));
+    
+    // Create all config objects with defaults
+    rtp_llm::MMModelConfig mm_model_config;
+    rtp_llm::ParallelismConfig parallelism_config;
+    rtp_llm::EPLBConfig eplb_config;
+    rtp_llm::PDSepConfig pd_sep_config;
+    rtp_llm::ConcurrencyConfig concurrency_config;
+    rtp_llm::FMHAConfig fmha_config;
+    rtp_llm::ProfilingDebugLoggingConfig profiling_debug_logging_config;
+    rtp_llm::HWKernelConfig hw_kernel_config;
+    rtp_llm::DeviceResourceConfig device_resource_config;
+    rtp_llm::MoeConfig moe_config;
+    rtp_llm::ModelSpecificConfig model_specific_config;
+    rtp_llm::SpeculativeExecutionConfig sp_config;
+    rtp_llm::CacheStoreConfig cache_store_config;
+    rtp_llm::MiscellaneousConfig misc_config;
+    rtp_llm::ArpcConfig arpc_config;
+    rtp_llm::FfnDisAggregateConfig ffn_disaggregate_config;
+    
+    rtp_llm::EngineInitParams rtp_llm_params(0, 
+                                             model_config,
+                                             mm_model_config,
+                                             parallelism_config,
+                                             runtime_config,
+                                             eplb_config,
+                                             pd_sep_config,
+                                             concurrency_config,
+                                             fmha_config,
+                                             kv_cache_config,
+                                             profiling_debug_logging_config,
+                                             hw_kernel_config,
+                                             device_resource_config,
+                                             moe_config,
+                                             model_specific_config,
+                                             sp_config,
+                                             cache_store_config,
+                                             misc_config,
+                                             arpc_config,
+                                             ffn_disaggregate_config,
+                                             std::move(*weights));
     return rtp_llm_params;
 }
 
 std::shared_ptr<NormalEngine>
-createMockEngine(DeviceBase* device, const CustomConfig& config, GptInitParameter& params) {
-    EngineInitParams              rtp_llm_params = createEngineInitParams(device, config, params);
+createMockEngine(DeviceBase* device, const CustomConfig& config) {
+    rtp_llm::ModelConfig model_config;
+    rtp_llm::RuntimeConfig runtime_config;
+    rtp_llm::KVCacheConfig kv_cache_config;
+    EngineInitParams              rtp_llm_params = createEngineInitParams(device, config, model_config, runtime_config, kv_cache_config);
     std::shared_ptr<NormalEngine> engine         = make_shared<NormalEngine>(rtp_llm_params);
     return engine;
 }
diff --git a/rtp_llm/cpp/normal_engine/test/NormalBatchStreamProcessorTest.cc b/rtp_llm/cpp/normal_engine/test/NormalBatchStreamProcessorTest.cc
index c1528c63d..f6ff89548 100644
--- a/rtp_llm/cpp/normal_engine/test/NormalBatchStreamProcessorTest.cc
+++ b/rtp_llm/cpp/normal_engine/test/NormalBatchStreamProcessorTest.cc
@@ -9,6 +9,7 @@
 #include "rtp_llm/cpp/core/Types.h"
 #include "rtp_llm/cpp/core/BufferHelper.h"
 #include "rtp_llm/cpp/devices/testing/TestBase.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 
 using namespace std;
 
@@ -18,16 +19,21 @@ class NormalBatchStreamProcessorTest: public DeviceTestBase {};
 
 TEST_F(NormalBatchStreamProcessorTest, testSimpleAssemble) {
     ResourceContext  resource_context;
-    GptInitParameter param;
-    param.max_seq_len_        = 2048;
-    param.vocab_size_         = 2048;
-    param.num_layers_         = 2;
-    param.kv_cache_data_type_ = DataType::TYPE_INT8;
-    NormalBatchStreamProcessor     processor(param, CacheConfig(), false);
+    ModelConfig model_config;
+    model_config.max_seq_len        = 2048;
+    model_config.vocab_size         = 2048;
+    model_config.num_layers         = 2;
+    model_config.attn_config.kv_cache_dtype = KvCacheDataType::INT8;
+    MMModelConfig mm_model_config;
+    PDSepConfig pd_sep_config;
+    ProfilingDebugLoggingConfig profiling_debug_logging_config;
+    CacheConfig cache_config;
+    RuntimeConfig runtime_config;
+    NormalBatchStreamProcessor     processor(model_config, mm_model_config, pd_sep_config, profiling_debug_logging_config, cache_config, false);
     std::shared_ptr<GenerateInput> query1 = make_shared<GenerateInput>();
     query1->input_ids                     = createBuffer<int32_t>({2}, {1, 2}, AllocationType::HOST);
     query1->generate_config               = make_shared<GenerateConfig>();
-    GenerateStreamPtr stream1             = make_shared<NormalGenerateStream>(query1, param, resource_context, nullptr);
+    GenerateStreamPtr stream1             = make_shared<NormalGenerateStream>(query1, model_config, runtime_config, resource_context, nullptr);
     query1->input_ids                     = createBuffer<int32_t>({1}, {1}, AllocationType::HOST);
     BatchKVCacheResource addr1;
     addr1.batch_block_id = {{1, 2, 3, 4}};
@@ -37,7 +43,7 @@ TEST_F(NormalBatchStreamProcessorTest, testSimpleAssemble) {
     std::shared_ptr<GenerateInput> query2 = make_shared<GenerateInput>();
     query2->input_ids                     = createBuffer<int32_t>({3}, {1, 2, 3}, AllocationType::HOST);
     query2->generate_config               = make_shared<GenerateConfig>();
-    GenerateStreamPtr stream2             = make_shared<NormalGenerateStream>(query2, param, resource_context, nullptr);
+    GenerateStreamPtr stream2             = make_shared<NormalGenerateStream>(query2, model_config, runtime_config, resource_context, nullptr);
     query2->input_ids                     = createBuffer<int32_t>({2}, {1, 2}, AllocationType::HOST);
     BatchKVCacheResource addr2;
     addr2.batch_block_id = {{5, 6, 7, 8}};
@@ -47,7 +53,7 @@ TEST_F(NormalBatchStreamProcessorTest, testSimpleAssemble) {
     std::shared_ptr<GenerateInput> query3 = make_shared<GenerateInput>();
     query3->input_ids                     = createBuffer<int32_t>({3}, {1, 2, 3}, AllocationType::HOST);
     query3->generate_config               = make_shared<GenerateConfig>();
-    GenerateStreamPtr    stream3          = make_shared<NormalGenerateStream>(query3, param, resource_context, nullptr);
+    GenerateStreamPtr    stream3          = make_shared<NormalGenerateStream>(query3, model_config, runtime_config, resource_context, nullptr);
     BatchKVCacheResource addr3;
     addr3.batch_block_id = {{9, 10}};
     stream3->setKVCache(addr3);
@@ -55,7 +61,7 @@ TEST_F(NormalBatchStreamProcessorTest, testSimpleAssemble) {
     std::shared_ptr<GenerateInput> query4 = make_shared<GenerateInput>();
     query4->input_ids                     = createBuffer<int32_t>({4}, {1, 2, 3, 4}, AllocationType::HOST);
     query4->generate_config               = make_shared<GenerateConfig>();
-    GenerateStreamPtr    stream4          = make_shared<NormalGenerateStream>(query4, param, resource_context, nullptr);
+    GenerateStreamPtr    stream4          = make_shared<NormalGenerateStream>(query4, model_config, runtime_config, resource_context, nullptr);
     BatchKVCacheResource addr4;
     addr4.batch_block_id = {{11, 12, 13, 14}};
     stream4->setKVCache(addr4);
@@ -90,7 +96,7 @@ TEST_F(NormalBatchStreamProcessorTest, testSimpleAssemble) {
         EXPECT_EQ(kv_cache_block_id, buffer2vector<int>(*model_input.kv_cache_block_id));
     }
     {
-        NormalBatchStreamProcessor processor(param, CacheConfig(), false);
+        NormalBatchStreamProcessor processor(model_config, mm_model_config, pd_sep_config, profiling_debug_logging_config, cache_config, false);
         StreamGroups               stream_groups(streams);
         auto                       merge_input_status = processor.gatherModelInput(stream_groups);
         EXPECT_TRUE(merge_input_status.ok());
@@ -101,16 +107,21 @@ TEST_F(NormalBatchStreamProcessorTest, testSimpleAssemble) {
 
 TEST_F(NormalBatchStreamProcessorTest, testSoftmaxProbs) {
     ResourceContext  resource_context;
-    GptInitParameter param;
-    param.max_seq_len_                            = 2048;
-    param.vocab_size_                             = 2;
-    param.num_layers_                             = 2;
+    ModelConfig model_config;
+    model_config.max_seq_len                            = 2048;
+    model_config.vocab_size                             = 2;
+    model_config.num_layers                             = 2;
+    MMModelConfig mm_model_config;
+    PDSepConfig pd_sep_config;
+    ProfilingDebugLoggingConfig profiling_debug_logging_config;
+    CacheConfig cache_config;
+    RuntimeConfig runtime_config;
     std::shared_ptr<GenerateInput> query1         = make_shared<GenerateInput>();
     query1->input_ids                             = createBuffer<int32_t>({1}, {1}, AllocationType::HOST);
     query1->generate_config                       = make_shared<GenerateConfig>();
     query1->generate_config->return_softmax_probs = true;
     // query1->generate_config->is_streaming   = true;
-    GenerateStreamPtr    stream1 = make_shared<NormalGenerateStream>(query1, param, resource_context, nullptr);
+    GenerateStreamPtr    stream1 = make_shared<NormalGenerateStream>(query1, model_config, runtime_config, resource_context, nullptr);
     BatchKVCacheResource addr1;
     addr1.batch_block_id = {{1}};
     stream1->setKVCache(addr1);
@@ -121,7 +132,7 @@ TEST_F(NormalBatchStreamProcessorTest, testSoftmaxProbs) {
     for (const auto& stream : streams) {
         stream->setRunning();
     }
-    NormalBatchStreamProcessor processor(param, CacheConfig(), false);
+    NormalBatchStreamProcessor processor(model_config, mm_model_config, pd_sep_config, profiling_debug_logging_config, cache_config, false);
     StreamGroups               stream_groups(streams);
     auto                       merge_input_status = processor.gatherModelInput(stream_groups);
     EXPECT_TRUE(merge_input_status.ok());
@@ -143,15 +154,20 @@ TEST_F(NormalBatchStreamProcessorTest, testSoftmaxProbs) {
 
 TEST_F(NormalBatchStreamProcessorTest, testLoss) {
     ResourceContext  resource_context;
-    GptInitParameter param;
-    param.max_seq_len_                      = 2048;
-    param.vocab_size_                       = 2048;
-    param.num_layers_                       = 2;
+    ModelConfig model_config;
+    model_config.max_seq_len                      = 2048;
+    model_config.vocab_size                       = 2048;
+    model_config.num_layers                       = 2;
+    MMModelConfig mm_model_config;
+    PDSepConfig pd_sep_config;
+    ProfilingDebugLoggingConfig profiling_debug_logging_config;
+    CacheConfig cache_config;
+    RuntimeConfig runtime_config;
     std::shared_ptr<GenerateInput> query1   = make_shared<GenerateInput>();
     query1->input_ids                       = createBuffer<int32_t>({1}, {1}, AllocationType::HOST);
     query1->generate_config                 = make_shared<GenerateConfig>();
     query1->generate_config->calculate_loss = 1;
-    GenerateStreamPtr    stream1 = make_shared<NormalGenerateStream>(query1, param, resource_context, nullptr);
+    GenerateStreamPtr    stream1 = make_shared<NormalGenerateStream>(query1, model_config, runtime_config, resource_context, nullptr);
     BatchKVCacheResource addr1;
     addr1.batch_block_id = {{1}};
     stream1->setKVCache(addr1);
@@ -160,7 +176,7 @@ TEST_F(NormalBatchStreamProcessorTest, testLoss) {
     query3->input_ids                       = createBuffer<int32_t>({2}, {0, 1}, AllocationType::HOST);
     query3->generate_config                 = make_shared<GenerateConfig>();
     query3->generate_config->calculate_loss = 2;
-    GenerateStreamPtr    stream3 = make_shared<NormalGenerateStream>(query3, param, resource_context, nullptr);
+    GenerateStreamPtr    stream3 = make_shared<NormalGenerateStream>(query3, model_config, runtime_config, resource_context, nullptr);
     BatchKVCacheResource addr3;
     addr3.batch_block_id = {{9}};
     stream3->setKVCache(addr3);
@@ -169,7 +185,7 @@ TEST_F(NormalBatchStreamProcessorTest, testLoss) {
     query4->input_ids                       = createBuffer<int32_t>({3}, {0, 1, 0}, AllocationType::HOST);
     query4->generate_config                 = make_shared<GenerateConfig>();
     query4->generate_config->calculate_loss = 1;
-    GenerateStreamPtr    stream4 = make_shared<NormalGenerateStream>(query4, param, resource_context, nullptr);
+    GenerateStreamPtr    stream4 = make_shared<NormalGenerateStream>(query4, model_config, runtime_config, resource_context, nullptr);
     BatchKVCacheResource addr4;
     addr4.batch_block_id = {{11, 12}};
     stream4->setKVCache(addr4);
@@ -182,7 +198,7 @@ TEST_F(NormalBatchStreamProcessorTest, testLoss) {
     for (const auto& stream : streams) {
         stream->setRunning();
     }
-    NormalBatchStreamProcessor processor(param, CacheConfig(), false);
+    NormalBatchStreamProcessor processor(model_config, mm_model_config, pd_sep_config, profiling_debug_logging_config, cache_config, false);
     StreamGroups               stream_groups(streams);
     auto                       merge_input_status = processor.gatherModelInput(stream_groups);
     EXPECT_TRUE(merge_input_status.ok());
@@ -211,26 +227,31 @@ TEST_F(NormalBatchStreamProcessorTest, testLoss) {
 
 TEST_F(NormalBatchStreamProcessorTest, testMultimodalGatherBatch) {
     ResourceContext  resource_context;
-    GptInitParameter param;
-    param.max_seq_len_        = 2048;
-    param.vocab_size_         = 2048;
-    param.num_layers_         = 2;
-    param.kv_cache_data_type_ = DataType::TYPE_INT8;
-    param.is_multimodal_      = true;
-    NormalBatchStreamProcessor     processor(param, CacheConfig(), false);
+    ModelConfig model_config;
+    model_config.max_seq_len        = 2048;
+    model_config.vocab_size         = 2048;
+    model_config.num_layers         = 2;
+    model_config.attn_config.kv_cache_dtype = KvCacheDataType::INT8;
+    MMModelConfig mm_model_config;
+    mm_model_config.is_multimodal_      = true;
+    PDSepConfig pd_sep_config;
+    ProfilingDebugLoggingConfig profiling_debug_logging_config;
+    CacheConfig cache_config;
+    RuntimeConfig runtime_config;
+    NormalBatchStreamProcessor     processor(model_config, mm_model_config, pd_sep_config, profiling_debug_logging_config, cache_config, false);
     std::shared_ptr<GenerateInput> query1 = make_shared<GenerateInput>();
     query1->input_ids                     = createBuffer<int32_t>({5}, {1, -1, -1, -1, 2}, AllocationType::HOST);
     query1->generate_config               = make_shared<GenerateConfig>();
     query1->mm_locs                       = createBuffer<int32_t>({1}, {1}, AllocationType::HOST);
     query1->text_tokens_mask              = createBuffer<int32_t>({5}, {1, 0, 0, 0, 1}, AllocationType::HOST);
     query1->multimodal_features           = {torch::rand({3, 10}, torch::kFloat16)};
-    GenerateStreamPtr stream1             = make_shared<NormalGenerateStream>(query1, param, resource_context, nullptr);
+    GenerateStreamPtr stream1             = make_shared<NormalGenerateStream>(query1, model_config, runtime_config, resource_context, nullptr);
     stream1->setIsContextStream(true);
 
     std::shared_ptr<GenerateInput> query2 = make_shared<GenerateInput>();
     query2->input_ids                     = createBuffer<int32_t>({3}, {3, 4, 5}, AllocationType::HOST);
     query2->generate_config               = make_shared<GenerateConfig>();
-    GenerateStreamPtr stream2             = make_shared<NormalGenerateStream>(query2, param, resource_context, nullptr);
+    GenerateStreamPtr stream2             = make_shared<NormalGenerateStream>(query2, model_config, runtime_config, resource_context, nullptr);
     stream2->setIsContextStream(true);
 
     std::shared_ptr<GenerateInput> query3 = make_shared<GenerateInput>();
@@ -239,7 +260,7 @@ TEST_F(NormalBatchStreamProcessorTest, testMultimodalGatherBatch) {
     query3->mm_locs                       = createBuffer<int32_t>({1}, {2}, AllocationType::HOST);
     query3->text_tokens_mask              = createBuffer<int32_t>({5}, {1, 1, 0, 0, 1}, AllocationType::HOST);
     query3->multimodal_features           = {torch::rand({2, 10}, torch::kFloat16)};
-    GenerateStreamPtr stream3             = make_shared<NormalGenerateStream>(query3, param, resource_context, nullptr);
+    GenerateStreamPtr stream3             = make_shared<NormalGenerateStream>(query3, model_config, runtime_config, resource_context, nullptr);
     stream3->setIsContextStream(true);
 
     std::list<GenerateStreamPtr> streams;
diff --git a/rtp_llm/cpp/pybind/BUILD b/rtp_llm/cpp/pybind/BUILD
index 138b7cf1b..d04f1518f 100644
--- a/rtp_llm/cpp/pybind/BUILD
+++ b/rtp_llm/cpp/pybind/BUILD
@@ -29,7 +29,10 @@ cc_library(
     ],
     deps = [
         ":py_utils",
-        "//rtp_llm/cpp/config:gpt_init_params",
+        "//rtp_llm/cpp/config:config_modules",
+        "//rtp_llm/cpp/config:model_config",  # Add ModelConfig implementation
+        "//rtp_llm/cpp/model_utils:model_utils",
+        "//rtp_llm/cpp/config:eplb_config",
         "//rtp_llm/cpp/utils:core_utils",
         "//rtp_llm/cpp/utils:hash_util",
     ],
diff --git a/rtp_llm/cpp/pybind/ComputeInit.cc b/rtp_llm/cpp/pybind/ComputeInit.cc
index 8fb5863f9..c276c1407 100644
--- a/rtp_llm/cpp/pybind/ComputeInit.cc
+++ b/rtp_llm/cpp/pybind/ComputeInit.cc
@@ -1,5 +1,5 @@
 #include <torch/library.h>
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 #include "rtp_llm/cpp/config/ConfigModules.h"
 #include "rtp_llm/cpp/devices/DeviceFactory.h"
 #include "rtp_llm/models_py/bindings/RegisterOps.h"
@@ -16,6 +16,7 @@ PYBIND11_MODULE(librtp_compute_ops, m) {
 
     py::module rtp_ops_m = m.def_submodule("rtp_llm_ops", "rtp llm custom ops");
     registerPyModuleOps(rtp_ops_m);
+
 }
 
 }  // namespace rtp_llm
diff --git a/rtp_llm/cpp/pybind/ConfigInit.cc b/rtp_llm/cpp/pybind/ConfigInit.cc
index 19eb5c6fc..ae043d1fd 100644
--- a/rtp_llm/cpp/pybind/ConfigInit.cc
+++ b/rtp_llm/cpp/pybind/ConfigInit.cc
@@ -1,112 +1,124 @@
-#include <torch/library.h>
+#define PYBIND11_DETAILED_ERROR_MESSAGES
+#include "rtp_llm/cpp/pybind/common/blockUtil.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/config/RoleTypes.h"
+#include "rtp_llm/cpp/config/SpecialTokens.h"
 #include "rtp_llm/cpp/model_utils/AttentionConfig.h"
 #include "rtp_llm/cpp/model_utils/MlaConfig.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
-#include "rtp_llm/cpp/config/ConfigModules.h"
-#include "rtp_llm/cpp/pybind/common/blockUtil.h"
+#include "rtp_llm/cpp/model_utils/QuantInfo.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
+#include "rtp_llm/cpp/config/EplbConfig.h"
 #include "pybind11/pybind11.h"
 #include "pybind11/cast.h"
 #include "pybind11/stl.h"
 
-namespace rtp_llm {
+namespace py = pybind11;
+using namespace rtp_llm;
+
+PYBIND11_MODULE(libth_transformer_config, m) {
+    // Register get_block_cache_keys function
+    registerCommon(m);
+
+    // Register enums
+    py::enum_<RoleType>(m, "RoleType")
+        .value("PDFUSION", RoleType::PDFUSION)
+        .value("PREFILL", RoleType::PREFILL)
+        .value("DECODE", RoleType::DECODE)
+        .value("VIT", RoleType::VIT)
+        .value("FRONTEND", RoleType::FRONTEND);
+
+    py::enum_<VitSeparation>(m, "VitSeparation")
+        .value("VIT_SEPARATION_LOCAL", VitSeparation::VIT_SEPARATION_LOCAL)
+        .value("VIT_SEPARATION_ROLE", VitSeparation::VIT_SEPARATION_ROLE)
+        .value("VIT_SEPARATION_REMOTE", VitSeparation::VIT_SEPARATION_REMOTE);
+
+    py::enum_<EplbMode>(m, "EplbMode")
+        .value("NONE", EplbMode::NONE)
+        .value("STATS", EplbMode::STATS)
+        .value("EPLB", EplbMode::EPLB)
+        .value("ALL", EplbMode::ALL);
 
-void registerFMHAType(py::module m) {
     py::enum_<FMHAType>(m, "FMHAType")
+        .value("FLASH_INFER", FMHAType::FLASH_INFER)
         .value("NONE", FMHAType::NONE)
-        .value("PAGED_TRT_V2", FMHAType::PAGED_TRT_V2)
-        .value("TRT_V2", FMHAType::TRT_V2)
-        .value("PAGED_OPEN_SOURCE", FMHAType::PAGED_OPEN_SOURCE)
         .value("OPEN_SOURCE", FMHAType::OPEN_SOURCE)
+        .value("PAGED_OPEN_SOURCE", FMHAType::PAGED_OPEN_SOURCE)
+        .value("PAGED_TRT_V2", FMHAType::PAGED_TRT_V2)
         .value("TRT_V1", FMHAType::TRT_V1)
-        .value("FLASH_INFER", FMHAType::FLASH_INFER)
+        .value("TRT_V2", FMHAType::TRT_V2)
         .value("XQA", FMHAType::XQA)
         .value("AITER_PREFILL", FMHAType::AITER_PREFILL)
         .value("AITER_DECODE", FMHAType::AITER_DECODE);
-}
-
-void register_parallelism_distributed_config(pybind11::module& m) {
-    pybind11::class_<ParallelismDistributedConfig>(m, "ParallelismDistributedConfig")
-        .def(pybind11::init<int, int, int, int, int, int, int, int>(),
-             pybind11::arg("tp_size")          = 1,
-             pybind11::arg("ep_size")          = 1,
-             pybind11::arg("dp_size")          = 1,
-             pybind11::arg("pp_size")          = 1,
-             pybind11::arg("world_size")       = 1,
-             pybind11::arg("world_rank")       = 0,
-             pybind11::arg("local_world_size") = 1,
-             pybind11::arg("ffn_sp_size")      = 1)
-        .def("to_string", &ParallelismDistributedConfig::to_string)
-        .def("update_from_env", &ParallelismDistributedConfig::update_from_env_for_test)
-        .def_readwrite("tp_size", &ParallelismDistributedConfig::tp_size)
-        .def_readwrite("ep_size", &ParallelismDistributedConfig::ep_size)
-        .def_readwrite("dp_size", &ParallelismDistributedConfig::dp_size)
-        .def_readwrite("pp_size", &ParallelismDistributedConfig::pp_size)
-        .def_readwrite("world_size", &ParallelismDistributedConfig::world_size)
-        .def_readwrite("world_rank", &ParallelismDistributedConfig::world_rank)
-        .def_readwrite("local_world_size", &ParallelismDistributedConfig::local_world_size)
-        .def_readwrite("ffn_sp_size", &ParallelismDistributedConfig::ffn_sp_size);
-}
-
-void register_arpc_config(pybind11::module& m) {
-    pybind11::class_<ArpcConfig>(m, "ArpcConfig")
-        .def(pybind11::init<int, int, int>(),
-             pybind11::arg("threadNum")   = 10,
-             pybind11::arg("queueNum")    = 50,
-             pybind11::arg("ioThreadNum") = 2)
-        .def("to_string", &ArpcConfig::to_string)
+
+    py::enum_<MlaOpsType>(m, "MlaOpsType")
+        .value("AUTO", MlaOpsType::AUTO)
+        .value("MHA", MlaOpsType::MHA)
+        .value("FLASH_INFER", MlaOpsType::FLASH_INFER)
+        .value("FLASH_MLA", MlaOpsType::FLASH_MLA);
+
+    // Register TaskType enum
+    py::enum_<TaskType>(m, "TaskType")
+        .value("DENSE_EMBEDDING", TaskType::DENSE_EMBEDDING)
+        .value("ALL_EMBEDDING", TaskType::ALL_EMBEDDING)
+        .value("SPARSE_EMBEDDING", TaskType::SPARSE_EMBEDDING)
+        .value("COLBERT_EMBEDDING", TaskType::COLBERT_EMBEDDING)
+        .value("LANGUAGE_MODEL", TaskType::LANGUAGE_MODEL)
+        .value("SEQ_CLASSIFICATION", TaskType::SEQ_CLASSIFICATION)
+        .value("RERANKER", TaskType::RERANKER)
+        .value("LINEAR_SOFTMAX", TaskType::LINEAR_SOFTMAX)
+        .value("BGE_M3", TaskType::BGE_M3);
+
+    // Register ArpcConfig
+    py::class_<ArpcConfig>(m, "ArpcConfig")
+        .def(py::init<>())
         .def_readwrite("threadNum", &ArpcConfig::threadNum)
         .def_readwrite("queueNum", &ArpcConfig::queueNum)
-        .def_readwrite("ioThreadNum", &ArpcConfig::ioThreadNum);
-}
-
-void register_ffn_disaggregate_config(pybind11::module& m) {
-    pybind11::class_<FfnDisAggregateConfig>(m, "FfnDisAggregateConfig")
-        .def(pybind11::init<bool, int, int, int, int, bool>(),
-             pybind11::arg("enable_ffn_disaggregate") = false,
-             pybind11::arg("attention_tp_size")       = 1,
-             pybind11::arg("attention_dp_size")       = 1,
-             pybind11::arg("ffn_tp_size")             = 1,
-             pybind11::arg("ffn_dp_size")             = 1,
-             pybind11::arg("is_ffn_rank")             = false)
-        .def("to_string", &FfnDisAggregateConfig::to_string)
-        .def("update_from_env", &FfnDisAggregateConfig::update_from_env_for_test)
-        .def("is_ffn_service", &FfnDisAggregateConfig::is_ffn_service)
-        .def_readwrite("enable_ffn_disaggregate", &FfnDisAggregateConfig::enable_ffn_disaggregate)
-        .def_readwrite("attention_tp_size", &FfnDisAggregateConfig::attention_tp_size)
-        .def_readwrite("attention_dp_size", &FfnDisAggregateConfig::attention_dp_size)
-        .def_readwrite("ffn_tp_size", &FfnDisAggregateConfig::ffn_tp_size)
-        .def_readwrite("ffn_dp_size", &FfnDisAggregateConfig::ffn_dp_size)
-        .def_readwrite("is_ffn_rank", &FfnDisAggregateConfig::is_ffn_rank);
-}
-
-// ConcurrencyConfig
-void register_concurrency_config(pybind11::module& m) {
-    pybind11::class_<ConcurrencyConfig>(m, "ConcurrencyConfig")
-        .def(pybind11::init<bool, int>(),
-             pybind11::arg("concurrency_with_block") = false,
-             pybind11::arg("concurrency_limit")      = 32)
-        .def("to_string", &ConcurrencyConfig::to_string)
-        .def("update_from_env", &ConcurrencyConfig::update_from_env_for_test)
+        .def_readwrite("ioThreadNum", &ArpcConfig::ioThreadNum)
+        .def("to_string", &ArpcConfig::to_string)
+        .def(py::pickle(
+            [](const ArpcConfig& self) {
+                return py::make_tuple(self.threadNum, self.queueNum, self.ioThreadNum);
+            },
+            [](py::tuple t) {
+                if (t.size() != 3) throw std::runtime_error("Invalid state!");
+                ArpcConfig c;
+                try {
+                    c.threadNum = t[0].cast<int>();
+                    c.queueNum = t[1].cast<int>();
+                    c.ioThreadNum = t[2].cast<int>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("ArpcConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register ConcurrencyConfig
+    py::class_<ConcurrencyConfig>(m, "ConcurrencyConfig")
+        .def(py::init<>())
         .def_readwrite("concurrency_with_block", &ConcurrencyConfig::concurrency_with_block)
-        .def_readwrite("concurrency_limit", &ConcurrencyConfig::concurrency_limit);
-}
-
-// FMHAConfig
-void register_fmha_config(pybind11::module& m) {
-    pybind11::class_<FMHAConfig>(m, "FMHAConfig")
-        .def(pybind11::init<bool, bool, bool, bool, bool, bool, bool, bool, bool, bool>(),
-             pybind11::arg("enable_fmha")                   = true,
-             pybind11::arg("enable_trt_fmha")               = true,
-             pybind11::arg("enable_paged_trt_fmha")         = true,
-             pybind11::arg("enable_open_source_fmha")       = true,
-             pybind11::arg("enable_paged_open_source_fmha") = true,
-             pybind11::arg("enable_trtv1_fmha")             = true,
-             pybind11::arg("fmha_perf_instrument")          = false,
-             pybind11::arg("fmha_show_params")              = false,
-             pybind11::arg("disable_flash_infer")           = false,
-             pybind11::arg("enable_xqa")                    = true)
-        .def("to_string", &FMHAConfig::to_string)
-        .def("update_from_env", &FMHAConfig::update_from_env_for_test)
+        .def_readwrite("concurrency_limit", &ConcurrencyConfig::concurrency_limit)
+        .def("to_string", &ConcurrencyConfig::to_string)
+        .def(py::pickle(
+            [](const ConcurrencyConfig& self) {
+                return py::make_tuple(self.concurrency_with_block, self.concurrency_limit);
+            },
+            [](py::tuple t) {
+                if (t.size() != 2) throw std::runtime_error("Invalid state!");
+                ConcurrencyConfig c;
+                try {
+                    c.concurrency_with_block = t[0].cast<int>();
+                    c.concurrency_limit = t[1].cast<int>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("ConcurrencyConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register FMHAConfig
+    py::class_<FMHAConfig>(m, "FMHAConfig")
+        .def(py::init<>())
         .def_readwrite("enable_fmha", &FMHAConfig::enable_fmha)
         .def_readwrite("enable_trt_fmha", &FMHAConfig::enable_trt_fmha)
         .def_readwrite("enable_paged_trt_fmha", &FMHAConfig::enable_paged_trt_fmha)
@@ -116,33 +128,45 @@ void register_fmha_config(pybind11::module& m) {
         .def_readwrite("fmha_perf_instrument", &FMHAConfig::fmha_perf_instrument)
         .def_readwrite("fmha_show_params", &FMHAConfig::fmha_show_params)
         .def_readwrite("disable_flash_infer", &FMHAConfig::disable_flash_infer)
-        .def_readwrite("enable_xqa", &FMHAConfig::enable_xqa);
-}
-
-// KVCacheConfig
-void register_kvcache_config(pybind11::module& m) {
-    pybind11::class_<KVCacheConfig>(m, "KVCacheConfig")
-        .def(pybind11::
-                 init<bool, std::string, std::string, bool, int, int, int, int, int, int, int64_t, int64_t, int, int>(),
-             pybind11::arg("reuse_cache")                        = false,
-             pybind11::arg("multi_task_prompt")                  = "",
-             pybind11::arg("multi_task_prompt_str")              = "",
-             pybind11::arg("enable_3fs")                         = false,
-             pybind11::arg("match_timeout_ms")                   = 1000,
-             pybind11::arg("rpc_get_cache_timeout_ms")           = 2000,
-             pybind11::arg("rpc_put_cache_timeout_ms")           = 2000,
-             pybind11::arg("threefs_read_timeout_ms")            = 1000,
-             pybind11::arg("threefs_write_timeout_ms")           = 2000,
-             pybind11::arg("max_block_size_per_item")            = 16,
-             pybind11::arg("threefs_read_iov_size")              = 1LL << 32,
-             pybind11::arg("threefs_write_iov_size")             = 1LL << 32,
-             pybind11::arg("memory_block_cache_size_mb")         = 0,
-             pybind11::arg("memory_block_cache_sync_timeout_ms") = 10000)
-        .def("to_string", &KVCacheConfig::to_string)
-        .def("update_from_env", &KVCacheConfig::update_from_env_for_test)
+        .def_readwrite("enable_xqa", &FMHAConfig::enable_xqa)
+        .def("to_string", &FMHAConfig::to_string)
+        .def(py::pickle(
+            [](const FMHAConfig& self) {
+                return py::make_tuple(
+                    self.enable_fmha, self.enable_trt_fmha, self.enable_paged_trt_fmha,
+                    self.enable_open_source_fmha, self.enable_paged_open_source_fmha,
+                    self.enable_trtv1_fmha, self.fmha_perf_instrument, self.fmha_show_params,
+                    self.disable_flash_infer, self.enable_xqa
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 10) throw std::runtime_error("Invalid state!");
+                FMHAConfig c;
+                try {
+                    c.enable_fmha = t[0].cast<bool>();
+                    c.enable_trt_fmha = t[1].cast<bool>();
+                    c.enable_paged_trt_fmha = t[2].cast<bool>();
+                    c.enable_open_source_fmha = t[3].cast<bool>();
+                    c.enable_paged_open_source_fmha = t[4].cast<bool>();
+                    c.enable_trtv1_fmha = t[5].cast<bool>();
+                    c.fmha_perf_instrument = t[6].cast<bool>();
+                    c.fmha_show_params = t[7].cast<bool>();
+                    c.disable_flash_infer = t[8].cast<bool>();
+                    c.enable_xqa = t[9].cast<bool>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("FMHAConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register KVCacheConfig
+    py::class_<KVCacheConfig>(m, "KVCacheConfig")
+        .def(py::init<>())
         .def_readwrite("reuse_cache", &KVCacheConfig::reuse_cache)
         .def_readwrite("multi_task_prompt", &KVCacheConfig::multi_task_prompt)
         .def_readwrite("multi_task_prompt_str", &KVCacheConfig::multi_task_prompt_str)
+        .def_readwrite("multi_task_prompt_tokens", &KVCacheConfig::multi_task_prompt_tokens)
         .def_readwrite("enable_3fs", &KVCacheConfig::enable_3fs)
         .def_readwrite("match_timeout_ms", &KVCacheConfig::match_timeout_ms)
         .def_readwrite("rpc_get_cache_timeout_ms", &KVCacheConfig::rpc_get_cache_timeout_ms)
@@ -153,52 +177,66 @@ void register_kvcache_config(pybind11::module& m) {
         .def_readwrite("threefs_read_iov_size", &KVCacheConfig::threefs_read_iov_size)
         .def_readwrite("threefs_write_iov_size", &KVCacheConfig::threefs_write_iov_size)
         .def_readwrite("memory_block_cache_size_mb", &KVCacheConfig::memory_block_cache_size_mb)
-        .def_readwrite("memory_block_cache_sync_timeout_ms", &KVCacheConfig::memory_block_cache_sync_timeout_ms);
-}
-
-// ProfilingDebugLoggingConfig
-void register_profiling_debug_logging_config(pybind11::module& m) {
-    pybind11::class_<ProfilingDebugLoggingConfig>(m, "ProfilingDebugLoggingConfig")
-        .def(pybind11::init<bool,
-                            bool,
-                            bool,
-                            bool,
-                            std::string,
-                            std::string,
-                            bool,
-                            std::string,
-                            std::string,
-                            int,
-                            std::string,
-                            bool,
-                            int,
-                            bool,
-                            bool,
-                            bool,
-                            bool,
-                            bool,
-                            bool>(),
-             pybind11::arg("trace_memory")              = false,
-             pybind11::arg("trace_malloc_stack")        = false,
-             pybind11::arg("enable_device_perf")        = false,
-             pybind11::arg("ft_core_dump_on_exception") = false,
-             pybind11::arg("ft_alog_conf_path")         = "",
-             pybind11::arg("log_level")                 = "INFO",
-             pybind11::arg("gen_timeline_sync")         = false,
-             pybind11::arg("torch_cuda_profiler_dir")   = "",
-             pybind11::arg("log_path")                  = "logs",
-             pybind11::arg("log_file_backup_count")     = 16,
-             pybind11::arg("nccl_debug_file")           = "",
-             pybind11::arg("debug_load_server")         = false,
-             pybind11::arg("hack_layer_num")            = 0,
-             pybind11::arg("debug_start_fake_process")  = false,
-             pybind11::arg("dg_print_reg_reuse")        = false,
-             pybind11::arg("qwen_agent_debug")          = false,
-             pybind11::arg("disable_dpc_random")        = false,
-             pybind11::arg("enable_detail_log")         = false,
-             pybind11::arg("check_nan")                 = false)
-        .def("to_string", &ProfilingDebugLoggingConfig::to_string)
-        .def("update_from_env", &ProfilingDebugLoggingConfig::update_from_env_for_test)
+        .def_readwrite("memory_block_cache_sync_timeout_ms", &KVCacheConfig::memory_block_cache_sync_timeout_ms)
+        .def_readwrite("int8_kv_cache", &KVCacheConfig::int8_kv_cache)
+        .def_readwrite("fp8_kv_cache", &KVCacheConfig::fp8_kv_cache)
+        .def_readwrite("kv_cache_mem_mb", &KVCacheConfig::kv_cache_mem_mb)
+        .def_readwrite("seq_size_per_block", &KVCacheConfig::seq_size_per_block)
+        .def_readwrite("test_block_num", &KVCacheConfig::test_block_num)
+        .def_readwrite("use_block_cache", &KVCacheConfig::use_block_cache)
+        .def_readwrite("blockwise_use_fp8_kv_cache", &KVCacheConfig::blockwise_use_fp8_kv_cache)
+        .def("insertMultiTaskPromptTokens", &KVCacheConfig::insertMultiTaskPromptTokens)
+        .def("to_string", &KVCacheConfig::to_string)
+        .def(py::pickle(
+            [](const KVCacheConfig& self) {
+                return py::make_tuple(
+                    self.reuse_cache, self.multi_task_prompt, self.multi_task_prompt_str,
+                    self.multi_task_prompt_tokens, self.enable_3fs, self.match_timeout_ms,
+                    self.rpc_get_cache_timeout_ms, self.rpc_put_cache_timeout_ms,
+                    self.threefs_read_timeout_ms, self.threefs_write_timeout_ms,
+                    self.max_block_size_per_item, self.threefs_read_iov_size,
+                    self.threefs_write_iov_size, self.memory_block_cache_size_mb,
+                    self.memory_block_cache_sync_timeout_ms, self.int8_kv_cache,
+                    self.fp8_kv_cache, self.kv_cache_mem_mb, self.seq_size_per_block,
+                    self.test_block_num, self.use_block_cache, self.blockwise_use_fp8_kv_cache
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 22) throw std::runtime_error("Invalid state!");
+                KVCacheConfig c;
+                try {
+                    c.reuse_cache = t[0].cast<bool>();
+                    c.multi_task_prompt = t[1].cast<std::string>();
+                    c.multi_task_prompt_str = t[2].cast<std::string>();
+                    c.multi_task_prompt_tokens = t[3].cast<std::map<std::string, std::vector<int>>>();
+                    c.enable_3fs = t[4].cast<bool>();
+                    c.match_timeout_ms = t[5].cast<int>();
+                    c.rpc_get_cache_timeout_ms = t[6].cast<int>();
+                    c.rpc_put_cache_timeout_ms = t[7].cast<int>();
+                    c.threefs_read_timeout_ms = t[8].cast<int>();
+                    c.threefs_write_timeout_ms = t[9].cast<int>();
+                    c.max_block_size_per_item = t[10].cast<int>();
+                    c.threefs_read_iov_size = t[11].cast<int64_t>();
+                    c.threefs_write_iov_size = t[12].cast<int64_t>();
+                    c.memory_block_cache_size_mb = t[13].cast<int64_t>();
+                    c.memory_block_cache_sync_timeout_ms = t[14].cast<int64_t>();
+                    c.int8_kv_cache = t[15].cast<int>();
+                    c.fp8_kv_cache = t[16].cast<int>();
+                    c.kv_cache_mem_mb = t[17].cast<int64_t>();
+                    c.seq_size_per_block = t[18].cast<int>();
+                    c.test_block_num = t[19].cast<int>();
+                    c.use_block_cache = t[20].cast<int>();
+                    c.blockwise_use_fp8_kv_cache = t[21].cast<int>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("KVCacheConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register ProfilingDebugLoggingConfig
+    py::class_<ProfilingDebugLoggingConfig>(m, "ProfilingDebugLoggingConfig")
+        .def(py::init<>())
         .def_readwrite("trace_memory", &ProfilingDebugLoggingConfig::trace_memory)
         .def_readwrite("trace_malloc_stack", &ProfilingDebugLoggingConfig::trace_malloc_stack)
         .def_readwrite("enable_device_perf", &ProfilingDebugLoggingConfig::enable_device_perf)
@@ -217,27 +255,53 @@ void register_profiling_debug_logging_config(pybind11::module& m) {
         .def_readwrite("qwen_agent_debug", &ProfilingDebugLoggingConfig::qwen_agent_debug)
         .def_readwrite("disable_dpc_random", &ProfilingDebugLoggingConfig::disable_dpc_random)
         .def_readwrite("enable_detail_log", &ProfilingDebugLoggingConfig::enable_detail_log)
-        .def_readwrite("check_nan", &ProfilingDebugLoggingConfig::check_nan);
-}
-
-void register_hwkernel_config(pybind11::module& m) {
-    pybind11::class_<HWKernelConfig>(m, "HWKernelConfig")
-        .def(pybind11::init<int, bool, bool, bool, bool, std::string, bool, bool, bool, bool, bool, bool, int>(),
-             pybind11::arg("deep_gemm_num_sm")             = -1,
-             pybind11::arg("arm_gemm_use_kai")             = false,
-             pybind11::arg("enable_stable_scatter_add")    = false,
-             pybind11::arg("enable_multi_block_mode")      = true,
-             pybind11::arg("ft_disable_custom_ar")         = true,
-             pybind11::arg("rocm_hipblaslt_config")        = "gemm_config.csv",
-             pybind11::arg("use_swizzleA")                 = false,
-             pybind11::arg("enable_cuda_graph")            = false,
-             pybind11::arg("enable_cuda_graph_debug_mode") = false,
-             pybind11::arg("use_aiter_pa")                 = true,
-             pybind11::arg("use_asm_pa")                   = true,
-             pybind11::arg("enable_native_cuda_graph")     = false,
-             pybind11::arg("num_native_cuda_graph")        = 200)
-        .def("to_string", &HWKernelConfig::to_string)
-        .def("update_from_env", &HWKernelConfig::update_from_env_for_test)
+        .def_readwrite("check_nan", &ProfilingDebugLoggingConfig::check_nan)
+        .def("to_string", &ProfilingDebugLoggingConfig::to_string)
+        .def(py::pickle(
+            [](const ProfilingDebugLoggingConfig& self) {
+                return py::make_tuple(
+                    self.trace_memory, self.trace_malloc_stack, self.enable_device_perf,
+                    self.ft_core_dump_on_exception, self.ft_alog_conf_path, self.log_level,
+                    self.gen_timeline_sync, self.torch_cuda_profiler_dir, self.log_path,
+                    self.log_file_backup_count, self.nccl_debug_file, self.debug_load_server,
+                    self.hack_layer_num, self.debug_start_fake_process, self.dg_print_reg_reuse,
+                    self.qwen_agent_debug, self.disable_dpc_random, self.enable_detail_log,
+                    self.check_nan
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 19) throw std::runtime_error("Invalid state!");
+                ProfilingDebugLoggingConfig c;
+                try {
+                    c.trace_memory = t[0].cast<bool>();
+                    c.trace_malloc_stack = t[1].cast<bool>();
+                    c.enable_device_perf = t[2].cast<bool>();
+                    c.ft_core_dump_on_exception = t[3].cast<bool>();
+                    c.ft_alog_conf_path = t[4].cast<std::string>();
+                    c.log_level = t[5].cast<std::string>();
+                    c.gen_timeline_sync = t[6].cast<bool>();
+                    c.torch_cuda_profiler_dir = t[7].cast<std::string>();
+                    c.log_path = t[8].cast<std::string>();
+                    c.log_file_backup_count = t[9].cast<int>();
+                    c.nccl_debug_file = t[10].cast<std::string>();
+                    c.debug_load_server = t[11].cast<bool>();
+                    c.hack_layer_num = t[12].cast<int>();
+                    c.debug_start_fake_process = t[13].cast<bool>();
+                    c.dg_print_reg_reuse = t[14].cast<bool>();
+                    c.qwen_agent_debug = t[15].cast<bool>();
+                    c.disable_dpc_random = t[16].cast<bool>();
+                    c.enable_detail_log = t[17].cast<bool>();
+                    c.check_nan = t[18].cast<bool>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("ProfilingDebugLoggingConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register HWKernelConfig
+    py::class_<HWKernelConfig>(m, "HWKernelConfig")
+        .def(py::init<>())
         .def_readwrite("deep_gemm_num_sm", &HWKernelConfig::deep_gemm_num_sm)
         .def_readwrite("arm_gemm_use_kai", &HWKernelConfig::arm_gemm_use_kai)
         .def_readwrite("enable_stable_scatter_add", &HWKernelConfig::enable_stable_scatter_add)
@@ -250,23 +314,45 @@ void register_hwkernel_config(pybind11::module& m) {
         .def_readwrite("use_aiter_pa", &HWKernelConfig::use_aiter_pa)
         .def_readwrite("use_asm_pa", &HWKernelConfig::use_asm_pa)
         .def_readwrite("enable_native_cuda_graph", &HWKernelConfig::enable_native_cuda_graph)
-        .def_readwrite("num_native_cuda_graph", &HWKernelConfig::num_native_cuda_graph);
-}
-
-// DeviceResourceConfig
-void register_device_resource_config(pybind11::module& m) {
-    pybind11::class_<DeviceResourceConfig>(m, "DeviceResourceConfig")
-        .def(pybind11::init<int64_t, int64_t, int, int, int, bool, int, bool>(),
-             pybind11::arg("device_reserve_memory_bytes") = 0,
-             pybind11::arg("host_reserve_memory_bytes")   = 4LL * 1024 * 1024 * 1024,
-             pybind11::arg("overlap_math_sm_count")       = 0,
-             pybind11::arg("overlap_comm_type")           = 0,
-             pybind11::arg("m_split")                     = 0,
-             pybind11::arg("enable_comm_overlap")         = true,
-             pybind11::arg("enable_layer_micro_batch")    = 0,
-             pybind11::arg("not_use_default_stream")      = false)
-        .def("to_string", &DeviceResourceConfig::to_string)
-        .def("update_from_env", &DeviceResourceConfig::update_from_env_for_test)
+        .def_readwrite("num_native_cuda_graph", &HWKernelConfig::num_native_cuda_graph)
+        .def("to_string", &HWKernelConfig::to_string)
+        .def(py::pickle(
+            [](const HWKernelConfig& self) {
+                return py::make_tuple(
+                    self.deep_gemm_num_sm, self.arm_gemm_use_kai, self.enable_stable_scatter_add,
+                    self.enable_multi_block_mode, self.ft_disable_custom_ar, self.rocm_hipblaslt_config,
+                    self.use_swizzleA, self.enable_cuda_graph, self.enable_cuda_graph_debug_mode,
+                    self.use_aiter_pa, self.use_asm_pa, self.enable_native_cuda_graph,
+                    self.num_native_cuda_graph
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 13) throw std::runtime_error("Invalid state!");
+                HWKernelConfig c;
+                try {
+                    c.deep_gemm_num_sm = t[0].cast<int>();
+                    c.arm_gemm_use_kai = t[1].cast<bool>();
+                    c.enable_stable_scatter_add = t[2].cast<bool>();
+                    c.enable_multi_block_mode = t[3].cast<bool>();
+                    c.ft_disable_custom_ar = t[4].cast<bool>();
+                    c.rocm_hipblaslt_config = t[5].cast<std::string>();
+                    c.use_swizzleA = t[6].cast<bool>();
+                    c.enable_cuda_graph = t[7].cast<bool>();
+                    c.enable_cuda_graph_debug_mode = t[8].cast<bool>();
+                    c.use_aiter_pa = t[9].cast<bool>();
+                    c.use_asm_pa = t[10].cast<bool>();
+                    c.enable_native_cuda_graph = t[11].cast<bool>();
+                    c.num_native_cuda_graph = t[12].cast<int>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("HWKernelConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register DeviceResourceConfig
+    py::class_<DeviceResourceConfig>(m, "DeviceResourceConfig")
+        .def(py::init<>())
         .def_readwrite("device_reserve_memory_bytes", &DeviceResourceConfig::device_reserve_memory_bytes)
         .def_readwrite("host_reserve_memory_bytes", &DeviceResourceConfig::host_reserve_memory_bytes)
         .def_readwrite("overlap_math_sm_count", &DeviceResourceConfig::overlap_math_sm_count)
@@ -274,65 +360,100 @@ void register_device_resource_config(pybind11::module& m) {
         .def_readwrite("m_split", &DeviceResourceConfig::m_split)
         .def_readwrite("enable_comm_overlap", &DeviceResourceConfig::enable_comm_overlap)
         .def_readwrite("enable_layer_micro_batch", &DeviceResourceConfig::enable_layer_micro_batch)
-        .def_readwrite("not_use_default_stream", &DeviceResourceConfig::not_use_default_stream);
-}
-
-// MoeConfig
-void register_moe_config(pybind11::module& m) {
-    pybind11::class_<MoeConfig>(m, "MoeConfig")
-        .def(pybind11::init<bool, bool, bool, bool, bool, int, bool, bool, int, int, int>(),
-             pybind11::arg("use_deepep_moe")                  = false,
-             pybind11::arg("use_deepep_internode")            = false,
-             pybind11::arg("use_deepep_low_latency")          = true,
-             pybind11::arg("use_deepep_p2p_low_latency")      = false,
-             pybind11::arg("fake_balance_expert")             = false,
-             pybind11::arg("eplb_control_step")               = 100,
-             pybind11::arg("eplb_test_mode")                  = false,
-             pybind11::arg("hack_moe_expert")                 = false,
-             pybind11::arg("eplb_balance_layer_per_step")     = 1,
-             pybind11::arg("deep_ep_num_sm")                  = 0,
-             pybind11::arg("max_moe_normal_masked_token_num") = 1024)
-        .def("to_string", &MoeConfig::to_string)
-        .def("update_from_env", &MoeConfig::update_from_env_for_test)
+        .def("to_string", &DeviceResourceConfig::to_string)
+        .def(py::pickle(
+            [](const DeviceResourceConfig& self) {
+                return py::make_tuple(
+                    self.device_reserve_memory_bytes, self.host_reserve_memory_bytes,
+                    self.overlap_math_sm_count, self.overlap_comm_type, self.m_split,
+                    self.enable_comm_overlap, self.enable_layer_micro_batch
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 7) throw std::runtime_error("Invalid state!");
+                DeviceResourceConfig c;
+                try {
+                    c.device_reserve_memory_bytes = t[0].cast<int64_t>();
+                    c.host_reserve_memory_bytes = t[1].cast<int64_t>();
+                    c.overlap_math_sm_count = t[2].cast<int>();
+                    c.overlap_comm_type = t[3].cast<int>();
+                    c.m_split = t[4].cast<int>();
+                    c.enable_comm_overlap = t[5].cast<bool>();
+                    c.enable_layer_micro_batch = t[6].cast<int>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("DeviceResourceConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register MoeConfig
+    py::class_<MoeConfig>(m, "MoeConfig")
+        .def(py::init<>())
         .def_readwrite("use_deepep_moe", &MoeConfig::use_deepep_moe)
         .def_readwrite("use_deepep_internode", &MoeConfig::use_deepep_internode)
         .def_readwrite("use_deepep_low_latency", &MoeConfig::use_deepep_low_latency)
         .def_readwrite("use_deepep_p2p_low_latency", &MoeConfig::use_deepep_p2p_low_latency)
         .def_readwrite("fake_balance_expert", &MoeConfig::fake_balance_expert)
-        .def_readwrite("eplb_control_step", &MoeConfig::eplb_control_step)
-        .def_readwrite("eplb_test_mode", &MoeConfig::eplb_test_mode)
         .def_readwrite("hack_moe_expert", &MoeConfig::hack_moe_expert)
-        .def_readwrite("eplb_balance_layer_per_step", &MoeConfig::eplb_balance_layer_per_step)
         .def_readwrite("deep_ep_num_sm", &MoeConfig::deep_ep_num_sm)
-        .def_readwrite("max_moe_normal_masked_token_num", &MoeConfig::max_moe_normal_masked_token_num);
-}
-
-// ModelSpecificConfig
-void register_model_specific_config(pybind11::module& m) {
-    pybind11::class_<ModelSpecificConfig>(m, "ModelSpecificConfig")
-        .def(pybind11::init<int64_t, bool>(),
-             pybind11::arg("max_lora_model_size") = -1,
-             pybind11::arg("load_python_model")   = false)
-        .def("to_string", &ModelSpecificConfig::to_string)
-        .def("update_from_env", &ModelSpecificConfig::update_from_env_for_test)
+        .def_readwrite("max_moe_normal_masked_token_num", &MoeConfig::max_moe_normal_masked_token_num)
+        .def_readwrite("use_all_gather", &MoeConfig::use_all_gather)
+        .def("to_string", &MoeConfig::to_string)
+        .def(py::pickle(
+            [](const MoeConfig& self) {
+                return py::make_tuple(
+                    self.use_deepep_moe, self.use_deepep_internode, self.use_deepep_low_latency,
+                    self.use_deepep_p2p_low_latency, self.fake_balance_expert, self.hack_moe_expert,
+                    self.deep_ep_num_sm, self.max_moe_normal_masked_token_num, self.use_all_gather
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 9) throw std::runtime_error("Invalid state!");
+                MoeConfig c;
+                try {
+                    c.use_deepep_moe = t[0].cast<bool>();
+                    c.use_deepep_internode = t[1].cast<bool>();
+                    c.use_deepep_low_latency = t[2].cast<bool>();
+                    c.use_deepep_p2p_low_latency = t[3].cast<bool>();
+                    c.fake_balance_expert = t[4].cast<bool>();
+                    c.hack_moe_expert = t[5].cast<bool>();
+                    c.deep_ep_num_sm = t[6].cast<int>();
+                    c.max_moe_normal_masked_token_num = t[7].cast<int>();
+                    c.use_all_gather = t[8].cast<bool>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("MoeConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register ModelSpecificConfig
+    py::class_<ModelSpecificConfig>(m, "ModelSpecificConfig")
+        .def(py::init<>())
         .def_readwrite("max_lora_model_size", &ModelSpecificConfig::max_lora_model_size)
-        .def_readwrite("load_python_model", &ModelSpecificConfig::load_python_model);
-}
-
-// SpeculativeExecutionConfig
-void register_speculative_execution_config(pybind11::module& m) {
-    pybind11::class_<SpeculativeExecutionConfig>(m, "SpeculativeExecutionConfig")
-        .def(pybind11::init<std::string, std::string, int64_t, int64_t, std::string, int, bool, bool>(),
-             pybind11::arg("sp_model_type")                 = "",
-             pybind11::arg("sp_type")                       = "",
-             pybind11::arg("sp_min_token_match")            = 2,
-             pybind11::arg("sp_max_token_match")            = 2,
-             pybind11::arg("tree_decode_config")            = "",
-             pybind11::arg("gen_num_per_cycle")             = 1,
-             pybind11::arg("force_stream_sample")           = false,
-             pybind11::arg("force_score_context_attention") = true)
-        .def("to_string", &SpeculativeExecutionConfig::to_string)
-        .def("update_from_env", &SpeculativeExecutionConfig::update_from_env_for_test)
+        .def_readwrite("load_python_model", &ModelSpecificConfig::load_python_model)
+        .def("to_string", &ModelSpecificConfig::to_string)
+        .def(py::pickle(
+            [](const ModelSpecificConfig& self) {
+                return py::make_tuple(self.max_lora_model_size, self.load_python_model);
+            },
+            [](py::tuple t) {
+                if (t.size() != 2) throw std::runtime_error("Invalid state!");
+                ModelSpecificConfig c;
+                try {
+                    c.max_lora_model_size = t[0].cast<int64_t>();
+                    c.load_python_model = t[1].cast<bool>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("ModelSpecificConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register SpeculativeExecutionConfig
+    py::class_<SpeculativeExecutionConfig>(m, "SpeculativeExecutionConfig")
+        .def(py::init<>())
         .def_readwrite("sp_model_type", &SpeculativeExecutionConfig::sp_model_type)
         .def_readwrite("sp_type", &SpeculativeExecutionConfig::sp_type)
         .def_readwrite("sp_min_token_match", &SpeculativeExecutionConfig::sp_min_token_match)
@@ -340,40 +461,43 @@ void register_speculative_execution_config(pybind11::module& m) {
         .def_readwrite("tree_decode_config", &SpeculativeExecutionConfig::tree_decode_config)
         .def_readwrite("gen_num_per_cycle", &SpeculativeExecutionConfig::gen_num_per_cycle)
         .def_readwrite("force_stream_sample", &SpeculativeExecutionConfig::force_stream_sample)
-        .def_readwrite("force_score_context_attention", &SpeculativeExecutionConfig::force_score_context_attention);
-}
-
-// ServiceDiscoveryConfig
-void register_service_discovery_config(pybind11::module& m) {
-    pybind11::class_<ServiceDiscoveryConfig>(m, "ServiceDiscoveryConfig")
-        .def(pybind11::init<bool, std::string, std::string, std::string, std::string>(),
-             pybind11::arg("use_local")                  = false,
-             pybind11::arg("remote_rpc_server_ip")       = "",
-             pybind11::arg("decode_cm2_config")          = "",
-             pybind11::arg("remote_vit_server_ip")       = "",
-             pybind11::arg("multimodal_part_cm2_config") = "")
-        .def("to_string", &ServiceDiscoveryConfig::to_string)
-        .def("update_from_env", &ServiceDiscoveryConfig::update_from_env_for_test)
-        .def_readwrite("use_local", &ServiceDiscoveryConfig::use_local)
-        .def_readwrite("remote_rpc_server_ip", &ServiceDiscoveryConfig::remote_rpc_server_ip)
-        .def_readwrite("decode_cm2_config", &ServiceDiscoveryConfig::decode_cm2_config)
-        .def_readwrite("remote_vit_server_ip", &ServiceDiscoveryConfig::remote_vit_server_ip)
-        .def_readwrite("multimodal_part_cm2_config", &ServiceDiscoveryConfig::multimodal_part_cm2_config);
-}
-
-void register_cache_store_config(pybind11::module& m) {
-    pybind11::class_<CacheStoreConfig>(m, "CacheStoreConfig")
-        .def(pybind11::init<bool, int, int, int, int, int, int, int>(),
-             pybind11::arg("cache_store_rdma_mode")        = false,
-             pybind11::arg("wrr_available_ratio")          = 80,
-             pybind11::arg("rank_factor")                  = 0,
-             pybind11::arg("thread_count")                 = 16,
-             pybind11::arg("rdma_connect_timeout_ms")      = 250,
-             pybind11::arg("rdma_qp_count_per_connection") = 2,
-             pybind11::arg("messager_io_thread_count")     = 2,
-             pybind11::arg("messager_worker_thread_count") = 16)
-        .def("to_string", &CacheStoreConfig::to_string)
-        .def("update_from_env", &CacheStoreConfig::update_from_env_for_test)
+        .def_readwrite("force_score_context_attention", &SpeculativeExecutionConfig::force_score_context_attention)
+        .def_readwrite("sp_quantization", &SpeculativeExecutionConfig::sp_quantization)
+        .def_readwrite("sp_checkpoint_path", &SpeculativeExecutionConfig::sp_checkpoint_path)
+        .def("to_string", &SpeculativeExecutionConfig::to_string)
+        .def(py::pickle(
+            [](const SpeculativeExecutionConfig& self) {
+                return py::make_tuple(
+                    self.sp_model_type, self.sp_type, self.sp_min_token_match,
+                    self.sp_max_token_match, self.tree_decode_config, self.gen_num_per_cycle,
+                    self.force_stream_sample, self.force_score_context_attention,
+                    self.sp_quantization, self.sp_checkpoint_path
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 10) throw std::runtime_error("Invalid state!");
+                SpeculativeExecutionConfig c;
+                try {
+                    c.sp_model_type = t[0].cast<std::string>();
+                    c.sp_type = t[1].cast<std::string>();
+                    c.sp_min_token_match = t[2].cast<int64_t>();
+                    c.sp_max_token_match = t[3].cast<int64_t>();
+                    c.tree_decode_config = t[4].cast<std::string>();
+                    c.gen_num_per_cycle = t[5].cast<int64_t>();
+                    c.force_stream_sample = t[6].cast<bool>();
+                    c.force_score_context_attention = t[7].cast<bool>();
+                    c.sp_quantization = t[8].cast<std::string>();
+                    c.sp_checkpoint_path = t[9].cast<std::string>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("SpeculativeExecutionConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register CacheStoreConfig
+    py::class_<CacheStoreConfig>(m, "CacheStoreConfig")
+        .def(py::init<>())
         .def_readwrite("cache_store_rdma_mode", &CacheStoreConfig::cache_store_rdma_mode)
         .def_readwrite("wrr_available_ratio", &CacheStoreConfig::wrr_available_ratio)
         .def_readwrite("rank_factor", &CacheStoreConfig::rank_factor)
@@ -381,392 +505,626 @@ void register_cache_store_config(pybind11::module& m) {
         .def_readwrite("rdma_connect_timeout_ms", &CacheStoreConfig::rdma_connect_timeout_ms)
         .def_readwrite("rdma_qp_count_per_connection", &CacheStoreConfig::rdma_qp_count_per_connection)
         .def_readwrite("messager_io_thread_count", &CacheStoreConfig::messager_io_thread_count)
-        .def_readwrite("messager_worker_thread_count", &CacheStoreConfig::messager_worker_thread_count);
-}
-
-// SchedulerConfig
-void register_scheduler_config(pybind11::module& m) {
-    pybind11::class_<SchedulerConfig>(m, "SchedulerConfig")
-        .def(pybind11::init<bool, bool>(),
-             pybind11::arg("use_batch_decode_scheduler") = false,
-             pybind11::arg("use_gather_batch_scheduler") = false)
-        .def("to_string", &SchedulerConfig::to_string)
-        .def("update_from_env", &SchedulerConfig::update_from_env_for_test)
-        .def_readwrite("use_batch_decode_scheduler", &SchedulerConfig::use_batch_decode_scheduler)
-        .def_readwrite("use_gather_batch_scheduler", &SchedulerConfig::use_gather_batch_scheduler);
-}
-
-// BatchDecodeSchedulerConfig
-void register_batch_decode_scheduler_config(pybind11::module& m) {
-    pybind11::class_<BatchDecodeSchedulerConfig>(m, "BatchDecodeSchedulerConfig")
-        .def(pybind11::init<int64_t, int64_t>(),
-             pybind11::arg("batch_decode_scheduler_batch_size")  = 1,
-             pybind11::arg("batch_decode_scheduler_warmup_type") = 0)
-        .def("to_string", &BatchDecodeSchedulerConfig::to_string)
-        .def("update_from_env", &BatchDecodeSchedulerConfig::update_from_env_for_test)
-        .def_readwrite("batch_decode_scheduler_batch_size",
-                       &BatchDecodeSchedulerConfig::batch_decode_scheduler_batch_size);
-}
-
-// FIFOSchedulerConfig
-void register_fifo_scheduler_config(pybind11::module& m) {
-    pybind11::class_<FIFOSchedulerConfig>(m, "FIFOSchedulerConfig")
-        .def(pybind11::init<int64_t, int, bool, bool, int64_t>(),
-             pybind11::arg("max_context_batch_size")           = 1,
-             pybind11::arg("scheduler_reserve_resource_ratio") = 5,
-             pybind11::arg("enable_fast_gen")                  = false,
-             pybind11::arg("enable_partial_fallback")          = false,
-             pybind11::arg("fast_gen_context_budget")          = -1)
-        .def("to_string", &FIFOSchedulerConfig::to_string)
-        .def("update_from_env", &FIFOSchedulerConfig::update_from_env_for_test)
-        .def_readwrite("max_context_batch_size", &FIFOSchedulerConfig::max_context_batch_size)
-        .def_readwrite("scheduler_reserve_resource_ratio", &FIFOSchedulerConfig::scheduler_reserve_resource_ratio)
-        .def_readwrite("enable_fast_gen", &FIFOSchedulerConfig::enable_fast_gen)
-        .def_readwrite("enable_partial_fallback", &FIFOSchedulerConfig::enable_partial_fallback)
-        .def_readwrite("fast_gen_context_budget", &FIFOSchedulerConfig::fast_gen_context_budget);
-}
+        .def_readwrite("messager_worker_thread_count", &CacheStoreConfig::messager_worker_thread_count)
+        .def("to_string", &CacheStoreConfig::to_string)
+        .def(py::pickle(
+            [](const CacheStoreConfig& self) {
+                return py::make_tuple(
+                    self.cache_store_rdma_mode, self.wrr_available_ratio, self.rank_factor,
+                    self.thread_count, self.rdma_connect_timeout_ms, self.rdma_qp_count_per_connection,
+                    self.messager_io_thread_count, self.messager_worker_thread_count
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 8) throw std::runtime_error("Invalid state!");
+                CacheStoreConfig c;
+                try {
+                    c.cache_store_rdma_mode = t[0].cast<bool>();
+                    c.wrr_available_ratio = t[1].cast<int>();
+                    c.rank_factor = t[2].cast<int>();
+                    c.thread_count = t[3].cast<int>();
+                    c.rdma_connect_timeout_ms = t[4].cast<int>();
+                    c.rdma_qp_count_per_connection = t[5].cast<int>();
+                    c.messager_io_thread_count = t[6].cast<int>();
+                    c.messager_worker_thread_count = t[7].cast<int>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("CacheStoreConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
 
-// MiscellaneousConfig
-void register_misc_config(pybind11::module& m) {
-    pybind11::class_<MiscellaneousConfig>(m, "MiscellaneousConfig")
-        .def(pybind11::init<bool, std::string>(), pybind11::arg("disable_pdl") = true, pybind11::arg("aux_string") = "")
-        .def("to_string", &MiscellaneousConfig::to_string)
-        .def("update_from_env", &MiscellaneousConfig::update_from_env_for_test)
+    // Register MiscellaneousConfig
+    py::class_<MiscellaneousConfig>(m, "MiscellaneousConfig")
+        .def(py::init<>())
         .def_readwrite("disable_pdl", &MiscellaneousConfig::disable_pdl)
-        .def_readwrite("aux_string", &MiscellaneousConfig::aux_string);
-}
-
-void registerGptInitParameter(py::module m) {
-    py::enum_<MlaOpsType>(m, "MlaOpsType")
-        .value("AUTO", MlaOpsType::AUTO)
-        .value("MHA", MlaOpsType::MHA)
-        .value("FLASH_INFER", MlaOpsType::FLASH_INFER)
-        .value("FLASH_MLA", MlaOpsType::FLASH_MLA);
-
-    py::enum_<EplbMode>(m, "EplbMode")
-        .value("NONE", EplbMode::NONE)
-        .value("STATS", EplbMode::STATS)
-        .value("EPLB", EplbMode::EPLB)
-        .value("ALL", EplbMode::ALL);
-
-    pybind11::class_<EplbConfig>(m, "EplbConfig")
-        .def(pybind11::init<>())
-        .def_readwrite("mode", &EplbConfig::mode)
-        .def_readwrite("update_time", &EplbConfig::update_time)
-        .def("__eq__", &EplbConfig::operator==)
-        .def("__ne__", &EplbConfig::operator!=)
-        .def("__str__", &EplbConfig::toString);
-
-#define DEF_PROPERTY(name) .def_readwrite(#name, &RoleSpecialTokens::name##_)
-
-#define REGISTER_PROPERTYS                                                                                             \
-    DEF_PROPERTY(token_ids)                                                                                            \
-    DEF_PROPERTY(eos_token_ids)
-
-    pybind11::class_<RoleSpecialTokens>(m, "RoleSpecialTokens").def(pybind11::init<>()) REGISTER_PROPERTYS;
-
-#undef DEF_PROPERTY
-#undef REGISTER_PROPERTYS
-
-#define DEF_PROPERTY(name) .def_readwrite(#name, &SpecialTokens::name##_)
+        .def_readwrite("aux_string", &MiscellaneousConfig::aux_string)
+        .def("to_string", &MiscellaneousConfig::to_string)
+        .def(py::pickle(
+            [](const MiscellaneousConfig& self) {
+                return py::make_tuple(self.disable_pdl, self.aux_string);
+            },
+            [](py::tuple t) {
+                if (t.size() != 2) throw std::runtime_error("Invalid state!");
+                MiscellaneousConfig c;
+                try {
+                    c.disable_pdl = t[0].cast<bool>();
+                    c.aux_string = t[1].cast<std::string>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("MiscellaneousConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
 
-#define REGISTER_PROPERTYS                                                                                             \
-    DEF_PROPERTY(bos_token_id)                                                                                         \
-    DEF_PROPERTY(eos_token_id)                                                                                         \
-    DEF_PROPERTY(decoder_start_token_id)                                                                               \
-    DEF_PROPERTY(user)                                                                                                 \
-    DEF_PROPERTY(assistant)                                                                                            \
-    DEF_PROPERTY(system)                                                                                               \
-    DEF_PROPERTY(stop_words_id_list)                                                                                   \
-    DEF_PROPERTY(stop_words_str_list)                                                                                  \
-    DEF_PROPERTY(pad_token_id)
+    // Register FfnDisAggregateConfig
+    py::class_<FfnDisAggregateConfig>(m, "FfnDisAggregateConfig")
+        .def(py::init<>())
+        .def_readwrite("enable_ffn_disaggregate", &FfnDisAggregateConfig::enable_ffn_disaggregate)
+        .def_readwrite("attention_tp_size", &FfnDisAggregateConfig::attention_tp_size)
+        .def_readwrite("attention_dp_size", &FfnDisAggregateConfig::attention_dp_size)
+        .def_readwrite("ffn_tp_size", &FfnDisAggregateConfig::ffn_tp_size)
+        .def_readwrite("ffn_dp_size", &FfnDisAggregateConfig::ffn_dp_size)
+        .def_readwrite("is_ffn_rank", &FfnDisAggregateConfig::is_ffn_rank)
+        .def("to_string", &FfnDisAggregateConfig::to_string)
+        .def("is_ffn_service", &FfnDisAggregateConfig::is_ffn_service)
+        .def(py::pickle(
+            [](const FfnDisAggregateConfig& self) {
+                return py::make_tuple(
+                    self.enable_ffn_disaggregate, self.attention_tp_size, self.attention_dp_size,
+                    self.ffn_tp_size, self.ffn_dp_size, self.is_ffn_rank
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 6) throw std::runtime_error("Invalid state!");
+                FfnDisAggregateConfig c;
+                try {
+                    c.enable_ffn_disaggregate = t[0].cast<bool>();
+                    c.attention_tp_size = t[1].cast<int>();
+                    c.attention_dp_size = t[2].cast<int>();
+                    c.ffn_tp_size = t[3].cast<int>();
+                    c.ffn_dp_size = t[4].cast<int>();
+                    c.is_ffn_rank = t[5].cast<bool>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("FfnDisAggregateConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
 
-    pybind11::class_<SpecialTokens>(m, "SpecialTokens").def(pybind11::init<>()) REGISTER_PROPERTYS;
+    // Register SpecialTokens
+    py::class_<RoleSpecialTokens>(m, "RoleSpecialTokens")
+        .def(py::init<>())
+        .def_readwrite("token_ids", &RoleSpecialTokens::token_ids)
+        .def_readwrite("eos_token_ids", &RoleSpecialTokens::eos_token_ids);
 
-#undef DEF_PROPERTY
-#undef REGISTER_PROPERTYS
+    py::class_<SpecialTokens>(m, "SpecialTokens")
+        .def(py::init<>())
+        .def_readwrite("bos_token_id", &SpecialTokens::bos_token_id)
+        .def_readwrite("eos_token_id", &SpecialTokens::eos_token_id)
+        .def_readwrite("pad_token_id", &SpecialTokens::pad_token_id)
+        .def_readwrite("decoder_start_token_id", &SpecialTokens::decoder_start_token_id)
+        .def_readwrite("user", &SpecialTokens::user)
+        .def_readwrite("assistant", &SpecialTokens::assistant)
+        .def_readwrite("system", &SpecialTokens::system)
+        .def_readwrite("stop_words_id_list", &SpecialTokens::stop_words_id_list)
+        .def_readwrite("stop_words_str_list", &SpecialTokens::stop_words_str_list);
 
-    pybind11::class_<QuantAlgo>(m, "QuantAlgo")
-        .def(pybind11::init<>())  // quant_pre_scales
-        .def("setQuantAlgo", &QuantAlgo::setQuantAlgo, py::arg("quant_method"), py::arg("bits"), py::arg("group_size"))
+    // Register QuantAlgo
+    py::class_<QuantAlgo>(m, "QuantAlgo")
+        .def(py::init<>())
+        .def(py::init<QuantMethod, int, int>(), py::arg("method"), py::arg("bits"), py::arg("group_size"))
         .def("isWeightOnlyPerCol", &QuantAlgo::isWeightOnlyPerCol)
+        .def("isPerTensorQuant", &QuantAlgo::isPerTensorQuant)
         .def("isGptq", &QuantAlgo::isGptq)
         .def("isAwq", &QuantAlgo::isAwq)
         .def("isSmoothQuant", &QuantAlgo::isSmoothQuant)
         .def("isOmniQuant", &QuantAlgo::isOmniQuant)
-        .def("isPerTensorQuant", &QuantAlgo::isPerTensorQuant)
         .def("isFp8", &QuantAlgo::isFp8)
         .def("isFp8PTPC", &QuantAlgo::isFp8PTPC)
         .def("isQuant", &QuantAlgo::isQuant)
         .def("isGroupwise", &QuantAlgo::isGroupwise)
+        .def("getQuantMethod", &QuantAlgo::getQuantMethod)
         .def("getGroupSize", &QuantAlgo::getGroupSize)
         .def("getWeightBits", &QuantAlgo::getWeightBits)
         .def("getActivationBits", &QuantAlgo::getActivationBits)
+        .def("setQuantAlgo", &QuantAlgo::setQuantAlgo);
+
+    // Register QuantMethod enum
+    py::enum_<QuantMethod>(m, "QuantMethod")
+        .value("None", QuantMethod::None)
+        .value("WeightOnlyPerCol", QuantMethod::WeightOnlyPerCol)
+        .value("GptQ", QuantMethod::GptQ)
+        .value("Awq", QuantMethod::Awq)
+        .value("SmoothQuant", QuantMethod::SmoothQuant)
+        .value("OmniQuant", QuantMethod::OmniQuant)
+        .value("PerTensorQuant", QuantMethod::PerTensorQuant)
+        .value("FP8Quant", QuantMethod::FP8Quant)
+        .value("FP8PTPC", QuantMethod::FP8PTPC);
+
+    // Register ParallelismConfig
+    py::class_<ParallelismConfig>(m, "ParallelismConfig")
+        .def(py::init<>())
+        .def_readwrite("tp_size", &ParallelismConfig::tp_size)
+        .def_readwrite("ep_size", &ParallelismConfig::ep_size)
+        .def_readwrite("dp_size", &ParallelismConfig::dp_size)
+        .def_readwrite("pp_size", &ParallelismConfig::pp_size)
+        .def_readwrite("world_size", &ParallelismConfig::world_size)
+        .def_readwrite("world_rank", &ParallelismConfig::world_rank)
+        .def_readwrite("local_world_size", &ParallelismConfig::local_world_size)
+        .def_readwrite("ffn_sp_size", &ParallelismConfig::ffn_sp_size)
+        .def_readwrite("tp_rank", &ParallelismConfig::tp_rank)
+        .def_readwrite("ep_rank", &ParallelismConfig::ep_rank)
+        .def_readwrite("dp_rank", &ParallelismConfig::dp_rank)
+        .def_readwrite("ffn_tp_size", &ParallelismConfig::ffn_tp_size)
+        .def_readwrite("ffn_tp_rank", &ParallelismConfig::ffn_tp_rank)
+        .def_readwrite("enable_sp", &ParallelismConfig::enable_sp)
+        .def_readwrite("nccl_ip", &ParallelismConfig::nccl_ip)
+        .def_readwrite("tp_nccl_port", &ParallelismConfig::tp_nccl_port)
+        .def_readwrite("dp_tp_nccl_port", &ParallelismConfig::dp_tp_nccl_port)
+        .def_readwrite("ffn_tp_nccl_port", &ParallelismConfig::ffn_tp_nccl_port)
+        .def_readwrite("th_nccl_port", &ParallelismConfig::th_nccl_port)
+        .def_readwrite("http_port", &ParallelismConfig::http_port)
+        .def_readwrite("model_rpc_port", &ParallelismConfig::model_rpc_port)
+        .def_readwrite("ffn_disaggregate_config", &ParallelismConfig::ffn_disaggregate_config)
+        .def("to_string", &ParallelismConfig::to_string)
         .def(py::pickle(
-            [](const QuantAlgo& quant_algo) {
-                return py::make_tuple(int(quant_algo.getQuantMethod()),
-                                      int(quant_algo.getWeightBits()),
-                                      int(quant_algo.getGroupSize()),
-                                      int(quant_algo.getActivationBits()));
+            [](const ParallelismConfig& self) {
+                return py::make_tuple(
+                    self.tp_size, self.ep_size, self.dp_size, self.pp_size,
+                    self.world_size, self.world_rank, self.local_world_size,
+                    self.ffn_sp_size, self.tp_rank, self.ep_rank, self.dp_rank,
+                    self.ffn_tp_size, self.ffn_tp_rank, self.enable_sp,
+                    self.nccl_ip, self.tp_nccl_port, self.dp_tp_nccl_port,
+                    self.ffn_tp_nccl_port, self.th_nccl_port, self.http_port,
+                    self.model_rpc_port, self.ffn_disaggregate_config
+                );
             },
-            [](py::tuple t) { return QuantAlgo(QuantMethod(t[0].cast<int>()), t[1].cast<int>(), t[2].cast<int>()); }));
+            [](py::tuple t) {
+                if (t.size() != 22) throw std::runtime_error("Invalid state!");
+                ParallelismConfig c;
+                try {
+                    c.tp_size = t[0].cast<int64_t>();
+                    c.ep_size = t[1].cast<int64_t>();
+                    c.dp_size = t[2].cast<int64_t>();
+                    c.pp_size = t[3].cast<int64_t>();
+                    c.world_size = t[4].cast<int64_t>();
+                    c.world_rank = t[5].cast<int64_t>();
+                    c.local_world_size = t[6].cast<int64_t>();
+                    c.ffn_sp_size = t[7].cast<int64_t>();
+                    c.tp_rank = t[8].cast<int64_t>();
+                    c.ep_rank = t[9].cast<int64_t>();
+                    c.dp_rank = t[10].cast<int64_t>();
+                    c.ffn_tp_size = t[11].cast<int64_t>();
+                    c.ffn_tp_rank = t[12].cast<int64_t>();
+                    c.enable_sp = t[13].cast<bool>();
+                    c.nccl_ip = t[14].cast<std::string>();
+                    c.tp_nccl_port = t[15].cast<int64_t>();
+                    c.dp_tp_nccl_port = t[16].cast<int64_t>();
+                    c.ffn_tp_nccl_port = t[17].cast<int64_t>();
+                    c.th_nccl_port = t[18].cast<int64_t>();
+                    c.http_port = t[19].cast<int64_t>();
+                    c.model_rpc_port = t[20].cast<int64_t>();
+                    c.ffn_disaggregate_config = t[21].cast<FfnDisAggregateConfig>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("ParallelismConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
 
-    pybind11::enum_<RoleType>(m, "RoleType")
-        .value("PDFUSION", RoleType::PDFUSION)
-        .value("PREFILL", RoleType::PREFILL)
-        .value("DECODE", RoleType::DECODE)
-        .value("VIT", RoleType::VIT)
-        .value("FRONTEND", RoleType::FRONTEND)
-        .def("__str__", [](RoleType role) {
-            switch (role) {
-                case RoleType::PDFUSION:
-                    return "pd_fusion";
-                case RoleType::PREFILL:
-                    return "prefill";
-                case RoleType::DECODE:
-                    return "decode";
-                case RoleType::VIT:
-                    return "vit";
-                case RoleType::FRONTEND:
-                    return "FRONTEND";
-                default:
-                    return "invalid";
+    // Register BatchDecodeSchedulerConfig
+    py::class_<BatchDecodeSchedulerConfig>(m, "BatchDecodeSchedulerConfig")
+        .def(py::init<>())
+        .def_readwrite("batch_decode_scheduler_batch_size", &BatchDecodeSchedulerConfig::batch_decode_scheduler_batch_size)
+        .def_readwrite("batch_decode_scheduler_warmup_type", &BatchDecodeSchedulerConfig::batch_decode_scheduler_warmup_type)
+        .def("to_string", &BatchDecodeSchedulerConfig::to_string)
+        .def(py::pickle(
+            [](const BatchDecodeSchedulerConfig& self) {
+                return py::make_tuple(self.batch_decode_scheduler_batch_size, self.batch_decode_scheduler_warmup_type);
+            },
+            [](py::tuple t) {
+                if (t.size() != 2) throw std::runtime_error("Invalid state!");
+                BatchDecodeSchedulerConfig c;
+                try {
+                    c.batch_decode_scheduler_batch_size = t[0].cast<int64_t>();
+                    c.batch_decode_scheduler_warmup_type = t[1].cast<int64_t>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("BatchDecodeSchedulerConfig unpickle error: ") + e.what());
+                }
+                return c;
             }
-        });
-
-#define DEF_PROPERTY(name, member) .def_readwrite(#name, &GptInitParameter::member)
-
-#define REGISTER_PROPERTYS                                                                                             \
-    DEF_PROPERTY(head_num, head_num_)                                                                                  \
-    DEF_PROPERTY(head_num_kv, head_num_kv_)                                                                            \
-    DEF_PROPERTY(size_per_head, size_per_head_)                                                                        \
-    DEF_PROPERTY(max_seq_len, max_seq_len_)                                                                            \
-    DEF_PROPERTY(max_batch_tokens_size, max_batch_tokens_size_)                                                        \
-    DEF_PROPERTY(vocab_size, vocab_size_)                                                                              \
-    DEF_PROPERTY(input_vocab_size, input_vocab_size_)                                                                  \
-    DEF_PROPERTY(hidden_size, hidden_size_)                                                                            \
-    DEF_PROPERTY(type_vocab_size, type_vocab_size_)                                                                    \
-    DEF_PROPERTY(embedding_size, embedding_size_)                                                                      \
-    DEF_PROPERTY(gen_num_per_circle, gen_num_per_circle_)                                                              \
-    DEF_PROPERTY(inter_size, inter_size_)                                                                              \
-    DEF_PROPERTY(inter_padding_size, inter_padding_size_)                                                              \
-    DEF_PROPERTY(moe_inter_padding_size, moe_inter_padding_size_)                                                      \
-    DEF_PROPERTY(is_sparse_head, is_sparse_head_)                                                                      \
-    DEF_PROPERTY(layer_head_num, layer_head_num_)                                                                      \
-    DEF_PROPERTY(layer_head_num_kv, layer_head_num_kv_)                                                                \
-    DEF_PROPERTY(layer_inter_size, layer_inter_size_)                                                                  \
-    DEF_PROPERTY(layer_inter_padding_size, layer_inter_padding_size_)                                                  \
-    DEF_PROPERTY(num_layers, num_layers_)                                                                              \
-    DEF_PROPERTY(layer_num, num_layers_)                                                                               \
-    DEF_PROPERTY(num_valid_layer, num_valid_layer_)                                                                    \
-    DEF_PROPERTY(expert_num, expert_num_)                                                                              \
-    DEF_PROPERTY(moe_k, moe_k_)                                                                                        \
-    DEF_PROPERTY(moe_normalize_expert_scale, moe_normalize_expert_scale_)                                              \
-    DEF_PROPERTY(moe_style, moe_style_)                                                                                \
-    DEF_PROPERTY(moe_layer_index, moe_layer_index_)                                                                    \
-    DEF_PROPERTY(scoring_func, scoring_func_)                                                                          \
-    DEF_PROPERTY(layernorm_eps, layernorm_eps_)                                                                        \
-    /* In python, the following types use strings for branch condition */                                              \
-    /* Everytime type changes, corresponding set type function should  */                                              \
-    /* be called.                                                      */                                              \
-    DEF_PROPERTY(layernorm_type, layernorm_type_str_)                                                                  \
-    DEF_PROPERTY(norm_type, norm_type_str_)                                                                            \
-    DEF_PROPERTY(activation_type, activation_type_str_)                                                                \
-    DEF_PROPERTY(rotary_embedding_dim, rotary_embedding_dim_)                                                          \
-    DEF_PROPERTY(kv_cache_data_type, kv_cache_data_type_str_)                                                          \
-    DEF_PROPERTY(rotary_embedding_style, rotary_embedding_style_)                                                      \
-    DEF_PROPERTY(position_ids_style, position_ids_style_)                                                              \
-    DEF_PROPERTY(position_id_len_factor, position_id_len_factor_)                                                      \
-    DEF_PROPERTY(rotary_embedding_base, rotary_embedding_base_)                                                        \
-    DEF_PROPERTY(rotary_embedding_scale, rotary_embedding_scale_)                                                      \
-    DEF_PROPERTY(org_embedding_max_pos, org_embedding_max_pos_)                                                        \
-    DEF_PROPERTY(rotary_factor1, rotary_factor1_)                                                                      \
-    DEF_PROPERTY(rotary_factor2, rotary_factor2_)                                                                      \
-    DEF_PROPERTY(partial_rotary_factor, partial_rotary_factor_)                                                        \
-    DEF_PROPERTY(mrope_section, mrope_section_)                                                                        \
-    DEF_PROPERTY(input_embedding_scalar, input_embedding_scalar_)                                                      \
-    DEF_PROPERTY(residual_scalar, residual_scalar_)                                                                    \
-    DEF_PROPERTY(use_norm_input_residual, use_norm_input_residual_)                                                    \
-    DEF_PROPERTY(use_norm_attn_out_residual, use_norm_attn_out_residual_)                                              \
-    DEF_PROPERTY(data_type, data_type_str_)                                                                            \
-    DEF_PROPERTY(has_positional_encoding, has_positional_encoding_)                                                    \
-    DEF_PROPERTY(has_pre_decoder_layernorm, has_pre_decoder_layernorm_)                                                \
-    DEF_PROPERTY(has_post_decoder_layernorm, has_post_decoder_layernorm_)                                              \
-    DEF_PROPERTY(has_moe_norm, has_moe_norm_)                                                                          \
-    DEF_PROPERTY(logit_scale, logit_scale_)                                                                            \
-    DEF_PROPERTY(has_lm_head, has_lm_head_)                                                                            \
-    DEF_PROPERTY(use_attention_linear_bias, use_attention_linear_bias_)                                                \
-    DEF_PROPERTY(use_fp32_to_compute_logit, use_fp32_to_compute_logit_)                                                \
-    DEF_PROPERTY(add_bias_linear, add_bias_linear_)                                                                    \
-    DEF_PROPERTY(tokenizer_path, tokenizer_path_)                                                                      \
-    DEF_PROPERTY(ckpt_path, ckpt_path_)                                                                                \
-    DEF_PROPERTY(pre_seq_len, pre_seq_len_)                                                                            \
-    DEF_PROPERTY(prefix_projection, prefix_projection_)                                                                \
-    DEF_PROPERTY(using_hf_sampling, using_hf_sampling_)                                                                \
-    DEF_PROPERTY(max_generate_batch_size, max_generate_batch_size_)                                                    \
-    DEF_PROPERTY(max_context_batch_size, max_context_batch_size_)                                                      \
-    DEF_PROPERTY(special_tokens, special_tokens_)                                                                      \
-    DEF_PROPERTY(quant_algo, quant_algo_)                                                                              \
-    DEF_PROPERTY(use_logn_attn, use_logn_attn_)                                                                        \
-    DEF_PROPERTY(q_scaling, q_scaling_)                                                                                \
-    DEF_PROPERTY(qk_norm, qk_norm_)                                                                                    \
-    DEF_PROPERTY(use_cross_attn, use_cross_attn_)                                                                      \
-    DEF_PROPERTY(cross_attn_input_len, cross_attn_input_len_)                                                          \
-    DEF_PROPERTY(is_multimodal, is_multimodal_)                                                                        \
-    DEF_PROPERTY(mm_sep_tokens, mm_sep_tokens_)                                                                        \
-    DEF_PROPERTY(include_sep_tokens, include_sep_tokens_)                                                              \
-    DEF_PROPERTY(mm_position_ids_style, mm_position_ids_style_)                                                        \
-    DEF_PROPERTY(pre_allocate_op_mem, pre_allocate_op_mem_)                                                            \
-    DEF_PROPERTY(seq_size_per_block, seq_size_per_block_)                                                              \
-    DEF_PROPERTY(max_block_size_per_item, max_block_size_per_item_)                                                    \
-    DEF_PROPERTY(block_nums, block_nums_)                                                                              \
-    DEF_PROPERTY(scheduler_reserve_resource_ratio, scheduler_reserve_resource_ratio_)                                  \
-    DEF_PROPERTY(kv_cache_mem_mb, kv_cache_mem_mb_)                                                                    \
-    DEF_PROPERTY(reserve_runtime_mem_mb, reserve_runtime_mem_mb_)                                                      \
-    DEF_PROPERTY(reuse_cache, reuse_cache_)                                                                            \
-    DEF_PROPERTY(enable_partial_fallback, enable_partial_fallback_)                                                    \
-    DEF_PROPERTY(enable_fast_gen, enable_fast_gen_)                                                                    \
-    DEF_PROPERTY(warm_up, warm_up_)                                                                                    \
-    DEF_PROPERTY(warm_up_with_loss, warm_up_with_loss_)                                                                \
-    DEF_PROPERTY(fast_gen_max_context_len, fast_gen_max_context_len_)                                                  \
-    DEF_PROPERTY(is_causal, is_causal_)                                                                                \
-    DEF_PROPERTY(nccl_ip, nccl_ip_)                                                                                    \
-    DEF_PROPERTY(tp_nccl_port, tp_nccl_port_)                                                                          \
-    DEF_PROPERTY(dp_tp_nccl_port, dp_tp_nccl_port_)                                                                    \
-    DEF_PROPERTY(ffn_tp_nccl_port, ffn_tp_nccl_port_)                                                                  \
-    DEF_PROPERTY(model_rpc_port, model_rpc_port_)                                                                      \
-    DEF_PROPERTY(http_port, http_port_)                                                                                \
-    DEF_PROPERTY(tp_size, tp_size_)                                                                                    \
-    DEF_PROPERTY(tp_rank, tp_rank_)                                                                                    \
-    DEF_PROPERTY(dp_size, dp_size_)                                                                                    \
-    DEF_PROPERTY(dp_rank, dp_rank_)                                                                                    \
-    DEF_PROPERTY(ffn_tp_size, ffn_tp_size_)                                                                            \
-    DEF_PROPERTY(ffn_tp_rank, ffn_tp_rank_)                                                                            \
-    DEF_PROPERTY(enable_sp, enable_sp_)                                                                                \
-    DEF_PROPERTY(world_size, world_size_)                                                                              \
-    DEF_PROPERTY(use_all_gather, use_all_gather_)                                                                      \
-    DEF_PROPERTY(cache_store_listen_port, cache_store_listen_port_)                                                    \
-    DEF_PROPERTY(cache_store_connect_port, cache_store_connect_port_)                                                  \
-    DEF_PROPERTY(cache_store_rdma_connect_port, cache_store_rdma_connect_port_)                                        \
-    DEF_PROPERTY(cache_store_rdma_listen_port, cache_store_rdma_listen_port_)                                          \
-    DEF_PROPERTY(worker_port_offset, worker_port_offset_)                                                              \
-    DEF_PROPERTY(worker_addrs, worker_addrs_)                                                                          \
-    DEF_PROPERTY(worker_grpc_addrs, worker_grpc_addrs_)                                                                \
-    DEF_PROPERTY(remote_rpc_server_port, remote_rpc_server_port_)                                                      \
-    DEF_PROPERTY(role_type, role_type_)                                                                                \
-    DEF_PROPERTY(cache_store_rdma_mode, cache_store_rdma_mode_)                                                        \
-    DEF_PROPERTY(prefill_retry_times, prefill_retry_times_)                                                            \
-    DEF_PROPERTY(prefill_retry_timeout_ms, prefill_retry_timeout_ms_)                                                  \
-    DEF_PROPERTY(prefill_max_wait_timeout_ms, prefill_max_wait_timeout_ms_)                                            \
-    DEF_PROPERTY(decode_retry_times, decode_retry_times_)                                                              \
-    DEF_PROPERTY(decode_retry_timeout_ms, decode_retry_timeout_ms_)                                                    \
-    DEF_PROPERTY(decode_polling_kv_cache_step_ms, decode_polling_kv_cache_step_ms_)                                    \
-    DEF_PROPERTY(decode_polling_call_prefill_ms, decode_polling_call_prefill_ms_)                                      \
-    DEF_PROPERTY(rdma_connect_retry_times, rdma_connect_retry_times_)                                                  \
-    DEF_PROPERTY(decode_entrance, decode_entrance_)                                                                    \
-    DEF_PROPERTY(load_cache_timeout_ms, load_cache_timeout_ms_)                                                        \
-    DEF_PROPERTY(max_rpc_timeout_ms, max_rpc_timeout_ms_)                                                              \
-    DEF_PROPERTY(ep_size, ep_size_)                                                                                    \
-    DEF_PROPERTY(ep_rank, ep_rank_)                                                                                    \
-    DEF_PROPERTY(use_kvcache, use_kvcache_)                                                                            \
-    DEF_PROPERTY(local_rank, local_rank_)                                                                              \
-    DEF_PROPERTY(rotary_embedding_mscale, rotary_embedding_mscale_)                                                    \
-    DEF_PROPERTY(rotary_embedding_offset, rotary_embedding_offset_)                                                    \
-    DEF_PROPERTY(rotary_embedding_extrapolation_factor, rotary_embedding_extrapolation_factor_)                        \
-    DEF_PROPERTY(use_mla, use_mla_)                                                                                    \
-    DEF_PROPERTY(mla_ops_type, mla_ops_type_)                                                                          \
-    DEF_PROPERTY(q_lora_rank, q_lora_rank_)                                                                            \
-    DEF_PROPERTY(kv_lora_rank, kv_lora_rank_)                                                                          \
-    DEF_PROPERTY(nope_head_dim, nope_head_dim_)                                                                        \
-    DEF_PROPERTY(rope_head_dim, rope_head_dim_)                                                                        \
-    DEF_PROPERTY(v_head_dim, v_head_dim_)                                                                              \
-    DEF_PROPERTY(moe_n_group, moe_n_group_)                                                                            \
-    DEF_PROPERTY(moe_topk_group, moe_topk_group_)                                                                      \
-    DEF_PROPERTY(routed_scaling_factor, routed_scaling_factor_)                                                        \
-    DEF_PROPERTY(softmax_extra_scale, softmax_extra_scale_)                                                            \
-    DEF_PROPERTY(vit_separation, vit_separation_)                                                                      \
-    DEF_PROPERTY(enable_speculative_decoding, enable_speculative_decoding_)                                            \
-    DEF_PROPERTY(model_name, model_name_)                                                                              \
-    DEF_PROPERTY(deepseek_rope_mscale, deepseek_rope_mscale_)                                                          \
-    DEF_PROPERTY(deepseek_mscale_all_dim, deepseek_mscale_all_dim_)                                                    \
-    DEF_PROPERTY(reverse_e_h_norm, reverse_e_h_norm_)                                                                  \
-    DEF_PROPERTY(enable_eplb, enable_eplb_)                                                                            \
-    DEF_PROPERTY(phy_exp_num, phy_exp_num_)                                                                            \
-    DEF_PROPERTY(eplb_update_time, eplb_update_time_)                                                                  \
-    DEF_PROPERTY(eplb_mode, eplb_mode_)                                                                                \
-    DEF_PROPERTY(py_eplb, py_eplb_)
-
-    pybind11::class_<GptInitParameter>(m, "GptInitParameter")
-        .def(pybind11::init<int64_t,  // head_num
-                            int64_t,  // size_per_head
-                            int64_t,  // num_layers
-                            int64_t,  // max_seq_len
-                            int64_t,  // vocab_size
-                            int64_t   // hidden_size
-                            >(),
-             py::arg("head_num"),
-             py::arg("size_per_head"),
-             py::arg("num_layers"),
-             py::arg("max_seq_len"),
-             py::arg("vocab_size"),
-             py::arg("hidden_size"))
-        .def("insertMultiTaskPromptTokens",
-             &GptInitParameter::insertMultiTaskPromptTokens,
-             py::arg("task_id"),
-             py::arg("tokens_id"))
-        .def("setLayerNormType", &GptInitParameter::setLayerNormType)
-        .def("setNormType", &GptInitParameter::setNormType)
-        .def("setActivationType", &GptInitParameter::setActivationType)
-        .def("setTaskType", &GptInitParameter::setTaskType, py::arg("task"))
-        .def("setDataType", &GptInitParameter::setDataType)
-        .def("setKvCacheDataType", &GptInitParameter::setKvCacheDataType)
-        .def("showDebugInfo", &GptInitParameter::showDebugInfo)
-        .def("isGatedActivation", &GptInitParameter::isGatedActivation)
-        .def("isKvCacheQuant", &GptInitParameter::isKvCacheQuant)
-        // 新增配置结构体成员暴露
-        .def_readwrite("parallelism_distributed_config", &GptInitParameter::parallelism_distributed_config)
-        .def_readwrite("concurrency_config", &GptInitParameter::concurrency_config)
-        .def_readwrite("fmha_config", &GptInitParameter::fmha_config)
-        .def_readwrite("kv_cache_config", &GptInitParameter::kv_cache_config)
-        .def_readwrite("profiling_debug_logging_config", &GptInitParameter::profiling_debug_logging_config)
-        .def_readwrite("hw_kernel_config", &GptInitParameter::hw_kernel_config)
-        .def_readwrite("device_resource_config", &GptInitParameter::device_resource_config)
-        .def_readwrite("moe_config", &GptInitParameter::moe_config)
-        .def_readwrite("model_specific_config", &GptInitParameter::model_specific_config)
-        .def_readwrite("sp_config", &GptInitParameter::sp_config)
-        .def_readwrite("service_discovery_config", &GptInitParameter::service_discovery_config)
-        .def_readwrite("cache_store_config", &GptInitParameter::cache_store_config)
-        .def_readwrite("scheduler_config", &GptInitParameter::scheduler_config)
-        .def_readwrite("batch_decode_scheduler_config", &GptInitParameter::batch_decode_scheduler_config)
-        .def_readwrite("fifo_scheduler_config", &GptInitParameter::fifo_scheduler_config)
-        .def_readwrite("misc_config", &GptInitParameter::misc_config)
-        .def_readwrite("arpc_config", &GptInitParameter::arpc_config)
-        .def_readwrite("ffn_disaggregate_config", &GptInitParameter::ffn_disaggregate_config) REGISTER_PROPERTYS;
-}
+        ));
 
-PYBIND11_MODULE(libth_transformer_config, m) {
-    register_parallelism_distributed_config(m);
-    register_concurrency_config(m);
-    register_fmha_config(m);
-    register_kvcache_config(m);
-    register_profiling_debug_logging_config(m);
-    register_hwkernel_config(m);
-    register_device_resource_config(m);
-    register_moe_config(m);
-    register_model_specific_config(m);
-    register_speculative_execution_config(m);
-    register_service_discovery_config(m);
-    register_cache_store_config(m);
-    register_scheduler_config(m);
-    register_batch_decode_scheduler_config(m);
-    register_fifo_scheduler_config(m);
-    register_misc_config(m);
-    register_arpc_config(m);
-    registerFMHAType(m);
-    register_ffn_disaggregate_config(m);
-    registerGptInitParameter(m);
+    // Register FIFOSchedulerConfig
+    py::class_<FIFOSchedulerConfig>(m, "FIFOSchedulerConfig")
+        .def(py::init<>())
+        .def_readwrite("enable_fast_gen", &FIFOSchedulerConfig::enable_fast_gen)
+        .def_readwrite("enable_partial_fallback", &FIFOSchedulerConfig::enable_partial_fallback)
+        .def_readwrite("fast_gen_context_budget", &FIFOSchedulerConfig::fast_gen_context_budget)
+        .def_readwrite("max_context_batch_size", &FIFOSchedulerConfig::max_context_batch_size)
+        .def_readwrite("scheduler_reserve_resource_ratio", &FIFOSchedulerConfig::scheduler_reserve_resource_ratio)
+        .def_readwrite("fast_gen_max_context_len", &FIFOSchedulerConfig::fast_gen_max_context_len)
+        .def_readwrite("max_batch_tokens_size", &FIFOSchedulerConfig::max_batch_tokens_size)
+        .def("to_string", &FIFOSchedulerConfig::to_string)
+        .def(py::pickle(
+            [](const FIFOSchedulerConfig& self) {
+                return py::make_tuple(
+                    self.enable_fast_gen, self.enable_partial_fallback, self.fast_gen_context_budget,
+                    self.max_context_batch_size, self.scheduler_reserve_resource_ratio,
+                    self.fast_gen_max_context_len, self.max_batch_tokens_size
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 7) throw std::runtime_error("Invalid state!");
+                FIFOSchedulerConfig c;
+                try {
+                    c.enable_fast_gen = t[0].cast<bool>();
+                    c.enable_partial_fallback = t[1].cast<bool>();
+                    c.fast_gen_context_budget = t[2].cast<int64_t>();
+                    c.max_context_batch_size = t[3].cast<int64_t>();
+                    c.scheduler_reserve_resource_ratio = t[4].cast<int64_t>();
+                    c.fast_gen_max_context_len = t[5].cast<int64_t>();
+                    c.max_batch_tokens_size = t[6].cast<int64_t>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("FIFOSchedulerConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
 
-    registerCommon(m);
-}
+    // Register RuntimeConfig - only expose its own members, not sub-config members
+    py::class_<RuntimeConfig> runtime_config(m, "RuntimeConfig");
+    runtime_config
+        .def(py::init<>())
+        .def_readwrite("max_generate_batch_size", &RuntimeConfig::max_generate_batch_size)
+        .def_readwrite("pre_allocate_op_mem", &RuntimeConfig::pre_allocate_op_mem)
+        .def_readwrite("max_block_size_per_item", &RuntimeConfig::max_block_size_per_item)
+        .def_readwrite("reserve_runtime_mem_mb", &RuntimeConfig::reserve_runtime_mem_mb)
+        .def_readwrite("warm_up", &RuntimeConfig::warm_up)
+        .def_readwrite("warm_up_with_loss", &RuntimeConfig::warm_up_with_loss)
+        .def_readwrite("use_batch_decode_scheduler", &RuntimeConfig::use_batch_decode_scheduler)
+        .def_readwrite("use_gather_batch_scheduler", &RuntimeConfig::use_gather_batch_scheduler)
+        .def_readwrite("model_name", &RuntimeConfig::model_name)
+        .def_readwrite("worker_grpc_addrs", &RuntimeConfig::worker_grpc_addrs)
+        // Fields merged from PyDeviceResourceConfig
+        .def_readwrite("specify_gpu_arch", &RuntimeConfig::specify_gpu_arch)
+        .def_readwrite("acext_gemm_config_dir", &RuntimeConfig::acext_gemm_config_dir)
+        // Add sub-configs as properties that return references
+        .def_property_readonly("batch_decode_scheduler_config", [](RuntimeConfig& self) -> BatchDecodeSchedulerConfig& {
+            return self.batch_decode_scheduler_config;
+        }, py::return_value_policy::reference_internal)
+        .def_property_readonly("fifo_scheduler_config", [](RuntimeConfig& self) -> FIFOSchedulerConfig& {
+            return self.fifo_scheduler_config;
+        }, py::return_value_policy::reference_internal)
+        .def("to_string", &RuntimeConfig::to_string)
+        .def(py::pickle(
+            [](const RuntimeConfig& self) {
+                return py::make_tuple(
+                    self.max_generate_batch_size, self.pre_allocate_op_mem,
+                    self.max_block_size_per_item, self.reserve_runtime_mem_mb,
+                    self.warm_up, self.warm_up_with_loss, self.use_batch_decode_scheduler,
+                    self.use_gather_batch_scheduler, self.batch_decode_scheduler_config,
+                    self.fifo_scheduler_config, self.model_name, self.worker_grpc_addrs,
+                    self.specify_gpu_arch, self.acext_gemm_config_dir
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 14) throw std::runtime_error("Invalid state!");
+                RuntimeConfig c;
+                try {
+                    c.max_generate_batch_size = t[0].cast<int64_t>();
+                    c.pre_allocate_op_mem = t[1].cast<bool>();
+                    c.max_block_size_per_item = t[2].cast<int64_t>();
+                    c.reserve_runtime_mem_mb = t[3].cast<int64_t>();
+                    c.warm_up = t[4].cast<bool>();
+                    c.warm_up_with_loss = t[5].cast<bool>();
+                    c.use_batch_decode_scheduler = t[6].cast<bool>();
+                    c.use_gather_batch_scheduler = t[7].cast<bool>();
+                    c.batch_decode_scheduler_config = t[8].cast<BatchDecodeSchedulerConfig>();
+                    c.fifo_scheduler_config = t[9].cast<FIFOSchedulerConfig>();
+                    c.model_name = t[10].cast<std::string>();
+                    c.worker_grpc_addrs = t[11].cast<std::vector<std::string>>();
+                    c.specify_gpu_arch = t[12].cast<std::string>();
+                    c.acext_gemm_config_dir = t[13].cast<std::string>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("RuntimeConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register DataType enum
+    py::enum_<DataType>(m, "DataType")
+        .value("TYPE_INVALID", DataType::TYPE_INVALID)
+        .value("TYPE_BOOL", DataType::TYPE_BOOL)
+        .value("TYPE_UINT8", DataType::TYPE_UINT8)
+        .value("TYPE_UINT16", DataType::TYPE_UINT16)
+        .value("TYPE_UINT32", DataType::TYPE_UINT32)
+        .value("TYPE_UINT64", DataType::TYPE_UINT64)
+        .value("TYPE_INT8", DataType::TYPE_INT8)
+        .value("TYPE_INT16", DataType::TYPE_INT16)
+        .value("TYPE_INT32", DataType::TYPE_INT32)
+        .value("TYPE_INT64", DataType::TYPE_INT64)
+        .value("TYPE_FP16", DataType::TYPE_FP16)
+        .value("TYPE_FP32", DataType::TYPE_FP32)
+        .value("TYPE_FP64", DataType::TYPE_FP64)
+        .value("TYPE_BYTES", DataType::TYPE_BYTES)
+        .value("TYPE_BF16", DataType::TYPE_BF16)
+        .value("TYPE_FP8_E4M3", DataType::TYPE_FP8_E4M3)
+        .value("TYPE_STR", DataType::TYPE_STR)
+        .value("TYPE_VOID", DataType::TYPE_VOID)
+        .value("TYPE_QINT8", DataType::TYPE_QINT8)
+        .value("TYPE_INT4X2", DataType::TYPE_INT4X2)
+        .value("TYPE_QINT4X2", DataType::TYPE_QINT4X2)
+        .value("TYPE_QFP8_E4M3", DataType::TYPE_QFP8_E4M3);
+
+    // Register KvCacheDataType enum
+    py::enum_<KvCacheDataType>(m, "KvCacheDataType")
+        .value("BASE", KvCacheDataType::BASE)
+        .value("INT8", KvCacheDataType::INT8)
+        .value("FP8", KvCacheDataType::FP8);
+
+    // Register RopeStyle enum
+    py::enum_<RopeStyle>(m, "RopeStyle")
+        .value("No", RopeStyle::No)
+        .value("Base", RopeStyle::Base)
+        .value("Glm2", RopeStyle::Glm2)
+        .value("DynamicNTK", RopeStyle::DynamicNTK)
+        .value("QwenDynamicNTK", RopeStyle::QwenDynamicNTK)
+        .value("Yarn", RopeStyle::Yarn)
+        .value("Llama3", RopeStyle::Llama3)
+        .value("Mrope", RopeStyle::Mrope);
+
+    // Register RopeConfig
+    py::class_<RopeConfig>(m, "RopeConfig")
+        .def(py::init<>())
+        .def_property("style",
+            [](const RopeConfig& self) { return self.style; },
+            [](RopeConfig& self, py::object value) {
+                if (py::isinstance<RopeStyle>(value)) {
+                    self.style = value.cast<RopeStyle>();
+                } else if (py::isinstance<py::int_>(value)) {
+                    self.style = static_cast<RopeStyle>(value.cast<int>());
+                } else {
+                    throw std::runtime_error("style must be RopeStyle enum or int");
+                }
+            },
+            py::return_value_policy::reference_internal)
+        .def_readwrite("dim", &RopeConfig::dim)
+        .def_readwrite("base", &RopeConfig::base)
+        .def_readwrite("scale", &RopeConfig::scale)
+        .def_readwrite("factor1", &RopeConfig::factor1)
+        .def_readwrite("factor2", &RopeConfig::factor2)
+        .def_readwrite("max_pos", &RopeConfig::max_pos)
+        .def_readwrite("extrapolation_factor", &RopeConfig::extrapolation_factor)
+        .def_readwrite("mscale", &RopeConfig::mscale)
+        .def_readwrite("offset", &RopeConfig::offset)
+        .def_readwrite("index_factor", &RopeConfig::index_factor)
+        .def_readwrite("mrope_dim1", &RopeConfig::mrope_dim1)
+        .def_readwrite("mrope_dim2", &RopeConfig::mrope_dim2)
+        .def_readwrite("mrope_dim3", &RopeConfig::mrope_dim3);
+
+    // Register AttentionConfigs
+    py::class_<AttentionConfigs>(m, "AttentionConfigs")
+        .def(py::init<>())
+        .def_readwrite("head_num", &AttentionConfigs::head_num)
+        .def_readwrite("kv_head_num", &AttentionConfigs::kv_head_num)
+        .def_readwrite("size_per_head", &AttentionConfigs::size_per_head)
+        .def_readwrite("rope_config", &AttentionConfigs::rope_config)
+        .def_readwrite("tokens_per_block", &AttentionConfigs::tokens_per_block)
+        .def_readwrite("q_scaling", &AttentionConfigs::q_scaling)
+        .def_readwrite("fuse_qkv_add_bias", &AttentionConfigs::fuse_qkv_add_bias)
+        .def_readwrite("use_logn_attn", &AttentionConfigs::use_logn_attn)
+        .def_readwrite("is_causal", &AttentionConfigs::is_causal)
+        .def_readwrite("use_mla", &AttentionConfigs::use_mla)
+        .def_readwrite("q_lora_rank", &AttentionConfigs::q_lora_rank)
+        .def_readwrite("kv_lora_rank", &AttentionConfigs::kv_lora_rank)
+        .def_readwrite("nope_head_dim", &AttentionConfigs::nope_head_dim)
+        .def_readwrite("rope_head_dim", &AttentionConfigs::rope_head_dim)
+        .def_readwrite("v_head_dim", &AttentionConfigs::v_head_dim)
+        .def_readwrite("softmax_extra_scale", &AttentionConfigs::softmax_extra_scale)
+        .def_readwrite("kv_cache_dtype", &AttentionConfigs::kv_cache_dtype)
+        .def_readwrite("skip_append_kv_cache", &AttentionConfigs::skip_append_kv_cache);
+
+    // Register ModelConfig
+    py::class_<ModelConfig>(m, "ModelConfig")
+        .def(py::init<>())
+        .def_readwrite("num_layers", &ModelConfig::num_layers)
+        .def_readwrite("max_seq_len", &ModelConfig::max_seq_len)
+        .def_readwrite("vocab_size", &ModelConfig::vocab_size)
+        .def_readwrite("hidden_size", &ModelConfig::hidden_size)
+        .def_readwrite("inter_size", &ModelConfig::inter_size)
+        .def_readwrite("inter_padding_size", &ModelConfig::inter_padding_size)
+        .def_readwrite("moe_inter_padding_size", &ModelConfig::moe_inter_padding_size)
+        .def_readwrite("attn_config", &ModelConfig::attn_config)
+        .def_readwrite("special_tokens", &ModelConfig::special_tokens)
+        .def_readwrite("quant_algo", &ModelConfig::quant_algo)
+        .def_readwrite("eplb_config", &ModelConfig::eplb_config)
+        // task_type is defined as property below
+        .def_readwrite("ckpt_path", &ModelConfig::ckpt_path)
+        .def_readwrite("tokenizer_path", &ModelConfig::tokenizer_path)
+        .def_readwrite("lora_infos", &ModelConfig::lora_infos)
+        .def_readwrite("position_ids_style", &ModelConfig::position_ids_style)
+        .def_readwrite("pre_seq_len", &ModelConfig::pre_seq_len)
+        .def_readwrite("use_kvcache", &ModelConfig::use_kvcache)
+        .def_readwrite("logit_scale", &ModelConfig::logit_scale)
+        .def_readwrite("qk_norm", &ModelConfig::qk_norm)
+        .def_readwrite("expert_num", &ModelConfig::expert_num)
+        .def_readwrite("moe_n_group", &ModelConfig::moe_n_group)
+        .def_readwrite("moe_k", &ModelConfig::moe_k)
+        .def_readwrite("moe_style", &ModelConfig::moe_style)
+        .def_readwrite("moe_layer_index", &ModelConfig::moe_layer_index)
+        // Properties with enum getter and string setter for type conversion
+        .def_property("data_type",
+            [](const ModelConfig& self) { return self.data_type; },
+            [](ModelConfig& self, const std::string& value) { self.set_data_type(value); })
+        .def_property("activation_type",
+            [](const ModelConfig& self) { return self.activation_type; },
+            [](ModelConfig& self, const std::string& value) { self.set_activation_type(value); })
+        .def_property("norm_type",
+            [](const ModelConfig& self) { return self.norm_type; },
+            [](ModelConfig& self, const std::string& value) { self.set_norm_type(value); })
+        .def_property("layernorm_type",
+            [](const ModelConfig& self) { return self.layernorm_type; },
+            [](ModelConfig& self, const std::string& value) { self.set_layer_norm_type(value); })
+        .def_property("task_type",
+            [](const ModelConfig& self) { return self.task_type; },
+            [](ModelConfig& self, const std::string& value) { self.set_task_type(value); })
+        .def_property("mla_ops_type",
+            [](const ModelConfig& self) { return self.mla_ops_type; },
+            [](ModelConfig& self, const std::string& value) { self.set_mla_ops_type(value); })
+        // Fields merged from PyModelConfig
+        .def_readwrite("extra_data_path", &ModelConfig::extra_data_path)
+        .def_readwrite("local_extra_data_path", &ModelConfig::local_extra_data_path)
+        .def_readwrite("act_type", &ModelConfig::act_type)
+        .def_readwrite("use_float32", &ModelConfig::use_float32)
+        .def_readwrite("original_checkpoint_path", &ModelConfig::original_checkpoint_path)
+        .def_readwrite("ft_plugin_path", &ModelConfig::ft_plugin_path)
+        .def_readwrite("model_type", &ModelConfig::model_type)
+        .def_readwrite("ptuning_path", &ModelConfig::ptuning_path)
+        .def_readwrite("json_model_override_args", &ModelConfig::json_model_override_args)
+        .def("getAttentionConfigs", &ModelConfig::getAttentionConfigs)
+        .def("isGatedActivation", &ModelConfig::isGatedActivation)
+        .def("isKvCacheQuant", &ModelConfig::isKvCacheQuant)
+        .def("to_string", &ModelConfig::to_string);
 
-}  // namespace rtp_llm
+    // Register MMModelConfig
+    py::class_<MMModelConfig>(m, "MMModelConfig")
+        .def(py::init<>())
+        .def_readwrite("is_multimodal", &MMModelConfig::is_multimodal)
+        .def_readwrite("mm_sep_tokens", &MMModelConfig::mm_sep_tokens)
+        .def_readwrite("include_sep_tokens", &MMModelConfig::include_sep_tokens)
+        .def_readwrite("mm_position_ids_style", &MMModelConfig::mm_position_ids_style);
+
+    // Register VitConfig
+    py::class_<VitConfig>(m, "VitConfig")
+        .def(py::init<>())
+        .def_readwrite("vit_separation", &VitConfig::vit_separation)
+        .def("to_string", &VitConfig::to_string)
+        .def(py::pickle(
+            [](const VitConfig& self) {
+                return py::make_tuple(self.vit_separation);
+            },
+            [](py::tuple t) {
+                if (t.size() != 1) throw std::runtime_error("Invalid state!");
+                VitConfig c;
+                try {
+                    c.vit_separation = t[0].cast<VitSeparation>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("VitConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+
+    // Register PDSepConfig
+    py::class_<PDSepConfig>(m, "PDSepConfig")
+        .def(py::init<>())
+        .def_readwrite("role_type", &PDSepConfig::role_type)
+        .def_readwrite("cache_store_rdma_mode", &PDSepConfig::cache_store_rdma_mode)
+        .def_readwrite("cache_store_listen_port", &PDSepConfig::cache_store_listen_port)
+        .def_readwrite("cache_store_connect_port", &PDSepConfig::cache_store_connect_port)
+        .def_readwrite("cache_store_rdma_listen_port", &PDSepConfig::cache_store_rdma_listen_port)
+        .def_readwrite("cache_store_rdma_connect_port", &PDSepConfig::cache_store_rdma_connect_port)
+        .def_readwrite("remote_rpc_server_port", &PDSepConfig::remote_rpc_server_port)
+        .def_readwrite("prefill_retry_times", &PDSepConfig::prefill_retry_times)
+        .def_readwrite("prefill_retry_timeout_ms", &PDSepConfig::prefill_retry_timeout_ms)
+        .def_readwrite("prefill_max_wait_timeout_ms", &PDSepConfig::prefill_max_wait_timeout_ms)
+        .def_readwrite("decode_retry_times", &PDSepConfig::decode_retry_times)
+        .def_readwrite("decode_retry_timeout_ms", &PDSepConfig::decode_retry_timeout_ms)
+        .def_readwrite("decode_polling_kv_cache_step_ms", &PDSepConfig::decode_polling_kv_cache_step_ms)
+        .def_readwrite("decode_polling_call_prefill_ms", &PDSepConfig::decode_polling_call_prefill_ms)
+        .def_readwrite("rdma_connect_retry_times", &PDSepConfig::rdma_connect_retry_times)
+        .def_readwrite("load_cache_timeout_ms", &PDSepConfig::load_cache_timeout_ms)
+        .def_readwrite("max_rpc_timeout_ms", &PDSepConfig::max_rpc_timeout_ms)
+        .def_readwrite("worker_port_offset", &PDSepConfig::worker_port_offset)
+        .def_readwrite("decode_entrance", &PDSepConfig::decode_entrance)
+        .def("to_string", &PDSepConfig::to_string)
+        .def(py::pickle(
+            [](const PDSepConfig& self) {
+                return py::make_tuple(
+                    self.role_type, self.cache_store_rdma_mode, self.cache_store_listen_port,
+                    self.cache_store_connect_port, self.cache_store_rdma_listen_port,
+                    self.cache_store_rdma_connect_port, self.remote_rpc_server_port,
+                    self.prefill_retry_times, self.prefill_retry_timeout_ms,
+                    self.prefill_max_wait_timeout_ms, self.decode_retry_times,
+                    self.decode_retry_timeout_ms, self.decode_polling_kv_cache_step_ms,
+                    self.decode_polling_call_prefill_ms, self.rdma_connect_retry_times,
+                    self.load_cache_timeout_ms, self.max_rpc_timeout_ms,
+                    self.worker_port_offset, self.decode_entrance
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 19) throw std::runtime_error("Invalid state!");
+                PDSepConfig c;
+                try {
+                    c.role_type = t[0].cast<RoleType>();
+                    c.cache_store_rdma_mode = t[1].cast<bool>();
+                    c.cache_store_listen_port = t[2].cast<int64_t>();
+                    c.cache_store_connect_port = t[3].cast<int64_t>();
+                    c.cache_store_rdma_listen_port = t[4].cast<int64_t>();
+                    c.cache_store_rdma_connect_port = t[5].cast<int64_t>();
+                    c.remote_rpc_server_port = t[6].cast<int64_t>();
+                    c.prefill_retry_times = t[7].cast<int64_t>();
+                    c.prefill_retry_timeout_ms = t[8].cast<int64_t>();
+                    c.prefill_max_wait_timeout_ms = t[9].cast<int64_t>();
+                    c.decode_retry_times = t[10].cast<int64_t>();
+                    c.decode_retry_timeout_ms = t[11].cast<int64_t>();
+                    c.decode_polling_kv_cache_step_ms = t[12].cast<int64_t>();
+                    c.decode_polling_call_prefill_ms = t[13].cast<int64_t>();
+                    c.rdma_connect_retry_times = t[14].cast<int64_t>();
+                    c.load_cache_timeout_ms = t[15].cast<int64_t>();
+                    c.max_rpc_timeout_ms = t[16].cast<int64_t>();
+                    c.worker_port_offset = t[17].cast<int64_t>();
+                    c.decode_entrance = t[18].cast<bool>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("PDSepConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+                
+    py::class_<EPLBConfig>(m, "EPLBConfig")
+        .def(py::init<>())
+        .def_readwrite("eplb_update_time", &EPLBConfig::eplb_update_time)
+        .def_readwrite("eplb_mode", &EPLBConfig::eplb_mode)
+        .def_readwrite("redundant_expert", &EPLBConfig::redundant_expert)
+        .def_readwrite("balance_method", &EPLBConfig::balance_method)
+        .def_readwrite("eplb_force_repack", &EPLBConfig::eplb_force_repack)
+        .def_readwrite("eplb_stats_window_size", &EPLBConfig::eplb_stats_window_size)
+        .def_readwrite("eplb_control_step", &EPLBConfig::eplb_control_step)
+        .def_readwrite("eplb_test_mode", &EPLBConfig::eplb_test_mode)
+        .def_readwrite("eplb_balance_layer_per_step", &EPLBConfig::eplb_balance_layer_per_step)
+        .def("enable_eplb", &EPLBConfig::enable_eplb, "Get enable_eplb status")
+        .def("phy_exp_num", &EPLBConfig::phy_exp_num, py::arg("expert_num"), "Get physical expert number")
+        .def(py::pickle(
+            [](const EPLBConfig& self) {
+                return py::make_tuple(
+                    self.eplb_update_time, self.eplb_mode, self.redundant_expert,
+                    self.balance_method, self.eplb_force_repack,
+                    self.eplb_stats_window_size, self.eplb_control_step, self.eplb_test_mode,
+                    self.eplb_balance_layer_per_step
+                );
+            },
+            [](py::tuple t) {
+                if (t.size() != 9) throw std::runtime_error("Invalid state!");
+                EPLBConfig c;
+                try {
+                    c.eplb_update_time = t[0].cast<int64_t>();
+                    c.eplb_mode = t[1].cast<EplbMode>();
+                    c.redundant_expert = t[2].cast<int64_t>();
+                    c.balance_method = t[3].cast<std::string>();
+                    c.eplb_force_repack = t[4].cast<int64_t>();
+                    c.eplb_stats_window_size = t[5].cast<int64_t>();
+                    c.eplb_control_step = t[6].cast<int>();
+                    c.eplb_test_mode = t[7].cast<bool>();
+                    c.eplb_balance_layer_per_step = t[8].cast<int>();
+                } catch (const std::exception& e) {
+                    throw std::runtime_error(std::string("EPLBConfig unpickle error: ") + e.what());
+                }
+                return c;
+            }
+        ));
+        
+}  // namespace rtp_llm
\ No newline at end of file
diff --git a/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpEmbeddingOp.cc b/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpEmbeddingOp.cc
index cda131e99..24b69fb65 100644
--- a/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpEmbeddingOp.cc
+++ b/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpEmbeddingOp.cc
@@ -8,6 +8,10 @@
 #include "rtp_llm/cpp/pybind/PyUtils.h"
 #include "rtp_llm/cpp/engine_base/EngineInitParams.h"
 #include "rtp_llm/cpp/engine_base/ProposeModelEngineInitParams.h"
+#include "rtp_llm/cpp/engine_base/WeightsConverter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/config/RoleTypes.h"
 
 using namespace std;
 
@@ -15,32 +19,91 @@ namespace th = torch;
 
 namespace rtp_llm {
 
-std::tuple<GptInitParameter, std::unique_ptr<Weights>> prepareEngineInitParams(py::object model, bool sp_model);
-
 RtpEmbeddingOp::RtpEmbeddingOp() {}
 
 void RtpEmbeddingOp::init(py::object model, py::object mm_process_engine) {
     try {
-        auto [gpt_init_params, gpt_weight] = prepareEngineInitParams(model, false);
-        auto                      py_model = model.attr("py_model");
-        EngineInitParams params(0, gpt_init_params, std::move(*gpt_weight), py_model);
+        py::object config_obj = model.attr("config");
+        // Extract individual config members from Python config object
+        auto model_config = config_obj.attr("py_model_config").cast<ModelConfig>();
+        auto mm_model_config = config_obj.attr("mm_model_config").cast<MMModelConfig>();
+        auto parallelism_config = config_obj.attr("parallelism_config").cast<ParallelismConfig>();
+        auto runtime_config = config_obj.attr("runtime_config").cast<RuntimeConfig>();
+        auto pd_sep_config = config_obj.attr("pd_sep_config").cast<PDSepConfig>();
+        auto concurrency_config = config_obj.attr("concurrency_config").cast<ConcurrencyConfig>();
+        auto fmha_config = config_obj.attr("fmha_config").cast<FMHAConfig>();
+        auto kv_cache_config = config_obj.attr("kv_cache_config").cast<KVCacheConfig>();
+        auto profiling_debug_logging_config = config_obj.attr("profiling_debug_logging_config").cast<ProfilingDebugLoggingConfig>();
+        auto hw_kernel_config = config_obj.attr("hw_kernel_config").cast<HWKernelConfig>();
+        auto device_resource_config = config_obj.attr("device_resource_config").cast<DeviceResourceConfig>();
+        auto moe_config = config_obj.attr("moe_config").cast<MoeConfig>();
+        auto model_specific_config = config_obj.attr("model_specific_config").cast<ModelSpecificConfig>();
+        auto sp_config = config_obj.attr("sp_config").cast<SpeculativeExecutionConfig>();
+        auto cache_store_config = config_obj.attr("cache_store_config").cast<CacheStoreConfig>();
+        auto misc_config = config_obj.attr("misc_config").cast<MiscellaneousConfig>();
+        auto arpc_config = config_obj.attr("arpc_config").cast<ArpcConfig>();
+        auto ffn_disaggregate_config = config_obj.attr("ffn_disaggregate_config").cast<FfnDisAggregateConfig>();
+        VitConfig vit_config;
+        if (py::hasattr(config_obj, "vit_config")) {
+            py::object py_vit_config = config_obj.attr("vit_config");
+            if (!py_vit_config.is_none()) {
+                vit_config.vit_separation = py_vit_config.attr("vit_separation").cast<VitSeparation>();
+            }
+        }
+        
+        py::object py_layers_weights = model.attr("weight").attr("weights");
+        py::object py_global_weights = model.attr("weight").attr("global_weights");
+        
+        auto convert    = WeightsConverter(false, model_config.quant_algo);
+        auto gpt_weight = convert.createGptWeights(py_layers_weights, py_global_weights);
+        
+        auto py_model = model.attr("py_model");
+        EngineInitParams params(0,
+                                model_config,
+                                mm_model_config,
+                                parallelism_config,
+                                runtime_config,
+                                pd_sep_config,
+                                concurrency_config,
+                                fmha_config,
+                                kv_cache_config,
+                                profiling_debug_logging_config,
+                                hw_kernel_config,
+                                device_resource_config,
+                                moe_config,
+                                model_specific_config,
+                                sp_config,
+                                cache_store_config,
+                                misc_config,
+                                arpc_config,
+                                ffn_disaggregate_config,
+                                vit_config,
+                                std::move(*gpt_weight),
+                                py_model);
         py::object                custom_module = model.attr("custom_module");
         py::object                py_render     = model.attr("custom_module").attr("renderer");
         py::object                py_tokenizer  = model.attr("tokenizer");
         py::object                py_handler    = model.attr("custom_module").attr("handler");
 
-        if (gpt_init_params.tp_rank_ == 0) {
+        if (parallelism_config.tp_rank == 0) {
             // kmon metric init
             (void)initKmonitorFactory();
             auto kmon_tags = kmonitor::MetricsTags();
-            kmon_tags.AddTag("dp_rank", std::to_string(gpt_init_params.dp_rank_));
+            kmon_tags.AddTag("dp_rank", std::to_string(parallelism_config.dp_rank));
             params.metrics_reporter.reset(new kmonitor::MetricsReporter("", "", kmon_tags));
         }
         embedding_engine_.reset(new EmbeddingEngine(params, py_handler));
         if (!mm_process_engine.is_none()) {
-            mm_processor_.reset(new LocalMultimodalProcessor(mm_process_engine, params.gpt_init_parameter));
+            mm_processor_.reset(new LocalMultimodalProcessor(mm_process_engine, params.mm_model_config_, params.model_config_.max_seq_len));
         }
-        startRpcServer(gpt_init_params, py_render, py_tokenizer, params.metrics_reporter, mm_processor_);
+        startRpcServer(parallelism_config.model_rpc_port,
+                       arpc_config.threadNum,
+                       arpc_config.queueNum,
+                       arpc_config.ioThreadNum,
+                       py_render,
+                       py_tokenizer,
+                       params.metrics_reporter,
+                       mm_processor_);
         startHttpServer(embedding_engine_, mm_processor_, params, custom_module);
     } catch (const std::exception& e) {
         RTP_LLM_FAIL("init embedding engine failed, error msg: %s", e.what());
@@ -69,7 +132,7 @@ void RtpEmbeddingOp::startHttpServer(std::shared_ptr<EmbeddingEngine>     embedd
                                      const EngineInitParams&              params,
                                      py::object                                    custom_module) {
     http_server_.reset(new HttpApiServer(embedding_engine, mm_processor, params, custom_module));
-    std::string http_server_address("tcp:0.0.0.0:" + std::to_string(params.gpt_init_parameter.http_port_));
+    std::string http_server_address("tcp:0.0.0.0:" + std::to_string(params.parallelism_config.http_port));
     if (http_server_->start(http_server_address)) {
         RTP_LLM_LOG_INFO("embedding HTTP Server listening on %s", http_server_address.c_str());
     } else {
@@ -77,20 +140,23 @@ void RtpEmbeddingOp::startHttpServer(std::shared_ptr<EmbeddingEngine>     embedd
     }
 }
 
-void RtpEmbeddingOp::startRpcServer(const GptInitParameter&              gpt_init_params,
+void RtpEmbeddingOp::startRpcServer(int64_t model_rpc_port,
+                                    int64_t arpc_thread_num,
+                                    int64_t arpc_queue_num,
+                                    int64_t arpc_io_thread_num,
                                     py::object                                    py_render,
                                     py::object                                    py_tokenizer,
                                     kmonitor::MetricsReporterPtr                  reporter,
                                     std::shared_ptr<MultimodalProcessor> mm_processor) {
     auto arpc_service = std::move(createEmbeddingArpcService(
-        gpt_init_params, py_render, py_tokenizer, mm_processor, embedding_engine_, reporter));
+        model_rpc_port, arpc_thread_num, arpc_queue_num, arpc_io_thread_num, py_render, py_tokenizer, mm_processor, embedding_engine_, reporter));
     if (arpc_service) {
         RTP_LLM_LOG_INFO("creating arpc service");
         embedding_rpc_service_.reset(new ArpcServerWrapper(std::move(arpc_service),
-                                                                    gpt_init_params.arpc_config.threadNum,
-                                                                    gpt_init_params.arpc_config.queueNum,
-                                                                    gpt_init_params.arpc_config.ioThreadNum,
-                                                                    gpt_init_params.model_rpc_port_));
+                                                                    arpc_thread_num,
+                                                                    arpc_queue_num,
+                                                                    arpc_io_thread_num,
+                                                                    model_rpc_port));
         embedding_rpc_service_->start();
     } else {
         RTP_LLM_LOG_INFO("Embedding RPC not supported, skip");
diff --git a/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpEmbeddingOp.h b/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpEmbeddingOp.h
index 622b0a88e..953af84ae 100644
--- a/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpEmbeddingOp.h
+++ b/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpEmbeddingOp.h
@@ -3,7 +3,8 @@
 #include <optional>
 #include <pybind11/pytypes.h>
 #include <vector>
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "rtp_llm/cpp/engine_base/stream/GenerateTypes.h"
 #include "rtp_llm/cpp/metrics/RtpLLMMetrics.h"
 #include "rtp_llm/cpp/embedding_engine/EmbeddingEngine.h"
@@ -34,7 +35,10 @@ public:
                       std::vector<MultimodalInput> multimodal_inputs = {});
 
 private:
-    void startRpcServer(const GptInitParameter&              gpt_init_params,
+    void startRpcServer(int64_t model_rpc_port,
+                        int64_t arpc_thread_num,
+                        int64_t arpc_queue_num,
+                        int64_t arpc_io_thread_num,
                         py::object                                    py_render,
                         py::object                                    py_tokenizer,
                         kmonitor::MetricsReporterPtr                  reporter,
diff --git a/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpLLMOp.cc b/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpLLMOp.cc
index 57c517b6e..97e3d32c6 100644
--- a/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpLLMOp.cc
+++ b/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpLLMOp.cc
@@ -7,7 +7,8 @@
 #include <grpcpp/resource_quota.h>
 #include "rtp_llm/cpp/metrics/RtpLLMMetrics.h"
 #include "rtp_llm/cpp/utils/AssertUtils.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 #include "rtp_llm/cpp/pybind/multi_gpu_gpt/RtpLLMOp.h"
 #include "rtp_llm/cpp/engine_base/EngineInitParams.h"
 #include "rtp_llm/cpp/engine_base/ProposeModelEngineInitParams.h"
@@ -24,58 +25,69 @@ namespace th = torch;
 
 namespace rtp_llm {
 
-std::tuple<GptInitParameter, std::unique_ptr<Weights>> prepareEngineInitParams(
-    py::object model, bool sp_model)
-{
-    if (sp_model) {
-        model = model.attr("model");
-    }
-    const GptInitParameter& gpt_init_params =
-    model.attr("config").attr("gpt_init_params").cast<GptInitParameter>();
-    py::object py_layers_weights = model.attr("weight").attr("weights");
-    py::object py_global_weights = model.attr("weight").attr("global_weights");
-    
-    auto convert    = WeightsConverter(false, gpt_init_params.quant_algo_);
-    auto gpt_weight = convert.createGptWeights(py_layers_weights, py_global_weights);
-    
-    return {gpt_init_params, std::move(gpt_weight)};
-}
-
 std::unique_ptr<ProposeModelEngineInitParams> prepareMTPEngineInitParams(size_t model_id, py::object model) {
     auto        sp_model           = model.attr("model");
     std::string sp_type            = model.attr("sp_type").cast<std::string>();
-    size_t      gen_num_per_circle = model.attr("gen_num_per_circle").cast<size_t>();
     RTP_LLM_CHECK(sp_type == "mtp" || sp_type == "eagle3" || sp_type == "eagle");
 
     std::unique_ptr<std::vector<std::unique_ptr<EngineInitParams>>> mtp_params =
         std::make_unique<std::vector<std::unique_ptr<EngineInitParams>>>();
-    const GptInitParameter& gpt_init_params =
-        sp_model.attr("config").attr("gpt_init_params").cast<GptInitParameter>();
+    py::object config_obj = sp_model.attr("config");
+    // Extract individual config members from Python config object
+    auto model_config = config_obj.attr("py_model_config").cast<ModelConfig>();
+    // Assign mm_model_config to model_config.mm_model_config
+    model_config.mm_model_config = config_obj.attr("mm_model_config").cast<MMModelConfig>();
+    auto parallelism_config = config_obj.attr("parallelism_config").cast<ParallelismConfig>();
+    auto runtime_config = config_obj.attr("runtime_config").cast<RuntimeConfig>();
+    auto pd_sep_config = config_obj.attr("pd_sep_config").cast<PDSepConfig>();
+    auto concurrency_config = config_obj.attr("concurrency_config").cast<ConcurrencyConfig>();
+    auto fmha_config = config_obj.attr("fmha_config").cast<FMHAConfig>();
+    auto kv_cache_config = config_obj.attr("kv_cache_config").cast<KVCacheConfig>();
+    auto profiling_debug_logging_config = config_obj.attr("profiling_debug_logging_config").cast<ProfilingDebugLoggingConfig>();
+    auto hw_kernel_config = config_obj.attr("hw_kernel_config").cast<HWKernelConfig>();
+    auto device_resource_config = config_obj.attr("device_resource_config").cast<DeviceResourceConfig>();
+    auto moe_config = config_obj.attr("moe_config").cast<MoeConfig>();
+    auto model_specific_config = config_obj.attr("model_specific_config").cast<ModelSpecificConfig>();
+    auto sp_config = config_obj.attr("sp_config").cast<SpeculativeExecutionConfig>();
+    auto cache_store_config = config_obj.attr("cache_store_config").cast<CacheStoreConfig>();
+    auto misc_config = config_obj.attr("misc_config").cast<MiscellaneousConfig>();
+    auto arpc_config = config_obj.attr("arpc_config").cast<ArpcConfig>();
+    auto ffn_disaggregate_config = config_obj.attr("ffn_disaggregate_config").cast<FfnDisAggregateConfig>();
+    auto vit_config = config_obj.attr("vit_config").cast<VitConfig>();
+
     py::object py_layers_weights     = sp_model.attr("weight").attr("weights");
     py::object py_global_weights     = sp_model.attr("weight").attr("global_weights");
-    auto       convert               = WeightsConverter(false, gpt_init_params.quant_algo_);
+    auto       convert               = WeightsConverter(false, model_config.quant_algo);
     auto       py_layers_weights_vec = convertPyObjectToVec(py_layers_weights);
     size_t     model_num             = py_layers_weights_vec.size();
-    if (gpt_init_params.gen_num_per_circle_ > 1 && py_layers_weights_vec.size() == 1) {
-        RTP_LLM_LOG_WARNING("duplicate py_layers_weights_vec from 1 to gpt_init_params.gen_num_per_circle_: %d",
-                            gpt_init_params.gen_num_per_circle_);
-        for (size_t i = 1; i < gpt_init_params.gen_num_per_circle_; i++) {
+    size_t     gen_num_per_cycle     = sp_config.gen_num_per_cycle;
+    if (gen_num_per_cycle > 1 && py_layers_weights_vec.size() == 1) {
+        RTP_LLM_LOG_WARNING("duplicate py_layers_weights_vec from 1 to sp_config.gen_num_per_cycle: %ld",
+                            gen_num_per_cycle);
+        for (size_t i = 1; i < gen_num_per_cycle; i++) {
             py_layers_weights_vec.push_back(py_layers_weights_vec[0]);
         }
-        model_num = gpt_init_params.gen_num_per_circle_;
+        model_num = gen_num_per_cycle;
     }
-    if (gpt_init_params.gen_num_per_circle_ != py_layers_weights_vec.size()) {
-        RTP_LLM_LOG_WARNING("gpt_init_params.gen_num_per_circle_: %d  != py_layers_weights_vec.size(): %d",
-                            gpt_init_params.gen_num_per_circle_,
+    if (gen_num_per_cycle != py_layers_weights_vec.size()) {
+        RTP_LLM_LOG_WARNING("sp_config.gen_num_per_cycle: %ld  != py_layers_weights_vec.size(): %ld",
+                            gen_num_per_cycle,
                             py_layers_weights_vec.size());
-        model_num = std::min(model_num, size_t(gpt_init_params.gen_num_per_circle_));
+        model_num = std::min(model_num, size_t(gen_num_per_cycle));
     }
     if (sp_type == "eagle" || sp_type == "eagle3") {
         model_num = 1;
     }
 
-    auto no_cast_gpt_init_params        = const_cast<GptInitParameter&>(gpt_init_params);
-    no_cast_gpt_init_params.num_layers_ = 1;
+    // Get py_eplb if available
+    py::object py_eplb = py::none();
+    if (py::hasattr(config_obj, "py_eplb")) {
+        py_eplb = config_obj.attr("py_eplb");
+    }
+    
+    // Create a temporary ModelConfig with num_layers = 1 for MTP
+    ModelConfig temp_model_config = model_config;
+    temp_model_config.num_layers = 1;
 
     for (int i = 0; i < model_num; i++) {
         auto     layer_weigths = py_layers_weights_vec[i];
@@ -83,12 +95,34 @@ std::unique_ptr<ProposeModelEngineInitParams> prepareMTPEngineInitParams(size_t
         tmp.append(layer_weigths);
         auto gpt_weight = convert.createGptWeights(tmp, py_global_weights);
         mtp_params->push_back(
-            std::move(std::make_unique<EngineInitParams>(model_id, gpt_init_params, std::move(*gpt_weight))));
+            std::move(std::make_unique<EngineInitParams>(
+                model_id,
+                temp_model_config,
+                parallelism_config,
+                runtime_config,
+                pd_sep_config,
+                concurrency_config,
+                fmha_config,
+                kv_cache_config,
+                profiling_debug_logging_config,
+                hw_kernel_config,
+                device_resource_config,
+                moe_config,
+                model_specific_config,
+                sp_config,
+                cache_store_config,
+                misc_config,
+                arpc_config,
+                ffn_disaggregate_config,
+                vit_config,
+                std::move(*gpt_weight),
+                py::none(),
+                py_eplb)));
         model_id++;
     }
 
     return std::move(
-        std::make_unique<ProposeModelEngineInitParams>(sp_type, gen_num_per_circle, std::move(mtp_params)));
+        std::make_unique<ProposeModelEngineInitParams>(sp_type, gen_num_per_cycle, std::move(mtp_params)));
 };
 
 RtpLLMOp::RtpLLMOp() {}
@@ -101,7 +135,7 @@ void RtpLLMOp::init(py::object model,
 
     EngineInitParams params = initModel(model);
     RTP_LLM_LOG_INFO("init engine params success");
-    params.showGptInitParameter();
+    params.showDebugInfo();
     std::unique_ptr<ProposeModelEngineInitParams> propose_params = initProposeModel(propose_model);
     pybind11::gil_scoped_release                           release;
     grpc_server_thread_ = std::thread(&RtpLLMOp::initRPCServer,
@@ -118,17 +152,78 @@ void RtpLLMOp::init(py::object model,
 
 EngineInitParams RtpLLMOp::initModel(py::object model) {
     try {
-        auto [gpt_init_params, gpt_weight] = prepareEngineInitParams(model, false);
-        auto py_model                      = model.attr("py_model");
+        py::object config_obj = model.attr("config");
+        // Extract individual config members from Python config object
+        auto model_config = config_obj.attr("py_model_config").cast<ModelConfig>();
+        // Assign mm_model_config to model_config.mm_model_config
+        model_config.mm_model_config = config_obj.attr("mm_model_config").cast<MMModelConfig>();
+        auto parallelism_config = config_obj.attr("parallelism_config").cast<ParallelismConfig>();
+        auto runtime_config = config_obj.attr("runtime_config").cast<RuntimeConfig>();
+        auto pd_sep_config = config_obj.attr("pd_sep_config").cast<PDSepConfig>();
+        auto concurrency_config = config_obj.attr("concurrency_config").cast<ConcurrencyConfig>();
+        auto fmha_config = config_obj.attr("fmha_config").cast<FMHAConfig>();
+        auto kv_cache_config = config_obj.attr("kv_cache_config").cast<KVCacheConfig>();
+        auto profiling_debug_logging_config = config_obj.attr("profiling_debug_logging_config").cast<ProfilingDebugLoggingConfig>();
+        auto hw_kernel_config = config_obj.attr("hw_kernel_config").cast<HWKernelConfig>();
+        auto device_resource_config = config_obj.attr("device_resource_config").cast<DeviceResourceConfig>();
+        auto moe_config = config_obj.attr("moe_config").cast<MoeConfig>();
+        auto model_specific_config = config_obj.attr("model_specific_config").cast<ModelSpecificConfig>();
+        auto sp_config = config_obj.attr("sp_config").cast<SpeculativeExecutionConfig>();
+        auto cache_store_config = config_obj.attr("cache_store_config").cast<CacheStoreConfig>();
+        auto misc_config = config_obj.attr("misc_config").cast<MiscellaneousConfig>();
+        auto arpc_config = config_obj.attr("arpc_config").cast<ArpcConfig>();
+        auto ffn_disaggregate_config = config_obj.attr("ffn_disaggregate_config").cast<FfnDisAggregateConfig>();
+        VitConfig vit_config;
+        if (py::hasattr(config_obj, "vit_config")) {
+            py::object py_vit_config = config_obj.attr("vit_config");
+            if (!py_vit_config.is_none()) {
+                vit_config.vit_separation = py_vit_config.attr("vit_separation").cast<VitSeparation>();
+            }
+        }
+        
+        py::object py_layers_weights = model.attr("weight").attr("weights");
+        py::object py_global_weights = model.attr("weight").attr("global_weights");
+        
+        auto convert    = WeightsConverter(false, model_config.quant_algo);
+        auto gpt_weight = convert.createGptWeights(py_layers_weights, py_global_weights);
+        
+        auto py_model = model.attr("py_model");
         // TODO(wangyin.yx): Only one of `py_model` and `gpt_weight` is actually needed.
 
-        EngineInitParams params(model_id_, gpt_init_params, std::move(*gpt_weight), py_model);
+        // Get py_eplb if available
+        py::object py_eplb = py::none();
+        if (py::hasattr(config_obj, "py_eplb")) {
+            py_eplb = config_obj.attr("py_eplb");
+        }
+
+        EngineInitParams params(model_id_,
+                                model_config,
+                                parallelism_config,
+                                runtime_config,
+                                pd_sep_config,
+                                concurrency_config,
+                                fmha_config,
+                                kv_cache_config,
+                                profiling_debug_logging_config,
+                                hw_kernel_config,
+                                device_resource_config,
+                                moe_config,
+                                model_specific_config,
+                                sp_config,
+                                cache_store_config,
+                                misc_config,
+                                arpc_config,
+                                ffn_disaggregate_config,
+                                vit_config,
+                                std::move(*gpt_weight),
+                                py_model,
+                                py_eplb);
         model_id_++;
-        if (gpt_init_params.tp_rank_ == 0) {
+        if (parallelism_config.tp_rank == 0) {
             // kmon metric init
             (void)initKmonitorFactory();
             auto kmon_tags = kmonitor::MetricsTags();
-            kmon_tags.AddTag("dp_rank", std::to_string(gpt_init_params.dp_rank_));
+            kmon_tags.AddTag("dp_rank", std::to_string(parallelism_config.dp_rank));
             params.metrics_reporter.reset(new kmonitor::MetricsReporter("", "", kmon_tags));
         }
         return params;
@@ -145,20 +240,88 @@ std::unique_ptr<ProposeModelEngineInitParams> RtpLLMOp::initProposeModel(py::obj
         }
         std::unique_ptr<ProposeModelEngineInitParams> params = nullptr;
         std::string sp_type            = propose_model.attr("sp_type").cast<std::string>();
-        size_t      gen_num_per_circle = propose_model.attr("gen_num_per_circle").cast<size_t>();
         if (sp_type == "vanilla") {
-            auto [gpt_init_params, gpt_weight] = prepareEngineInitParams(propose_model, true);
+            py::object sp_model = propose_model.attr("model");
+            py::object config_obj = sp_model.attr("config");
+            // Extract individual config members from Python config object
+            auto model_config = config_obj.attr("py_model_config").cast<ModelConfig>();
+            // Assign mm_model_config to model_config.mm_model_config
+            model_config.mm_model_config = config_obj.attr("mm_model_config").cast<MMModelConfig>();
+            auto parallelism_config = config_obj.attr("parallelism_config").cast<ParallelismConfig>();
+            auto runtime_config = config_obj.attr("runtime_config").cast<RuntimeConfig>();
+            auto pd_sep_config = config_obj.attr("pd_sep_config").cast<PDSepConfig>();
+            auto concurrency_config = config_obj.attr("concurrency_config").cast<ConcurrencyConfig>();
+            auto fmha_config = config_obj.attr("fmha_config").cast<FMHAConfig>();
+            auto kv_cache_config = config_obj.attr("kv_cache_config").cast<KVCacheConfig>();
+            auto profiling_debug_logging_config = config_obj.attr("profiling_debug_logging_config").cast<ProfilingDebugLoggingConfig>();
+            auto hw_kernel_config = config_obj.attr("hw_kernel_config").cast<HWKernelConfig>();
+            auto device_resource_config = config_obj.attr("device_resource_config").cast<DeviceResourceConfig>();
+            auto moe_config = config_obj.attr("moe_config").cast<MoeConfig>();
+            auto model_specific_config = config_obj.attr("model_specific_config").cast<ModelSpecificConfig>();
+            auto sp_config = config_obj.attr("sp_config").cast<SpeculativeExecutionConfig>();
+            auto cache_store_config = config_obj.attr("cache_store_config").cast<CacheStoreConfig>();
+            auto misc_config = config_obj.attr("misc_config").cast<MiscellaneousConfig>();
+            auto arpc_config = config_obj.attr("arpc_config").cast<ArpcConfig>();
+            auto ffn_disaggregate_config = config_obj.attr("ffn_disaggregate_config").cast<FfnDisAggregateConfig>();
+            VitConfig vit_config;
+            if (py::hasattr(config_obj, "vit_config")) {
+                py::object py_vit_config = config_obj.attr("vit_config");
+                if (!py_vit_config.is_none()) {
+                    vit_config.vit_separation = py_vit_config.attr("vit_separation").cast<VitSeparation>();
+                }
+            }
+            
+            py::object py_layers_weights = sp_model.attr("weight").attr("weights");
+            py::object py_global_weights = sp_model.attr("weight").attr("global_weights");
+            
+            auto convert    = WeightsConverter(false, model_config.quant_algo);
+            auto gpt_weight = convert.createGptWeights(py_layers_weights, py_global_weights);
+            
+            // Get py_eplb if available
+            py::object py_eplb = py::none();
+            if (py::hasattr(config_obj, "py_eplb")) {
+                py_eplb = config_obj.attr("py_eplb");
+            }
+            
+            size_t gen_num_per_cycle = sp_config.gen_num_per_cycle;
             params                             = std::make_unique<ProposeModelEngineInitParams>(
-                model_id_, sp_type, gen_num_per_circle, gpt_init_params, std::move(*gpt_weight));
+                model_id_, sp_type, gen_num_per_cycle,
+                model_config,
+                parallelism_config,
+                runtime_config,
+                pd_sep_config,
+                concurrency_config,
+                fmha_config,
+                kv_cache_config,
+                profiling_debug_logging_config,
+                hw_kernel_config,
+                device_resource_config,
+                moe_config,
+                model_specific_config,
+                sp_config,
+                cache_store_config,
+                misc_config,
+                arpc_config,
+                ffn_disaggregate_config,
+                vit_config,
+                std::move(*gpt_weight));
             model_id_++;
         } else if (sp_type == "mtp") {
             params = prepareMTPEngineInitParams(model_id_, propose_model);
-            model_id_ += gen_num_per_circle;
+            // Get gen_num_per_cycle from sp_config
+            py::object config_obj = propose_model.attr("model").attr("config");
+            auto sp_config = config_obj.attr("sp_config").cast<SpeculativeExecutionConfig>();
+            size_t gen_num_per_cycle = sp_config.gen_num_per_cycle;
+            model_id_ += gen_num_per_cycle;
         } else if (sp_type == "eagle" || sp_type == "eagle3") {
             params = prepareMTPEngineInitParams(model_id_, propose_model);
             model_id_++;
         } else if (sp_type == "deterministic") {
-            params = std::make_unique<ProposeModelEngineInitParams>(sp_type, gen_num_per_circle);
+            // Get gen_num_per_cycle from sp_config
+            py::object config_obj = propose_model.attr("config");
+            auto sp_config = config_obj.attr("sp_config").cast<SpeculativeExecutionConfig>();
+            size_t gen_num_per_cycle = sp_config.gen_num_per_cycle;
+            params = std::make_unique<ProposeModelEngineInitParams>(sp_type, gen_num_per_cycle);
         } else {
             RTP_LLM_FAIL("sp_type %s not support", sp_type.c_str());
         }
@@ -201,9 +364,9 @@ void RtpLLMOp::initRPCServer(const EngineInitParams                        maga_
                              py::object                                             mm_process_engine,
                              std::unique_ptr<ProposeModelEngineInitParams> propose_params,
                              py::object                                             token_processor) {
-    auto http_port      = maga_init_params.gpt_init_parameter.http_port_;
-    auto model_rpc_port = maga_init_params.gpt_init_parameter.model_rpc_port_;
-    auto role_type      = maga_init_params.gpt_init_parameter.role_type_;
+    auto http_port      = maga_init_params.parallelism_config.http_port;
+    auto model_rpc_port = maga_init_params.parallelism_config.model_rpc_port;
+    auto role_type      = maga_init_params.pd_sep_config.role_type;
     // NOTE: ip/ip段可自定义为所需范围。
     std::string server_address("0.0.0.0:" + std::to_string(model_rpc_port));
     {
@@ -270,7 +433,7 @@ void RtpLLMOp::updateSchedulerInfo(const std::string& scheduler_info) {
     model_rpc_service_->getEngine()->getScheduler().updateSchedulerInfo(scheduler_info);
 }
 
-bool RtpLLMOp::updateEplbConfig(const EplbConfig& config) {
+bool RtpLLMOp::updateEplbConfig(const EPLBConfig& config) {
     if (model_rpc_service_) {
         pybind11::gil_scoped_release release;
         return model_rpc_service_->getEngine()->updateEplbConfig(config);
diff --git a/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpLLMOp.h b/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpLLMOp.h
index d7b672b97..47e6b8600 100644
--- a/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpLLMOp.h
+++ b/rtp_llm/cpp/pybind/multi_gpu_gpt/RtpLLMOp.h
@@ -32,7 +32,7 @@ public:
     KVCacheInfo        getCacheStatusInfo(int64_t latest_cache_version);
     // currently only used in BatchDecodeScheduler
     void updateSchedulerInfo(const std::string& scheduler_info);
-    bool updateEplbConfig(const EplbConfig& config);
+    bool updateEplbConfig(const EPLBConfig& config);
     void pause();
     void restart();
 
diff --git a/rtp_llm/cpp/rocm/rocmFmhaWrapper.cc b/rtp_llm/cpp/rocm/rocmFmhaWrapper.cc
index a12164cb1..337bbe586 100644
--- a/rtp_llm/cpp/rocm/rocmFmhaWrapper.cc
+++ b/rtp_llm/cpp/rocm/rocmFmhaWrapper.cc
@@ -92,13 +92,11 @@ uint32_t rocmFmhaWrapper::runCKFmha(void*  q,
     auto lse           = softmax_lse_ ? true : false;
 
     std::string msk_str;
-    if (mtype_ == AttentionMaskType::noMask) {
+    if (!is_causal_) {
         msk_str = "0";
-    } else if (mtype_ == AttentionMaskType::causalMask) {
+    } else {
         msk_str = "b";
         // RTP_LLM_LOG_INFO("Using causal_bottom_right Mask");
-    } else {
-        RTP_LLM_LOG_ERROR("Mask type not supported");
     }
 
     bias_info bias = bias_info::decode(linear_bias_slopes ? "a" : "n");
@@ -357,13 +355,11 @@ uint32_t rocmFmhaWrapper::runCKFmhaV2(void*  q,
     auto lse           = softmax_lse_ ? true : false;
 
     std::string msk_str;
-    if (mtype_ == AttentionMaskType::noMask) {
+    if (!is_causal_) {
         msk_str = "0";
-    } else if (mtype_ == AttentionMaskType::causalMask) {
+    } else {
         msk_str = "b";
         // RTP_LLM_LOG_INFO("Using causal_bottom_right Mask");
-    } else {
-        RTP_LLM_LOG_ERROR("Mask type not supported");
     }
 
     bias_info bias = bias_info::decode(linear_bias_slopes ? "a" : "n");
@@ -622,13 +618,11 @@ uint32_t rocmFmhaWrapper::runCKFmhaMLA(void*  q,
     auto lse           = softmax_lse_ ? true : false;
 
     std::string msk_str;
-    if (mtype_ == AttentionMaskType::noMask) {
+    if (!is_causal_) {
         msk_str = "0";
-    } else if (mtype_ == AttentionMaskType::causalMask) {
+    } else {
         msk_str = "b";
         // RTP_LLM_LOG_INFO("Using causal_bottom_right Mask");
-    } else {
-        RTP_LLM_LOG_ERROR("Mask type not supported");
     }
 
     bias_info bias = bias_info::decode(linear_bias_slopes ? "a" : "n");
diff --git a/rtp_llm/cpp/rocm/rocmFmhaWrapper.h b/rtp_llm/cpp/rocm/rocmFmhaWrapper.h
index 0035fabbc..42d477219 100644
--- a/rtp_llm/cpp/rocm/rocmFmhaWrapper.h
+++ b/rtp_llm/cpp/rocm/rocmFmhaWrapper.h
@@ -7,8 +7,8 @@ namespace rtp_llm {
 class rocmFmhaWrapper {
 private:
     /* data */
-    DataType          dtype_;
-    AttentionMaskType mtype_;
+    DataType dtype_;
+    bool     is_causal_;
 
     size_t head_num_;
     size_t kv_head_num_;
@@ -24,14 +24,14 @@ public:
     void init(hipStream_t stream) {
         stream_ = stream;
     }
-    void setup(DataType          dtype,
-               AttentionMaskType mtype,
-               size_t            head_num,
-               size_t            kv_head_num,
-               size_t            size_per_head,
-               float             q_scaling) {
+    void setup(DataType dtype,
+               bool     is_causal,
+               size_t   head_num,
+               size_t   kv_head_num,
+               size_t   size_per_head,
+               float    q_scaling) {
         dtype_         = dtype;
-        mtype_         = mtype;
+        is_causal_     = is_causal;
         head_num_      = head_num;
         kv_head_num_   = kv_head_num;
         size_per_head_ = size_per_head;
diff --git a/rtp_llm/cpp/speculative_engine/SpeculativeEngine.cc b/rtp_llm/cpp/speculative_engine/SpeculativeEngine.cc
index c9ba8189e..a666f9589 100644
--- a/rtp_llm/cpp/speculative_engine/SpeculativeEngine.cc
+++ b/rtp_llm/cpp/speculative_engine/SpeculativeEngine.cc
@@ -44,7 +44,7 @@ std::shared_ptr<GenerateStream> SpeculativeEngine::createMinFakeStream(int32_t m
         device_->allocateBuffer({rtp_llm::DataType::TYPE_INT32, {(size_t)1}, rtp_llm::AllocationType::HOST});
 
     // std::default_random_engine         generator;
-    // std::uniform_int_distribution<int> distribution(0, score_model_params_.gpt_init_parameter.vocab_size_ - 1);
+    // std::uniform_int_distribution<int> distribution(0, score_model_params_.model_config_.vocab_size - 1);
     // for (size_t i = 0; i < fake_input->input_ids->size(); ++i) {
     //     *fake_input->input_ids->dataWithOffset<int32_t>(i) = distribution(generator);
     // }
@@ -64,17 +64,17 @@ std::shared_ptr<GenerateStream> SpeculativeEngine::createMinFakeStream(int32_t m
     stream->fakeInitKVBlock();
 
     if (fake_hidden_states) {
-        auto      dtype = score_model_params_.gpt_init_parameter.data_type_;
+        auto      dtype = score_model_params_.model_config_.data_type;
         BufferPtr fake_hidden_states;
         if (sp_type_ == "eagle3") {
             fake_hidden_states =
                 device_->allocateBuffer({dtype,
-                                         {1, (size_t)score_model_params_.gpt_init_parameter.hidden_size_ * 3},
+                                         {1, (size_t)score_model_params_.model_config_.hidden_size * 3},
                                          rtp_llm::AllocationType::DEVICE});
         } else {
             fake_hidden_states =
                 device_->allocateBuffer({dtype,
-                                         {1, (size_t)score_model_params_.gpt_init_parameter.hidden_size_},
+                                         {1, (size_t)score_model_params_.model_config_.hidden_size},
                                          rtp_llm::AllocationType::DEVICE});
         }
         // avoid logits nan
@@ -106,13 +106,12 @@ std::shared_ptr<GenerateStream> SpeculativeEngine::createMinFakeStream(int32_t m
 absl::Status SpeculativeEngine::init() {
     RTP_LLM_LOG_INFO(__PRETTY_FUNCTION__);
     std::optional<WarmUpResult> warm_up_result = std::nullopt;
-    if (score_model_params_.gpt_init_parameter.warm_up_) {
+    if (score_model_params_.runtime_config.warm_up) {
         // warm up
-        const rtp_llm::GptInitParameter& score_gpt_params = score_model_params_.gpt_init_parameter;
         RTP_LLM_LOG_INFO("warm up (max_context_batch_size %d, max_seq_len %d calculate_loss %d) query begin",
-                         score_gpt_params.max_context_batch_size_,
-                         score_gpt_params.max_seq_len_,
-                         int(score_gpt_params.warm_up_with_loss_));
+                         score_model_params_.runtime_config.fifo_scheduler_config.max_context_batch_size,
+                         score_model_params_.model_config_.max_seq_len,
+                         int(score_model_params_.runtime_config.warm_up_with_loss));
         warm_up_result = warmUp();
         RTP_LLM_LOG_INFO(
             "warm up done, max runtime used memory: %ld bytes (%ld MiB), device reserved memory: %ld bytes (%ld MiB)",
@@ -133,18 +132,28 @@ absl::Status SpeculativeEngine::init() {
     score_executor_.reset(
         new ScoreExecutor(score_model_params_, device_, resource_context_.cache_manager, getLoraManager()));
 
-    if (score_model_params_.gpt_init_parameter.scheduler_config.use_gather_batch_scheduler) {
+    if (score_model_params_.runtime_config.use_gather_batch_scheduler) {
         RTP_LLM_LOG_INFO("create speculative gather batch scheduler");
-        scheduler_.reset(new SpeculativeGatherBatchScheduler(score_model_params_.gpt_init_parameter,
-                                                             resource_context_.cache_manager,
-                                                             metrics_reporter_,
-                                                             propose_model_params_->genNumPerCircle() + 1));
+        scheduler_.reset(new SpeculativeGatherBatchScheduler(
+            score_model_params_.runtime_config,
+            score_model_params_.model_config_,
+            score_model_params_.pd_sep_config,
+            score_model_params_.parallelism_config,
+            score_model_params_.model_specific_config,
+            resource_context_.cache_manager,
+            metrics_reporter_,
+            propose_model_params_->genNumPerCircle() + 1));
     } else {
         RTP_LLM_LOG_INFO("create speculative scheduler");
-        scheduler_.reset(new SpeculativeScheduler(score_model_params_.gpt_init_parameter,
-                                                  resource_context_.cache_manager,
-                                                  metrics_reporter_,
-                                                  propose_model_params_->genNumPerCircle() + 1));
+        scheduler_.reset(new SpeculativeScheduler(
+            score_model_params_.runtime_config,
+            score_model_params_.model_config_,
+            score_model_params_.pd_sep_config,
+            score_model_params_.parallelism_config,
+            score_model_params_.model_specific_config,
+            resource_context_.cache_manager,
+            metrics_reporter_,
+            propose_model_params_->genNumPerCircle() + 1));
     }
     speculative_sampler_ = std::make_unique<SpeculativeSampler>(device_);
     RTP_LLM_LOG_INFO("create speculative sampler");
@@ -156,7 +165,8 @@ absl::StatusOr<GenerateStreamPtr> SpeculativeEngine::preRun(const std::shared_pt
                                                             preRunMode                            mode) {
     std::shared_ptr<GenerateStream> score_stream =
         std::make_shared<NormalGenerateStream>(generate_input,
-                                               score_model_params_.gpt_init_parameter,
+                                               score_model_params_.model_config_,
+                                               score_model_params_.runtime_config,
                                                resource_context_,
                                                nullptr,
                                                0,
@@ -184,8 +194,17 @@ absl::StatusOr<GenerateStreamPtr> SpeculativeEngine::preRun(const std::shared_pt
 
 absl::Status SpeculativeEngine::initCacheManager(std::optional<WarmUpResult> warm_up_result) {
     if (propose_model_params_->draftModel()) {
-        const auto& config                 = CacheConfigCreator::createSpConfig(score_model_params_.gpt_init_parameter,
-                                                                propose_model_params_->getGptInitParameter(),
+        const auto& propose_params = propose_model_params_->getEngineInitParams();
+        const auto& config                 = CacheConfigCreator::createSpConfig(
+                                                                score_model_params_.model_config_,
+                                                                score_model_params_.parallelism_config,
+                                                                score_model_params_.runtime_config,
+                                                                score_model_params_.kv_cache_config,
+                                                                score_model_params_.sp_config,
+                                                                propose_params.model_config_,
+                                                                propose_params.parallelism_config,
+                                                                propose_params.runtime_config,
+                                                                propose_params.kv_cache_config,
                                                                 warm_up_result,
                                                                 isMTPEagle(),
                                                                 isEagle());
@@ -194,9 +213,9 @@ absl::Status SpeculativeEngine::initCacheManager(std::optional<WarmUpResult> war
         scorer_cache_config.mtp_model_type = "score_model";
         proposer_cache_config.mtp_model_type = "propose_model";
         resource_context_.cache_manager      = make_shared<CacheManager>(
-            scorer_cache_config, device_, false, metrics_reporter_, score_model_params_.gpt_init_parameter);
+            scorer_cache_config, device_, false, metrics_reporter_, score_model_params_.kv_cache_config, score_model_params_.parallelism_config, score_model_params_.runtime_config);
         if (isMTPEagle()) {
-            auto layer_num = propose_model_params_->getGptInitParameter().gen_num_per_circle_;
+            auto layer_num = propose_model_params_->genNumPerCircle();
             if (isEagle()) {
                 layer_num = 1;
             }
@@ -204,30 +223,34 @@ absl::Status SpeculativeEngine::initCacheManager(std::optional<WarmUpResult> war
             for (int i = 0; i < layer_num; i++) {
                 RTP_LLM_CHECK(proposer_cache_config.layer_num == 1);
                 resource_context_.mtp_cache_managers.push_back(std::make_shared<CacheManager>(
-                    proposer_cache_config, device_, false, metrics_reporter_, score_model_params_.gpt_init_parameter));
+                    proposer_cache_config, device_, false, metrics_reporter_, score_model_params_.kv_cache_config, score_model_params_.parallelism_config, score_model_params_.runtime_config));
             }
         } else {
             resource_context_.propose_cache_manager = make_shared<CacheManager>(
-                proposer_cache_config, device_, false, metrics_reporter_, score_model_params_.gpt_init_parameter);
+                proposer_cache_config, device_, false, metrics_reporter_, score_model_params_.kv_cache_config, score_model_params_.parallelism_config, score_model_params_.runtime_config);
         }
 
     } else {
-        const auto& config = CacheConfigCreator::createConfig(score_model_params_.gpt_init_parameter, warm_up_result);
+        const auto& config = CacheConfigCreator::createConfig(
+                                    score_model_params_.model_config_,
+                                    score_model_params_.parallelism_config,
+                                    score_model_params_.runtime_config,
+                                    score_model_params_.kv_cache_config,
+                                    warm_up_result);
         resource_context_.cache_manager = make_shared<CacheManager>(
-            config, device_, false, metrics_reporter_, score_model_params_.gpt_init_parameter);
+            config, device_, false, metrics_reporter_, score_model_params_.kv_cache_config, score_model_params_.parallelism_config, score_model_params_.runtime_config);
     }
     return absl::OkStatus();
 }
 
 WarmUpResult SpeculativeEngine::warmUp() {
-    const rtp_llm::GptInitParameter& socre_gpt_params = score_model_params_.gpt_init_parameter;
     std::shared_ptr<GenerateInput>   fake_input       = make_shared<GenerateInput>();
     fake_input->input_ids                             = device_->allocateBuffer(
-        {rtp_llm::DataType::TYPE_INT32, {(size_t)socre_gpt_params.max_seq_len_ - 1}, rtp_llm::AllocationType::HOST});
+        {rtp_llm::DataType::TYPE_INT32, {(size_t)score_model_params_.model_config_.max_seq_len - 1}, rtp_llm::AllocationType::HOST});
     std::memset(fake_input->input_ids->data(), 0, fake_input->input_ids->sizeBytes());
     fake_input->generate_config                       = make_shared<GenerateConfig>();
-    fake_input->generate_config->num_return_sequences = socre_gpt_params.max_context_batch_size_;
-    fake_input->generate_config->calculate_loss       = int(socre_gpt_params.warm_up_with_loss_);
+    fake_input->generate_config->num_return_sequences = score_model_params_.runtime_config.fifo_scheduler_config.max_context_batch_size;
+    fake_input->generate_config->calculate_loss       = int(score_model_params_.runtime_config.warm_up_with_loss);
     fake_input->generate_config->top_k                = 2;
     fake_input->begin_time_us                         = autil::TimeUtility::currentTimeInMicroSeconds();
     device_->setTraceMemory(true);
@@ -257,15 +280,15 @@ WarmUpResult SpeculativeEngine::warmUp() {
 }
 
 absl::Status SpeculativeEngine::initSystemPrompt() {
-    resource_context_.reuse_cache = score_model_params_.gpt_init_parameter.reuse_cache_;
-    resource_context_.enable_3fs  = score_model_params_.gpt_init_parameter.kv_cache_config.enable_3fs;
+    resource_context_.reuse_cache = score_model_params_.kv_cache_config.reuse_cache;
+    resource_context_.enable_3fs  = score_model_params_.kv_cache_config.enable_3fs;
     resource_context_.enable_memory_block_cache =
-        score_model_params_.gpt_init_parameter.kv_cache_config.memory_block_cache_size_mb > 0;
+        score_model_params_.kv_cache_config.memory_block_cache_size_mb > 0;
 
-    if (!score_model_params_.gpt_init_parameter.multi_task_prompt_tokens_.empty()) {
+    if (!score_model_params_.kv_cache_config.multi_task_prompt_tokens.empty()) {
         resource_context_.reuse_cache = true;
         CHECK_AND_RETURN_REF(system_prompt_param,
-                             SystemPromptConstructor::construct(score_model_params_.gpt_init_parameter,
+                             SystemPromptConstructor::construct(score_model_params_.kv_cache_config,
                                                                 this,
                                                                 resource_context_.cache_manager.get(),
                                                                 device_->getDeviceProperties().tp_rank == 0));
@@ -313,7 +336,8 @@ absl::Status SpeculativeEngine::trySaveStepError() const {
 std::shared_ptr<GenerateStream> SpeculativeEngine::makeStream(const std::shared_ptr<GenerateInput>& input) {
     std::shared_ptr<GenerateStream> stream =
         std::make_shared<NormalGenerateStream>(input,
-                                               score_model_params_.gpt_init_parameter,
+                                               score_model_params_.model_config_,
+                                               score_model_params_.runtime_config,
                                                resource_context_,
                                                metrics_reporter_,
                                                propose_model_params_->gen_num_per_circle);
@@ -361,10 +385,10 @@ absl::Status SpeculativeEngine::step() {
         CHECK_AND_ASSIGN(streams, scheduler_->schedule(reserve_step));
 
         if (streams.empty()) {
-            if (score_model_params_.gpt_init_parameter.dp_size_ > 1) {
+            if (score_model_params_.parallelism_config.dp_size > 1) {
                 CHECK_AND_ASSIGN(streams, scheduler_->schedule(reserve_step));
                 if (streams.empty()) {
-                    if (score_model_params_.gpt_init_parameter.role_type_ == RoleType::PREFILL) {
+                    if (score_model_params_.pd_sep_config.role_type == RoleType::PREFILL) {
                         streams.emplace_back(createMinFakeStream(1, false));
                     } else {
                         streams.emplace_back(createMinFakeStream(1, true));
@@ -374,8 +398,8 @@ absl::Status SpeculativeEngine::step() {
                 return absl::OkStatus();
             }
         }
-        if (score_model_params_.gpt_init_parameter.dp_size_ > 1
-            && score_model_params_.gpt_init_parameter.role_type_ != RoleType::PREFILL) {
+        if (score_model_params_.parallelism_config.dp_size > 1
+            && score_model_params_.pd_sep_config.role_type != RoleType::PREFILL) {
             bool has_hidden_states = false;
             for (auto stream : streams) {
                 if (stream->getLastHiddenStates() != nullptr) {
@@ -586,7 +610,7 @@ absl::Status SpeculativeEngine::prefillMtpStep(std::list<GenerateStreamPtr>& str
 }
 
 absl::Status SpeculativeEngine::mtpStep(std::list<GenerateStreamPtr>& streams) {
-    if (score_model_params_.gpt_init_parameter.role_type_ == RoleType::PREFILL) {
+    if (score_model_params_.pd_sep_config.role_type == RoleType::PREFILL) {
         return prefillMtpStep(streams);
     }
 
@@ -625,7 +649,7 @@ absl::Status SpeculativeEngine::mtpStep(std::list<GenerateStreamPtr>& streams) {
         bool skip_propose = propose_streams.empty();
         tpSyncDisableSPRun(skip_propose);
         // dp 情况下不允许skip propose，有可能步骤不同步
-        RTP_LLM_CHECK_WITH_INFO(score_model_params_.gpt_init_parameter.dp_size_ <= 1 || !skip_propose,
+        RTP_LLM_CHECK_WITH_INFO(score_model_params_.parallelism_config.dp_size <= 1 || !skip_propose,
                                 "skip propose not allowed now");
         if (!skip_propose) {
             RTP_LLM_LOG_DEBUG("propose step");
@@ -721,7 +745,7 @@ void SpeculativeEngine::reportMetrics() {
                                                                                                        &collector);
 }
 
-bool SpeculativeEngine::updateEplbConfig(const EplbConfig& config) {
+bool SpeculativeEngine::updateEplbConfig(const EPLBConfig& config) {
     if (score_executor_ && propose_executor_) {
         return score_executor_->updateEplbConfig(config) && propose_executor_->updateEplbConfig(config);
     }
diff --git a/rtp_llm/cpp/speculative_engine/SpeculativeEngine.h b/rtp_llm/cpp/speculative_engine/SpeculativeEngine.h
index f62df3a97..aa6a76cd5 100644
--- a/rtp_llm/cpp/speculative_engine/SpeculativeEngine.h
+++ b/rtp_llm/cpp/speculative_engine/SpeculativeEngine.h
@@ -138,7 +138,7 @@ private:
 
     std::list<GenerateStreamPtr> extractFirstPrefillStreams(std::list<GenerateStreamPtr>& streams);
 
-    bool updateEplbConfig(const EplbConfig& config) override;
+    bool updateEplbConfig(const EPLBConfig& config) override;
 
 private:
     kmonitor::MetricsReporterPtr                  metrics_reporter_ = nullptr;
diff --git a/rtp_llm/cpp/speculative_engine/SpeculativeGatherBatchScheduler.h b/rtp_llm/cpp/speculative_engine/SpeculativeGatherBatchScheduler.h
index 1cc3cfcb2..a64410f5d 100644
--- a/rtp_llm/cpp/speculative_engine/SpeculativeGatherBatchScheduler.h
+++ b/rtp_llm/cpp/speculative_engine/SpeculativeGatherBatchScheduler.h
@@ -12,12 +12,16 @@ struct SpeculativeGatherBatchSchedulerConfigLocal: public autil::legacy::Jsoniza
 
 class SpeculativeGatherBatchScheduler: public SpeculativeScheduler, public GatherBatchScheduler {
 public:
-    explicit SpeculativeGatherBatchScheduler(const rtp_llm::GptInitParameter&     params,
+    explicit SpeculativeGatherBatchScheduler(const RuntimeConfig&                 runtime_config,
+                                             const ModelConfig&                   model_config,
+                                             const PDSepConfig&                  pd_sep_config,
+                                             const ParallelismConfig&            parallelism_config,
+                                             const ModelSpecificConfig&          model_specific_config,
                                              const std::shared_ptr<CacheManager>& cache_manager,
                                              const kmonitor::MetricsReporterPtr   metrics_reporter = nullptr,
                                              const int                            max_score_len    = 1):
-        FIFOScheduler(params, cache_manager, metrics_reporter, max_score_len),
-        SpeculativeScheduler(params, cache_manager, metrics_reporter, max_score_len),
-        GatherBatchScheduler(params, cache_manager, metrics_reporter, max_score_len) {}
+        FIFOScheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager, metrics_reporter, max_score_len),
+        SpeculativeScheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager, metrics_reporter, max_score_len),
+        GatherBatchScheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager, metrics_reporter, max_score_len) {}
 };
 }  // namespace rtp_llm
\ No newline at end of file
diff --git a/rtp_llm/cpp/speculative_engine/SpeculativeScheduler.h b/rtp_llm/cpp/speculative_engine/SpeculativeScheduler.h
index 64cd77d56..77dfbdc5e 100644
--- a/rtp_llm/cpp/speculative_engine/SpeculativeScheduler.h
+++ b/rtp_llm/cpp/speculative_engine/SpeculativeScheduler.h
@@ -6,11 +6,15 @@ namespace rtp_llm {
 
 class SpeculativeScheduler: virtual public FIFOScheduler {
 public:
-    explicit SpeculativeScheduler(const rtp_llm::GptInitParameter&     params,
+    explicit SpeculativeScheduler(const RuntimeConfig&                 runtime_config,
+                                  const ModelConfig&                   model_config,
+                                  const PDSepConfig&                  pd_sep_config,
+                                  const ParallelismConfig&            parallelism_config,
+                                  const ModelSpecificConfig&          model_specific_config,
                                   const std::shared_ptr<CacheManager>& cache_manager,
                                   const kmonitor::MetricsReporterPtr   metrics_reporter = nullptr,
                                   const int                            max_score_len    = 1):
-        FIFOScheduler(params, cache_manager, metrics_reporter, max_score_len) {}
+        FIFOScheduler(runtime_config, model_config, pd_sep_config, parallelism_config, model_specific_config, cache_manager, metrics_reporter, max_score_len) {}
 
     absl::StatusOr<std::list<GenerateStreamPtr>> schedule(size_t reserve_step = 0) override;
     bool                                         empty() override {
diff --git a/rtp_llm/cpp/speculative_engine/propose_executor/DeterministicExecutor.h b/rtp_llm/cpp/speculative_engine/propose_executor/DeterministicExecutor.h
index 38551a235..865fcf656 100644
--- a/rtp_llm/cpp/speculative_engine/propose_executor/DeterministicExecutor.h
+++ b/rtp_llm/cpp/speculative_engine/propose_executor/DeterministicExecutor.h
@@ -15,7 +15,7 @@ public:
         ProposeExecutor(device) {
 
         propose_step_ = std::min(propose_model_engine_init_params->gen_num_per_circle,
-                                 (size_t)score_model_engine_init_params.gpt_init_parameter.max_seq_len_);
+                                 (size_t)score_model_engine_init_params.model_config_.max_seq_len);
 
         min_token_match_len_ = device->initParams().sp_config.sp_min_token_match;
         max_token_match_len_ = device->initParams().sp_config.sp_max_token_match;
diff --git a/rtp_llm/cpp/speculative_engine/propose_executor/MTPBatchStreamProcessor.h b/rtp_llm/cpp/speculative_engine/propose_executor/MTPBatchStreamProcessor.h
index 33ff3728b..0f68ef09f 100644
--- a/rtp_llm/cpp/speculative_engine/propose_executor/MTPBatchStreamProcessor.h
+++ b/rtp_llm/cpp/speculative_engine/propose_executor/MTPBatchStreamProcessor.h
@@ -5,8 +5,12 @@ namespace rtp_llm {
 
 class MTPBatchStreamProcessor: public NormalBatchStreamProcessor {
 public:
-    MTPBatchStreamProcessor(const rtp_llm::GptInitParameter& params, const CacheConfig& cache_config, bool warm_up):
-        NormalBatchStreamProcessor(params, cache_config, warm_up) {};
+    MTPBatchStreamProcessor(const ModelConfig& model_config,
+                               const PDSepConfig& pd_sep_config,
+                               const ProfilingDebugLoggingConfig& profiling_debug_logging_config,
+                               const CacheConfig& cache_config,
+                               bool warm_up):
+        NormalBatchStreamProcessor(model_config, pd_sep_config, profiling_debug_logging_config, cache_config, warm_up) {};
 
     absl::StatusOr<GptModelInputs> gatherModelInput(const StreamGroups& stream_groups) const override;
 };
diff --git a/rtp_llm/cpp/speculative_engine/propose_executor/MTPExecutor.cc b/rtp_llm/cpp/speculative_engine/propose_executor/MTPExecutor.cc
index bed57fc76..e1b6cb5cd 100644
--- a/rtp_llm/cpp/speculative_engine/propose_executor/MTPExecutor.cc
+++ b/rtp_llm/cpp/speculative_engine/propose_executor/MTPExecutor.cc
@@ -59,7 +59,7 @@ absl::Status MTPExecutor::propose(const std::list<GenerateStreamPtr>& streams, b
     return absl::OkStatus();
 }
 
-bool MTPExecutor::updateEplbConfig(const EplbConfig& config) {
+bool MTPExecutor::updateEplbConfig(const EPLBConfig& config) {
     for (auto& executor : mtp_executors_) {
         if (executor) {
             executor->updateEplbConfig(config);
diff --git a/rtp_llm/cpp/speculative_engine/propose_executor/MTPExecutor.h b/rtp_llm/cpp/speculative_engine/propose_executor/MTPExecutor.h
index 8f01c3acc..5872d75f3 100644
--- a/rtp_llm/cpp/speculative_engine/propose_executor/MTPExecutor.h
+++ b/rtp_llm/cpp/speculative_engine/propose_executor/MTPExecutor.h
@@ -44,11 +44,11 @@ public:
                 std::make_shared<NormalExecutor>(*mtp_params, cache_manager, device_, lora_manager, warm_up);
             const auto& cache_config = cache_manager ? cache_manager->cacheConfig() : CacheConfig();
             executor->setBatchProcessor(std::move(
-                std::make_unique<MTPBatchStreamProcessor>(mtp_params->gpt_init_parameter, cache_config, warm_up)));
+                std::make_unique<MTPBatchStreamProcessor>(mtp_params->model_config_, mtp_params->pd_sep_config, mtp_params->profiling_debug_logging_config, cache_config, warm_up)));
             auto model_params = GptModelInitParams(
                 {device_,
                  mtp_params->gpt_weights,
-                 Executor::genModelDescription(mtp_params->gpt_init_parameter),
+                 Executor::genModelDescription(mtp_params->model_config_, mtp_params->parallelism_config, mtp_params->eplb_config, mtp_params->moe_config),
                  cache_manager ? ((std::optional<KVCacheAllocator::KVCacheBuffer>)cache_manager->kvCacheBuffer()) :
                                  std::nullopt,
                  mtp_params->model_id});
@@ -91,7 +91,7 @@ public:
         return propose_step_;
     }
 
-    bool updateEplbConfig(const EplbConfig& config) override;
+    bool updateEplbConfig(const EPLBConfig& config) override;
 
 protected:
     std::string                                  sp_type_;
diff --git a/rtp_llm/cpp/speculative_engine/propose_executor/ProposeExecutor.h b/rtp_llm/cpp/speculative_engine/propose_executor/ProposeExecutor.h
index 2f2d37649..b66c4943c 100644
--- a/rtp_llm/cpp/speculative_engine/propose_executor/ProposeExecutor.h
+++ b/rtp_llm/cpp/speculative_engine/propose_executor/ProposeExecutor.h
@@ -19,7 +19,7 @@ public:
     virtual size_t       reserveStep() const                                                           = 0;
     virtual absl::Status normalProcess(const std::list<GenerateStreamPtr>& streams)                    = 0;
 
-    virtual bool updateEplbConfig(const EplbConfig& config) {
+    virtual bool updateEplbConfig(const EPLBConfig& config) {
         return true;
     }
 
diff --git a/rtp_llm/cpp/speculative_engine/score_executor/ScoreBatchStreamProcessor.h b/rtp_llm/cpp/speculative_engine/score_executor/ScoreBatchStreamProcessor.h
index da898f5af..a72c9dfbb 100644
--- a/rtp_llm/cpp/speculative_engine/score_executor/ScoreBatchStreamProcessor.h
+++ b/rtp_llm/cpp/speculative_engine/score_executor/ScoreBatchStreamProcessor.h
@@ -3,8 +3,12 @@
 namespace rtp_llm {
 class ScoreBatchStreamProcessor: public NormalBatchStreamProcessor {
 public:
-    ScoreBatchStreamProcessor(const rtp_llm::GptInitParameter& params, const CacheConfig& cache_config, bool warm_up):
-        NormalBatchStreamProcessor(params, cache_config, warm_up) {}
+    ScoreBatchStreamProcessor(const ModelConfig& model_config,
+                               const PDSepConfig& pd_sep_config,
+                               const ProfilingDebugLoggingConfig& profiling_debug_logging_config,
+                               const CacheConfig& cache_config,
+                               bool warm_up):
+        NormalBatchStreamProcessor(model_config, pd_sep_config, profiling_debug_logging_config, cache_config, warm_up) {}
     virtual absl::Status dispatch(const StreamGroups& stream_groups, const MergedOutput& merge_outputs) const override;
     virtual absl::StatusOr<GptModelInputs> gatherModelInput(const StreamGroups& stream_groups) const override;
     virtual absl::StatusOr<SamplerInputs>  gatherSamplerInput(const StreamGroups&    stream_groups,
diff --git a/rtp_llm/cpp/speculative_engine/score_executor/ScoreExecutor.cc b/rtp_llm/cpp/speculative_engine/score_executor/ScoreExecutor.cc
index 7d4b61ebf..1d7d7497b 100644
--- a/rtp_llm/cpp/speculative_engine/score_executor/ScoreExecutor.cc
+++ b/rtp_llm/cpp/speculative_engine/score_executor/ScoreExecutor.cc
@@ -45,7 +45,7 @@ absl::Status ScoreExecutor::score(const std::list<GenerateStreamPtr>& streams, b
     return absl::OkStatus();
 }
 
-bool ScoreExecutor::updateEplbConfig(const EplbConfig& config) {
+bool ScoreExecutor::updateEplbConfig(const EPLBConfig& config) {
     score_normal_executor_.updateEplbConfig(config);
     normal_executor_.updateEplbConfig(config);
     return true;
diff --git a/rtp_llm/cpp/speculative_engine/score_executor/ScoreExecutor.h b/rtp_llm/cpp/speculative_engine/score_executor/ScoreExecutor.h
index da8fcd1f1..ba69fa97c 100644
--- a/rtp_llm/cpp/speculative_engine/score_executor/ScoreExecutor.h
+++ b/rtp_llm/cpp/speculative_engine/score_executor/ScoreExecutor.h
@@ -27,7 +27,7 @@ public:
         normal_executor_(params, cache_manager, device_, lora_manager, warm_up) {
         const auto& cache_config = cache_manager ? cache_manager->cacheConfig() : CacheConfig();
         score_normal_executor_.setBatchProcessor(
-            std::move(std::make_unique<ScoreBatchStreamProcessor>(params.gpt_init_parameter, cache_config, warm_up)));
+            std::move(std::make_unique<ScoreBatchStreamProcessor>(params.model_config_, params.pd_sep_config, params.profiling_debug_logging_config, cache_config, warm_up)));
     }
 
     absl::Status normalProcess(const std::list<GenerateStreamPtr>& streams) {
@@ -36,7 +36,7 @@ public:
 
     absl::Status score(const std::list<GenerateStreamPtr>& streams, bool skip_check = false);
 
-    bool updateEplbConfig(const EplbConfig& config);
+    bool updateEplbConfig(const EPLBConfig& config);
 
 private:
     rtp_llm::DeviceBase* device_;
diff --git a/rtp_llm/cpp/speculative_engine/test/SpeculativeEngineTest.cc b/rtp_llm/cpp/speculative_engine/test/SpeculativeEngineTest.cc
index 417ceb194..c26ffc5b4 100644
--- a/rtp_llm/cpp/speculative_engine/test/SpeculativeEngineTest.cc
+++ b/rtp_llm/cpp/speculative_engine/test/SpeculativeEngineTest.cc
@@ -18,9 +18,7 @@ public:
 
 TEST_F(SpeculativeNormalEngineTest, testSimple) {
     CustomConfig config;
-    auto         gpt_init_params                    = rtp_llm::GptInitParameter();
-    gpt_init_params.fmha_config.disable_flash_infer = true;
-    auto engine                                     = createVanillaSpeculativeEngine(device_, config, gpt_init_params);
+    auto engine = createVanillaSpeculativeEngine(device_, config);
 
     ASSERT_TRUE(engine->resourceContext().cache_manager);
     ASSERT_FALSE(engine->resourceContext().system_prompt);
@@ -83,9 +81,7 @@ TEST_F(SpeculativeNormalEngineTest, testSystemPrompt) {
     vector<int>  prompt_1                           = {1, 2, 3};
     vector<int>  prompt_2                           = {4, 5, 6, 7, 8, 9};
     config.multi_task_prompt_tokens                 = {{"1", prompt_1}, {"2", prompt_2}};
-    auto gpt_init_params                            = rtp_llm::GptInitParameter();
-    gpt_init_params.fmha_config.disable_flash_infer = true;
-    auto engine                                     = createVanillaSpeculativeEngine(device_, config, gpt_init_params);
+    auto engine = createVanillaSpeculativeEngine(device_, config);
     ASSERT_TRUE(engine->resourceContext().cache_manager);
     ASSERT_TRUE(engine->resourceContext().system_prompt);
     ASSERT_TRUE(engine->resourceContext().reuse_cache);
@@ -153,9 +149,7 @@ TEST_F(SpeculativeNormalEngineTest, testSystemPrompt) {
 TEST_F(SpeculativeNormalEngineTest, testReuseCache) {
     CustomConfig config;
     config.reuse_cache                              = true;
-    auto gpt_init_params                            = rtp_llm::GptInitParameter();
-    gpt_init_params.fmha_config.disable_flash_infer = true;
-    auto engine                                     = createVanillaSpeculativeEngine(device_, config, gpt_init_params);
+    auto engine = createVanillaSpeculativeEngine(device_, config);
     ASSERT_TRUE(engine->resourceContext().reuse_cache);
     {
         std::shared_ptr<GenerateInput> query = make_shared<GenerateInput>();
diff --git a/rtp_llm/cpp/speculative_engine/test/SpeculativeMockEngine.h b/rtp_llm/cpp/speculative_engine/test/SpeculativeMockEngine.h
index 3b0a729dd..6cf0e4e1a 100644
--- a/rtp_llm/cpp/speculative_engine/test/SpeculativeMockEngine.h
+++ b/rtp_llm/cpp/speculative_engine/test/SpeculativeMockEngine.h
@@ -10,11 +10,21 @@ using namespace std;
 namespace rtp_llm {
 
 std::shared_ptr<SpeculativeEngine>
-createVanillaSpeculativeEngine(DeviceBase* device, const CustomConfig& config, GptInitParameter& params) {
-    EngineInitParams                              score_params   = createEngineInitParams(device, config, params);
-    EngineInitParams                              vanilla_params = createEngineInitParams(device, config, params);
+createVanillaSpeculativeEngine(DeviceBase* device, const CustomConfig& config) {
+    rtp_llm::ModelConfig model_config;
+    rtp_llm::RuntimeConfig runtime_config;
+    rtp_llm::KVCacheConfig kv_cache_config;
+    EngineInitParams                              score_params   = createEngineInitParams(device, config, model_config, runtime_config, kv_cache_config);
+    EngineInitParams                              vanilla_params = createEngineInitParams(device, config, model_config, runtime_config, kv_cache_config);
     std::unique_ptr<ProposeModelEngineInitParams> propose_params = std::make_unique<ProposeModelEngineInitParams>(
-        0, "vanilla", 1, vanilla_params.gpt_init_parameter, std::move(vanilla_params.gpt_weights));
+        0, "vanilla", 1, vanilla_params.model_config_, vanilla_params.mm_model_config_, vanilla_params.parallelism_config,
+        vanilla_params.runtime_config, vanilla_params.pd_sep_config,
+        vanilla_params.concurrency_config, vanilla_params.fmha_config, vanilla_params.kv_cache_config,
+        vanilla_params.profiling_debug_logging_config, vanilla_params.hw_kernel_config,
+        vanilla_params.device_resource_config, vanilla_params.moe_config, vanilla_params.model_specific_config,
+        vanilla_params.sp_config, vanilla_params.cache_store_config, vanilla_params.misc_config,
+        vanilla_params.arpc_config, vanilla_params.ffn_disaggregate_config, std::move(vanilla_params.gpt_weights),
+        py::none(), vanilla_params.py_eplb);
     std::shared_ptr<SpeculativeEngine> engine = make_shared<SpeculativeEngine>(score_params, std::move(propose_params));
     THROW_IF_STATUS_ERROR(engine->init());
     return engine;
diff --git a/rtp_llm/cpp/test/BUILD b/rtp_llm/cpp/test/BUILD
index ebe2ead5e..d3245c845 100644
--- a/rtp_llm/cpp/test/BUILD
+++ b/rtp_llm/cpp/test/BUILD
@@ -18,7 +18,7 @@ cc_library(
 test_deps = [
     "//rtp_llm/cpp/devices/testing:device_test_utils",
     "//rtp_llm/cpp/models:models",
-    "//rtp_llm/cpp/config:gpt_init_params",
+    "//rtp_llm/cpp/config:config_modules",
     "//rtp_llm/cpp/engine_base:weights_converter",
     ":test_headers",
     "@com_google_googletest//:gtest",
diff --git a/rtp_llm/cpp/test/GptModelTest.cc b/rtp_llm/cpp/test/GptModelTest.cc
index 95fc57b68..c0feec76c 100644
--- a/rtp_llm/cpp/test/GptModelTest.cc
+++ b/rtp_llm/cpp/test/GptModelTest.cc
@@ -28,7 +28,7 @@ TEST_F(GptModelTest, testSimple) {
     attention_conf.rope_config.style     = RopeStyle::Base;
     attention_conf.rope_config.dim       = 64;
     attention_conf.rope_config.base      = 1000000;
-    attention_conf.mask_type             = AttentionMaskType::causalMask;
+    attention_conf.is_causal             = true;
 
     const auto  cache_block_num = 128;
     CacheConfig cache_config(KVCacheParam({static_cast<uint>(weights->layers.size()),
diff --git a/rtp_llm/cpp/test/PyModelTest.cc b/rtp_llm/cpp/test/PyModelTest.cc
index 567b5e457..c3380b868 100644
--- a/rtp_llm/cpp/test/PyModelTest.cc
+++ b/rtp_llm/cpp/test/PyModelTest.cc
@@ -32,7 +32,7 @@ TEST_F(PyModelTest, testSimple) {
     attention_conf.rope_config.style     = RopeStyle::Base;
     attention_conf.rope_config.dim       = 64;
     attention_conf.rope_config.base      = 1000000;
-    attention_conf.mask_type             = AttentionMaskType::causalMask;
+    attention_conf.is_causal             = true;
 
     const auto  cache_block_num = 128;
     CacheConfig cache_config(KVCacheParam({static_cast<uint>(weights->layers.size()),
diff --git a/rtp_llm/device/device_base.py b/rtp_llm/device/device_base.py
index 54dd17bee..8889a9f75 100644
--- a/rtp_llm/device/device_base.py
+++ b/rtp_llm/device/device_base.py
@@ -18,8 +18,8 @@ class MemInfo:
 class DeviceBase:
     def __init__(self, exported_device: DeviceExporter):
         self.exported_device = exported_device
-        self.py_env_configs = PyEnvConfigs()
-        self.py_env_configs.update_from_env()
+        from rtp_llm.server.server_args.server_args import setup_args
+        self.py_env_configs = setup_args()
 
     def get_device_type(self) -> DeviceType:
         return self.exported_device.get_device_type()
diff --git a/rtp_llm/device/device_impl.py b/rtp_llm/device/device_impl.py
index 015fbdf01..134036b13 100644
--- a/rtp_llm/device/device_impl.py
+++ b/rtp_llm/device/device_impl.py
@@ -140,7 +140,7 @@ class GpuImpl(DeviceBase):
 
     @property
     def specify_gpu_arch(self):
-        return self.py_env_configs.py_device_resource_config.specify_gpu_arch
+        return self.py_env_configs.runtime_config.specify_gpu_arch
 
     def apply_int8(self, tensor: torch.Tensor, device: str):
         shape = tensor.shape
@@ -347,7 +347,7 @@ class RocmImpl(GpuImpl):
                 logging.warn(f"Cannot get ROCm device gfx version: {e}")
         # 如果无法获取，则使用环境变量或默认值
         specify_gpu_arch = (
-            self.py_env_configs.py_device_resource_config.specify_gpu_arch
+            self.py_env_configs.runtime_config.specify_gpu_arch
         )
         return "900" if specify_gpu_arch == "" else specify_gpu_arch
 
@@ -438,7 +438,7 @@ class RocmImpl(GpuImpl):
                 logging.warn(f"Cannot get ROCm device gfx version: {e}")
         # 如果无法获取，则使用环境变量或默认值
         specify_gpu_arch = (
-            self.py_env_configs.py_device_resource_config.specify_gpu_arch
+            self.py_env_configs.runtime_config.specify_gpu_arch
         )
         return "900" if specify_gpu_arch == "" else specify_gpu_arch
 
diff --git a/rtp_llm/distribute/gang_info.py b/rtp_llm/distribute/gang_info.py
index 93de32237..188cb30b4 100644
--- a/rtp_llm/distribute/gang_info.py
+++ b/rtp_llm/distribute/gang_info.py
@@ -5,16 +5,19 @@ import os
 import socket
 from typing import Any, Dict, List, NamedTuple, Optional
 
-from rtp_llm.config.py_config_modules import StaticConfig
 from rtp_llm.distribute.worker_info import WorkerInfo, g_parallel_info, g_worker_info
 
 CONFIG_FILE_ENV = "DISTRIBUTE_CONFIG_FILE"
 JSON_GANG_PARTS_ENV = "JSON_GANG_PARTS"
-from rtp_llm.config.py_config_modules import StaticConfig
-from rtp_llm.distribute.worker_info import WorkerInfo, g_parallel_info, g_worker_info
 
 
-def members_from_json(gang_info_json: Dict[str, Any]) -> List[WorkerInfo]:
+def members_from_json(gang_info_json: Dict[str, Any], zone_name: Optional[str] = None) -> List[WorkerInfo]:
+    """Create members list from JSON gang info.
+    
+    Args:
+        gang_info_json: Dictionary containing gang member information.
+        zone_name: Zone name to filter members. If None, no filtering is applied.
+    """
     members: List[WorkerInfo] = []
     # here is only the fake ip
     for name, info in gang_info_json.items():
@@ -38,7 +41,6 @@ def members_from_json(gang_info_json: Dict[str, Any]) -> List[WorkerInfo]:
                 info=info,
             )
         )
-    zone_name = StaticConfig.gang_config.zone_name
     if zone_name:
         members = [
             member for member in members if member.name.split("_")[-2] == zone_name
@@ -98,12 +100,17 @@ app.c2.io/biz-detail-ganginfo="{\"llama13B_2A10_PCIE_1_inference_part0\":{\"name
 """
 
 
-def get_c2_members():
-    file_name = StaticConfig.gang_config.gang_annocation_path
-    if not os.path.exists(file_name):
-        raise Exception(f"not found file: {file_name}")
+def get_c2_members(gang_annocation_path: str, zone_name: Optional[str] = None) -> List[WorkerInfo]:
+    """Get members from C2 annotation file.
+    
+    Args:
+        gang_annocation_path: Path to gang annotation file.
+        zone_name: Zone name to filter members. If None, no filtering is applied.
+    """
+    if not os.path.exists(gang_annocation_path):
+        raise Exception(f"not found file: {gang_annocation_path}")
 
-    with open(file_name, "r") as reader:
+    with open(gang_annocation_path, "r") as reader:
         content = reader.read()
 
     infos = [x for x in content.split("\n") if "app.c2.io/biz-detail-ganginfo" in x]
@@ -114,7 +121,7 @@ def get_c2_members():
     logging.info(f"gang info: {gang_info[gang_info.index('=') + 2: -1]}")
     gang_info_json = json.loads(gang_info[gang_info.index("=") + 2 : -1])
     logging.info(f"gang info json: {gang_info_json}")
-    return members_from_json(gang_info_json)
+    return members_from_json(gang_info_json, zone_name)
 
 
 def get_leader_ip(leader_address: str) -> str:
@@ -125,10 +132,15 @@ def get_leader_ip(leader_address: str) -> str:
         return socket.gethostbyname(leader_address)
 
 
-def get_leader_members(env_str: str) -> List[WorkerInfo]:
+def get_leader_members(env_str: str, zone_name: Optional[str] = None) -> List[WorkerInfo]:
+    """Get leader members from leader address.
+    
+    Args:
+        env_str: Leader address string.
+        zone_name: Zone name. If None, uses default naming.
+    """
     ip_str = get_leader_ip(env_str)
     members: List[WorkerInfo] = []
-    zone_name = StaticConfig.gang_config.zone_name
     member_info = {}
     member_info["name"] = "part0"
     if zone_name:
@@ -185,15 +197,26 @@ def get_leader_members(env_str: str) -> List[WorkerInfo]:
     return members
 
 
-def get_members_from_file():
-    file = StaticConfig.gang_config.distribute_config_file
-    with open(file, "r") as reader:
+def get_members_from_file(distribute_config_file: str, zone_name: Optional[str] = None) -> List[WorkerInfo]:
+    """Get members from config file.
+    
+    Args:
+        distribute_config_file: Path to distribute config file.
+        zone_name: Zone name to filter members. If None, no filtering is applied.
+    """
+    with open(distribute_config_file, "r") as reader:
         config_json = json.loads(reader.read())
-    return members_from_json(config_json)
+    return members_from_json(config_json, zone_name)
 
 
-def get_members_from_json_env(env_str: str) -> List[WorkerInfo]:
-    return members_from_json(json.loads(env_str))
+def get_members_from_json_env(env_str: str, zone_name: Optional[str] = None) -> List[WorkerInfo]:
+    """Get members from JSON environment string.
+    
+    Args:
+        env_str: JSON string containing gang member information.
+        zone_name: Zone name to filter members. If None, no filtering is applied.
+    """
+    return members_from_json(json.loads(env_str), zone_name)
 
 
 class GangInfo(NamedTuple):
@@ -207,35 +230,53 @@ class GangInfo(NamedTuple):
         return [member for member in self.members if not member.equals(self.master)]
 
 
-def get_gang_info() -> GangInfo:
+def get_gang_info(
+    distribute_config_file: Optional[str] = None,
+    gang_config_string: Optional[str] = None,
+    json_gang_parts: Optional[str] = None,
+    leader_address: Optional[str] = None,
+    gang_annocation_path: Optional[str] = None,
+    zone_name: Optional[str] = None,
+) -> GangInfo:
+    """Get gang information from configuration.
+    
+    Args:
+        distribute_config_file: Path to distribute config file.
+        gang_config_string: Gang config string for distributed test.
+        json_gang_parts: JSON string containing gang parts.
+        leader_address: Leader address for leader-based configuration.
+        gang_annocation_path: Path to gang annotation file.
+        zone_name: Zone name to filter members.
+    """
     only_leader = False
+    
     if g_parallel_info.local_world_size < g_parallel_info.world_size:
         # from config file
-        if StaticConfig.gang_config.distribute_config_file:
-            members = get_members_from_file()
+        if distribute_config_file:
+            members = get_members_from_file(distribute_config_file, zone_name)
         # for distributed test
-        elif StaticConfig.gang_config.gang_config_string:
+        elif gang_config_string:
             logging.info(
-                f"use GANG_CONFIG_STRING: {StaticConfig.gang_config.gang_config_string}"
+                f"use GANG_CONFIG_STRING: {gang_config_string}"
             )
-            members = members_from_test_env(StaticConfig.gang_config.gang_config_string)
+            members = members_from_test_env(gang_config_string)
         # from env json
-        elif StaticConfig.gang_config.json_gang_parts:
+        elif json_gang_parts:
             logging.info(
-                f"use JSON_GANG_PARTS_ENV: {StaticConfig.gang_config.json_gang_parts}"
-            )
-            members = get_members_from_json_env(
-                StaticConfig.gang_config.json_gang_parts
+                f"use JSON_GANG_PARTS_ENV: {json_gang_parts}"
             )
+            members = get_members_from_json_env(json_gang_parts, zone_name)
         # for lws
-        elif StaticConfig.gang_config.leader_address:
+        elif leader_address:
             logging.info(
-                f"use LEADER_ADDRESS: {StaticConfig.gang_config.leader_address}"
+                f"use LEADER_ADDRESS: {leader_address}"
             )
-            members = get_leader_members(StaticConfig.gang_config.leader_address)
+            members = get_leader_members(leader_address, zone_name)
         # from c2 annotation
         else:
-            members = get_c2_members()
+            if gang_annocation_path is None:
+                raise ValueError("gang_annocation_path must be provided when other config options are not set")
+            members = get_c2_members(gang_annocation_path, zone_name)
     else:
         members = [
             WorkerInfo(
@@ -256,7 +297,7 @@ def get_gang_info() -> GangInfo:
                 None,
             )
         ]
-    if StaticConfig.gang_config.leader_address:
+    if leader_address:
         only_leader = True
 
     # 假设 GPU 均匀分布，可以整除
diff --git a/rtp_llm/distribute/gang_server.py b/rtp_llm/distribute/gang_server.py
index 06c42f63d..2de6bd5aa 100644
--- a/rtp_llm/distribute/gang_server.py
+++ b/rtp_llm/distribute/gang_server.py
@@ -15,7 +15,7 @@ import uvicorn
 from fastapi import FastAPI
 import torch.distributed
 
-from rtp_llm.config.py_config_modules import PyEnvConfigs, StaticConfig
+from rtp_llm.config.py_config_modules import GangConfig, ServerConfig
 from rtp_llm.config.uvicorn_config import UVICORN_LOGGING_CONFIG
 from rtp_llm.distribute.gang_info import GangInfo, get_gang_info
 
@@ -79,7 +79,9 @@ class FailedRankInfo:
 
 
 class GangServer:
-    def __init__(self, py_env_configs: PyEnvConfigs):
+    def __init__(self, gang_config: GangConfig, server_config: ServerConfig):
+        self.gang_config = gang_config
+        self.server_config = server_config
         self._initialized: bool = False
         self._gang_status: Dict[str, str] = {}
         self._gang_parts: Dict[str, str] = {}
@@ -88,7 +90,6 @@ class GangServer:
         self._failure_events: Dict[int, FailedRankInfo] = {}
         self._delay_exit_loops: int = 0
         self._max_delay_times: int = 3
-        self.py_env_configs = py_env_configs
 
     def _start_server(self):
         app = FastAPI()
@@ -111,7 +112,7 @@ class GangServer:
         @app.post("/broadcast_parts")
         def broadcast_parts(req: Dict[str, Any]):
             logging.debug("broadcast parts recv: %s", json.dumps(req))
-            StaticConfig.gang_config.json_gang_parts = json.dumps(req)
+            self.gang_config.json_gang_parts = json.dumps(req)
 
         @app.post("/report_failure")
         def handle_failure_report(req: Dict[str, Any]):
@@ -208,13 +209,20 @@ class GangServer:
             )
 
     def _wait_ready(self):
-        timeout_minutes = self.py_env_configs.gang_config.gang_timeout_min
-        sleep_time = self.py_env_configs.gang_config.gang_sleep_time
+        timeout_minutes = self.gang_config.gang_timeout_min
+        sleep_time = self.gang_config.gang_sleep_time
         start_time = datetime.datetime.now()
         retry_time = 0
         while True:
             try:
-                self._gang_info = get_gang_info()
+                self._gang_info = get_gang_info(
+                    distribute_config_file=self.gang_config.distribute_config_file,
+                    gang_config_string=self.gang_config.gang_config_string,
+                    json_gang_parts=self.gang_config.json_gang_parts,
+                    leader_address=self.gang_config.leader_address,
+                    gang_annocation_path=self.gang_config.gang_annocation_path,
+                    zone_name=self.gang_config.zone_name,
+                )
                 if self._gang_info.only_leader:
                     self._exchange_gang_info(self._gang_info)
                     self._check_gang_info(self._gang_info)
@@ -294,7 +302,7 @@ class GangServer:
                         part_info["name"] = name
                         part_info["ip"] = address
                         parts[name] = part_info
-                    StaticConfig.gang_config.json_gang_parts = json.dumps(parts)
+                    self.gang_config.json_gang_parts = json.dumps(parts)
                     try:
                         result = requests.post(broadcast_url, json=parts, timeout=1)
                     except:
@@ -383,7 +391,7 @@ class GangServer:
             logging.error(f"first failure node is {first_failure_info}")
 
     def start_health_check(self):
-        sleep_time = self.py_env_configs.gang_config.gang_sleep_time
+        sleep_time = self.gang_config.gang_sleep_time
 
         def wrapper():
             while True:
@@ -413,7 +421,7 @@ class GangServer:
     def start(self):
         if g_parallel_info.world_size == 1:
             logging.info("world_size==1, do not start gang_server")
-            update_master_info("", self.py_env_configs.server_config.start_port)
+            update_master_info("", self.server_config.start_port)
             return
         self._start()
         self._wait_ready()
@@ -425,7 +433,7 @@ class GangServer:
             f"tcp://{g_master_info.ip}:{self._gang_info.master.server_port - 1}"
         )
         logging.info(f"gang worker {g_parallel_info} init_process_group {master_url}")
-        init_process_timeout = self.py_env_configs.gang_config.dist_barrier_timeout
+        init_process_timeout = self.gang_config.dist_barrier_timeout
         os.environ["TORCH_DIST_INIT_BARRIER"] = "1"
         torch.distributed.init_process_group(
             backend=torch.distributed.Backend.NCCL,
diff --git a/rtp_llm/distribute/test/fake_model.py b/rtp_llm/distribute/test/fake_model.py
index bd8de1c9e..003b2fce6 100644
--- a/rtp_llm/distribute/test/fake_model.py
+++ b/rtp_llm/distribute/test/fake_model.py
@@ -1,6 +1,6 @@
 from typing import Any, List
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.base_model import BaseModel
 
@@ -36,19 +36,15 @@ class FakeModel(BaseModel):
         pass
 
     @classmethod
-    def create_config(cls, model_config: Any) -> GptInitModelParameters:
-        config = GptInitModelParameters(
-            head_num=2,
-            size_per_head=128,
-            layer_num=2,
-            max_seq_len=2048,
-            vocab_size=500000,
-            multi_task_prompt=None,
-        )
-        config.lora_infos = None
-        config.multi_task_prompt = None
-        config.is_sparse_head = False
-        config.tokenizer_path = model_config.tokenizer_path
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
+        config = ModelConfig()
+        config.head_num_ = 2
+        config.size_per_head_ = 128
+        config.num_layers = 2
+        config.max_seq_len = 2048
+        config.vocab_size = 500000
+        config.lora_infos = {}
+        config.is_sparse_head_ = False
         return config
 
     @classmethod
diff --git a/rtp_llm/distribute/test/gang_test.py b/rtp_llm/distribute/test/gang_test.py
index 34780e311..b6739dc52 100644
--- a/rtp_llm/distribute/test/gang_test.py
+++ b/rtp_llm/distribute/test/gang_test.py
@@ -12,12 +12,11 @@ from unittest import mock
 import requests
 import torch
 
-from rtp_llm.config.py_config_modules import StaticConfig
-
 torch.cuda.set_device = lambda x: None
 
 from rtp_llm.distribute.gang_info import get_c2_members, get_gang_info
 from rtp_llm.distribute.worker_info import WorkerInfo, g_parallel_info
+from rtp_llm.config.py_config_modules import MIN_WORKER_INFO_PORT_NUM
 from rtp_llm.openai.openai_endpoint import OpenaiEndpoint
 from rtp_llm.frontend.frontend_server import FrontendWorker
 from rtp_llm.start_backend_server import main
@@ -48,9 +47,8 @@ class GangTest(unittest.TestCase):
         {"GANG_ANNOCATION_PATH": "rtp_llm/distribute/test/testdata/annocation"},
     )
     def test_annocation(self):
-        StaticConfig.update_from_env()
-        # os.environ['GANG_ANNOCATION_PATH'] = "rtp_llm/distribute/test/testdata/annocation"
-        gang_members = get_c2_members()
+        gang_annocation_path = os.environ.get("GANG_ANNOCATION_PATH", "rtp_llm/distribute/test/testdata/annocation")
+        gang_members = get_c2_members(gang_annocation_path)
         self.assertEqual(len(gang_members), 2)
         self.assertEqual(gang_members[0].name, "llama_7b_a10_part2_new_inference_part0")
         self.assertEqual(gang_members[0].ip, "33.115.125.211")
@@ -69,8 +67,8 @@ class GangTest(unittest.TestCase):
         },
     )
     def test_multi_gpu_gang_info(self):
-        StaticConfig.update_from_env()
-        g_parallel_info.reload()
+        worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+        g_parallel_info.reload(worker_info_port_num)
         gang_info = get_gang_info()
         self.assertEqual(len(gang_info.members), 2)
         self.assertEqual(gang_info.members[0].ip, self.get_self_ip())
@@ -96,9 +94,10 @@ class GangTest(unittest.TestCase):
         },
     )
     def test_multi_worker_gang_info(self):
-        StaticConfig.update_from_env()
-        g_parallel_info.reload()
-        gang_info = get_gang_info()
+        worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+        g_parallel_info.reload(worker_info_port_num)
+        gang_annocation_path = os.environ.get("GANG_ANNOCATION_PATH", "rtp_llm/distribute/test/testdata/annocation")
+        gang_info = get_gang_info(gang_annocation_path=gang_annocation_path)
         self.assertEqual(len(gang_info.members), 2)
         self.assertEqual(gang_info.members[0].ip, "33.115.125.211")
         self.assertEqual(
@@ -127,9 +126,10 @@ class GangTest(unittest.TestCase):
         },
     )
     def test_multi_worker_gpu_gang_info(self):
-        StaticConfig.update_from_env()
-        g_parallel_info.reload()
-        gang_info = get_gang_info()
+        worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+        g_parallel_info.reload(worker_info_port_num)
+        gang_annocation_path = os.environ.get("GANG_ANNOCATION_PATH", "rtp_llm/distribute/test/testdata/annocation")
+        gang_info = get_gang_info(gang_annocation_path=gang_annocation_path)
         self.assertEqual(len(gang_info.members), 4)
         self.assertEqual(gang_info.members[0].ip, "33.115.125.211")
         self.assertEqual(
@@ -175,9 +175,10 @@ class GangTest(unittest.TestCase):
         },
     )
     def test_multi_worker_gang_info_from_json(self):
-        StaticConfig.update_from_env()
-        g_parallel_info.reload()
-        gang_info = get_gang_info()
+        worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+        g_parallel_info.reload(worker_info_port_num)
+        distribute_config_file = os.environ.get("DISTRIBUTE_CONFIG_FILE", "rtp_llm/distribute/test/testdata/parallel.json")
+        gang_info = get_gang_info(distribute_config_file=distribute_config_file)
         self.assertEqual(len(gang_info.members), 2)
         self.assertEqual(gang_info.members[0].ip, "11.161.48.116")
         self.assertEqual(
@@ -190,7 +191,6 @@ class GangTest(unittest.TestCase):
             gang_info.members[1].name, "llama13B_2A10_PCIE_1_inference_part1_0"
         )
         self.assertEqual(gang_info.members[1].server_port, 20000)
-        StaticConfig.gang_config.distribute_config_file = ""
 
     @mock.patch("torch.cuda.device_count")
     @mock.patch.dict(
@@ -216,16 +216,15 @@ class GangTest(unittest.TestCase):
         },
     )
     def test_server_start(self, torch_device_count):
-        StaticConfig.update_from_env()
         try:
             multiprocessing.set_start_method("spawn")
         except RuntimeError as e:
             logging.warn(str(e))
 
         torch_device_count.return_value = 2
-        g_parallel_info.reload()
+        worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+        g_parallel_info.reload(worker_info_port_num)
         procs: List[Process] = list()
-        StaticConfig.update_from_env()
         procs = main()
 
         try:
@@ -265,8 +264,8 @@ class GangTest(unittest.TestCase):
         },
     )
     def test_get_world_rank_from_world_index(self):
-        StaticConfig.update_from_env()
-        g_parallel_info.reload()
+        worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+        g_parallel_info.reload(worker_info_port_num)
         self.assertEqual(g_parallel_info.world_rank, 2)
 
     @mock.patch.dict(
@@ -282,9 +281,11 @@ class GangTest(unittest.TestCase):
         },
     )
     def test_multi_worker_gang_info_from_leader(self):
-        StaticConfig.update_from_env()
-        g_parallel_info.reload()
-        gang_info = get_gang_info()
+        worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+        g_parallel_info.reload(worker_info_port_num)
+        leader_address = os.environ.get("LEADER_ADDRESS", "33.115.125.211")
+        zone_name = os.environ.get("ZONE_NAME", "prefill")
+        gang_info = get_gang_info(leader_address=leader_address, zone_name=zone_name)
         self.assertEqual(len(gang_info.members), 2)
         self.assertEqual(gang_info.members[0].ip, "33.115.125.211")
         self.assertEqual(gang_info.members[0].name, "prefill_part0_0")
@@ -296,8 +297,6 @@ class GangTest(unittest.TestCase):
         self.assertEqual(
             gang_info.members[1].server_port, WorkerInfo.server_port_offset(0)
         )
-        StaticConfig.gang_config.zone_name = ""
-        StaticConfig.gang_config.leader_address = None
 
     @mock.patch("torch.cuda.device_count")
     @mock.patch.dict(
@@ -324,16 +323,15 @@ class GangTest(unittest.TestCase):
         },
     )
     def test_server_start_leader(self, torch_device_count):
-        StaticConfig.update_from_env()
         try:
             multiprocessing.set_start_method("spawn")
         except RuntimeError as e:
             logging.warn(str(e))
 
         torch_device_count.return_value = 2
-        g_parallel_info.reload()
+        worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+        g_parallel_info.reload(worker_info_port_num)
         procs: List[Process] = list()
-        StaticConfig.update_from_env()
         procs = main()
 
         try:
@@ -360,7 +358,6 @@ class GangTest(unittest.TestCase):
             self.assertEqual(hb_response.json()["initializing"], False)
 
         finally:
-            StaticConfig.gang_config.leader_address = None
             for proc in procs:
                 if proc.is_alive():
                     proc.terminate()
@@ -390,16 +387,15 @@ class GangTest(unittest.TestCase):
         },
     )
     def test_server_start_worker(self, torch_device_count):
-        StaticConfig.update_from_env()
         try:
             multiprocessing.set_start_method("spawn")
         except RuntimeError as e:
             logging.warn(str(e))
 
         torch_device_count.return_value = 2
-        g_parallel_info.reload()
+        worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+        g_parallel_info.reload(worker_info_port_num)
         procs: List[Process] = list()
-        StaticConfig.update_from_env()
         procs = main()
 
         try:
@@ -436,7 +432,6 @@ class GangTest(unittest.TestCase):
             self.assertEqual(hb_response.json()["initializing"], False)
 
         finally:
-            StaticConfig.gang_config.leader_address = None
             for proc in procs:
                 if proc.is_alive():
                     proc.terminate()
diff --git a/rtp_llm/distribute/worker_info.py b/rtp_llm/distribute/worker_info.py
index 5ccd6fc32..12150f5ec 100644
--- a/rtp_llm/distribute/worker_info.py
+++ b/rtp_llm/distribute/worker_info.py
@@ -10,26 +10,9 @@ import torch
 from rtp_llm.config.py_config_modules import (
     MASTER_INFO_PORT_NUM,
     MIN_WORKER_INFO_PORT_NUM,
-    WORKER_INFO_PORT_NUM,
-    StaticConfig,
 )
 
 
-def get_worker_port_num():
-    global WORKER_INFO_PORT_NUM
-    global MIN_WORKER_INFO_PORT_NUM
-    WORKER_INFO_PORT_NUM = int(StaticConfig.worker_config.worker_info_port_num)
-    logging.info(f"env WORKER_INFO_PORT_NUM: {WORKER_INFO_PORT_NUM}")
-    if WORKER_INFO_PORT_NUM < MIN_WORKER_INFO_PORT_NUM:
-        raise Exception(
-            f"env worker info port num {WORKER_INFO_PORT_NUM} "
-            f"is small than min worker info port num {MIN_WORKER_INFO_PORT_NUM}"
-        )
-
-
-get_worker_port_num()
-
-
 class FrontendServerInfo(object):
     def __init__(self, frontend_server_id: int):
         self.frontend_server_id = frontend_server_id
@@ -37,7 +20,6 @@ class FrontendServerInfo(object):
     def __str__(self):
         return f"FrontendServerInfo:[ frontend_server_id={self.frontend_server_id} ]"
 
-
 class ParallelInfo(object):
     # EP从TP里分
     def __init__(
@@ -50,6 +32,7 @@ class ParallelInfo(object):
         world_size: int,
         world_rank: int,
         local_world_size: int,
+        worker_info_port_num: int,
     ):
         self.tp_size = tp_size
         self.ep_size = ep_size
@@ -60,6 +43,15 @@ class ParallelInfo(object):
         self.world_size = world_size
         self.world_rank = world_rank
         self.local_world_size = local_world_size
+        self.worker_info_port_num = int(worker_info_port_num)
+        
+        logging.info(f"ParallelInfo worker_info_port_num: {self.worker_info_port_num}")
+        
+        if self.worker_info_port_num < MIN_WORKER_INFO_PORT_NUM:
+            raise Exception(
+                f"worker info port num {self.worker_info_port_num} "
+                f"is smaller than min worker info port num {MIN_WORKER_INFO_PORT_NUM}"
+            )
         logging.info(
             f"ParallelInfo:[ tp_size={self.tp_size} ep_size={self.ep_size} pp_size={self.pp_size} world_size={self.world_size} world_rank={self.world_rank} local_world_size={self.local_world_size} ffn_sp_size={self.ffn_sp_size} ffn_tp_size={self.ffn_tp_size}]"
         )
@@ -100,19 +92,20 @@ class ParallelInfo(object):
         return self.world_rank == 0
 
     @staticmethod
-    def from_env() -> "ParallelInfo":
-        return ParallelInfo.from_params(dict(os.environ))
+    def from_env(worker_info_port_num: int) -> "ParallelInfo":
+        return ParallelInfo.from_params(dict(os.environ), worker_info_port_num)
 
     @staticmethod
-    def from_params(params: Dict[str, str]) -> "ParallelInfo":
+    def from_params(params: Dict[str, str], worker_info_port_num: int) -> "ParallelInfo":
         world_size = int(params.get("WORLD_SIZE", "1"))
         if "LOCAL_WORLD_SIZE" in params:
             local_world_size = int(params["LOCAL_WORLD_SIZE"])
         else:
             local_world_size = min(torch.cuda.device_count(), world_size)
-            local_world_size = max(
-                local_world_size, 1
-            )  # make sure local_world_size >= 1
+        local_world_size = max(
+            local_world_size, 1
+        )  # make sure local_world_size >= 1
+        
         info = ParallelInfo(
             tp_size=int(params.get("TP_SIZE", "1")),
             ep_size=int(params.get("EP_SIZE", params.get("WORLD_SIZE", "1"))),
@@ -122,6 +115,7 @@ class ParallelInfo(object):
             world_size=world_size,
             world_rank=int(params.get("WORLD_RANK", "0")),
             local_world_size=local_world_size,
+            worker_info_port_num=worker_info_port_num,
         )
         if ("WORLD_INDEX" in params) and ("WORLD_RANK" not in params):
             world_index = int(params["WORLD_INDEX"])
@@ -179,24 +173,23 @@ class ParallelInfo(object):
                     f"try decode ACCL_NIC_GPU_AFFINITY failed, content is {content}"
                 )
 
+        logging.info(f"ParallelInfo from_params: {info}")
         return info
 
     # used for ut
-    def reload(self):
-        new_info = self.from_env()
+    def reload(self, worker_info_port_num: int):
+        new_info = self.from_env(worker_info_port_num)
         self.tp_size = new_info.tp_size
         self.pp_size = new_info.pp_size
         self.world_size = new_info.world_size
         self.world_rank = new_info.world_rank
         self.local_world_size = new_info.local_world_size
+        self.worker_info_port_num = new_info.worker_info_port_num
+        logging.info(f"ParallelInfo reload: {self}")
 
     def __str__(self):
         return f"ParallelInfo:[ tp_size={self.tp_size} pp_size={self.pp_size} world_size={self.world_size} world_rank={self.world_rank} local_world_size={self.local_world_size} tp_rank={self.tp_rank} dp_rank={self.dp_rank} ep_size={self.ep_size} dp_size={self.dp_size} ep_rank={self.ep_rank} local_rank={self.local_rank} ffn_sp_size={self.ffn_sp_size} ]"
 
-
-g_parallel_info = ParallelInfo.from_env()
-
-
 class WorkerInfo(object):
     def __init__(
         self,
@@ -236,81 +229,82 @@ class WorkerInfo(object):
         return self.ip == other.ip and self.server_port == other.server_port
 
     @staticmethod
-    def from_env():
+    def from_env(start_port):
+        worker_info_port_num = g_parallel_info.worker_info_port_num
+        local_rank = g_parallel_info.local_rank
+        world_rank = g_parallel_info.world_rank
+
         info = WorkerInfo(
             ip=socket.gethostbyname(socket.gethostname()),
-            server_port=WorkerInfo.server_port_offset(g_parallel_info.local_rank),
-            gang_hb_port=WorkerInfo.gang_hb_port_offset(g_parallel_info.local_rank),
-            http_port=WorkerInfo.http_port_offset(g_parallel_info.local_rank),
+            server_port=WorkerInfo.server_port_offset(local_rank, start_port, worker_info_port_num),
+            gang_hb_port=WorkerInfo.gang_hb_port_offset(local_rank, start_port, worker_info_port_num),
+            http_port=WorkerInfo.http_port_offset(local_rank, start_port, worker_info_port_num),
             rpc_server_port=WorkerInfo.rpc_server_port_offset(
-                g_parallel_info.local_rank
+                local_rank, start_port, worker_info_port_num
             ),
             remote_rpc_server_port=WorkerInfo.rpc_server_port_offset(
-                g_parallel_info.local_rank, int(os.environ.get("REMOTE_SERVER_PORT", 0))
+                local_rank, int(os.environ.get("REMOTE_SERVER_PORT", 0)), worker_info_port_num
             ),
             cache_store_listen_port=WorkerInfo.cache_store_listen_port_offset(
-                g_parallel_info.local_rank
+                local_rank, start_port, worker_info_port_num
             ),
             cache_store_connect_port=WorkerInfo.cache_store_listen_port_offset(
-                g_parallel_info.local_rank, int(os.environ.get("REMOTE_SERVER_PORT", 0))
+                local_rank, int(os.environ.get("REMOTE_SERVER_PORT", 0)), worker_info_port_num
             ),
             cache_store_rdma_listen_port=WorkerInfo.cache_store_rdma_listen_port_offset(
-                g_parallel_info.local_rank
+                local_rank, start_port, worker_info_port_num
             ),
             cache_store_rdma_connect_port=WorkerInfo.cache_store_rdma_listen_port_offset(
-                g_parallel_info.local_rank, int(os.environ.get("REMOTE_SERVER_PORT", 0))
+                local_rank, int(os.environ.get("REMOTE_SERVER_PORT", 0)), worker_info_port_num
             ),
             backend_server_port=WorkerInfo.backend_server_port_offset(
-                g_parallel_info.local_rank
+                local_rank, start_port, worker_info_port_num
             ),
-            local_rank=g_parallel_info.local_rank,
-            world_rank=g_parallel_info.world_rank,
+            local_rank=local_rank,
+            world_rank=world_rank,
             name="",
             info=None,
         )
-        return info
+        logging.info(f"WorkerInfo from_env: {info}")
 
-    @staticmethod
-    def self_server_port():
-        return StaticConfig.server_config.start_port
+        return info
 
     @staticmethod
-    def server_port_offset(local_rank: int, server_port: int = -1) -> int:
-        if server_port != -1:
-            base_port = server_port
-        else:
-            base_port = WorkerInfo.self_server_port()
-        return base_port + local_rank * WORKER_INFO_PORT_NUM
+    def server_port_offset(local_rank: int, server_port: int, worker_info_port_num: int = None) -> int:
+        base_port = server_port
+        if worker_info_port_num is None:
+            worker_info_port_num = g_parallel_info.worker_info_port_num
+        return base_port + local_rank * worker_info_port_num
 
     @staticmethod
-    def rpc_server_port_offset(local_rank: int, server_port: int = -1) -> int:
-        return WorkerInfo.server_port_offset(local_rank, server_port) + 1
+    def rpc_server_port_offset(local_rank: int, server_port: int, worker_info_port_num: int = None) -> int:
+        return WorkerInfo.server_port_offset(local_rank, server_port, worker_info_port_num) + 1
 
     @staticmethod
-    def cache_store_listen_port_offset(local_rank: int, server_port: int = -1) -> int:
-        return WorkerInfo.server_port_offset(local_rank, server_port) + 2
+    def cache_store_listen_port_offset(local_rank: int, server_port: int, worker_info_port_num: int = None) -> int:
+        return WorkerInfo.server_port_offset(local_rank, server_port, worker_info_port_num) + 2
 
     @staticmethod
-    def gang_hb_port_offset(local_rank: int, server_port: int = -1) -> int:
-        return WorkerInfo.server_port_offset(local_rank, server_port) + 3
+    def gang_hb_port_offset(local_rank: int, server_port: int, worker_info_port_num: int = None) -> int:
+        return WorkerInfo.server_port_offset(local_rank, server_port, worker_info_port_num) + 3
 
     @staticmethod
     def cache_store_rdma_listen_port_offset(
-        local_rank: int, server_port: int = -1
+        local_rank: int, server_port: int, worker_info_port_num: int = None
     ) -> int:
-        return WorkerInfo.server_port_offset(local_rank, server_port) + 4
+        return WorkerInfo.server_port_offset(local_rank, server_port, worker_info_port_num) + 4
 
     @staticmethod
-    def http_port_offset(local_rank: int, server_port: int = -1) -> int:
-        return WorkerInfo.server_port_offset(local_rank, server_port) + 5
+    def http_port_offset(local_rank: int, server_port: int, worker_info_port_num: int = None) -> int:
+        return WorkerInfo.server_port_offset(local_rank, server_port, worker_info_port_num) + 5
 
     @staticmethod
-    def backend_server_port_offset(local_rank: int, server_port: int = -1) -> int:
-        return WorkerInfo.server_port_offset(local_rank, server_port) + 6
+    def backend_server_port_offset(local_rank: int, server_port: int, worker_info_port_num: int = None) -> int:
+        return WorkerInfo.server_port_offset(local_rank, server_port, worker_info_port_num) + 6
 
     # used for ut
-    def reload(self):
-        new_info = self.from_env()
+    def reload(self, start_port):
+        new_info = self.from_env(start_port)
         self.ip = new_info.ip
         self.server_port = new_info.server_port
         self.gang_hb_port = new_info.gang_hb_port
@@ -324,6 +318,7 @@ class WorkerInfo(object):
         self.world_rank = new_info.world_rank
         self.name = new_info.name
         self.info = new_info.info
+        logging.info(f"WorkerInfo reload: {self}")
 
     def __str__(self):
         return f"""
@@ -333,10 +328,6 @@ class WorkerInfo(object):
         local_rank={self.local_rank} world_rank={self.world_rank} name={self.name} info={self.info} ]
         """
 
-
-g_worker_info = WorkerInfo.from_env()
-
-
 @dataclass
 class MasterInfo:
     ip: str
@@ -371,10 +362,20 @@ def update_master_info(ip: str, base_port: int):
     g_master_info.ffn_tp_nccl_port = base_port - 5
     if g_parallel_info.ffn_sp_size != g_parallel_info.tp_size:
         base_port -= g_parallel_info.ffn_sp_size
+    logging.info(f"g_master_info: {g_master_info}")
 
 
 def total_need_port_num() -> int:
     return (
         MASTER_INFO_PORT_NUM * g_parallel_info.dp_size
-        + WORKER_INFO_PORT_NUM * g_parallel_info.tp_size
+        + g_parallel_info.worker_info_port_num * g_parallel_info.tp_size
     )
+
+
+# Initialize global variables after class definitions
+g_parallel_info = ParallelInfo.from_env(MIN_WORKER_INFO_PORT_NUM)
+g_worker_info = WorkerInfo.from_env(start_port=0)
+
+def update_worker_info(start_port: int, worker_info_port_num: int):
+    g_parallel_info.reload(worker_info_port_num)
+    g_worker_info.reload(start_port)
diff --git a/rtp_llm/eplb/ep_balancer.py b/rtp_llm/eplb/ep_balancer.py
index 34c45cc79..e1a414393 100644
--- a/rtp_llm/eplb/ep_balancer.py
+++ b/rtp_llm/eplb/ep_balancer.py
@@ -8,11 +8,11 @@ import traceback
 from collections import deque
 from enum import Enum
 from queue import Queue
-from typing import Any, Deque, Sequence
+from typing import Any, Deque, Optional, Sequence
 
 import torch
 
-from rtp_llm.config.py_config_modules import PyEnvConfigs
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.device import get_current_device
 from rtp_llm.eplb.eplb import rebalance_experts
 from rtp_llm.model_loader.load_config import LoadConfig
@@ -57,14 +57,29 @@ class ExpertBalancer:
         compute_dtype: torch.dtype,
         phy2log: Any,
         database: BaseDatabase,
-        py_env_configs: PyEnvConfigs,
+        model_config: Optional[ModelConfig] = None,
     ):
+        """
+        Initialize ExpertBalancer.
+        
+        Args:
+            weights_info: Model weight information
+            compute_dtype: Compute data type
+            phy2log: Physical to logical expert mapping
+            database: Database for loading weights
+            model_config: Optional ModelConfig (used to get eplb_config and use_float32)
+        """
         self.database: BaseDatabase = database
         self._weights_info: ModelDeployWeightInfo = weights_info
         self._model_weight_info: ModelWeightInfo = (
             self._weights_info.create_model_weight_info(database)
         )
-        use_fp32 = py_env_configs.model_config.use_float32
+        
+        # Get use_float32 from model_config
+        use_fp32 = False
+        if model_config is not None:
+            use_fp32 = model_config.use_float32
+        
         if use_fp32:
             compute_dtype = torch.float32
 
@@ -81,14 +96,23 @@ class ExpertBalancer:
 
         self.time_prefix = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
         self.queue: Queue[int] = Queue()
-        select_layer_method = py_env_configs.py_eplb_config.balance_method
-        self.select_layer_method = SelectLayerMethod(select_layer_method)
+        
+        # Get balance_method from model_config.eplb_config
+        balance_method = "mix"  # default
+        if model_config is not None:
+            balance_method = model_config.eplb_config.balance_method
+        
+        self.select_layer_method = SelectLayerMethod(balance_method)
         self.phy2log = phy2log
         self.update_cnt = 0
 
-        window_size = py_env_configs.py_eplb_config.eplb_stats_window_size
+        # Get window_size from model_config.eplb_config
+        eplb_stats_window_size = 10  # default
+        if model_config is not None:
+            eplb_stats_window_size = model_config.eplb_config.eplb_stats_window_size
+        
         self.history_log_stats = HistoryStats(
-            window_size=window_size,
+            window_size=eplb_stats_window_size,
             shape=(self._load_config.num_layers, self._load_config.expert_num),
         )
 
@@ -100,7 +124,12 @@ class ExpertBalancer:
             dtype=torch.int32,
         )
 
-        self.force_repack = py_env_configs.py_eplb_config.eplb_force_repack == 1
+        # Get force_repack from model_config.eplb_config
+        eplb_force_repack = 0  # default
+        if model_config is not None:
+            eplb_force_repack = model_config.eplb_config.eplb_force_repack
+        
+        self.force_repack = eplb_force_repack == 1
 
     @torch.inference_mode()
     def get_balanced_layer(self, gpu_loads: torch.Tensor) -> int:
diff --git a/rtp_llm/frontend/frontend_app.py b/rtp_llm/frontend/frontend_app.py
index 2bdc9bb67..1fc2275be 100644
--- a/rtp_llm/frontend/frontend_app.py
+++ b/rtp_llm/frontend/frontend_app.py
@@ -16,7 +16,7 @@ from typing_extensions import override
 from uvicorn import Config, Server
 from uvicorn.loops.auto import auto_loop_setup
 
-from rtp_llm.config.py_config_modules import PyEnvConfigs, StaticConfig
+from rtp_llm.config.py_config_modules import PyEnvConfigs, ServerConfig
 from rtp_llm.config.uvicorn_config import UVICORN_LOGGING_CONFIG
 from rtp_llm.distribute.worker_info import WorkerInfo, g_worker_info
 from rtp_llm.frontend.frontend_server import FrontendServer
@@ -49,25 +49,26 @@ class GracefulShutdownServer(Server):
 class FrontendApp(object):
     def __init__(
         self,
-        py_env_configs: PyEnvConfigs = StaticConfig,
+        py_env_configs: PyEnvConfigs,
         separated_frontend: bool = False,
     ):
-        self.py_env_configs = py_env_configs
+        self.server_config = py_env_configs.server_config
         self.frontend_server = FrontendServer(
             separated_frontend,
-            py_env_configs.server_config.rank_id,
-            py_env_configs.server_config.frontend_server_id,
+            self.server_config.rank_id,
+            self.server_config.frontend_server_id,
+            py_env_configs,
         )
         self.separated_frontend = separated_frontend
         g_worker_info.server_port = WorkerInfo.server_port_offset(
-            self.py_env_configs.server_config.rank_id, g_worker_info.server_port
+            self.server_config.rank_id, g_worker_info.server_port
         )
         g_worker_info.backend_server_port = WorkerInfo.server_port_offset(
-            self.py_env_configs.server_config.rank_id, g_worker_info.backend_server_port
+            self.server_config.rank_id, g_worker_info.backend_server_port
         )
         logging.info(
-            f"rank_id = {self.py_env_configs.server_config.rank_id}, "
-            f"server_port = {g_worker_info.server_port}, backend_server_port = {g_worker_info.backend_server_port}, frontend_server_id = {py_env_configs.server_config.frontend_server_id}"
+            f"rank_id = {self.server_config.rank_id}, "
+            f"server_port = {g_worker_info.server_port}, backend_server_port = {g_worker_info.backend_server_port}, frontend_server_id = {self.server_config.frontend_server_id}"
         )
 
     def start(self):
@@ -86,7 +87,7 @@ class FrontendApp(object):
         sock.bind(("0.0.0.0", g_worker_info.server_port))
         sock.listen()
         fd = sock.fileno()
-        timeout_keep_alive = self.py_env_configs.server_config.timeout_keep_alive
+        timeout_keep_alive = self.server_config.timeout_keep_alive
 
         config = Config(
             app,
diff --git a/rtp_llm/frontend/frontend_server.py b/rtp_llm/frontend/frontend_server.py
index cfcc9ecc1..6106ffebd 100644
--- a/rtp_llm/frontend/frontend_server.py
+++ b/rtp_llm/frontend/frontend_server.py
@@ -10,9 +10,8 @@ from fastapi.responses import ORJSONResponse, StreamingResponse
 from pydantic import BaseModel
 
 from rtp_llm.access_logger.access_logger import AccessLogger
-from rtp_llm.config.generate_config import RoleType
-from rtp_llm.config.py_config_modules import StaticConfig
-from rtp_llm.config.task_type import TaskType
+from rtp_llm.config.model_config import get_task_type_from_ckpt_path
+from rtp_llm.ops import TaskType
 from rtp_llm.frontend.frontend_worker import FrontendWorker, TokenizerEncodeResponse
 from rtp_llm.metrics import AccMetrics, GaugeMetrics, kmonitor
 from rtp_llm.openai.api_datatype import ChatCompletionRequest
@@ -34,9 +33,10 @@ USAGE_HEADER = "USAGE"
 
 class FrontendServer(object):
     def __init__(
-        self, separated_frontend: bool = False, rank_id: int = 0, server_id: int = 0
+        self, separated_frontend: bool = False, rank_id: int = 0, server_id: int = 0, py_env_configs = None
     ):
-        self._access_logger = AccessLogger()
+        self.py_env_configs = py_env_configs
+        self._access_logger = AccessLogger(py_env_configs.profiling_debug_config.log_path, py_env_configs.profiling_debug_config.log_file_backup_count)
         self._frontend_worker = None
         self._openai_endpoint = None
         self.thread_lock_ = threading.Lock()
@@ -47,26 +47,59 @@ class FrontendServer(object):
         kmonitor.init()
 
     def start(self):
-        if StaticConfig.profiling_debug_config.debug_start_fake_process == 1:
+        if self.py_env_configs.profiling_debug_config.debug_start_fake_process == 1:
             # for debug online
             logging.info("DEBUG_START_FAKE_PROCESS is set, start fake server")
             self._frontend_worker = None
-        else:
-            self._frontend_worker = FrontendWorker(self.separated_frontend)
-            self._openai_endpoint = None
-            self.is_embedding = False
-            if (
-                self._frontend_worker.model_config is not None
-                and self._frontend_worker.model_config.task_type
-                != TaskType.LANGUAGE_MODEL
-            ):
-                self.is_embedding = True
-            else:
-                self._openai_endpoint = OpenaiEndpoint(
-                    self._frontend_worker.model_config,
-                    self._frontend_worker.tokenizer,
-                    self._frontend_worker.backend_rpc_server_visitor,
-                )
+            return
+
+        # Initialize SpecialTokens before creating FrontendWorker
+        # This will be reused by both FrontendWorker and OpenaiEndpoint
+        from rtp_llm.ops import SpecialTokens
+        from rtp_llm.config.model_config import (
+            update_stop_words_from_env,
+            update_tokenizer_special_tokens,
+        )
+        
+        # Create a temporary tokenizer to initialize special_tokens
+        # We'll update it with the actual tokenizer after FrontendWorker is created
+        special_tokens = SpecialTokens()
+        if self.py_env_configs.generate_env_config:
+            update_stop_words_from_env(special_tokens, self.py_env_configs.generate_env_config)
+        
+        # Create FrontendWorker with special_tokens
+        self._frontend_worker = FrontendWorker(self.separated_frontend, self.py_env_configs, special_tokens)
+        
+        # Update special_tokens with actual tokenizer
+        update_tokenizer_special_tokens(special_tokens, self._frontend_worker.tokenizer)
+        
+        self._openai_endpoint = None
+        self.is_embedding = False
+        
+        # Get task_type from ckpt_path
+        ckpt_path = self.py_env_configs.model_args.ckpt_path
+        task_type = get_task_type_from_ckpt_path(
+            ckpt_path,
+            self.py_env_configs.embedding_config,
+        )
+        
+        # Only initialize OpenaiEndpoint for LANGUAGE_MODEL task type
+        if task_type == TaskType.LANGUAGE_MODEL:
+            
+            self._openai_endpoint = OpenaiEndpoint(
+                model_args=self.py_env_configs.model_args,
+                generate_env_config=self.py_env_configs.generate_env_config,
+                render_config=self.py_env_configs.render_config,
+                misc_config=self.py_env_configs.misc_config,
+                vit_config=self.py_env_configs.vit_config,
+                special_tokens=special_tokens,
+                max_seq_len=self.py_env_configs.model_args.max_seq_len,
+                template_type=None,
+                model_name=self.py_env_configs.model_args.model_type,
+                ckpt_path=ckpt_path,
+                tokenizer=self._frontend_worker.tokenizer,
+                backend_rpc_server_visitor=self._frontend_worker.backend_rpc_server_visitor,
+            )
 
     def stop(self):
         if self._frontend_worker is not None:
diff --git a/rtp_llm/frontend/frontend_worker.py b/rtp_llm/frontend/frontend_worker.py
index 95c994a01..5657d682f 100644
--- a/rtp_llm/frontend/frontend_worker.py
+++ b/rtp_llm/frontend/frontend_worker.py
@@ -7,8 +7,6 @@ import traceback
 from functools import partial
 from typing import Any, AsyncGenerator, Dict, List, Optional, Set, Tuple, Union
 
-from rtp_llm.config.py_config_modules import StaticConfig
-
 current_file_path = pathlib.Path(__file__).parent.absolute()
 sys.path.append(str(current_file_path.parent.absolute()))
 
@@ -17,7 +15,6 @@ from pydantic import BaseModel
 from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
 from rtp_llm.config.generate_config import GenerateConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizer_factory import TokenizerFactory
-from rtp_llm.model_factory import ModelFactory
 from rtp_llm.pipeline.pipeline import Pipeline
 from rtp_llm.structure.request_extractor import Request, RequestExtractor
 from rtp_llm.utils.base_model_datatypes import GenerateResponse
@@ -25,7 +22,6 @@ from rtp_llm.utils.complete_response_async_generator import (
     CompleteResponseAsyncGenerator,
 )
 
-
 class PipelineResponse(BaseModel):
     response: str = ""
     finished: bool = True
@@ -55,16 +51,51 @@ class TokenizerEncodeResponse(BaseModel):
 
 
 class FrontendWorker:
-    def __init__(self, separated_frontend: bool) -> None:
+    def __init__(self, separated_frontend: bool, py_env_configs, special_tokens=None) -> None:
         logging.info("starting frontend worker")
-        self.model_config = ModelFactory.create_frontend_config(
-            ModelFactory.create_normal_model_config()
+        from rtp_llm.config.engine_config import EngineConfig
+        from rtp_llm.ops import SpecialTokens
+        from rtp_llm.config.model_config import (
+            update_stop_words_from_env,
+            update_tokenizer_special_tokens,
+        )
+        
+        # Get tokenizer parameters from py_env_configs.model_args
+        ckpt_path = py_env_configs.model_args.ckpt_path or ""
+        tokenizer_path = py_env_configs.model_args.tokenizer_path or ckpt_path
+        # Get model_type from py_env_configs
+        model_type = py_env_configs.model_args.model_type
+        # Paths should already be local after fetch_model_files_to_local()
+        self.tokenizer = TokenizerFactory.create(ckpt_path, tokenizer_path, model_type)   
+        engine_config = EngineConfig.create(py_env_configs, gang_info=None)
+        
+        # Use provided special_tokens or create new one
+        if special_tokens is None:
+            special_tokens = SpecialTokens()
+            update_stop_words_from_env(special_tokens, py_env_configs.generate_env_config)
+        # Update special_tokens with tokenizer (needed for tokenizer-specific tokens)
+        update_tokenizer_special_tokens(special_tokens, self.tokenizer)
+        
+        # Get max_seq_len and seq_size_per_block
+        max_seq_len = py_env_configs.model_args.max_seq_len
+        seq_size_per_block = py_env_configs.kv_cache_config.seq_size_per_block
+
+        self.pipeline = Pipeline(
+            special_tokens=special_tokens,
+            pd_sep_config=engine_config.pd_sep_config,
+            runtime_config=engine_config.runtime_config,
+            ffn_disaggregate_config=engine_config.parallelism_config.ffn_disaggregate_config,
+            max_seq_len=max_seq_len,
+            seq_size_per_block=seq_size_per_block,
+            tokenizer=self.tokenizer,
+            sp_config=py_env_configs.sp_config,
+            separated_frontend=separated_frontend,
+            mm_related_params=None,  # Frontend doesn't need mm_related_params
+            gang_info=None,
         )
-        self.tokenizer = TokenizerFactory.create_from_env()
-        self.model_config.update_task_prompt_tokens_id(self.tokenizer)
-        self.model_config.update_tokenizer_special_tokens(self.tokenizer)
-        self.pipeline = Pipeline(self.model_config, self.tokenizer, separated_frontend)
         self.backend_rpc_server_visitor = self.pipeline.backend_rpc_server_visitor
+        self.generate_env_config = py_env_configs.generate_env_config
+
         logging.info("frontend worker start done.")
 
     def tokenizer_offset_mapping(self, prompt: str) -> Any:
@@ -218,6 +249,7 @@ class FrontendWorker:
             request_id=request_id,
             urls=urls,
             generate_config=generate_config,
+            generate_env_config=self.generate_env_config,
             **kwargs,
         )
         async for generate_response in stream:
diff --git a/rtp_llm/frontend/test/frontend_worker_test.py b/rtp_llm/frontend/test/frontend_worker_test.py
index 4042a2e03..6dc487ce5 100644
--- a/rtp_llm/frontend/test/frontend_worker_test.py
+++ b/rtp_llm/frontend/test/frontend_worker_test.py
@@ -44,7 +44,7 @@ class FrontendWorkerTest(TestCase):
         port_list, _ = PortManager().get_consecutive_ports(1)
         os.environ["START_PORT"] = str(port_list[0])
         update_master_info("0.0.0.0", int(port_list[0]))
-        g_worker_info.reload()
+        g_worker_info.reload(port_list[0])
         self.fake_model_loader = FakeModelLoader(
             model_type="llama",
             tokenizer_path=self.tokenizer_path,
@@ -52,7 +52,16 @@ class FrontendWorkerTest(TestCase):
             max_seq_len=2048,
         )
         model: AsyncModel = self.fake_model_loader.load_model()
-        pipeline = Pipeline(model.config, model.tokenizer)
+        pipeline = Pipeline(
+            special_tokens=model.model.py_model_config.special_tokens,
+            pd_sep_config=model.model.engine_config.pd_sep_config,
+            runtime_config=model.model.engine_config.runtime_config,
+            ffn_disaggregate_config=model.model.engine_config.parallelism_config.ffn_disaggregate_config,
+            max_seq_len=model.model.py_model_config.max_seq_len,
+            seq_size_per_block=model.model.engine_config.kv_cache_config.seq_size_per_block,
+            tokenizer=model.tokenizer,
+            py_env_configs=model.model.py_env_configs,
+        )
         return FakeFrontendWorker(model, pipeline)
 
     async def _run(self, frontend_worker, **kwargs):
diff --git a/rtp_llm/frontend/tokenizer_factory/tokenizer_factory.py b/rtp_llm/frontend/tokenizer_factory/tokenizer_factory.py
index 4de08461d..ed0776ae4 100644
--- a/rtp_llm/frontend/tokenizer_factory/tokenizer_factory.py
+++ b/rtp_llm/frontend/tokenizer_factory/tokenizer_factory.py
@@ -1,41 +1,27 @@
 import json
 import logging
 import os
-import sys
-from typing import Any, Dict, Optional, Type, Union
 
-from transformers import AutoTokenizer
-
-from rtp_llm.config.py_config_modules import StaticConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizer_factory_register import (
     _tokenizer_factory,
 )
 from rtp_llm.frontend.tokenizer_factory.tokenizers.base_tokenizer import BaseTokenizer
-from rtp_llm.utils.fuser import fetch_remote_file_to_local
-from rtp_llm.utils.util import check_with_info
 
 
 class TokenizerFactory:
-    @staticmethod
-    def create_from_env():
-        ckpt_path = StaticConfig.model_config.checkpoint_path
-        tokenizer_path = StaticConfig.model_config.tokenizer_path
-        model_type = StaticConfig.model_config.model_type
-
-        tokenizer_path = fetch_remote_file_to_local(tokenizer_path)
-        ckpt_path = fetch_remote_file_to_local(ckpt_path)
-
-        return TokenizerFactory.create(ckpt_path, tokenizer_path, model_type)
-
-    @staticmethod
-    def create_from_config(config):
-        ckpt_path = config.ckpt_path
-        tokenizer_path = config.tokenizer_path
-        model_type = StaticConfig.model_config.model_type
-        return TokenizerFactory.create(ckpt_path, tokenizer_path, model_type)
-
     @staticmethod
     def create(ckpt_path: str, tokenizer_path: str, model_type: str):
+        """
+        Create a tokenizer from the given parameters.
+        
+        Args:
+            ckpt_path: Path to the checkpoint directory
+            tokenizer_path: Path to the tokenizer directory or file
+            model_type: Type of the model (e.g., "gpt", "llama", etc.)
+        
+        Returns:
+            A tokenizer instance
+        """
         global _tokenizer_factory
         config_json = {}
         config_json_path = os.path.join(ckpt_path, "config.json")
diff --git a/rtp_llm/frontend/tokenizer_factory/tokenizers/base_tokenizer.py b/rtp_llm/frontend/tokenizer_factory/tokenizers/base_tokenizer.py
index a2d1b8e51..d96755d4a 100644
--- a/rtp_llm/frontend/tokenizer_factory/tokenizers/base_tokenizer.py
+++ b/rtp_llm/frontend/tokenizer_factory/tokenizers/base_tokenizer.py
@@ -113,7 +113,7 @@ class BaseTokenizer:
 
     @property
     def special_tokens_map(self):
-        return self.tokenizer.special_tokens_map
+        return self.tokenizer.special_tokensmap
 
     def save_pretrained(self, save_directory, **kwargs):
         return self.tokenizer.save_pretrained(save_directory, **kwargs)
diff --git a/rtp_llm/lora/lora_manager.py b/rtp_llm/lora/lora_manager.py
index 898fe16c7..20b0e7565 100644
--- a/rtp_llm/lora/lora_manager.py
+++ b/rtp_llm/lora/lora_manager.py
@@ -19,13 +19,12 @@ class LoraManager:
     database_: CkptDatabase
     weights_loader_: ModelLoader
 
-    def __init__(self, model: AsyncModel) -> None:
+    def __init__(self, model: AsyncModel, max_lora_model_size: int = -1) -> None:
         self.model_ = model
         self.lora_infos_ = {}
-        self.max_lora_model_size_ = (
-            model.config.py_env_configs.model_specific_config.max_lora_model_size
-        )
-        self.device: str = self.model_.model.device
+        self.max_lora_model_size_ = max_lora_model_size
+        from rtp_llm.distribute.worker_info import g_parallel_info
+        self.device: str = g_parallel_info.device
         assert isinstance(self.model_.decoder_engine_, RPCEngine)
         self.lora_cpp_wrapper_ = self.model_.decoder_engine_.rtp_llm_op_.ft_op
         assert isinstance(self.model_.model.database, CkptDatabase)
@@ -33,7 +32,7 @@ class LoraManager:
         assert isinstance(self.model_.model.model_weights_loader, ModelLoader)
         self.weights_loader_ = self.model_.model.model_weights_loader
         with Timer() as timer:
-            model_lora_infos = self.model_.model.config.lora_infos
+            model_lora_infos = self.model_.model.py_model_config.lora_infos
             if model_lora_infos is not None and len(model_lora_infos) > 1:
                 logging.info(f"model_lora_infos is {model_lora_infos}")
                 for key, value in model_lora_infos.items():
diff --git a/rtp_llm/model_factory.py b/rtp_llm/model_factory.py
index 2133ba57c..9eb8ab8db 100644
--- a/rtp_llm/model_factory.py
+++ b/rtp_llm/model_factory.py
@@ -2,25 +2,25 @@ import json
 import logging
 import os
 import sys
-from typing import Any, Dict, Optional, Type, Union
-
-import torch
-
-from rtp_llm.config.py_config_modules import StaticConfig
+from typing import Any, Optional
 
 CUR_PATH = os.path.dirname(os.path.abspath(__file__))
 sys.path.append(os.path.join(str(CUR_PATH), ".."))
 
-from rtp_llm.config.gpt_init_model_parameters import ConfigMode, GptInitModelParameters
-from rtp_llm.distribute.gang_info import get_gang_info
-from rtp_llm.distribute.worker_info import g_parallel_info
-from rtp_llm.model_factory_register import _model_factory
-from rtp_llm.tools.api.hf_model_helper import get_model_info_from_hf
-from rtp_llm.utils.base_model_datatypes import ModelConfig
-from rtp_llm.utils.dump_config_utils import dump_model_to_table
-from rtp_llm.utils.fuser import fetch_remote_file_to_local
+from rtp_llm.async_decoder_engine.async_model import AsyncModel
+from rtp_llm.config.engine_config import EngineConfig, finalize_scheduler_config
+from rtp_llm.config.model_config import ModelConfig, build_py_model_config
+from rtp_llm.config.model_args import ModelArgs
+from rtp_llm.config.py_config_modules import (
+    VitConfig,
+    LoraConfig,
+    GenerateEnvConfig,
+    EmbeddingConfig,
+)
+from rtp_llm.model_factory_register import ModelDict, _model_factory
+from rtp_llm.models.propose_model.propose_model import ProposeModel
+from rtp_llm.ops import MMModelConfig
 from rtp_llm.utils.util import check_with_info
-from rtp_llm.utils.weight_type import WEIGHT_TYPE
 
 
 class ModelFactory:
@@ -49,264 +49,176 @@ class ModelFactory:
         return model_cls
 
     @staticmethod
-    def create_frontend_config(model_config: ModelConfig):
-        config = GptInitModelParameters(0, 0, 0, 0, 0)
-        config.update_common(
-            ckpt_path=model_config.ckpt_path,
-            tokenizer_path=model_config.tokenizer_path,
-            quantization=model_config.quantization,
-            data_type=model_config.act_type,
-            kv_cache_type=model_config.kv_cache_type,
-            max_seq_len=model_config.max_seq_len,
-            seq_size_per_block=model_config.seq_size_per_block,
-            gen_num_per_circle=model_config.gen_num_per_circle,
-            lora_infos=model_config.lora_infos,
-            ptuning_path=model_config.ptuning_path,
-            ref_module=model_config.ref_module,
-            ref_dict=model_config.ref_dict,
-            parallel_info=g_parallel_info,
-            gang_info=get_gang_info(),
-            config_mode=ConfigMode.SimpleMode,
-        )
-        config.seq_size_per_block = model_config.seq_size_per_block
-
-        return config
-
-    @staticmethod
-    def _create_model(model_config: ModelConfig):
-        global _model_factory
-        if model_config.model_type not in _model_factory:
-            raise Exception(f"model type {model_config.model_type} not registered!")
-        model_cls = _model_factory[model_config.model_type]
-        config: GptInitModelParameters = model_cls.create_config(model_config)
-        config.model_name = model_cls.__name__
-        model = model_cls.from_config(config)
-        dump_model_to_table(
-            ModelFactory.model_config_json(model_cls, model_config, config)
+    def _create_model(
+        model_config: ModelConfig,
+        mm_model_config: MMModelConfig,
+        engine_config: EngineConfig,
+        vit_config: Optional[VitConfig] = None,
+        merge_lora: bool = False,
+    ):
+        """Create model from independent config objects.
+        
+        All model metadata (template_type, model_name, lora_infos) is now stored in model_config.
+        
+        Args:
+            model_config: Model configuration
+            mm_model_config: Multimodal model configuration
+            engine_config: Engine configuration
+            vit_config: Optional VitConfig (needed for multimodal models)
+            merge_lora: Whether to merge LoRA weights
+        """
+        model_type = model_config.model_type
+        model_cls = ModelFactory.get_model_cls(model_type)
+        
+        # Get model_name from model_config (default to model class name if not set)
+        model_name = model_config.model_name or model_cls.__name__
+        model_config.model_name = model_name
+        engine_config.runtime_config.model_name = model_name
+        
+        model = model_cls.from_config(
+            py_model_config=model_config,
+            mm_model_config=mm_model_config,
+            engine_config=engine_config,
+            vit_config=vit_config,
+            merge_lora=merge_lora,
         )
         return model
 
     @staticmethod
-    def _create_sp_model(
-        score_model_gpt_config: GptInitModelParameters, model_config: ModelConfig
-    ):
-        from rtp_llm.models.propose_model.propose_model import ProposeModel
-
-        model = None
-        global _model_factory
+    def get_sp_model(
+        model_config: ModelConfig,
+        propose_model_config: Optional[ModelConfig],
+        engine_config: EngineConfig,
+    ) -> Optional[Any]:
+        """Get and create ProposeModel from engine_config and propose_model_config.
+        
+        This function handles sp_type determination and ProposeModel creation logic.
+        
+        Args:
+            model_config: Main ModelConfig (for max_seq_len alignment)
+            propose_model_config: Optional propose ModelConfig
+            engine_config: EngineConfig containing sp_config
+            
+        Returns:
+            ProposeModel instance or None if no propose model needed
+        """
+        if not propose_model_config:
+            return None
+            
+        sp_type = engine_config.sp_config.sp_type
+        if sp_type == "none" or not sp_type:
+            return None
+        
+        # Adjust sp_type based on propose model type if needed
         if (
-            model_config.sp_type == "vanilla"
-            or model_config.sp_type == "mtp"
-            or model_config.sp_type == "eagle3"
-            or model_config.sp_type == "eagle"
+            sp_type == "vanilla"
+            or sp_type == "mtp"
+            or sp_type == "eagle3"
+            or sp_type == "eagle"
         ):
-            if model_config.model_type not in _model_factory:
-                raise Exception(f"model type {model_config.model_type} not registered!")
+            model_type = propose_model_config.model_type
             if (
-                model_config.model_type == "deepseek-v3-mtp"
-                or model_config.model_type == "mixtbstars-mtp"
+                model_type == "deepseek-v3-mtp"
+                or model_type == "mixtbstars-mtp"
             ):
                 logging.warning(
-                    f"create sp model type is {model_config.model_type}, so change the sp type to mtp"
+                    f"create sp model type is {model_type}, so change the sp type to mtp"
                 )
-                model_config.sp_type = "mtp"
-            if model_config.model_type == "qwen_3_moe-mtp":
+                engine_config.sp_config.sp_type = "mtp"
+                sp_type = "mtp"
+            elif model_type == "qwen_3_moe-mtp":
                 logging.warning(
-                    f"create sp model type is {model_config.model_type}, so change the sp type to eagle3"
+                    f"create sp model type is {model_type}, so change the sp type to eagle3"
                 )
-                model_config.sp_type = "eagle3"
-            model_cls = _model_factory[model_config.model_type]
-            # propose model's max seq len must be equal to score model's max seq len
-            model_config.max_seq_len = score_model_gpt_config.max_seq_len
-            config: GptInitModelParameters = model_cls.create_config(model_config)
-            gpt_model = model_cls.from_config(config)
-            dump_model_to_table(
-                ModelFactory.model_config_json(model_cls, model_config, config)
-            )
-            model = ProposeModel(
-                model_config.sp_type, model_config.gen_num_per_circle, gpt_model
-            )
-        elif model_config.sp_type == "deterministic":
-            model = ProposeModel(model_config.sp_type, model_config.gen_num_per_circle)
-        return model
-
-    # TODO: remove model_config, get all info from gpt_config
-    @staticmethod
-    def model_config_json(
-        model_cls: Type[Any], model_config: ModelConfig, config: GptInitModelParameters
-    ) -> Dict[str, Any]:
-        config_json = {
-            "model_type": model_cls.__name__,
-            "act_type": str(model_config.act_type),
-            "max_seq_len": config.max_seq_len,
-            "use_sparse_head": config.is_sparse_head,
-            "use_multi_task_prompt": config.multi_task_prompt,
-            "lora_infos": config.lora_infos,
-        }
-        return config_json
-
-    @staticmethod
-    def from_model_config(
-        model_config: ModelConfig, propose_model_config: Optional[ModelConfig] = None
-    ):
-        from rtp_llm.async_decoder_engine.async_model import AsyncModel
-
-        model = ModelFactory._create_model(model_config)
-        if model_config.model_type == "fake_model" or model.config.vit_separation == 1:
-            return model
-        propose_model = (
-            None
-            if propose_model_config is None
-            else ModelFactory._create_sp_model(model.config, propose_model_config)
-        )
-        if propose_model:
-            logging.info("set enable_speculative_decoding")
-            model.config.enable_speculative_decoding = True
-        model = AsyncModel(model, propose_model)
-        if propose_model:
-            logging.info("create propose model done")
-        logging.info("create rpc model done")
-        return model
-
-    @staticmethod
-    def from_huggingface(
-        model_path_or_name: str,
-        revision: Optional[str] = None,
-        model_config: ModelConfig = ModelConfig(),
-    ):
-        model_path, model_type = get_model_info_from_hf(model_path_or_name, revision)
-        new_model_config = model_config
-        new_model_config = new_model_config._replace(
-            model_type=model_type, ckpt_path=model_path, tokenizer_path=model_path
-        )
-        return ModelFactory.from_model_config(new_model_config)
-
-    @staticmethod
-    def creat_standalone_py_model_from_huggingface(
-        model_path_or_name: str,
-        revision: Optional[str] = None,
-        model_config: ModelConfig = ModelConfig(),
-    ):
-        assert os.environ["LOAD_PYTHON_MODEL"] == "1"
-        new_model_config = model_config
-        new_model_config = new_model_config._replace(
-            model_type=model_type, ckpt_path=model_path, tokenizer_path=model_path
-        )
-        model = ModelFactory._create_model(model_config)
-        return model
-
-    @staticmethod
-    def create_normal_model_config():
-        model_type = StaticConfig.model_config.model_type
-        ckpt_path = StaticConfig.model_config.checkpoint_path
-        tokenizer_path = StaticConfig.model_config.tokenizer_path
-        if tokenizer_path == "":
-            tokenizer_path = ckpt_path
-        lora_infos = StaticConfig.lora_config.lora_info
-        max_seq_len = StaticConfig.engine_config.max_seq_len
-        seq_size_per_block = StaticConfig.py_kv_cache_config.seq_size_per_block
-
-        tokenizer_path = fetch_remote_file_to_local(tokenizer_path)
-        ckpt_path = fetch_remote_file_to_local(ckpt_path)
-
-        extra_data_path = StaticConfig.model_config.extra_data_path
-        if extra_data_path:
-            extra_data_path = fetch_remote_file_to_local(extra_data_path)
-            StaticConfig.model_config.local_extra_data_path = extra_data_path
-
-        ptuning_path = StaticConfig.model_config.ptuning_path
-        if ptuning_path is not None:
-            ptuning_path = fetch_remote_file_to_local(ptuning_path)
-
-        lora_infos = json.loads(lora_infos)
-        for lora_name, lora_path in lora_infos.items():
-            lora_infos[lora_name] = fetch_remote_file_to_local(lora_path)
-
-        logging.info(
-            f"load model from tokenizer_path: {tokenizer_path}, ckpt_path: {ckpt_path}, lora_infos: {lora_infos}, ptuning_path: {ptuning_path}"
-        )
-
-        act_type = None
-        # TODO(xinfei.sxf) fix this
-        act_type = StaticConfig.model_config.act_type
-        kv_cache_type = StaticConfig.py_kv_cache_config.kv_cache_dtype
-        quantization = StaticConfig.quantization_config.quantization
-        model_config = ModelConfig(
-            model_type=model_type,
-            ckpt_path=ckpt_path,
-            tokenizer_path=tokenizer_path,
-            act_type=act_type,
-            kv_cache_type=kv_cache_type,
-            max_seq_len=max_seq_len,
-            seq_size_per_block=seq_size_per_block,
-            lora_infos=lora_infos,
-            ptuning_path=ptuning_path,
-            quantization=quantization,
-        )
-
-        return model_config
-
-    @staticmethod
-    def create_propose_model_config(normal_model_config: ModelConfig):
-        propose_model_config = None
-
-        sp_type = StaticConfig.py_speculative_execution_config.sp_type
+                engine_config.sp_config.sp_type = "eagle3"
+                sp_type = "eagle3"
+        
+        gen_num_per_circle = engine_config.sp_config.gen_num_per_cycle
+        
         if (
             sp_type == "vanilla"
             or sp_type == "mtp"
             or sp_type == "eagle3"
             or sp_type == "eagle"
         ):
-            logging.info("use vanilla speculative model")
-            propose_model_type = (
-                StaticConfig.py_speculative_execution_config.sp_model_type
-            )
-            gen_num_per_circle = (
-                StaticConfig.py_speculative_execution_config.gen_num_per_circle
-            )
-            origin_ckpt_path = (
-                StaticConfig.py_speculative_execution_config.sp_checkpoint_path
-            )
-            if origin_ckpt_path is None:
-                logging.error("sp is disabled since SP_CHECKPOINT_PATH is not set")
-                return None
-            propose_ckpt_path = fetch_remote_file_to_local(origin_ckpt_path)
-            logging.info(f"load propose model from ckpt_path: {propose_ckpt_path}")
-
-            propose_act_type = WEIGHT_TYPE.from_str(
-                StaticConfig.model_config.act_type
-            ).to_str()
-            quantization = StaticConfig.py_speculative_execution_config.sp_quantization
-            kv_cache_type = (
-                StaticConfig.py_speculative_execution_config.sp_kv_cache_dtype
-            )
-
-            propose_model_config = ModelConfig(
-                model_type=propose_model_type,
-                ckpt_path=propose_ckpt_path,
-                tokenizer_path=normal_model_config.tokenizer_path,
-                lora_infos=None,
-                act_type=propose_act_type,
-                kv_cache_type=kv_cache_type,
-                max_seq_len=normal_model_config.max_seq_len,
-                gen_num_per_circle=gen_num_per_circle,
-                sp_type=sp_type,
-                quantization=quantization,
+            # Need to create GPT model for propose model
+            model_cls = ModelFactory.get_model_cls(propose_model_config.model_type)
+            # propose model's max seq len must be equal to score model's max seq len
+            propose_model_config.max_seq_len = model_config.max_seq_len
+            # Create MMModelConfig for propose model
+            propose_mm_model_config = MMModelConfig()
+            gpt_model = model_cls.from_config(
+                py_model_config=propose_model_config,
+                mm_model_config=propose_mm_model_config,
+                engine_config=engine_config,
+                vit_config=None,  # Propose model doesn't need vit_config
+                merge_lora=False,  # Propose model doesn't need merge_lora
             )
+            return ProposeModel(sp_type, gen_num_per_circle, gpt_model)
         elif sp_type == "deterministic":
-            gen_num_per_circle = (
-                StaticConfig.py_speculative_execution_config.gen_num_per_circle
-            )
-            propose_model_config = ModelConfig(
-                sp_type=sp_type, gen_num_per_circle=gen_num_per_circle
-            )
-            logging.info("use deterministic speculative model")
+            return ProposeModel(sp_type, gen_num_per_circle)
+        
+        return None
 
-        return propose_model_config
+    @staticmethod
+    def from_model_configs(
+        model_config: ModelConfig,
+        mm_model_config: MMModelConfig,
+        engine_config: EngineConfig,
+        gang_info,
+        vit_config: Optional[VitConfig] = None,
+        merge_lora: bool = False,
+        propose_model_config: Optional[ModelConfig] = None,
+    ):
+        """Create model from independent config objects, with optional propose model.
+        
+        All model metadata (template_type, model_name, lora_infos) should be set in model_config before calling this method.
+        
+        This replaces from_gpt_config().
+        
+        Args:
+            model_config: Model configuration
+            mm_model_config: Multimodal model configuration
+            engine_config: Engine configuration
+            gang_info: GangInfo instance from GangServer
+            vit_config: Optional VitConfig (needed for multimodal models)
+            merge_lora: Whether to merge LoRA weights
+            propose_model_config: Optional propose model configuration
+        """
+        model = ModelFactory._create_model(
+            model_config=model_config,
+            mm_model_config=mm_model_config,
+            engine_config=engine_config,
+            vit_config=vit_config,
+            merge_lora=merge_lora,
+        )
+        from rtp_llm.ops import VitSeparation
+        model_type = model_config.model_type
+        if model_type == "fake_model" or (vit_config is not None and vit_config.vit_separation == VitSeparation.VIT_SEPARATION_ROLE):
+            return model
+        
+        # Create propose model if provided
+        propose_model = ModelFactory.get_sp_model(
+            model_config=model_config,
+            propose_model_config=propose_model_config,
+            engine_config=engine_config,
+        )
+
+        model = AsyncModel(model, gang_info, propose_model)
+        logging.info("create rpc model done")
+        return model
 
     @staticmethod
-    def load_default_generate_config(model):
-        generation_config_path = StaticConfig.generate_env_config.generation_config_path
+    def load_default_generate_config(model, generate_env_config: Optional[Any] = None):
+        """Load default generate config from GenerateEnvConfig.
+        
+        Args:
+            model: Model instance to update
+            generate_env_config: Optional GenerateEnvConfig object
+        """
+        if generate_env_config is None:
+            return
+        generation_config_path = generate_env_config.generation_config_path
         if generation_config_path:
             model.default_generate_config.update(
                 json.load(
@@ -318,33 +230,138 @@ class ModelFactory:
                          {json.dumps(model.default_generate_config.model_dump(), indent=4)}"
             )
 
+
     @staticmethod
-    def create_from_env():
-        normal_model_config = ModelFactory.create_normal_model_config()
-        propose_model_config = ModelFactory.create_propose_model_config(
-            normal_model_config
+    def create_model_configs(
+        engine_config: EngineConfig,
+        model_args: ModelArgs,
+        lora_config: LoraConfig,
+        generate_env_config: Optional[GenerateEnvConfig] = None,
+        embedding_config: Optional[EmbeddingConfig] = None,
+    ) -> tuple[ModelConfig, Optional[ModelConfig]]:
+        """Create ModelConfig and optional propose ModelConfig from configuration objects.
+        
+        This method handles all ModelConfig construction and initialization logic.
+        EngineConfig should already be built before calling this method.
+        
+        The flow is:
+        1. Use provided ModelArgs (already populated by server_args)
+        2. Call model's _create_config to create ModelConfig with model architecture
+        3. Apply ModelArgs to ModelConfig (overwrite with user-provided values)
+        4. Build ModelConfig with build_py_model_config
+        
+        Args:
+            engine_config: Already built EngineConfig
+            model_args: ModelArgs containing model configuration
+            lora_config: LoraConfig containing LoRA configuration
+            generate_env_config: Optional GenerateEnvConfig for generation settings
+            embedding_config: Optional EmbeddingConfig for embedding settings
+            
+        Returns:
+            Tuple of (model_config, propose_model_config)
+            propose_py_model_config will be None if not needed
+        """
+        # Step 1: Use provided ModelArgs (already populated by server_args)
+        
+        # Step 1.5: Infer model_type from checkpoint if not provided
+        if not model_args.model_type and model_args.ckpt_path:
+            try:
+                config_json = ModelFactory.get_config_json(model_args.ckpt_path)
+                inferred_model_type = ModelDict.get_ft_model_type_by_config(config_json)
+                if inferred_model_type:
+                    model_args.model_type = inferred_model_type
+                    logging.info(f"Inferred model_type '{inferred_model_type}' from checkpoint path: {model_args.ckpt_path}")
+                else:
+                    # Try to get from architectures field directly
+                    if "architectures" in config_json and config_json["architectures"]:
+                        architecture = config_json["architectures"][0]
+                        # Try to map architecture to model_type
+                        inferred_model_type = ModelDict.get_ft_model_type_by_hf_architectures(architecture)
+                        if inferred_model_type:
+                            model_args.model_type = inferred_model_type
+                            logging.info(f"Inferred model_type '{inferred_model_type}' from architecture '{architecture}'")
+            except Exception as e:
+                logging.warning(f"Failed to infer model_type from checkpoint: {e}")
+        
+        # Check if model_type is still empty
+        if not model_args.model_type:
+            raise ValueError(
+                f"model_type is not set and could not be inferred from checkpoint path: {model_args.ckpt_path}. "
+                f"Please provide --model_type or MODEL_TYPE environment variable."
+            )
+        
+        # Step 2: Get model class and create ModelConfig with model architecture
+        model_cls = ModelFactory.get_model_cls(model_args.model_type)
+
+        # Call _create_config to get model architecture config
+        model_config = model_cls._create_config(model_args.ckpt_path)
+        # Step 4: Build ModelConfig (setup paths, quantization, etc.)
+        build_py_model_config(
+            py_model_config=model_config,
+            model_args=model_args,
+            kv_cache_config=engine_config.kv_cache_config,
+            py_hw_kernel_config=engine_config.hw_kernel_config,
+            profiling_debug_logging_config=engine_config.profiling_debug_logging_config,
+            parallelism_config=engine_config.parallelism_config,
+            embedding_config=embedding_config,
         )
-        model = ModelFactory.from_model_config(
-            normal_model_config, propose_model_config
+         
+        # Set model metadata fields
+        # Set lora_infos from lora_config (direct assignment)
+        if lora_config.lora_info:
+            lora_infos = json.loads(lora_config.lora_info)
+            model_config.lora_infos = lora_infos if lora_infos else {}
+        
+        # Set model_name (default to model class name)
+        model_config.model_name = model_cls.__name__
+        
+        # Finalize scheduler config based on ModelConfig (only once, for main model)
+        finalize_scheduler_config(
+            fifo_scheduler_config=engine_config.runtime_config.fifo_scheduler_config,
+            max_seq_len=model_config.max_seq_len,
         )
-        ModelFactory.load_default_generate_config(model)
-
-        return model
-
-    @staticmethod
-    def create_from_module(ref_module: torch.nn.Module):
-        normal_model_config = ModelFactory.create_normal_model_config()
-        normal_model_config.add_ref_module(ref_module)
-        model = ModelFactory.from_model_config(normal_model_config)
-        ModelFactory.load_default_generate_config(model)
-
-        return model
-
-    @staticmethod
-    def create_from_dict(ref_dict: Dict[str, torch.Tensor]):
-        normal_model_config = ModelFactory.create_normal_model_config()
-        normal_model_config.add_ref_dict(ref_dict)
-        model = ModelFactory.from_model_config(normal_model_config)
-        ModelFactory.load_default_generate_config(model)
+        
+        # Set model_name to engine_config.runtime_config.model_name (for backward compatibility)
+        engine_config.runtime_config.model_name = model_config.model_name
+        
+        # Create propose model config if needed
+        propose_model_config = None
+        sp_type = engine_config.sp_config.sp_type
+        if sp_type and sp_type != "none":
+            propose_model_type = engine_config.sp_config.sp_model_type
+            propose_ckpt_path = engine_config.sp_config.sp_checkpoint_path
+            if propose_ckpt_path:
+                # Create ModelArgs for propose model (reuse main model args, but override ckpt_path)
+                propose_model_args = ModelArgs()
+                propose_model_args.ckpt_path = propose_ckpt_path
+                propose_model_args.tokenizer_path = model_args.tokenizer_path
+                propose_model_args.model_type = propose_model_type
+                propose_model_args.act_type = model_args.act_type
+                propose_model_args.use_float32 = model_args.use_float32
+                propose_model_args.mla_ops_type = model_args.mla_ops_type
+                
+                # Create propose ModelConfig using _create_config
+                propose_model_cls = ModelFactory.get_model_cls(propose_model_type)
+                propose_model_config = propose_model_cls._create_config(propose_ckpt_path)
+                # Ensure max_seq_len matches main model
+                propose_model_config.max_seq_len = model_config.max_seq_len
+                propose_model_config.quantization = engine_config.sp_config.sp_quantization
+                
+                logging.info(
+                    f"load propose model from tokenizer_path: {propose_model_config.tokenizer_path}, "
+                    f"ckpt_path: {propose_model_config.ckpt_path}, quantization: {propose_model_config.quantization}"
+                )
+                
+                # Build propose model config (no finalize_scheduler_config for propose model)
+                build_py_model_config(
+                    py_model_config=propose_model_config,
+                    model_args=propose_model_args,
+                    kv_cache_config=engine_config.kv_cache_config,
+                    py_hw_kernel_config=engine_config.hw_kernel_config,
+                    profiling_debug_logging_config=engine_config.profiling_debug_logging_config,
+                    parallelism_config=engine_config.parallelism_config,
+                    embedding_config=None,  # Propose model doesn't need embedding_config
+                )
+        
+        return model_config, propose_model_config
 
-        return model
diff --git a/rtp_llm/model_loader/load_config.py b/rtp_llm/model_loader/load_config.py
index 8dbc7c391..f249dee88 100644
--- a/rtp_llm/model_loader/load_config.py
+++ b/rtp_llm/model_loader/load_config.py
@@ -3,16 +3,18 @@ import logging
 from typing import Any, List, Optional, Union
 
 import torch
-from pydantic import BaseModel, field_validator, model_validator
+from pydantic import BaseModel, ConfigDict, field_validator, model_validator
 
-from rtp_llm.config.py_config_modules import StaticConfig
+from rtp_llm.config.py_config_modules import LoadConfig as PyLoadConfig
 from rtp_llm.device.device_base import DeviceBase
 from rtp_llm.utils.database import BaseDatabase
 from rtp_llm.utils.fuser import fetch_remote_file_to_local
 from rtp_llm.utils.util import check_with_info
-
+from rtp_llm.ops import VitSeparation
 
 class LoadConfig(BaseModel):
+    model_config = ConfigDict(arbitrary_types_allowed=True)
+    
     database: Any
     num_layers: int
     hidden_size: int
@@ -38,11 +40,10 @@ class LoadConfig(BaseModel):
     ffn_tp_size: int
     ffn_tp_rank: int
     num_nodes: int
-    tp_split_emb_and_lm_head: bool = False
     bit: int = 16
     merge_lora: bool = False
 
-    vit_separation: int = 0
+    vit_separation: VitSeparation = VitSeparation.VIT_SEPARATION_LOCAL  # VitSeparation enum
     compute_dtype: Any = torch.float16
 
     quant_algo: Any = None
@@ -54,10 +55,20 @@ class LoadConfig(BaseModel):
     phy2log: Optional[List[List[int]]] = None
     use_swizzleA: bool = False
 
-    @field_validator("database", "compute_dtype", "quant_algo", "exported_device")
+    @field_validator("database", "compute_dtype", "quant_algo", "exported_device", "vit_separation")
     @classmethod
     def validate_custom_types(cls, value: Any, info) -> Any:
         field_name = info.field_name
+        if field_name == "vit_separation":
+            from rtp_llm.ops import VitSeparation
+            if value is None:
+                return VitSeparation.VIT_SEPARATION_LOCAL
+            if not isinstance(value, VitSeparation):
+                raise TypeError(
+                    f"Field 'vit_separation' expects type VitSeparation, got {type(value)}"
+                )
+            return value
+        
         expected_types = {
             "database": BaseDatabase,
             "compute_dtype": torch.dtype,
@@ -74,16 +85,8 @@ class LoadConfig(BaseModel):
 
     @model_validator(mode="after")
     def _set_default_phy2log(self) -> "LoadConfig":
-        if self.phy2log is None and self.expert_num > 0:
-            phy2log = self.create_redundant_expert(
-                layer_num=self.num_layers,
-                expert_num=self.expert_num,
-                phy_exp_num=self.phy_exp_num,
-                ep_size=self.ep_size,
-                num_nodes=self.num_nodes,
-                PHY2LOG_PATH_KEY="PHY2LOG_PATH",
-            )
-            return self.model_copy(update={"phy2log": phy2log})
+        # phy2log should be set during LoadConfig creation, not in validator
+        # Validator removed - phy2log must be provided explicitly
         return self
 
     def get_selected_experts(self, layer_id: int, expert_num):
@@ -125,6 +128,7 @@ class LoadConfig(BaseModel):
     def create_redundant_expert(
         layer_num: int,
         expert_num: int,
+        load_config: PyLoadConfig,
         phy_exp_num: int,
         ep_size: int,
         num_nodes: int,
@@ -149,7 +153,7 @@ class LoadConfig(BaseModel):
         layer_num = layer_num
 
         phy2log: List[List[int]] = []
-        phy2log_path = fetch_remote_file_to_local(StaticConfig.load_config.phy2log_path)
+        phy2log_path = fetch_remote_file_to_local(load_config.phy2log_path) if load_config.phy2log_path else None
 
         if phy2log_path:
             with open(phy2log_path, "r") as f:
diff --git a/rtp_llm/model_loader/loader.py b/rtp_llm/model_loader/loader.py
index f95406411..3c0855429 100644
--- a/rtp_llm/model_loader/loader.py
+++ b/rtp_llm/model_loader/loader.py
@@ -9,10 +9,9 @@ import safetensors
 import torch
 import torch.nn.functional as F
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.config.py_config_modules import PyEnvConfigs, StaticConfig
-from rtp_llm.config.task_type import TaskType
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.device import get_current_device
+from rtp_llm.ops import TaskType
 from rtp_llm.eplb.ep_balancer import ExpertBalancer
 from rtp_llm.lora.lora_weights import LoRAWeights
 from rtp_llm.model_loader.load_config import LoadConfig
@@ -28,32 +27,42 @@ from rtp_llm.utils.fuser import fetch_remote_file_to_local
 from rtp_llm.utils.model_weight import W, WeightStyle
 from rtp_llm.utils.time_util import timer_wrapper
 from rtp_llm.utils.util import check_with_info
-
+from rtp_llm.ops import VitSeparation
 
 class ModelLoader:
     WeightInfo = NamedTuple("WeightInfo", [("weight", WeightModule), ("layer_id", Optional[int]), ("collector", TensorCollector)])
 
     def __init__(
         self,
-        task_type: TaskType,
+        model_config: ModelConfig,
         weights_info: ModelDeployWeightInfo,
         misc_weights_info: Optional[CustomAtomicWeight],
         compute_dtype: torch.dtype,
         database: BaseDatabase,
-        py_env_configs: PyEnvConfigs = StaticConfig,
-        is_attn_model: bool = False,
     ):
-        self._task_type = task_type
+        # self.model_config = model_config
+        # Get task_type from model_config (C++ enum)
+        self._task_type = model_config.task_type
+        
         self._weights_info = weights_info
         self._misc_weights_info: Optional[CustomAtomicWeight] = misc_weights_info
         self._model_weights_info: Optional[ModelWeightInfo] = (
             self._weights_info.create_model_weight_info(database)
         )
-        self.py_env_configs = py_env_configs
-        use_fp32 = py_env_configs.model_config.use_float32
+        
+        # Get use_fp32 from model_config
+        use_fp32 = model_config.use_float32
         if use_fp32:
             compute_dtype = torch.float32
-        self._is_attn_model = is_attn_model
+        logging.info(f'load use type {compute_dtype}')
+        
+        # Get is_attn_model from engine_config
+        engine_config = weights_info.config.engine_config
+        ffn_config = engine_config.parallelism_config.ffn_disaggregate_config
+        self._is_attn_model = (
+            ffn_config.enable_ffn_disaggregate
+            and not ffn_config.is_ffn_service()
+        )
         self._init_eplb_config(self._weights_info, compute_dtype)
 
         self._load_config: LoadConfig = self._weights_info.create_load_config(
@@ -226,7 +235,9 @@ class ModelLoader:
         return self._load_from_scratch(device)
     
     def _is_memory_enough_for_fastsafetensor(self):
-        model_size = self._weights_info.config.eval_model_size()
+        # Get task_type from C++ ModelConfig (enum)
+        task_type = self._weights_info.config.py_model_config.task_type
+        model_size = self._weights_info.config.py_model_config.eval_model_size()
         device_mem_info = self._load_config.exported_device.get_mem_info()
         max_file_size = self._load_config.database.get_max_file_size()
         if device_mem_info is None:
@@ -287,7 +298,7 @@ class ModelLoader:
         return model_weights
 
     def prepare_weights(self, device: str):
-        if self._load_config.vit_separation != 1 and not self._is_attn_model:
+        if self._load_config.vit_separation != VitSeparation.VIT_SEPARATION_ROLE and not self._is_attn_model:
             for id in range(self._load_config.num_layers):
                 results = self._load_layer_weights(id, device)
                 for name, tensor in results.items():
@@ -314,7 +325,7 @@ class ModelLoader:
         WeightInfo = ModelLoader.WeightInfo
         tensor_to_weight_map: Dict[str, WeightInfo] = {}
         weight_info_list: List[WeightInfo] = []
-        if self._load_config.vit_separation != 1:
+        if self._load_config.vit_separation != VitSeparation.VIT_SEPARATION_ROLE:
             for layer_id in range(self._load_config.num_layers):
                 layer_weights = self._model_weights_info.layer_weights[layer_id]
                 if isinstance(layer_weights, WeightModule):
@@ -375,7 +386,9 @@ class ModelLoader:
     def _choose_weight_convert_device(self, current_device):
         if "FORCE_CPU_LOAD_WEIGHTS" in os.environ:
             return "cpu"
-        model_size = self._weights_info.config.eval_model_size()
+        # Get task_type from C++ ModelConfig (enum)
+        task_type = self._weights_info.config.py_model_config.task_type
+        model_size = self._weights_info.config.py_model_config.eval_model_size()
         device_mem_info = self._load_config.exported_device.get_mem_info()
         if device_mem_info is None:
             return "cpu"
@@ -394,7 +407,7 @@ class ModelLoader:
         for layer_id, name, tensor in self.prepare_weights(convert_device):
             if convert_device != device:
                 tensor = tensor.to(device)
-            if layer_id is not None and self._load_config.vit_separation != 1:
+            if layer_id is not None and self._load_config.vit_separation != VitSeparation.VIT_SEPARATION_ROLE:
                 weights.set_layer_weight(layer_id, name, tensor)
             else:
                 weights.set_global_weight(name, tensor)
@@ -432,20 +445,21 @@ class ModelLoader:
 
         embedding_weight = weight.global_weights.get(W.embedding, None)
         if embedding_weight != None:
-            self._weights_info.config.embedding_size = embedding_weight.shape[0]
+            self._weights_info.config.py_model_config.embedding_size_ = embedding_weight.shape[0]
             logging.info(
-                f"embedding_size is {self._weights_info.config.embedding_size}, vocab size is {self._weights_info.config.vocab_size}"
+                f"embedding_size is {self._weights_info.config.py_model_config.embedding_size_}, vocab size is {self._weights_info.config.py_model_config.vocab_size}"
             )
 
-        if self._load_config.vit_separation != 1:
+        if self._load_config.vit_separation != VitSeparation.VIT_SEPARATION_ROLE:
             if self._task_type == TaskType.LANGUAGE_MODEL:
                 lm_head_w = weight.steal_global_weight(W.lm_head)
                 if lm_head_w == None:
                     lm_head_w = weight.global_weights[W.embedding]
-                if self._weights_info.config.normalize_lm_head_weight:
+                if self._weights_info.config.py_model_config.normalize_lm_head_weight:
                     lm_head_w = F.normalize(lm_head_w)
-                if self._weights_info.config.logit_scale != 1.0:
-                    lm_head_w = self._weights_info.config.scale_logit * lm_head_w
+                logit_scale = self._weights_info.config.py_model_config.logit_scale
+                if logit_scale != 1.0:
+                    lm_head_w = logit_scale * lm_head_w
                 weight.set_global_weight(W.lm_head, lm_head_w)
             else:
                 # Some LLM can be used for other tasks, e.g. classification, in which case lm_head is not needed
@@ -453,11 +467,12 @@ class ModelLoader:
 
             pos_weight = weight.global_weights.get(W.positional_embedding, None)
             if pos_weight != None:
-                if pos_weight.shape[0] < self._weights_info.config.max_seq_len:
+                max_seq_len = self._weights_info.config.py_model_config.max_seq_len
+                if pos_weight.shape[0] < max_seq_len:
                     raise Exception(
-                        f"positon_weight has shape: {pos_weight.shape}, but max_seq_len is: {self._weights_info.config.max_seq_len} > {pos_weight.shape[0]}"
+                        f"positon_weight has shape: {pos_weight.shape}, but max_seq_len is: {max_seq_len} > {pos_weight.shape[0]}"
                     )
-                pos_weight = pos_weight[: self._weights_info.config.max_seq_len].to(
+                pos_weight = pos_weight[: max_seq_len].to(
                     device
                 )
                 weight.set_global_weight(W.positional_embedding, pos_weight)
@@ -472,46 +487,61 @@ class ModelLoader:
                         dynamic_weight.name, dynamic_w.get(dynamic_weight.name)
                     )
 
-    def _init_redundant_expert(self, config: GptInitModelParameters):
-        if config.expert_num == 0:
+    def _init_redundant_expert(self, weights_info: ModelDeployWeightInfo):
+        if weights_info.expert_num_ == 0:
             return
 
-        expert_num = config.expert_num
-        ep_size = config.ep_size
-        layer_num = config.layer_num
-        phy_exp_num = config.phy_exp_num
-
+        expert_num = weights_info.expert_num_
+        ep_size = weights_info.ep_size
+        layer_num = weights_info._num_layers
+        phy_exp_num = weights_info.phy_exp_num_
+
+        # Get load_config from py_env_configs
+        from rtp_llm.config.py_config_modules import PyEnvConfigs
+        py_env_configs = weights_info.config.py_env_configs
+        load_config = py_env_configs.load_config if isinstance(py_env_configs, PyEnvConfigs) else None
+        if load_config is None:
+            # Fallback: create a minimal LoadConfig object if py_env_configs is not available
+            from rtp_llm.config.py_config_modules import LoadConfig as PyLoadConfig
+            load_config = PyLoadConfig()
+        
         phy2log = LoadConfig.create_redundant_expert(
             layer_num=layer_num,
             expert_num=expert_num,
+            load_config=load_config,
             phy_exp_num=phy_exp_num,
             ep_size=ep_size,
-            num_nodes=config.num_nodes,
+            num_nodes=weights_info.num_nodes,
         )
-        config.phy2log = phy2log
+        weights_info.phy2log = phy2log
 
     def _init_eplb_config(
         self, weights_info: ModelDeployWeightInfo, compute_dtype: torch.dtype
     ):
-        self._init_redundant_expert(weights_info.config)
-        if weights_info.config.enable_eplb:
+        self._init_redundant_expert(weights_info)
+        if weights_info.enable_eplb_:
             model_path = None
-            if weights_info.config.is_mtp:
-                model_path = weights_info.config.ckpt_path
+            if weights_info.config.py_model_config.is_mtp:
+                model_path = weights_info.config.py_model_config.ckpt_path
             else:
-                path = self.py_env_configs.model_config.original_checkpoint_path
+                # Get original_checkpoint_path from model_config or use ckpt_path
+                path = None
+                if weights_info.config.py_env_configs and weights_info.config.py_env_configs.model_args:
+                    path = weights_info.config.py_env_configs.model_args.original_checkpoint_path
                 if path is None:
-                    path = weights_info.config.ckpt_path
+                    path = weights_info.config.py_model_config.ckpt_path
                 model_path = fetch_remote_file_to_local(path)
 
             ep_lb_database = CkptDatabase(model_path)
+            
             self.ep_balancer = ExpertBalancer(
                 weights_info=weights_info,
                 compute_dtype=compute_dtype,
-                phy2log=weights_info.config.phy2log,
+                phy2log=weights_info.phy2log,
                 database=ep_lb_database,
-                py_env_configs=self.py_env_configs,
+                model_config=self.model_config,
             )
+            weights_info.py_eplb = self.ep_balancer
             weights_info.config.py_eplb = self.ep_balancer
 
     def _init_eplb_weight(self, weight: ModelWeights, device: str):
@@ -521,7 +551,7 @@ class ModelLoader:
         phy2log = self._load_config.phy2log
 
         if expert_num == 0 or (
-            not self._weights_info.config.enable_eplb and redundant_expert == 0
+            not self._weights_info.enable_eplb_ and redundant_expert == 0
         ):
             logging.info("don't need to init eplb weight, skip...")
             return
@@ -546,12 +576,11 @@ class ModelLoader:
 
 
 def get_model_loader(
-    task_type: TaskType,
+    model_config: ModelConfig,
     weights_info: ModelDeployWeightInfo,
     misc_weights_info: Optional[CustomAtomicWeight],
     compute_dtype: torch.dtype,
     database: BaseDatabase,
-    is_attn_model: bool,
 ) -> ModelLoader:
     if weights_info._head_num % weights_info.tp_size != 0:
         raise Exception(
@@ -567,10 +596,9 @@ def get_model_loader(
             % (weights_info.tp_size, weights_info._head_num_kv)
         )
     return ModelLoader(
-        task_type,
+        model_config,
         weights_info,
         misc_weights_info,
         compute_dtype,
         database,
-        is_attn_model=is_attn_model,
     )
diff --git a/rtp_llm/model_loader/model_weight_info.py b/rtp_llm/model_loader/model_weight_info.py
index 1d3bbfa9b..6b10993c4 100644
--- a/rtp_llm/model_loader/model_weight_info.py
+++ b/rtp_llm/model_loader/model_weight_info.py
@@ -1,12 +1,9 @@
 import functools
-import gc
 import logging
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.config.py_config_modules import StaticConfig
 from rtp_llm.config.quant_config import QuantizationConfig
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight, AttnConfig
 from rtp_llm.model_loader.ffn_weight import FfnConfig, FfnWeight, MoeWithSharedWeight
@@ -27,6 +24,16 @@ from rtp_llm.utils.model_weight import (
     tolerate_failed,
 )
 from rtp_llm.utils.weight_type import WEIGHT_TYPE
+from rtp_llm.ops import VitSeparation
+
+# Forward references for type hints
+from typing import TYPE_CHECKING
+
+# Import BaseMultiModalWeightInfo for isinstance check
+from rtp_llm.models.multimodal.multimodal_mixin import BaseMultiModalWeightInfo
+if TYPE_CHECKING:
+    from rtp_llm.config.model_config import ModelConfig
+    from rtp_llm.config.engine_config import EngineConfig
 
 
 def create_scalar_ones(ts: List[torch.Tensor]):
@@ -144,71 +151,89 @@ class ModelDeployWeightInfo:
         W.post_ln_beta: "transformer.layers.{i}.post_layernorm.bias",
     }
 
-    def __init__(self, config: GptInitModelParameters, tp_size: int, tp_rank: int):
-        self.config = config
-        self._use_swizzleA = config.hw_kernel_config.use_swizzleA
-        self._use_qk_norm = config.qk_norm
-        self._hidden_size = config.hidden_size
-        self._inter_size = config.inter_size
-        self._inter_padding_size = config.inter_padding_size
-        self._moe_inter_padding_size = config.moe_inter_padding_size
-        self._head_num = config.head_num
-        self._head_num_kv = config.head_num_kv
+    def __init__(
+        self,
+        py_model_config: "ModelConfig",
+        engine_config: "EngineConfig",
+        merge_lora: bool = False,
+        tp_size: int = 1,
+        tp_rank: int = 0,
+        vit_config: Optional["VitConfig"] = None,
+    ):
+        """Initialize ModelDeployWeightInfo with independent configuration objects."""
+        self.py_model_config = py_model_config
+        self.engine_config = engine_config
+        self.merge_lora = merge_lora
+        
+        self._use_swizzleA = engine_config.hw_kernel_config.use_swizzleA
+        self._use_qk_norm = py_model_config.qk_norm
+        self._hidden_size = py_model_config.hidden_size
+        self._inter_size = py_model_config.inter_size
+        self._inter_padding_size = py_model_config.inter_padding_size
+        self._moe_inter_padding_size = py_model_config.moe_inter_padding_size
+        self._head_num = py_model_config.attn_config.head_num
+        self._head_num_kv = py_model_config.attn_config.kv_head_num
         self.tp_size = tp_size
         self.tp_rank = tp_rank
-        self.ep_size = config.ep_size
-        self.ep_rank = config.ep_rank
-        self.dp_size = config.dp_size
-        self.dp_rank = config.dp_rank
-        self.num_nodes: int = config.num_nodes
-        self.ffn_tp_rank = config.ffn_tp_rank
-        self.ffn_tp_size = config.ffn_tp_size
-        self._size_per_head = config.size_per_head
+        self.ep_size = engine_config.parallelism_config.ep_size
+        self.ep_rank = engine_config.parallelism_config.ep_rank
+        self.dp_size = engine_config.parallelism_config.dp_size
+        self.dp_rank = engine_config.parallelism_config.dp_rank
+        # num_nodes should come from gang_info, not from config
+        self.num_nodes: int = 1  # Will be set from gang_info later
+        self.ffn_tp_rank = engine_config.parallelism_config.ffn_tp_rank
+        self.ffn_tp_size = engine_config.parallelism_config.ffn_tp_size
+        self._size_per_head = py_model_config.attn_config.size_per_head
         if self._head_num_kv == -1:
             self._head_num_kv = self._head_num
-        self._quant_algo = config.quant_algo
-        self._quant_config = config.quant_config
-        self._num_layers = config.num_layers
-        self._layer_head_num = config.layer_head_num
-        self._layer_inter_padding_size = config.layer_inter_padding_size
+        self._quant_algo = py_model_config.quant_algo
+        self._quant_config = py_model_config.quant_config
+        self._num_layers = py_model_config.num_layers
         self._has_prefix_encoder = False
-        self._is_sparse_head = config.is_sparse_head
-        self._layer_head_num = config.layer_head_num
-        self._src_quantization_bit = config.src_quantization_bit
-        self.tp_split_emb_and_lm_head = config.tp_split_emb_and_lm_head
-
-        self._is_gated_activation = config.gpt_init_params.isGatedActivation()
-        self.expert_num_ = config.gpt_init_params.expert_num
-        self.moe_n_group_ = config.moe_n_group
-        self.enable_eplb_ = config.enable_eplb
-        self.phy_exp_num_ = config.phy_exp_num
-        self.moe_k_ = config.gpt_init_params.moe_k
-        self.moe_layer_index_ = config.gpt_init_params.moe_layer_index
-        self.moe_style_ = config.gpt_init_params.moe_style
-        self._moe_inter_padding_size = config.moe_inter_padding_size
-
-        self.tie_word_embeddings = config.tie_word_embeddings
+        self._src_quantization_bit = py_model_config.src_quantization_bit
+
+        self._is_gated_activation = py_model_config.isGatedActivation()
+        self.expert_num_ = py_model_config.expert_num
+        self.moe_n_group_ = py_model_config.moe_n_group
+        self.enable_eplb_ = py_model_config.eplb_config.enable_eplb()
+        self.phy_exp_num_ = py_model_config.eplb_config.phy_exp_num(py_model_config.expert_num)
+        self.moe_k_ = py_model_config.moe_k
+        self.moe_layer_index_ = py_model_config.moe_layer_index
+        self.moe_style_ = py_model_config.moe_style
+        self._moe_inter_padding_size = py_model_config.moe_inter_padding_size
+
+        self.tie_word_embeddings = py_model_config.tie_word_embeddings
         self.weight_style = WeightStyle.NONE
 
         # for mla
-        self.kv_lora_rank = config.kv_lora_rank
-        self.nope_head_dim = config.nope_head_dim
-        self.rope_head_dim = config.rope_head_dim
-        self.v_head_dim = config.v_head_dim
-
-        # for vit sep
-        self.vit_separation = config.vit_separation
+        self.kv_lora_rank = py_model_config.attn_config.kv_lora_rank
+        self.nope_head_dim = py_model_config.attn_config.nope_head_dim
+        self.rope_head_dim = py_model_config.attn_config.rope_head_dim
+        self.v_head_dim = py_model_config.attn_config.v_head_dim
+        self.vit_separation = vit_config.vit_separation
 
         # for eplb
-        self.phy2log = config.phy2log
+        # phy2log should be loaded from LoadConfig, not from config
+        self.phy2log = None  # Will be set in create_load_config
+
         # for moe
         self._use_stack_weight = False
 
-        self.kv_cache_data_type = config.kv_cache_data_type
+        self.kv_cache_data_type = py_model_config.kv_cache_data_type
 
         self.is_ffn_service = (
-            config.gpt_init_params.ffn_disaggregate_config.is_ffn_service()
+            engine_config.parallelism_config.ffn_disaggregate_config.is_ffn_service()
         )
+        
+        # Create a config wrapper for backward compatibility
+        class ConfigWrapper:
+            def __init__(self, py_model_config, engine_config):
+                self.py_model_config = py_model_config
+                self.engine_config = engine_config
+                self.py_env_configs = None  # Will be set if needed
+                self.py_eplb = None  # Will be set if needed
+        
+        self.config = ConfigWrapper(py_model_config, engine_config)
 
     @property
     def support_lora(self):
@@ -216,13 +241,19 @@ class ModelDeployWeightInfo:
 
     @property
     def attn_config(self):
+        use_fp8_kv_cache = False
+        kv_cache_config = self.engine_config.kv_cache_config
+        if kv_cache_config:
+            use_fp8_kv_cache = (
+                self.kv_cache_data_type == WEIGHT_TYPE.FP8.to_str()
+                and kv_cache_config.blockwise_use_fp8_kv_cache == 1
+            )        
         attn_config = AttnConfig(
             hidden_size=self._hidden_size,
             size_per_head=self._size_per_head,
             head_num=self._head_num,
             head_num_kv=self._head_num_kv,
-            use_fp8_kv_cache=self.kv_cache_data_type == WEIGHT_TYPE.FP8.to_str()
-            and StaticConfig.py_kv_cache_config.blockwise_use_fp8_kv_cache == 1,
+            use_fp8_kv_cache=use_fp8_kv_cache,
         )
         return attn_config
 
@@ -237,7 +268,11 @@ class ModelDeployWeightInfo:
 
     def get_weight_info(self) -> ModelWeightInfo:
         weight_info = self._get_weight_info()
-        use_fp32 = self.config.py_env_configs.model_config.use_float32
+        if (isinstance(self, BaseMultiModalWeightInfo) and
+            self.vit_separation != VitSeparation.VIT_SEPARATION_REMOTE and
+            self.tp_rank == 0):
+            weight_info = self._get_vit_info(weight_info)
+        use_fp32 = self.py_model_config.use_float32
         if use_fp32:
             weight_info = weight_info.set_weight_dtype(torch.float32)
 
@@ -267,10 +302,6 @@ class ModelDeployWeightInfo:
         if self.tie_word_embeddings:
             logging.info("fix tie_word_embeddings")
             weight_info = self._fix_tie_lm_head(weight_info)
-        if self._is_sparse_head:
-            logging.info("Skiping load empty weight for head_num == 0")
-            weight_info = self._process_sparse_weight(weight_info)
-
         return weight_info
 
     def _fix_weight_style_layer_weight(self, origin_weight_info: ModelWeightInfo):
@@ -431,34 +462,6 @@ class ModelDeployWeightInfo:
         origin_weight_info.weights[lm_head_idx] = lm_head
         return origin_weight_info
 
-    def _process_sparse_weight(
-        self, origin_weight_info: ModelWeightInfo
-    ) -> ModelWeightInfo:
-        if not isinstance(origin_weight_info.layer_weights[0], list):
-            raise Exception("model weight use sparse config should be list(list())")
-        new_layer_weights = []
-
-        skip_weights_list = [
-            W.attn_qkv_w,
-            W.attn_qkv_b,
-            W.attn_ln_gamma,
-            W.attn_ln_beta,
-            W.qk_ln_gamma,
-            W.attn_o_w,
-        ]
-
-        for i, layer_weight in enumerate(origin_weight_info.layer_weights):
-            if self._layer_head_num[i] == 0:
-                new_weights = [
-                    weight
-                    for weight in layer_weight
-                    if weight.name not in skip_weights_list
-                ]
-            else:
-                new_weights = layer_weight
-            new_layer_weights.append(new_weights)
-        return ModelWeightInfo(origin_weight_info.weights, new_layer_weights)
-
     def _get_weight_info(self) -> ModelWeightInfo:
         raise NotImplementedError()
 
@@ -525,7 +528,7 @@ class ModelDeployWeightInfo:
         if not database.is_ft_style:
             merge_lora = (
                 database.has_lora()
-                and self.config.py_env_configs.lora_config.merge_lora
+                and self.merge_lora
             )
 
         if database.has_lora() and not self.support_lora:
@@ -536,7 +539,7 @@ class ModelDeployWeightInfo:
         if (
             database.is_ft_style
             and database.ft_weight_params
-            and self.vit_separation != 1
+            and self.vit_separation != VitSeparation.VIT_SEPARATION_ROLE
         ):
             # check ft_style ParallelInfo is match weight's ParallelInfo
             src_tp_size = int(database.ft_weight_params.get("TP_SIZE", self.tp_size))
@@ -577,14 +580,13 @@ class ModelDeployWeightInfo:
             num_nodes=self.num_nodes,
             ffn_tp_rank=self.ffn_tp_rank,
             ffn_tp_size=self.ffn_tp_size,
-            tp_split_emb_and_lm_head=self.tp_split_emb_and_lm_head,
             merge_lora=merge_lora,
             vit_separation=self.vit_separation,
             compute_dtype=compute_dtype,
             quant_algo=self._quant_algo,
             bit=self._quant_algo.getWeightBits(),
             is_ft_style_weight=database.is_ft_style,
-            phy2log=self.config.phy2log,  # Notice use config, because phy2log init after ModelDeployWeightInfo.__init__
+            phy2log=self.phy2log,  # phy2log should be set before create_load_config is called
             exported_device=exported_device,
             use_swizzleA=self._use_swizzleA
         )
diff --git a/rtp_llm/model_loader/weight_manager.py b/rtp_llm/model_loader/weight_manager.py
index 61030ad24..41e3a512c 100644
--- a/rtp_llm/model_loader/weight_manager.py
+++ b/rtp_llm/model_loader/weight_manager.py
@@ -13,6 +13,7 @@ from rtp_llm.model_loader.model_weight_info import ModelWeights
 
 # Assuming these imports are from your project and accessible
 from rtp_llm.model_loader.weight_module import WeightModule
+from rtp_llm.distribute.worker_info import g_parallel_info
 
 from .tipc import CudaIpcHelper, SharedMemIpcMeta, SharedMemoryIPCHelper
 
@@ -108,7 +109,7 @@ class WeightManager:
         """
         self._model = model
         self._s_helper = SharedMemoryIPCHelper()
-        self._device: torch.device = model.model.device
+        self._device: torch.device = torch.device(g_parallel_info.device)
         self._weights: ModelWeights = model.model.weight
         self._weights_loader: ModelLoader = model.model.model_weights_loader
         self._weight_module = self._weights_loader._model_weights_info
diff --git a/rtp_llm/model_loader/weight_module.py b/rtp_llm/model_loader/weight_module.py
index f04a24ef8..8eee5e94d 100644
--- a/rtp_llm/model_loader/weight_module.py
+++ b/rtp_llm/model_loader/weight_module.py
@@ -607,17 +607,6 @@ class AtomicWeight(WeightModule):
         ):
             return {self.name: raw_tensor}
 
-        tp_split_emb_and_lm_head = load_config.tp_split_emb_and_lm_head
-
-        if not tp_split_emb_and_lm_head and self.name in [
-            W.lm_head,
-            W.lm_head_b,
-            W.embedding,
-            W.positional_embedding,
-            W.token_type_embedding,
-        ]:
-            return {self.name: raw_tensor}
-
         split_func = self._get_split_func()
 
         ts = (
diff --git a/rtp_llm/models/base_model.py b/rtp_llm/models/base_model.py
index d3524964e..3cd95fae0 100644
--- a/rtp_llm/models/base_model.py
+++ b/rtp_llm/models/base_model.py
@@ -1,13 +1,10 @@
 import logging
-from typing import Any, Dict, List, NamedTuple, Optional, Union
+from typing import Optional, Union
 
 import torch
-from pydantic import BaseModel as PyBaseModel
 
-from rtp_llm.config.generate_config import GenerateConfig, RoleAddr, RoleType
-from rtp_llm.config.gpt_init_model_parameters import ConfigMode, GptInitModelParameters
-from rtp_llm.config.task_type import TaskType
-from rtp_llm.distribute.gang_info import get_gang_info
+from rtp_llm.config.generate_config import GenerateConfig
+from rtp_llm.config.engine_config import EngineConfig
 from rtp_llm.distribute.worker_info import ParallelInfo, g_parallel_info
 from rtp_llm.frontend.tokenizer_factory.tokenizer_factory import (
     BaseTokenizer,
@@ -19,68 +16,67 @@ from rtp_llm.models.downstream_modules.custom_module import CustomModule
 from rtp_llm.models.downstream_modules.utils import create_custom_module
 from rtp_llm.models.multimodal.multimodal_mixin import MultiModalMixin
 from rtp_llm.models_py.model_desc.module_base import GptModelBase
-from rtp_llm.utils.base_model_datatypes import (
-    AuxInfo,
-    EmbeddingOutput,
-    GenerateContext,
-    GenerateInput,
-    GenerateOutput,
-    GenerateOutputs,
-    GenerateResponse,
-    ModelConfig,
-)
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.py_config_modules import VitConfig
+from rtp_llm.config.kv_cache_config import KVCacheConfig
+from rtp_llm.ops import MMModelConfig
 from rtp_llm.utils.database import CkptDatabase
-from rtp_llm.utils.multimodal_util import MultimodalInput
 from rtp_llm.utils.time_util import timer_wrapper
 from rtp_llm.utils.util import to_torch_dtype
 
-FT_DEFAULT_MAX_NEW_TOKENS = 2048
-
-
 class BaseModel(object):
 
-    config: GptInitModelParameters
-    vocab_size_padded: int
-    device: str
+    # Independent configuration objects
+    py_model_config: ModelConfig
+    mm_model_config: MMModelConfig
+    engine_config: EngineConfig
+
+    def __init__(
+        self,
+        py_model_config: ModelConfig,
+        mm_model_config: MMModelConfig,
+        engine_config: EngineConfig,
+        vit_config: Optional[VitConfig] = None,
+        merge_lora: bool = False,
+    ) -> None:
+        """Initialize BaseModel with independent configuration objects.
+        Args:
+            py_model_config: Model configuration (contains template_type, model_name, lora_infos)
+            mm_model_config: Multimodal model configuration
+            engine_config: Engine configuration
+            vit_config: Optional VitConfig (needed for multimodal models)
+            merge_lora: Whether to merge LoRA weights
+        """
+        self.py_model_config = py_model_config
+        self.mm_model_config = mm_model_config
+        self.engine_config = engine_config
+        self.vit_config = vit_config
+        self.merge_lora = merge_lora
 
-    def __init__(self, config: GptInitModelParameters) -> None:
-        self.config = config
         self.weight = None
-
-        self.linear_bias_slopes: Optional[torch.Tensor] = None
-        self.prefix_tokens: Optional[torch.Tensor] = None
         self.tokenizer: Optional[BaseTokenizer] = None
-        self.max_input_buffer_len: int = 0
-
-        self.task_type: TaskType = TaskType.LANGUAGE_MODEL
         self.custom_module: Optional[CustomModule] = None
-        self.is_attn_model = False
-        if self.config:
-            self.is_attn_model = (
-                config.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate
-                and not config.gpt_init_params.ffn_disaggregate_config.is_ffn_service()
-            )
-
         self.default_generate_config: GenerateConfig = GenerateConfig()
         self.load_tokenizer()
 
         self.py_model: Optional[GptModelBase] = None
 
+
     @timer_wrapper(description="load model")
     def load(self, parallel_info: ParallelInfo = g_parallel_info):
         if (
-            self.config.model_specific_config.load_python_model
-            and self.config.hw_kernel_config.enable_cuda_graph
+            self.engine_config.model_specific_config.load_python_model
+            and self.engine_config.hw_kernel_config.enable_cuda_graph
             and self.support_cuda_graph() is False
         ):
             raise Exception("current model can't support cuda graph in py model mode")
 
         self.model_weights_loader = self.create_model_loader(parallel_info)
-        self._load(self.device)
+        self._load(parallel_info.device)
 
-        if self.config.model_specific_config.load_python_model:
+        if self.engine_config.model_specific_config.load_python_model:
             logging.info(
-                f"Creating python model for {self.config.ckpt_path} on {self.device}"
+                f"Creating python model for {self.py_model_config.ckpt_path} on {parallel_info.device}"
             )
             self._create_python_model()
         else:
@@ -95,62 +91,95 @@ class BaseModel(object):
     def _load(self, device: str):
         # set empty weights for attention service
         self.weight: ModelWeights = self.model_weights_loader.load_weights(
-            device=self.device
+            device=device
         )
         self._load_custom_module()
         self._load_multimodal()
         self.model_weights_loader.force_clean_cuda_memory()
 
     @classmethod
-    def create_config(
-        cls,
-        model_config: ModelConfig,
-        parallel_info: ParallelInfo = g_parallel_info,
-        config_mode: ConfigMode = ConfigMode.ComplexMode,
-    ) -> GptInitModelParameters:
-        config: GptInitModelParameters = cls._create_config(model_config.ckpt_path)
-        if config.hidden_size == 0:
-            config.hidden_size = config.size_per_head * config.head_num
-        config.update_common(
-            ckpt_path=model_config.ckpt_path,
-            tokenizer_path=model_config.tokenizer_path,
-            quantization=model_config.quantization,
-            data_type=model_config.act_type,
-            kv_cache_type=model_config.kv_cache_type,
-            max_seq_len=model_config.max_seq_len,
-            seq_size_per_block=model_config.seq_size_per_block,
-            gen_num_per_circle=model_config.gen_num_per_circle,
-            lora_infos=model_config.lora_infos,
-            ptuning_path=model_config.ptuning_path,
-            ref_module=model_config.ref_module,
-            ref_dict=model_config.ref_dict,
-            parallel_info=parallel_info,
-            gang_info=get_gang_info(),
-            config_mode=config_mode,
-        )
-        cls._update_config(config)
-        return config
-
-    @classmethod
-    def _create_config(cls, ckpt_path: str) -> GptInitModelParameters:
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         raise NotImplementedError()
 
-    @classmethod
-    def _update_config(cls, config: GptInitModelParameters):
-        pass
-
     @classmethod
     def from_config(
-        cls, config: Any, parallel_info: ParallelInfo = g_parallel_info
+        cls,
+        py_model_config: ModelConfig,
+        mm_model_config: MMModelConfig,
+        engine_config: EngineConfig,
+        parallel_info: ParallelInfo = g_parallel_info,
+        vit_config: Optional[VitConfig] = None,
+        merge_lora: bool = False,
     ) -> "BaseModel":
-        model = cls(config)
+        """Create model from independent configuration objects.
+        
+        Args:
+            py_model_config: Model configuration (contains template_type, model_name, lora_infos)
+            mm_model_config: Multimodal model configuration
+            engine_config: Engine configuration
+            parallel_info: Parallel information for loading
+            vit_config: Optional VitConfig (needed for multimodal models)
+            merge_lora: Whether to merge LoRA weights
+        """
+        # All metadata is in py_model_config
+        model = cls(
+            py_model_config=py_model_config,
+            mm_model_config=mm_model_config,
+            engine_config=engine_config,
+            vit_config=vit_config,
+            merge_lora=merge_lora,
+        )
         model.load(parallel_info)
         return model
-
     @staticmethod
     def get_weight_cls() -> ModelDeployWeightInfo:
         raise NotImplementedError
 
+    @property
+    def config(self):
+        """Configuration wrapper that combines py_model_config, engine_config, and vit_config.
+        
+        This property provides a unified interface for accessing all configuration objects
+        needed by AsyncModel, RPCEngine, and other components.
+        """
+        # Create a simple wrapper object that combines all configs
+        class ConfigWrapper:
+            def __init__(self, base_model):
+                self.base_model = base_model
+                # Direct access to config objects
+                self.py_model_config = base_model.py_model_config
+                self.mm_model_config = base_model.mm_model_config
+                self.engine_config = base_model.engine_config
+                self.vit_config = base_model.vit_config
+                
+                # Expose engine_config sub-configs for compatibility
+                self.parallelism_config = base_model.engine_config.parallelism_config
+                self.runtime_config = base_model.engine_config.runtime_config
+                self.pd_sep_config = base_model.engine_config.pd_sep_config
+                self.concurrency_config = base_model.engine_config.concurrency_config
+                self.fmha_config = base_model.engine_config.fmha_config
+                self.kv_cache_config = base_model.engine_config.kv_cache_config
+                self.profiling_debug_logging_config = base_model.engine_config.profiling_debug_logging_config
+                self.hw_kernel_config = base_model.engine_config.hw_kernel_config
+                self.device_resource_config = base_model.engine_config.device_resource_config
+                self.moe_config = base_model.engine_config.moe_config
+                self.model_specific_config = base_model.engine_config.model_specific_config
+                self.sp_config = base_model.engine_config.sp_config
+                self.cache_store_config = base_model.engine_config.cache_store_config
+                self.misc_config = base_model.engine_config.misc_config
+                self.arpc_config = base_model.engine_config.arpc_config
+                self.ffn_disaggregate_config = base_model.engine_config.parallelism_config.ffn_disaggregate_config
+                
+                # Expose commonly accessed attributes
+                self.special_tokens = base_model.py_model_config.special_tokens
+                self.max_seq_len = base_model.py_model_config.max_seq_len
+                self.role_type = base_model.engine_config.pd_sep_config.role_type
+                self.is_multimodal = base_model.mm_model_config.is_multimodal
+        
+        if not hasattr(self, '_config_wrapper'):
+            self._config_wrapper = ConfigWrapper(self)
+        return self._config_wrapper
+
     @property
     def dtype(self) -> Union[str, torch.dtype]:
         assert self.weight is not None
@@ -160,46 +189,94 @@ class BaseModel(object):
     def _may_init_multimodal(self):
         if self.is_multimodal():
             assert isinstance(self, MultiModalMixin)  # for syntax check
-            self.config.is_multimodal = True
+            self.mm_model_config.is_multimodal_ = True
             if self.parallel_info.tp_rank == 0:
-                self.init_multimodal(self.config, self.device)
-
+                if self.vit_config is None:
+                    raise ValueError("vit_config is required for multimodal models")
+                # Only initialize multimodal if vit_separation != REMOTE
+                from rtp_llm.ops import VitSeparation
+                vit_separation = self.vit_config.vit_separation
+                if vit_separation != VitSeparation.VIT_SEPARATION_REMOTE:
+                    self.init_multimodal(
+                        mm_model_config=self.mm_model_config,
+                        vit_config=self.vit_config,
+                        device=self.parallel_info.device,
+                    )
     @timer_wrapper(description="init custom_module")
     def _init_misc(self):
         self._may_init_multimodal()
-        self.task_type = self.config.task_type
         self.custom_module = self._init_custom_module()
-        self.compute_dtype: torch.dtype = to_torch_dtype(self.config.data_type)
 
     def _init_custom_module(self) -> Optional[CustomModule]:
-        return create_custom_module(self.task_type, self.config, self.tokenizer)
+        return create_custom_module(self.py_model_config.task_type, self, self.tokenizer)
 
     def load_tokenizer(self) -> None:
-        if self.config:
-            self.tokenizer = TokenizerFactory.create_from_config(self.config)
-        else:
-            self.tokenizer = TokenizerFactory.create_from_env()
-        if hasattr(self.tokenizer, "eos_token_id") and self.tokenizer.eos_token_id:
-            self.config.special_tokens.eos_token_id = self.tokenizer.eos_token_id
-            self.config.update_task_prompt_tokens_id(self.tokenizer)
+        # Get tokenizer parameters from config
+        ckpt_path = self.py_model_config.ckpt_path
+        tokenizer_path = self.py_model_config.tokenizer_path
+        
+        # Get model_type from config.json or py_model_config
+        import json
+        import os
+        model_type = ""
+        # First try to get from py_model_config.model_type
+        if self.py_model_config.model_type:
+            model_type = self.py_model_config.model_type.lower()
+        
+        # If not found, try to get from config.json
+        if not model_type:
+            config_json_path = os.path.join(ckpt_path, "config.json")
+            if os.path.exists(config_json_path):
+                with open(config_json_path, "r", encoding="utf-8") as reader:
+                    config_json = json.loads(reader.read())
+                    # Try to get model_type from architectures field
+                    if "architectures" in config_json and config_json["architectures"]:
+                        model_type = config_json["architectures"][0].lower()
+                    elif "model_type" in config_json:
+                        model_type = config_json["model_type"].lower()
+        
+        # model_type should be found in config.json or py_model_config
+        if not model_type:
+            raise ValueError("model_type not found in config.json and cannot be determined")
+
+        self.tokenizer = TokenizerFactory.create(ckpt_path, tokenizer_path, model_type)
+    
+        # Load task prompt config and update token IDs if tokenizer is available
+        # Ensure kv_cache_config is Python KVCacheConfig instance (not C++ object)
+        if not isinstance(self.engine_config.kv_cache_config, KVCacheConfig):
+            # Replace C++ object with Python instance
+            cpp_config = self.engine_config.kv_cache_config
+            python_config = KVCacheConfig()
+            # Copy all attributes from C++ object to Python object
+            for attr in dir(cpp_config):
+                if not attr.startswith('_') and hasattr(cpp_config, attr):
+                    try:
+                        setattr(python_config, attr, getattr(cpp_config, attr))
+                    except (AttributeError, TypeError):
+                        pass
+            self.engine_config.kv_cache_config = python_config
+        
+        if self.engine_config.kv_cache_config.multi_task_prompt or self.engine_config.kv_cache_config.multi_task_prompt_str:
+            self.engine_config.kv_cache_config.load_and_update_task_prompt_config(self.tokenizer)
+
+        if self.tokenizer.eos_token_id:
+            self.py_model_config.special_tokens.eos_token_id = self.tokenizer.eos_token_id
 
     def is_multimodal(self) -> bool:
         return isinstance(self, MultiModalMixin)
 
     def _init_database(self):
-        self.database = CkptDatabase(self.config.ckpt_path, self.config.ptuning_path)
-        # static lora load
-        self.static_lora: bool = (
-            self.config.lora_infos is not None and len(self.config.lora_infos) == 1
-        )
+        self.database = CkptDatabase(self.py_model_config.ckpt_path, self.py_model_config.ptuning_path)
+        lora_infos = self.py_model_config.lora_infos
+        self.static_lora: bool = len(lora_infos) == 1
         if self.static_lora:
-            for name, path in self.config.lora_infos.items():
+            for name, path in lora_infos.items():
                 self.database.load_lora(name, path)
             self.database.dump_lora_info()
-
+                
     def _load_model_weights(self):
         self.weight: ModelWeights = self.model_weights_loader.load_weights(
-            device=self.device
+            device=self.parallel_info.device
         )
 
     @timer_wrapper(description="load custom module")
@@ -209,62 +286,22 @@ class BaseModel(object):
 
     @timer_wrapper(description="load multimodal")
     def _load_multimodal(self):
-        if self.config.vit_separation != 2 and self.is_multimodal():
+        from rtp_llm.ops import VitSeparation
+        if self.vit_config is not None and self.vit_config.vit_separation != VitSeparation.VIT_SEPARATION_REMOTE and self.is_multimodal():
+            assert isinstance(self, MultiModalMixin)  # for syntax check
+            # Convert torch.dtype to string for load_mm_weight
+            dtype_str = self.py_model_config.data_type
             self.load_mm_weight(
-                self.compute_dtype,
-                self.config.tp_size,
-                self.config.tp_rank,
-                self.device,
+                py_model_config=self.py_model_config,
+                mm_model_config=self.mm_model_config,
+                ctype=dtype_str,
+                tp_size=self.engine_config.parallelism_config.tp_size,
+                tp_rank=self.engine_config.parallelism_config.tp_rank,
+                device=self.parallel_info.device,
             )
 
-    def dup_dim0_for_beam_search(
-        self, t: torch.Tensor, beam_width: int
-    ) -> torch.Tensor:
-        shape = list(t.shape)
-        return (
-            t.unsqueeze(1)
-            .repeat([1, beam_width] + [1] * len(shape[1:]))
-            .reshape([-1] + shape[1:])
-            .contiguous()
-        )
-
-    def extend_context_combo_token_types(self, token_types: List[int]) -> List[int]:
-        return []
-
-    def extend_generate_combo_token_types(self, combo_tokens: List[int]) -> List[int]:
-        return []
-
-    def create_context_position_ids(
-        self, input_lengths: Union[List[int], torch.Tensor]
-    ):
-        return torch.concat(
-            [
-                torch.arange(int(input_length), dtype=torch.int32)
-                for input_length in input_lengths
-            ],
-            dim=0,
-        )
-
-    def create_context_decoder_mask(self, input_lengths: List[int]):
-        batch_size = len(input_lengths)
-        max_input_length = max(input_lengths)
-        attention_mask = torch.ones(
-            (max_input_length, max_input_length), dtype=torch.bool, device=self.device
-        )
-        if self.config.is_causal:
-            attention_mask = attention_mask.tril()
-        attention_mask = (
-            attention_mask.unsqueeze_(0).tile(batch_size, 1, 1).to(self.dtype)
-        )
-        for b, input_length in enumerate(input_lengths):
-            attention_mask[b, input_length:, ...] = 0
-            if not self.config.is_causal:
-                attention_mask[b, :, input_length:] = 0
-        return attention_mask
-
     def create_model_loader(self, parallel_info: ParallelInfo) -> ModelLoader:
         self.parallel_info = parallel_info
-        self.device = self.parallel_info.device
 
         self._init_misc()
         self._init_database()
@@ -273,24 +310,16 @@ class BaseModel(object):
         tp_size = self.parallel_info.tp_size
 
         weights_info: ModelDeployWeightInfo = self.get_weight_cls()(
-            self.config, tp_size, tp_rank
+            self.py_model_config, self.engine_config, self.merge_lora, tp_size, tp_rank, self.vit_config
         )
         misc_weights_info = (
             self.custom_module.get_custom_weight_info() if self.custom_module else []
         )
         return get_model_loader(
-            self.task_type,
+            self.py_model_config,
             weights_info,
             misc_weights_info,
-            self.compute_dtype,
+            to_torch_dtype(self.py_model_config.data_type),
             self.database,
-            self.is_attn_model,
         )
 
-    @staticmethod
-    def eval_model_size(config: GptInitModelParameters):
-        return config.eval_model_size()
-
-    @staticmethod
-    def eval_model_param_count(config: GptInitModelParameters):
-        return config.model_param_count
diff --git a/rtp_llm/models/bert.py b/rtp_llm/models/bert.py
index 1d0edfc0c..86b7a8755 100644
--- a/rtp_llm/models/bert.py
+++ b/rtp_llm/models/bert.py
@@ -3,11 +3,8 @@ import logging
 import os
 from typing import Any, Dict, List, Optional
 
-import torch
-from transformers import AutoTokenizer, BertTokenizer
-
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.config.task_type import TaskType
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.ops import TaskType
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.base_model import BaseModel
 from rtp_llm.models.bert_weight import BertWeightInfo, RobertaWeightInfo
@@ -29,23 +26,17 @@ class Bert(BaseModel):
         return BertWeightInfo
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0,
-            size_per_head=0,
-            layer_num=0,
-            max_seq_len=0,
-            vocab_size=0,
-            ckpt_path=ckpt_path,
-            activation_type="gelu",
-            norm_type="layernorm",
-            rotary_embedding_dim=0,
-            rotary_embedding_style=0,
-            has_positional_encoding=True,
-            has_pre_decoder_layernorm=True,
-            layernorm_type="post_layernorm",
-            is_causal=False,
-        )
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
+        config = ModelConfig()
+        config.ckpt_path = ckpt_path
+        config.activation_type = "gelu"
+        config.norm_type = "layernorm"
+        config.rotary_embedding_dim_ = 0
+        config.rotary_embedding_style_ = 0
+        config.has_positional_encoding_ = True
+        config.has_pre_decoder_layernorm_ = True
+        config.layernorm_type = "post_layernorm"
+        config.is_causal_ = False
         # hugggingface
         config_path = os.path.join(ckpt_path, "config.json")
         if not os.path.exists(config_path):
@@ -60,36 +51,53 @@ class Bert(BaseModel):
         return True
 
     def _create_python_model(self) -> Optional[GptModelBase]:
-        self.py_model = BertModel(self.config, self.weight)
+        model_config = self.model_config
+        parallelism_config = self.engine_config.parallelism_config
+        device_resource_config = self.engine_config.device_resource_config
+        quant_config = self.model_config.quant_config
+        vocab_size = self.model_config.vocab_size
+        fmha_config = self.engine_config.fmha_config
+        py_hw_kernel_config = self.engine_config.hw_kernel_config
+        
+        self.py_model = BertModel(
+            model_config,
+            parallelism_config,
+            device_resource_config,
+            self.weight,
+            quant_config,
+            vocab_size,
+            fmha_config=fmha_config,
+            py_hw_kernel_config=py_hw_kernel_config,
+        )
 
     def _init_custom_module(self) -> Optional[CustomModule]:
-        if self.task_type == TaskType.SEQ_CLASSIFICATION:
+        if self.py_model_config.task_type == TaskType.SEQ_CLASSIFICATION:
             logging.info("using BertClassifierModule as custom module")
-            return BertClassifierModule(self.config, self.tokenizer)
-        if self.task_type == TaskType.RERANKER:
+            return BertClassifierModule(self.model_config, self.tokenizer)
+        if self.py_model_config.task_type == TaskType.RERANKER:
             logging.info("using BertRerankerModule as custom module")
-            return BertRerankerModule(self.config, self.tokenizer)
+            return BertRerankerModule(self.model_config, self.tokenizer)
         return super()._init_custom_module()
 
     @classmethod
     def from_huggingface(
-        cls, config: GptInitModelParameters, config_json: Dict[str, Any]
+        cls, config: ModelConfig, config_json: Dict[str, Any]
     ):
         # check position_embedding_type == absolute
-        config.head_num = config_json["num_attention_heads"]
+        config.head_num_ = config_json["num_attention_heads"]
         # bert has no group attention
-        config.head_num_kv = config.head_num
-        config.size_per_head = (
+        config.head_num_kv_ = config.head_num_
+        config.size_per_head_ = (
             config_json["hidden_size"] // config_json["num_attention_heads"]
         )
         config.hidden_size = config_json["hidden_size"]
-        config.layer_num = config_json["num_hidden_layers"]
+        config.num_layers = config_json["num_hidden_layers"]
         config.max_seq_len = config_json.get("max_position_embeddings", 512)
         config.vocab_size = config_json["vocab_size"]
-        config.type_vocab_size = config_json.get("type_vocab_size", 0)
-        config.layernorm_eps = config_json["layer_norm_eps"]
+        config.type_vocab_size_ = config_json.get("type_vocab_size", 0)
+        config.layernorm_eps_ = config_json["layer_norm_eps"]
         config.inter_size = config_json["intermediate_size"]
-        config.config_dtype = config_json.get("torch_dtype", None)
+        config.config_dtype_ = config_json.get("torch_dtype", None)
 
 
 class Roberta(Bert):
@@ -98,13 +106,13 @@ class Roberta(Bert):
         return RobertaWeightInfo
 
     def _init_custom_module(self) -> Optional[CustomModule]:
-        logging.info(f"task_type : {self.task_type}")
-        if self.task_type == TaskType.SEQ_CLASSIFICATION:
+        logging.info(f"task_type : {self.py_model_config.task_type}")
+        if self.py_model_config.task_type == TaskType.SEQ_CLASSIFICATION:
             logging.info("using RobertaClassifierModule as custom module")
-            return RobertaClassifierModule(self.config, self.tokenizer)
-        elif self.task_type == TaskType.RERANKER:
+            return RobertaClassifierModule(self.model_config, self.tokenizer)
+        elif self.py_model_config.task_type == TaskType.RERANKER:
             logging.info("using RobertaRerankerModule as custom module")
-            return RobertaRerankerModule(self.config, self.tokenizer)
+            return RobertaRerankerModule(self.model_config, self.tokenizer)
         return super()._init_custom_module()
 
     def support_cuda_graph(self) -> bool:
@@ -112,25 +120,12 @@ class Roberta(Bert):
 
     @classmethod
     def from_huggingface(
-        cls, config: GptInitModelParameters, config_json: Dict[str, Any]
+        cls, config: ModelConfig, config_json: Dict[str, Any]
     ):
         Bert.from_huggingface(config, config_json)
         config.special_tokens.pad_token_id = config_json["pad_token_id"]
         config.position_ids_style = 1
 
-    def create_context_position_ids(self, input_lengths: List[int]):
-        pad_index = self.config.special_tokens.pad_token_id
-        return torch.concat(
-            [
-                torch.arange(
-                    pad_index + 1, input_length + pad_index + 1, dtype=torch.int32
-                )
-                for input_length in input_lengths
-            ],
-            dim=0,
-        )
-
-
 register_model(
     "bert", Bert, ["BertModel", "BertForMaskedLM", "BertForSequenceClassification"]
 )
diff --git a/rtp_llm/models/bloom.py b/rtp_llm/models/bloom.py
index 06d339092..64f24cbe2 100644
--- a/rtp_llm/models/bloom.py
+++ b/rtp_llm/models/bloom.py
@@ -3,7 +3,8 @@ from typing import Any, Dict
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight
 from rtp_llm.model_loader.ffn_weight import FfnAtomicWeight
@@ -187,53 +188,54 @@ class Bloom(BaseModel):
     @staticmethod
     def from_huggingface(config_json: Dict[str, Any]):
         model_type = config_json["model_type"]
-        config = GptInitModelParameters(
-            head_num=32,
-            size_per_head=128,
-            layer_num=30,
-            max_seq_len=2048,
-            vocab_size=250682,
-        )
+        config = ModelConfig()
+        config.head_num_ = 32
+        config.size_per_head_ = 128
+        config.layer_num_ = 30
+        config.max_seq_len = 2048
+        config.vocab_size = 250682
         if model_type != "bloom":
             raise BaseException(f"model type is not bloom: {model_type}")
-        config.head_num = config_json.get(
+        config.head_num_ = config_json.get(
             "num_attention_heads", config_json.get("n_head")
         )
-        config.head_num_kv = config.head_num
+        config.head_num_kv_ = config.head_num_
         config.hidden_size = config_json.get("n_embed", config_json.get("hidden_size"))
-        config.size_per_head = config.hidden_size // config.head_num
-        config.layer_num = config_json["n_layer"]
+        config.size_per_head_ = config.hidden_size // config.head_num_
+        config.layer_num_ = config_json["n_layer"]
         config.max_seq_len = config_json.get("seq_length", 2048)
         config.vocab_size = config_json["vocab_size"]
-        config.layernorm_eps = config_json["layer_norm_epsilon"]
+        config.layernorm_eps_ = config_json["layer_norm_epsilon"]
         config.inter_size = config.hidden_size * 4
+        if config.special_tokens is None:
+            from rtp_llm.config.model_config import SpecialTokens
+            config.special_tokens = SpecialTokens()
         config.special_tokens.eos_token_id = config_json.get("eos_token_id", 0)
-        config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
-        config.config_dtype = config_json.get("torch_dtype", None)
+        config.tie_word_embeddings_ = config_json.get("tie_word_embeddings", False)
+        config.config_dtype_ = config_json.get("torch_dtype", None)
         return config
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config_dict = get_config_from_path(ckpt_path)
         if config_dict:
             config = Bloom.from_huggingface(config_dict)
         else:
-            config = GptInitModelParameters(
-                head_num=32,
-                head_num_kv=32,
-                size_per_head=128,
-                inter_size=4 * 32 * 128,
-                layer_num=30,
-                max_seq_len=2048,
-                vocab_size=250880,
-            )
-        config.layernorm_eps = 1e-5
+            config = ModelConfig()
+            config.head_num_ = 32
+            config.head_num_kv_ = 32
+            config.size_per_head_ = 128
+            config.inter_size = 4 * 32 * 128
+            config.layer_num_ = 30
+            config.max_seq_len = 2048
+            config.vocab_size = 250880
+        config.layernorm_eps_ = 1e-5
         config.layernorm_type = "pre_layernorm"
         config.activation_type = "gelu"
-        config.has_positional_encoding = False
-        config.has_pre_decoder_layernorm = True
-        config.has_post_decoder_layernorm = True
-        config.use_attention_linear_bias = True
+        config.has_positional_encoding_ = False
+        config.has_pre_decoder_layernorm_ = True
+        config.has_post_decoder_layernorm_ = True
+        config.use_attention_linear_bias_ = True
         return config
 
 
diff --git a/rtp_llm/models/chat_glm_v2.py b/rtp_llm/models/chat_glm_v2.py
index b41316d89..b1910907e 100644
--- a/rtp_llm/models/chat_glm_v2.py
+++ b/rtp_llm/models/chat_glm_v2.py
@@ -1,6 +1,6 @@
 from typing import Any, Dict
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.base_model import BaseModel
 from rtp_llm.models.glm_v2_weight import GlmV2WeightInfo
@@ -21,75 +21,79 @@ class ChatGlmV2(BaseModel):
         "fp32_residual_connection": false,
         "original_rope": true,
         """
-        config = GptInitModelParameters(
-            head_num=32,
-            size_per_head=128,
-            layer_num=32,
-            max_seq_len=8192,
-            vocab_size=65024,
-        )
-        config.head_num = config_json["num_attention_heads"]
+        config = ModelConfig()
+        config.head_num_ = 32
+        config.size_per_head_ = 128
+        config.layer_num_ = 32
+        config.max_seq_len = 8192
+        config.vocab_size = 65024
+        config.head_num_ = config_json["num_attention_heads"]
         if config_json.get("multi_query_attention", False):
-            config.head_num_kv = config_json["multi_query_group_num"]
+            config.head_num_kv_ = config_json["multi_query_group_num"]
         else:
-            config.head_num_kv = config.head_num
-        config.size_per_head = (
+            config.head_num_kv_ = config.head_num_
+        config.size_per_head_ = (
             config_json["hidden_size"] // config_json["num_attention_heads"]
         )
-        config.layer_num = config_json["num_layers"]
+        config.layer_num_ = config_json["num_layers"]
         config.max_seq_len = config_json.get("seq_length", 8192)
         config.vocab_size = config_json["padded_vocab_size"]
-        config.layernorm_eps = config_json["layernorm_epsilon"]
+        config.layernorm_eps_ = config_json["layernorm_epsilon"]
         config.inter_size = config_json["ffn_hidden_size"]
-        config.add_bias_linear = config_json["add_bias_linear"]
-        config.has_post_decoder_layernorm = config_json["post_layer_norm"]
+        config.add_bias_linear_ = config_json["add_bias_linear"]
+        config.has_post_decoder_layernorm_ = config_json["post_layer_norm"]
         if "pre_seq_len" in config_json:
             config.pre_seq_len = config_json["pre_seq_len"]
         if "prefix_projection" in config_json:
-            config.prefix_projection = config_json["prefix_projection"]
-        config.src_quantization_bit = config_json.get("quantization_bit", 0)
-        config.rotary_embedding_dim = config.size_per_head
-        config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
+            config.prefix_projection_ = config_json["prefix_projection"]
+        config.src_quantization_bit_ = config_json.get("quantization_bit", 0)
+        config.rope_config.dim = config.size_per_head_
+        config.tie_word_embeddings_ = config_json.get("tie_word_embeddings", False)
+        if config.special_tokens is None:
+            from rtp_llm.config.model_config import SpecialTokens
+            config.special_tokens = SpecialTokens()
         config.special_tokens.pad_token_id = config_json.get("pad_token_id", 0)
         config = cls.get_rotary_embedding_scale(config, config_json)
         cls.update_stop_words(config, config_json)
-        config.config_dtype = config_json.get("torch_dtype", None)
+        config.config_dtype_ = config_json.get("torch_dtype", None)
         return config
 
     @classmethod
     def update_stop_words(
-        cls, config: GptInitModelParameters, config_json: Dict[str, Any]
+        cls, config: ModelConfig, config_json: Dict[str, Any]
     ):
+        if config.special_tokens is None:
+            from rtp_llm.config.model_config import SpecialTokens
+            config.special_tokens = SpecialTokens()
         config.special_tokens.eos_token_id = config_json.get("eos_token_id", 2)
 
     @staticmethod
-    def get_rotary_embedding_scale(config, config_json):
-        config.rotary_embedding_scale = config_json.get("rope_ratio", 1)
+    def get_rotary_embedding_scale(config: ModelConfig, config_json):
+        config.rope_config.scale = config_json.get("rope_ratio", 1)
         return config
 
     @staticmethod
     def default_config():
-        config = GptInitModelParameters(
-            head_num=32,
-            head_num_kv=2,
-            size_per_head=128,
-            layer_num=32,
-            max_seq_len=8192,
-            vocab_size=65024,
-            layernorm_eps=1e-5,
-            inter_size=13696,
-            add_bias_linear=False,
-            has_post_decoder_layernorm=False,
-        )
+        config = ModelConfig()
+        config.head_num_ = 32
+        config.head_num_kv_ = 2
+        config.size_per_head_ = 128
+        config.layer_num_ = 32
+        config.max_seq_len = 8192
+        config.vocab_size = 65024
+        config.layernorm_eps_ = 1e-5
+        config.inter_size = 13696
+        config.add_bias_linear_ = False
+        config.has_post_decoder_layernorm_ = False
         return config
 
     @staticmethod
-    def modify_config(config):
-        config.use_attention_linear_bias = False
+    def modify_config(config: ModelConfig):
+        config.use_attention_linear_bias_ = False
         config.activation_type = "SiGLU"
         config.norm_type = "rmsnorm"
-        config.rotary_embedding_dim = 128
-        config.rotary_embedding_style = 2
+        config.rope_config.dim = 128
+        config.rope_config.style = 2
 
         return config
 
diff --git a/rtp_llm/models/chat_glm_v3.py b/rtp_llm/models/chat_glm_v3.py
index 528149485..10be4d036 100644
--- a/rtp_llm/models/chat_glm_v3.py
+++ b/rtp_llm/models/chat_glm_v3.py
@@ -1,4 +1,3 @@
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.chat_glm_v2 import ChatGlmV2
 from rtp_llm.utils.util import get_config_from_path
diff --git a/rtp_llm/models/chat_glm_v4.py b/rtp_llm/models/chat_glm_v4.py
index 28a33c751..af967696b 100644
--- a/rtp_llm/models/chat_glm_v4.py
+++ b/rtp_llm/models/chat_glm_v4.py
@@ -2,7 +2,7 @@ from typing import Any, Dict
 
 from transformers import PreTrainedTokenizerBase
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.chat_glm_v3 import ChatGlmV3
 
@@ -10,9 +10,12 @@ from rtp_llm.models.chat_glm_v3 import ChatGlmV3
 class ChatGlmV4(ChatGlmV3):
     @classmethod
     def update_stop_words(
-        cls, config: GptInitModelParameters, config_json: Dict[str, Any]
+        cls, config: ModelConfig, config_json: Dict[str, Any]
     ):
         # chatglm4 config.json is list[int], bad format
+        if config.special_tokens is None:
+            from rtp_llm.config.model_config import SpecialTokens
+            config.special_tokens = SpecialTokens()
         if isinstance(config_json.get("eos_token_id"), list):
             config.special_tokens.eos_token_id = config_json["eos_token_id"][0]
             config.special_tokens.stop_words_id_list = [
diff --git a/rtp_llm/models/chat_glm_v4_vision.py b/rtp_llm/models/chat_glm_v4_vision.py
index b07bd128c..4a3790dde 100644
--- a/rtp_llm/models/chat_glm_v4_vision.py
+++ b/rtp_llm/models/chat_glm_v4_vision.py
@@ -1,6 +1,5 @@
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.chat_glm_v4 import ChatGlmV4
 from rtp_llm.models.chat_glm_v4_vision_weight import (
@@ -23,9 +22,11 @@ class ChatGlmV4VisionImageEmbedding(EVA2CLIPImageEmbedding):
 
 
 class ChatGlmV4Vision(ChatGlmV4, MultiModalMixin):
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = ChatGlmV4VisionImageEmbedding(config)
-        config.mm_related_params.vit_weights = ChatGlmV4VisionVitWeights(
+    def _init_multimodal(self, mm_model_config, vit_config):
+        if mm_model_config.mm_related_params is None:
+            raise ValueError("mm_model_config.mm_related_params is required for ChatGlmV4Vision")
+        self.mm_part = ChatGlmV4VisionImageEmbedding(mm_model_config.mm_related_params)
+        mm_model_config.mm_related_params.vit_weights = ChatGlmV4VisionVitWeights(
             {"vit": self.mm_part.vit}
         )
 
@@ -35,7 +36,6 @@ class ChatGlmV4Vision(ChatGlmV4, MultiModalMixin):
         config_dict = get_config_from_path(ckpt_path)
         vit_config = config_dict["vision_config"]
         config.mm_related_params.config.update(vit_config)
-        config.build_position_ids = True
         # use initial hidden size for linear_proj and conv layer in eva2clip
         config.mm_related_params.config["use_vision_hidden_size"] = False
         config.mm_related_params.config["boi_token_id"] = config_dict.get(
diff --git a/rtp_llm/models/chat_glm_v4_vision_weight.py b/rtp_llm/models/chat_glm_v4_vision_weight.py
index 422d9c41f..35463cf76 100644
--- a/rtp_llm/models/chat_glm_v4_vision_weight.py
+++ b/rtp_llm/models/chat_glm_v4_vision_weight.py
@@ -1,4 +1,4 @@
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models.glm_v2_weight import GlmV2WeightInfo
 from rtp_llm.models.multimodal.multimodal_mixin import (
     BaseMultiModalWeightInfo,
@@ -13,11 +13,6 @@ class ChatGlmV4VisionVitWeights(BaseVitWeights):
 
 
 class ChatGlmV4VisionWeightInfo(GlmV2WeightInfo, BaseMultiModalWeightInfo):
-    def __init__(self, config: GptInitModelParameters, tp_size: int, tp_rank: int):
-        GlmV2WeightInfo.__init__(self, config, tp_size, tp_rank)
-        BaseMultiModalWeightInfo.__init__(self, config)
-
-    def _get_weight_info(self):
-        glm_4v_weight = super()._get_weight_info()
-        glm_4v_weight = self._get_vit_info(glm_4v_weight)
-        return glm_4v_weight
+    def __init__(self, vit_weights, **kwargs):
+        GlmV2WeightInfo.__init__(self, **kwargs)
+        BaseMultiModalWeightInfo.__init__(self, vit_weights=vit_weights, **kwargs)
diff --git a/rtp_llm/models/cosyvoice_qwen.py b/rtp_llm/models/cosyvoice_qwen.py
index d6bb3bda5..6e4d91cb8 100644
--- a/rtp_llm/models/cosyvoice_qwen.py
+++ b/rtp_llm/models/cosyvoice_qwen.py
@@ -1,7 +1,6 @@
 import json
 import os
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.qwen_v2 import QWenV2
 
@@ -10,24 +9,14 @@ class CosyVoiceQwen(QWenV2):
     @classmethod
     def _create_config(cls, ckpt_path: str):
         config = QWenV2._create_config(ckpt_path)
-        CosyVoiceQwen._update_config(config, ckpt_path)
+        config_path = os.path.join(ckpt_path, "config.json")
+        if os.path.exists(config_path):
+            with open(config_path) as reader:
+                content = reader.read()
+                config_json = json.loads(content)
+            config.input_vocab_size = config_json.get(
+            "input_vocab_size", config.vocab_size + 151938)
         config.mm_sep_tokens = [[-200]]  # TODO(yinzhi): for SFT support
         return config
 
-    @classmethod
-    def _update_config(cls, config: GptInitModelParameters, ckpt_path: str):
-        config_path = os.path.join(ckpt_path, "config.json")
-
-        if not os.path.exists(config_path):
-            return
-        with open(config_path) as reader:
-            content = reader.read()
-            config_json = json.loads(content)
-
-        # input vocab size = speech vocab_size + (LLM vocab size + 2)
-        config.input_vocab_size = config_json.get(
-            "input_vocab_size", config.vocab_size + 151938
-        )
-
-
 register_model("cosyvoice_qwen", CosyVoiceQwen, ["CosyQwen2ForCausalLM"])
diff --git a/rtp_llm/models/deepseek_v2.py b/rtp_llm/models/deepseek_v2.py
index 3a13cd0ad..1304bd36c 100644
--- a/rtp_llm/models/deepseek_v2.py
+++ b/rtp_llm/models/deepseek_v2.py
@@ -6,8 +6,8 @@ from typing import List, Optional
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters, MlaOpsType
-from rtp_llm.config.py_config_modules import StaticConfig
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.ops import MlaOpsType
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import MlaAttnAtomicWeight, MlaConfig
 from rtp_llm.model_loader.ffn_weight import (
@@ -54,8 +54,8 @@ class DeepSeekV2Weight(ModelDeployWeightInfo):
     q_use_lora = False
     has_e_score_correction_bias = False
 
-    def __init__(self, config: GptInitModelParameters, tp_size: int, tp_rank: int):
-        super().__init__(config, tp_size, tp_rank)
+    def __init__(self, py_model_config: ModelConfig, engine_config, merge_lora: bool = False, tp_size: int = 1, tp_rank: int = 0):
+        super().__init__(py_model_config, engine_config, merge_lora=merge_lora, tp_size=tp_size, tp_rank=tp_rank)
 
     def _process_meta(self, meta_dict, weight_keys):
         if "model.layers.0.self_attn.q_a_proj.weight" in weight_keys:
@@ -76,7 +76,7 @@ class DeepSeekV2Weight(ModelDeployWeightInfo):
             kv_lora_rank=self.kv_lora_rank,
             ope_head_dim=self.nope_head_dim,
             v_head_dim=self.v_head_dim,
-            use_mla=self.config.use_mla and self.config.mla_ops_type != MlaOpsType.MHA,
+            use_mla=self.py_model_config.use_mla_ and self.py_model_config.mla_ops_type != MlaOpsType.MHA,
             q_use_lora=self.q_use_lora,
         )
         layer_weights = [
@@ -228,7 +228,7 @@ class DeepSeekV2Weight(ModelDeployWeightInfo):
                 )
             )
 
-        if self.config.use_mla and self.config.mla_ops_type != MlaOpsType.MHA:
+        if self.py_model_config.use_mla_ and self.py_model_config.mla_ops_type != MlaOpsType.MHA:
             mla_layer_weights.append(
                 MlaAttnAtomicWeight(
                     W.mla_kc,
@@ -446,26 +446,26 @@ class DeepSeekV2Weight(ModelDeployWeightInfo):
             ]
 
     def _create_rope_w(self) -> Optional[AtomicWeight]:
-        if self.config.mla_ops_type == MlaOpsType.MHA:
+        if self.py_model_config.mla_ops_type == MlaOpsType.MHA:
             return None
-        config: GptInitModelParameters = self.config
+        config = self.py_model_config
 
-        def __create_rope_w(ts: List[torch.Tensor], config: GptInitModelParameters):
+        def __create_rope_w(ts: List[torch.Tensor], config: ModelConfig):
             logging.info(
                 f"initialize rope cos sin cache with seq_len: {config.max_seq_len}"
             )
             rotary_emb = DeepseekV3YarnRotaryEmbedding(
-                config.rotary_embedding_dim,
+                config.rope_config.dim,
                 config.max_seq_len,
-                config.rotary_embedding_base,
-                scaling_factor=config.rotary_embedding_scale,
+                config.rope_config.base,
+                scaling_factor=config.rope_config.scale,
                 original_max_position_embeddings=config.org_embedding_max_pos,
                 beta_fast=config.rotary_factor2,
                 beta_slow=config.rotary_factor1,
                 mscale=config.deepseek_rope_mscale,
                 mscale_all_dim=config.deepseek_mscale_all_dim,
             )
-            half_rope_dim = config.rotary_embedding_dim // 2
+            half_rope_dim = config.rope_config.dim // 2
             cos_cache = rotary_emb.cos_cached[:, :half_rope_dim]
             sin_cache = rotary_emb.sin_cached[:, :half_rope_dim]
             # cos sin cache must be float32
@@ -507,27 +507,40 @@ class DeepSeekV2Weight(ModelDeployWeightInfo):
 class DeepSeekV2(BaseModel):
     @classmethod
     def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0,
-            head_num_kv=0,
-            size_per_head=0,
-            layer_num=0,
-            inter_size=0,
-            vocab_size=102400,
-            max_seq_len=8192,
-            norm_type="rmsnorm",
-            has_post_decoder_layernorm=True,
-        )
+        config = ModelConfig()
+        config.head_num_ = 0
+        config.head_num_kv_ = 0
+        config.size_per_head_ = 0
+        config.num_layers = 0
+        config.inter_size = 0
+        config.vocab_size = 102400
+        config.max_seq_len = 8192
+        config.norm_type = "rmsnorm"
+        config.has_post_decoder_layernorm = True
         # config.activation_type = "gated-silu"
         config.activation_type = "SiGLU"
         DeepSeekV2._from_hf(config, ckpt_path)
         return config
 
     def _create_python_model(self) -> Optional[GptModelBase]:
-        self.py_model = DeepSeekV2Model(self.config, self.weight)
+        from rtp_llm.config.engine_config import EngineConfig
+        py_model_config = self.py_model_config
+        parallelism_config = self.engine_config.parallelism_config
+        device_resource_config = self.engine_config.device_resource_config
+        quant_config = py_model_config.quant_config
+        vocab_size = py_model_config.vocab_size
+        
+        self.py_model = DeepSeekV2Model(
+            py_model_config,
+            parallelism_config,
+            device_resource_config,
+            self.weight,
+            vocab_size,
+            quant_config,
+        )
 
     @staticmethod
-    def _from_hf(config: GptInitModelParameters, ckpt_path: str):
+    def _from_hf(config: ModelConfig, ckpt_path: str):
         config_path = os.path.join(ckpt_path, "config.json")
         if not os.path.exists(config_path):
             return
@@ -535,38 +548,34 @@ class DeepSeekV2(BaseModel):
             content = reader.read()
             config_json = json.loads(content)
             config.inter_size = config_json["intermediate_size"]
-            config.head_num = config_json["num_attention_heads"]
-            config.head_num_kv = config_json.get("num_key_value_heads", config.head_num)
-            config.layer_num = config_json["num_hidden_layers"]
-            config.rotary_embedding_base = config_json.get(
-                "rope_theta", config.rotary_embedding_base
+            config.head_num_ = config_json["num_attention_heads"]
+            config.head_num_kv_ = config_json.get("num_key_value_heads", config.head_num_)
+            config.num_layers = config_json["num_hidden_layers"]
+            config.rope_config.base = config_json.get(
+                "rope_theta", config.rope_config.base
             )
             config.vocab_size = config_json["vocab_size"]
-            config.layernorm_eps = config_json.get("rms_norm_eps", 1e-06)
+            config.layernorm_eps_ = config_json.get("rms_norm_eps", 1e-06)
             config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
             config.hidden_size = config_json["hidden_size"]
 
             # MLA config
-            config.use_mla = True
-            config.mla_ops_type = MlaOpsType.__members__[
-                StaticConfig.model_config.mla_ops_type
-            ]
-            logging.info(f"deepseek2 mla_ops_type: {config.mla_ops_type.name}")
+            config.use_mla_ = True
             config.q_lora_rank = config_json["q_lora_rank"]
             config.kv_lora_rank = config_json["kv_lora_rank"]
             config.nope_head_dim = config_json["qk_nope_head_dim"]
             config.rope_head_dim = config_json["qk_rope_head_dim"]
             config.v_head_dim = config_json["v_head_dim"]
-            config.size_per_head = config.nope_head_dim + config.rope_head_dim
-            config.rotary_embedding_dim = config.rope_head_dim
+            config.size_per_head_ = config.nope_head_dim + config.rope_head_dim
+            config.rope_config.dim = config.rope_head_dim
 
             # yarn rotary config
             if config.mla_ops_type != MlaOpsType.MHA:
-                config.rotary_embedding_style = 0
+                config.rope_config.style = 0
             else:
-                config.rotary_embedding_style = 5
+                config.rope_config.style = 5
             rope_scaling = config_json.get("rope_scaling")
-            config.rotary_embedding_scale = rope_scaling["factor"]
+            config.rope_config.scale = rope_scaling["factor"]
             config.rotary_factor1 = float(rope_scaling.get("beta_slow", 1))
             config.rotary_factor2 = float(rope_scaling.get("beta_fast", 32))
             config.org_embedding_max_pos = rope_scaling[
@@ -607,7 +616,7 @@ class DeepSeekV2(BaseModel):
             n_shared_experts = config_json["n_shared_experts"]
             config.inter_size = n_shared_experts * config.moe_inter_padding_size
 
-            config.layernorm_eps = config_json.get("rms_norm_eps", 1e-06)
+            config.layernorm_eps_ = config_json.get("rms_norm_eps", 1e-06)
             config.has_moe_norm = config_json.get("norm_topk_prob", False)
             config.moe_style = 2  # shared + expert
 
@@ -615,13 +624,13 @@ class DeepSeekV2(BaseModel):
             first_k_dense_replace = config_json["first_k_dense_replace"]
             config.moe_layer_index = [
                 i
-                for i in range(config.layer_num)
+                for i in range(config.num_layers)
                 if i >= first_k_dense_replace and i % moe_step == 0
             ]
 
             ffn_inter_size = config_json.get("intermediate_size", config.inter_size)
             layer_inter_size = []
-            for i in range(config.layer_num):
+            for i in range(config.num_layers):
                 if i in config.moe_layer_index:
                     layer_inter_size.append(config.inter_size)
                 else:
@@ -636,8 +645,8 @@ class DeepSeekV2(BaseModel):
 
 class DeepSeekV3MtpWeight(DeepSeekV2Weight):
 
-    def __init__(self, config: GptInitModelParameters, tp_size: int, tp_rank: int):
-        super().__init__(config, tp_size, tp_rank)
+    def __init__(self, py_model_config: ModelConfig, engine_config, merge_lora: bool = False, tp_size: int = 1, tp_rank: int = 0):
+        super().__init__(py_model_config, engine_config, merge_lora=merge_lora, tp_size=tp_size, tp_rank=tp_rank)
 
     def _get_weight_info(self):
         layer_weights: List[List[WeightModule]] = []
diff --git a/rtp_llm/models/downstream_modules/classifier/bert_classifier.py b/rtp_llm/models/downstream_modules/classifier/bert_classifier.py
index 46b3077dd..cc77237d9 100644
--- a/rtp_llm/models/downstream_modules/classifier/bert_classifier.py
+++ b/rtp_llm/models/downstream_modules/classifier/bert_classifier.py
@@ -2,7 +2,7 @@ from typing import Dict, List
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.model_loader.weight_module import CustomAtomicWeight
 from rtp_llm.models.downstream_modules.custom_module import CustomHandler, CustomModule
@@ -16,7 +16,7 @@ from .util import load_num_labels
 
 class BertClassifierModule(CustomModule):
 
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = ClassifierRenderer(self.config_, self.tokenizer_)
         self.handler = BertClassifierHandler(self.config_)
@@ -24,7 +24,7 @@ class BertClassifierModule(CustomModule):
 
 class BertClassifierHandler(CustomHandler):
 
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__(config)
         num_labels = load_num_labels(self.config_.ckpt_path)
         self.dense = torch.nn.Linear(self.config_.hidden_size, self.config_.hidden_size)
diff --git a/rtp_llm/models/downstream_modules/classifier/classifier.py b/rtp_llm/models/downstream_modules/classifier/classifier.py
index 5066c8ea7..ae21922e4 100644
--- a/rtp_llm/models/downstream_modules/classifier/classifier.py
+++ b/rtp_llm/models/downstream_modules/classifier/classifier.py
@@ -3,7 +3,7 @@ from typing import Any, Dict, List
 import torch
 
 from rtp_llm.async_decoder_engine.embedding.interface import EngineInputs, EngineOutputs
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.model_loader.weight_module import CustomAtomicWeight
 from rtp_llm.models.downstream_modules.classifier.api_datatype import (
@@ -30,14 +30,14 @@ from .util import load_num_labels
 
 # Normal Classifier
 class ClassifierModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = ClassifierRenderer(self.config_, self.tokenizer_)
         self.handler = ClassifierHandler(self.config_)
 
 
 class ClassifierRenderer(CustomRenderer):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.generator = CommonInputGenerator(tokenizer, config)
 
@@ -59,7 +59,7 @@ class ClassifierRenderer(CustomRenderer):
 
 
 class ClassifierHandler(CustomHandler):
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__(config)
         num_labels = load_num_labels(self.config_.ckpt_path)
         self.linear = torch.nn.Linear(self.config_.hidden_size, num_labels)
@@ -87,7 +87,7 @@ class ClassifierHandler(CustomHandler):
         input_lengths: torch.Tensor,
     ) -> List[torch.Tensor]:
         # TODO test it
-        if self.config_.is_causal:
+        if self.config_.is_causal_:
             last_tokens = get_last_token_from_combo_tokens(hidden_states, input_lengths)
             return self.linear(last_tokens)
         else:
diff --git a/rtp_llm/models/downstream_modules/classifier/roberta_classifier.py b/rtp_llm/models/downstream_modules/classifier/roberta_classifier.py
index 1f0faf7f5..9ce6be49a 100644
--- a/rtp_llm/models/downstream_modules/classifier/roberta_classifier.py
+++ b/rtp_llm/models/downstream_modules/classifier/roberta_classifier.py
@@ -2,7 +2,7 @@ from typing import Dict, List
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.model_loader.weight_module import CustomAtomicWeight
 from rtp_llm.models.downstream_modules.custom_module import CustomHandler, CustomModule
@@ -15,14 +15,14 @@ from .util import load_num_labels
 
 
 class RobertaClassifierModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = ClassifierRenderer(self.config_, self.tokenizer_)
         self.handler = RobertaClassifierHandler(self.config_)
 
 
 class RobertaClassifierHandler(CustomHandler):
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__(config)
         num_labels = load_num_labels(self.config_.ckpt_path)
         self.out_proj = torch.nn.Linear(self.config_.hidden_size, num_labels)
diff --git a/rtp_llm/models/downstream_modules/common_input_generator.py b/rtp_llm/models/downstream_modules/common_input_generator.py
index ef7a5c376..09a4ba62d 100644
--- a/rtp_llm/models/downstream_modules/common_input_generator.py
+++ b/rtp_llm/models/downstream_modules/common_input_generator.py
@@ -7,7 +7,7 @@ import torch
 
 from rtp_llm.async_decoder_engine.embedding.interface import EngineInputs
 from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.metrics import GaugeMetrics, kmonitor
 from rtp_llm.models.downstream_modules.embedding.api_datatype import (
@@ -18,7 +18,7 @@ from rtp_llm.utils.time_util import current_time_ms
 
 
 class CommonInputGenerator(object):
-    def __init__(self, tokenizer: BaseTokenizer, config: GptInitModelParameters):
+    def __init__(self, tokenizer: BaseTokenizer, config: ModelConfig):
         self.tokenizer_ = tokenizer
         self.config_ = config
         from rtp_llm.models.downstream_modules.openai_render import (
diff --git a/rtp_llm/models/downstream_modules/custom_module.py b/rtp_llm/models/downstream_modules/custom_module.py
index 034bd1a3a..567cd9e50 100644
--- a/rtp_llm/models/downstream_modules/custom_module.py
+++ b/rtp_llm/models/downstream_modules/custom_module.py
@@ -4,7 +4,7 @@ import torch
 from pydantic import BaseModel
 
 from rtp_llm.async_decoder_engine.embedding.interface import EngineInputs, EngineOutputs
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.model_loader.model_weight_info import ModelWeights
 from rtp_llm.model_loader.weight_module import CustomAtomicWeight
@@ -18,7 +18,7 @@ class CustomModule(object):
     renderer: "CustomRenderer"
     handler: "CustomHandler"
 
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         self.config_ = config
         self.tokenizer_ = tokenizer
 
@@ -46,7 +46,7 @@ class CustomModule(object):
 
 
 class CustomHandler(object):
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         self.config_ = config
         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
@@ -95,7 +95,7 @@ class CustomHandler(object):
 
 
 class CustomRenderer(object):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         self.config_ = config
         self.tokenizer_ = tokenizer
 
diff --git a/rtp_llm/models/downstream_modules/embedding/all_embedding_module.py b/rtp_llm/models/downstream_modules/embedding/all_embedding_module.py
index 76fafbc8b..15f8cf001 100644
--- a/rtp_llm/models/downstream_modules/embedding/all_embedding_module.py
+++ b/rtp_llm/models/downstream_modules/embedding/all_embedding_module.py
@@ -4,7 +4,7 @@ from typing import Any, Dict, List, Union
 import torch
 
 from rtp_llm.async_decoder_engine.embedding.interface import EngineInputs, EngineOutputs
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.models.downstream_modules.custom_module import CustomHandler, CustomModule
 from rtp_llm.models.downstream_modules.embedding.api_datatype import (
@@ -23,7 +23,7 @@ from rtp_llm.models.downstream_modules.embedding.misc import (
 
 
 class ALLEmbeddingModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = ALLEmbeddingRenderer(config, tokenizer)
         self.handler = NormalHandler(config)
@@ -80,7 +80,7 @@ class ALLEmbeddingRenderer(EmbeddingRendererBase):
 
 
 class NormalHandler(CustomHandler):
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__(config)
 
     def forward(
diff --git a/rtp_llm/models/downstream_modules/embedding/bge_m3_embedding_module.py b/rtp_llm/models/downstream_modules/embedding/bge_m3_embedding_module.py
index 23364b2ae..e76a23cab 100644
--- a/rtp_llm/models/downstream_modules/embedding/bge_m3_embedding_module.py
+++ b/rtp_llm/models/downstream_modules/embedding/bge_m3_embedding_module.py
@@ -5,7 +5,7 @@ from pydantic import BaseModel
 
 from rtp_llm.async_decoder_engine.embedding.interface import EngineOutputs
 from rtp_llm.config.base_model_config import PyDanticModelBase
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.embedding.embedding_type import TYPE_STR, EmbeddingType
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.model_loader.weight_module import CustomAtomicWeight
@@ -40,7 +40,7 @@ class RequestTuple(PyDanticModelBase):
 
 
 class BgeM3EmbeddingModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         self._colbert_module = ColBertEmbeddingModule(config, tokenizer)
         self._sparse_module = SparseEmbeddingModule(config, tokenizer)
         self._dense_module = DenseEmbeddingModule(config, tokenizer)
diff --git a/rtp_llm/models/downstream_modules/embedding/colbert_embedding_module.py b/rtp_llm/models/downstream_modules/embedding/colbert_embedding_module.py
index 378f70e42..b331522f6 100644
--- a/rtp_llm/models/downstream_modules/embedding/colbert_embedding_module.py
+++ b/rtp_llm/models/downstream_modules/embedding/colbert_embedding_module.py
@@ -3,7 +3,7 @@ from typing import Any, Dict, List, Union
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.models.downstream_modules.custom_module import CustomHandler, CustomModule
 from rtp_llm.models.downstream_modules.embedding.api_datatype import (
@@ -20,14 +20,14 @@ from rtp_llm.utils.util import to_torch_dtype
 
 
 class ColBertEmbeddingModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = ColbertEmbeddingRenderer(config, tokenizer)
         self.handler = ColBertEmbeddingHandler(config)
 
 
 class ColbertEmbeddingRenderer(EmbeddingRendererBase):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.embedding_type = EmbeddingResponseType.COLBERT
 
@@ -61,7 +61,7 @@ class ColbertEmbeddingRenderer(EmbeddingRendererBase):
 
 
 class ColBertEmbeddingHandler(CustomHandler):
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__(config)
         self.colbert_linear_path_ = os.path.join(
             self.config_.ckpt_path, "colbert_linear.pt"
diff --git a/rtp_llm/models/downstream_modules/embedding/dense_embedding_module.py b/rtp_llm/models/downstream_modules/embedding/dense_embedding_module.py
index 31a9a7c13..916886717 100644
--- a/rtp_llm/models/downstream_modules/embedding/dense_embedding_module.py
+++ b/rtp_llm/models/downstream_modules/embedding/dense_embedding_module.py
@@ -11,7 +11,7 @@ import torch.nn as nn
 from sentence_transformers.models import Normalize, Transformer
 from sentence_transformers.util import import_from_string
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.models.downstream_modules.custom_module import CustomHandler, CustomModule
 from rtp_llm.models.downstream_modules.embedding.api_datatype import (
@@ -32,7 +32,7 @@ from rtp_llm.utils.util import to_torch_dtype
 
 
 class DenseEmbeddingModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = DenseEmbeddingRenderer(config, tokenizer)
         if os.path.exists(os.path.join(self.config_.ckpt_path, "modules.json")):
@@ -77,9 +77,9 @@ class DenseEmbeddingRenderer(EmbeddingRendererBase):
 
 
 class NormalHandler(CustomHandler):
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__(config)
-        self.is_causal = config.is_causal
+        self.is_causal = config.is_causal_
 
     def forward(
         self,
@@ -104,7 +104,7 @@ class SentenceTransformerHandler(CustomHandler):
     def rename_args(cls, lst: List[str]) -> List[str]:
         return [cls.arg_name_mapping.get(x, x) for x in lst]
 
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__(config)
         sys.path.append(config.ckpt_path)
         dtype = to_torch_dtype(config.data_type)
diff --git a/rtp_llm/models/downstream_modules/embedding/minicpmv_embedding_module.py b/rtp_llm/models/downstream_modules/embedding/minicpmv_embedding_module.py
index 0b9de9d7b..108758e31 100644
--- a/rtp_llm/models/downstream_modules/embedding/minicpmv_embedding_module.py
+++ b/rtp_llm/models/downstream_modules/embedding/minicpmv_embedding_module.py
@@ -1,6 +1,6 @@
 import copy
 import math
-from typing import Any, Dict, List, Union
+from typing import Any, Dict, List, Optional, Union
 
 import numpy as np
 import torch
@@ -9,7 +9,8 @@ from PIL import Image
 
 from rtp_llm.async_decoder_engine.embedding.interface import EngineInputs
 from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.py_config_modules import VitConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.metrics import GaugeMetrics, kmonitor
 from rtp_llm.models.downstream_modules.custom_module import CustomHandler, CustomModule
@@ -147,10 +148,13 @@ def split_to_patches(image, grid):
 
 class MiniCPMVInputGenerator(object):
 
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer, vit_config: Optional[VitConfig] = None):
         self.tokenizer_ = tokenizer
         self.config_ = config
+        if config.mm_related_params is None:
+            raise ValueError("mm_related_params is required for MiniCPMV")
         self.vit_config = config.mm_related_params.config
+        self.vit_config_obj = vit_config  # Store VitConfig object for download_headers and url_cache_item_num
         self.im_start = self.tokenizer_.im_start
         self.im_end = self.tokenizer_.im_end
         self.slice_start = self.tokenizer_.slice_start
@@ -213,7 +217,13 @@ class MiniCPMVInputGenerator(object):
 
     def _render_image(self, url: str):
         content = ""
-        image = get_bytes_io_from_url(url)
+        # Get vit_config from parameter if available
+        download_headers = ""
+        url_cache_size = 10
+        if self.vit_config_obj is not None:
+            download_headers = getattr(self.vit_config_obj, 'download_headers', "")
+            url_cache_size = getattr(self.vit_config_obj, 'url_cache_item_num', 10)
+        image = get_bytes_io_from_url(url, download_headers=download_headers, url_cache_size=url_cache_size)
         image = Image.open(image).convert("RGB")
         if self.slice_mode:
             _, final_placeholder = self.get_slice_image_placeholder(
@@ -308,7 +318,7 @@ class MiniCPMVInputGenerator(object):
 
 class MiniCPMVHandler(CustomHandler):
 
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__(config)
 
     def forward(
@@ -336,10 +346,10 @@ class MiniCPMVHandler(CustomHandler):
 
 class MiniCPMVRenderer(EmbeddingRendererBase):
 
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer, vit_config: Optional[VitConfig] = None):
         super().__init__(config, tokenizer)
         self.embedding_type = EmbeddingResponseType.DENSE
-        self.generator = MiniCPMVInputGenerator(config, tokenizer)
+        self.generator = MiniCPMVInputGenerator(config, tokenizer, vit_config=vit_config)
 
     def similar_func(
         self, left: EmbeddingResponseFormat, right: EmbeddingResponseFormat
@@ -382,7 +392,7 @@ class MiniCPMVRenderer(EmbeddingRendererBase):
 
 class MiniCPMVModule(CustomModule):
 
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer, vit_config: Optional[VitConfig] = None):
         super().__init__(config, tokenizer)
-        self.renderer = MiniCPMVRenderer(config, tokenizer)
+        self.renderer = MiniCPMVRenderer(config, tokenizer, vit_config=vit_config)
         self.handler = MiniCPMVHandler(config)
diff --git a/rtp_llm/models/downstream_modules/embedding/misc.py b/rtp_llm/models/downstream_modules/embedding/misc.py
index a5ad20a92..b38c8e27e 100644
--- a/rtp_llm/models/downstream_modules/embedding/misc.py
+++ b/rtp_llm/models/downstream_modules/embedding/misc.py
@@ -5,7 +5,7 @@ from pydantic import BaseModel
 from torch.nn.utils.rnn import pad_sequence
 
 from rtp_llm.async_decoder_engine.embedding.interface import EngineInputs, EngineOutputs
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.distribute.worker_info import g_parallel_info
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.models.downstream_modules.common_input_generator import (
@@ -137,7 +137,7 @@ def combo_to_list(
 class EmbeddingRendererBase(CustomRenderer):
     embedding_type: EmbeddingResponseType
 
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.generator = CommonInputGenerator(tokenizer, config)
 
diff --git a/rtp_llm/models/downstream_modules/embedding/sparse_emebdding_module.py b/rtp_llm/models/downstream_modules/embedding/sparse_emebdding_module.py
index ec304bbac..612b5afa9 100644
--- a/rtp_llm/models/downstream_modules/embedding/sparse_emebdding_module.py
+++ b/rtp_llm/models/downstream_modules/embedding/sparse_emebdding_module.py
@@ -4,7 +4,7 @@ from typing import Any, Dict, List, Union
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.model_loader.weight_module import CustomAtomicWeight
 from rtp_llm.models.downstream_modules.custom_module import CustomHandler, CustomModule
@@ -22,14 +22,14 @@ from rtp_llm.utils.util import to_torch_dtype
 
 
 class SparseEmbeddingModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = SparseEmbeddingRenderer(config, tokenizer)
         self.handler = SparseEmbeddingHandler(config)
 
 
 class SparseEmbeddingRenderer(EmbeddingRendererBase):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.embedding_type = EmbeddingResponseType.SPARSE
         self.unused_tokens = set(
@@ -87,7 +87,7 @@ class SparseEmbeddingRenderer(EmbeddingRendererBase):
 
 
 class SparseEmbeddingHandler(CustomHandler):
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__(config)
         self.sparse_linear = torch.nn.Linear(
             in_features=self.config_.hidden_size, out_features=1
diff --git a/rtp_llm/models/downstream_modules/openai_render.py b/rtp_llm/models/downstream_modules/openai_render.py
index b68b8d49c..1b38fe1dd 100644
--- a/rtp_llm/models/downstream_modules/openai_render.py
+++ b/rtp_llm/models/downstream_modules/openai_render.py
@@ -1,8 +1,9 @@
 import logging
+from typing import Optional
 
 from transformers import PreTrainedTokenizerBase
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.openai.renderer_factory import ChatRendererFactory
 from rtp_llm.openai.renderers.basic_renderer import BasicRenderer
@@ -10,7 +11,7 @@ from rtp_llm.openai.renderers.custom_renderer import CustomChatRenderer, Rendere
 
 
 class OpenAIRenderBasicInfo(object):
-    def __init__(self, tokenizer: BaseTokenizer, config: GptInitModelParameters):
+    def __init__(self, tokenizer: BaseTokenizer, config: ModelConfig, model_type: Optional[str] = None):
         self.config = config
         self.max_seq_len = self.config.max_seq_len
 
@@ -24,8 +25,10 @@ class OpenAIRenderBasicInfo(object):
 
         self.stop_word_ids_list = self.config.special_tokens.stop_words_id_list
 
+        if model_type is None:
+            model_type = config.model_type
         render_params = RendererParams(
-            model_type=config.py_env_configs.model_config.model_type,
+            model_type=model_type,
             max_seq_len=self.max_seq_len,
             eos_token_id=self.eos_token_id,
             stop_word_ids_list=self.stop_word_ids_list,
diff --git a/rtp_llm/models/downstream_modules/plugin_loader.py b/rtp_llm/models/downstream_modules/plugin_loader.py
deleted file mode 100644
index 197687b3c..000000000
--- a/rtp_llm/models/downstream_modules/plugin_loader.py
+++ /dev/null
@@ -1,26 +0,0 @@
-import importlib.util
-import logging
-import os
-import sys
-from types import ModuleType
-
-
-def load_module(file_path: str) -> ModuleType:
-    if not os.path.exists(file_path):
-        logging.info("file_path[%s] not exist", file_path)
-        raise Exception(f"file_path[{file_path}] not exist")
-    _, file_name = os.path.split(file_path)
-    module_name, _ = os.path.splitext(file_name)
-    spec = importlib.util.spec_from_file_location(module_name, file_path)
-    logging.info("load from %s %s", module_name, file_path)
-    module = importlib.util.module_from_spec(spec)
-    sys.modules[module_name] = module
-    spec.loader.exec_module(module)
-    return module
-
-
-class UserModuleLoader(object):
-    @staticmethod
-    def load(module_path: str):
-        module = load_module(module_path)
-        return module.UserModule
diff --git a/rtp_llm/models/downstream_modules/reranker/qwen3_reranker.py b/rtp_llm/models/downstream_modules/reranker/qwen3_reranker.py
index 2a1aed360..d31d8c3cd 100644
--- a/rtp_llm/models/downstream_modules/reranker/qwen3_reranker.py
+++ b/rtp_llm/models/downstream_modules/reranker/qwen3_reranker.py
@@ -6,7 +6,7 @@ import torch
 from transformers import PreTrainedTokenizerBase
 
 from rtp_llm.async_decoder_engine.embedding.interface import EngineInputs
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.metrics import GaugeMetrics, kmonitor
 from rtp_llm.model_loader.weight_module import CustomAtomicWeight
 from rtp_llm.models.downstream_modules.custom_module import CustomHandler, CustomModule
@@ -23,7 +23,7 @@ from rtp_llm.utils.util import to_torch_dtype
 class Qwen3RerankerModule(CustomModule):
 
     def __init__(
-        self, config: GptInitModelParameters, tokenizer: PreTrainedTokenizerBase
+        self, config: ModelConfig, tokenizer: PreTrainedTokenizerBase
     ):
         super().__init__(config, tokenizer)
         self.renderer = Qwen3RerankerRenderer(self.config_, self.tokenizer_)
@@ -34,7 +34,7 @@ class Qwen3RerankerModule(CustomModule):
 
 class Qwen3RerankerRenderer(RerankerRenderer):
     def __init__(
-        self, config: GptInitModelParameters, tokenizer: PreTrainedTokenizerBase
+        self, config: ModelConfig, tokenizer: PreTrainedTokenizerBase
     ):
         super().__init__(config, tokenizer)
         prefix = '<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>\n<|im_start|>user\n'
@@ -102,7 +102,7 @@ class Qwen3RerankerRenderer(RerankerRenderer):
 class Qwen3RerankerHandler(CustomHandler):
 
     def __init__(
-        self, config: GptInitModelParameters, token_false_id: int, token_true_id: int
+        self, config: ModelConfig, token_false_id: int, token_true_id: int
     ):
         super().__init__(config)
         self.token_false_id = token_false_id
diff --git a/rtp_llm/models/downstream_modules/reranker/reranker_module.py b/rtp_llm/models/downstream_modules/reranker/reranker_module.py
index 81e36ff5a..775a71e38 100644
--- a/rtp_llm/models/downstream_modules/reranker/reranker_module.py
+++ b/rtp_llm/models/downstream_modules/reranker/reranker_module.py
@@ -3,7 +3,7 @@ from typing import Any, Dict, List, Tuple
 import numpy as np
 
 from rtp_llm.async_decoder_engine.embedding.interface import EngineInputs, EngineOutputs
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.models.downstream_modules.classifier.bert_classifier import (
     BertClassifierHandler,
@@ -24,28 +24,28 @@ from rtp_llm.models.downstream_modules.reranker.api_datatype import (
 
 
 class RerankerModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = RerankerRenderer(self.config_, self.tokenizer_)
         self.handler = ClassifierHandler(self.config_)
 
 
 class BertRerankerModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = RerankerRenderer(self.config_, self.tokenizer_)
         self.handler = BertClassifierHandler(self.config_)
 
 
 class RobertaRerankerModule(CustomModule):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.renderer = RerankerRenderer(self.config_, self.tokenizer_)
         self.handler = RobertaClassifierHandler(self.config_)
 
 
 class RerankerRenderer(CustomRenderer):
-    def __init__(self, config: GptInitModelParameters, tokenizer: BaseTokenizer):
+    def __init__(self, config: ModelConfig, tokenizer: BaseTokenizer):
         super().__init__(config, tokenizer)
         self.generator = CommonInputGenerator(tokenizer, config)
 
diff --git a/rtp_llm/models/downstream_modules/utils.py b/rtp_llm/models/downstream_modules/utils.py
index dc74c7d34..15f2695e8 100644
--- a/rtp_llm/models/downstream_modules/utils.py
+++ b/rtp_llm/models/downstream_modules/utils.py
@@ -1,7 +1,7 @@
 from typing import Optional
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.config.task_type import TaskType
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.ops import TaskType
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.models.downstream_modules import (
     ALLEmbeddingModule,
@@ -16,7 +16,7 @@ from rtp_llm.models.downstream_modules import (
 
 def create_custom_module(
     task_type: TaskType,
-    config: GptInitModelParameters,
+    config: ModelConfig,
     tokenizer: Optional[BaseTokenizer],
 ):
     # try import internal module
diff --git a/rtp_llm/models/eva2clip_vit.py b/rtp_llm/models/eva2clip_vit.py
index 1d4f5fc97..466a53a79 100644
--- a/rtp_llm/models/eva2clip_vit.py
+++ b/rtp_llm/models/eva2clip_vit.py
@@ -6,7 +6,7 @@ import torch
 from torch import nn
 from transformers.activations import ACT2FN
 
-from rtp_llm.config.py_config_modules import StaticConfig
+from rtp_llm.config.model_config import VitParameters
 from rtp_llm.models.multimodal.multimodal_common import (
     ImageEmbeddingInterface,
     ImageTransform,
@@ -14,12 +14,20 @@ from rtp_llm.models.multimodal.multimodal_common import (
 
 
 class EVA2CLIPImageEmbedding(ImageEmbeddingInterface):
-    def __init__(self, config):
+    def __init__(self, mm_related_params: VitParameters, vit_trt: int = None):
+        """Initialize EVA2CLIPImageEmbedding.
+        
+        Args:
+            mm_related_params: VitParameters object containing vision config.
+        """
+        if mm_related_params is None or not hasattr(mm_related_params, 'config'):
+            raise ValueError("mm_related_params.config is required for EVA2CLIPImageEmbedding")
+        self.mm_related_params = mm_related_params
         # EVA2CLIPModel is too big, create it in cpu
-        self.config = config
-        self.vit = EVA2CLIPModel(config).cpu()
+        # Pass mm_related_params.config to EVA2CLIPModel
+        self.vit = EVA2CLIPModel(mm_related_params.config, vit_trt).cpu()
         self.image_transform = ImageTransform(
-            config.mm_related_params.config["image_size"]
+            mm_related_params.config["image_size"]
         )
 
     @property
@@ -58,12 +66,18 @@ class PatchEmbedding(nn.Module):
 
 
 class Attention(nn.Module):
-    def __init__(self, config):
+    def __init__(self, config, vit_trt: int = None):
+        """Initialize Attention module.
+        
+        Args:
+            config: Vision config object.
+        """
         super().__init__()
         self.num_heads = config.num_heads
         self.query_key_value = nn.Linear(config.hidden_size, config.hidden_size * 3)
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
         self.output_dropout = torch.nn.Dropout(config.dropout_prob)
+        self.vit_trt = vit_trt
 
     def forward(self, x: "tensor(B, L, D)") -> "tensor(B, L, D)":
         B, L, _ = x.shape
@@ -77,7 +91,7 @@ class Attention(nn.Module):
         # Here we maintain two versions of scaled_dot_product_attention, the original math attention is for tensorrt/
         # the optimized scaled_dot_product_attention is for users who don't want to use tensorrt, it's much faster and
         # memory efficient than the original version.
-        if StaticConfig.vit_config.vit_trt == 1:
+        if self.vit_trt == 1:
             attn_weights = torch.matmul(q / math.sqrt(q.shape[-1]), k.transpose(-1, -2))
             attn_weights = attn_weights.softmax(dim=-1)
             attn_out = torch.matmul(attn_weights, v)
@@ -107,12 +121,17 @@ class MLP(nn.Module):
 
 
 class TransformerLayer(nn.Module):
-    def __init__(self, config):
+    def __init__(self, config, vit_trt: int = None):
+        """Initialize TransformerLayer.
+        
+        Args:
+            config: Vision config object.
+        """
         super().__init__()
         self.input_layernorm = nn.LayerNorm(
             config.hidden_size, eps=config.layer_norm_eps
         )
-        self.attention = Attention(config)
+        self.attention = Attention(config, vit_trt)
         self.mlp = MLP(config)
         self.post_attention_layernorm = nn.LayerNorm(
             config.hidden_size, eps=config.layer_norm_eps
@@ -129,10 +148,15 @@ class TransformerLayer(nn.Module):
 
 
 class Transformer(nn.Module):
-    def __init__(self, config):
+    def __init__(self, config, vit_trt: int = None):
+        """Initialize Transformer.
+        
+        Args:
+            config: Vision config object.
+        """
         super().__init__()
         self.layers = nn.ModuleList(
-            [TransformerLayer(config) for _ in range(config.num_hidden_layers)]
+            [TransformerLayer(config, vit_trt) for _ in range(config.num_hidden_layers)]
         )
 
     def forward(self, hidden_states):
@@ -165,11 +189,16 @@ class GLU(nn.Module):
 
 
 class EVA2CLIPModel(nn.Module):
-    def __init__(self, config):
+    def __init__(self, config, vit_trt: int = None):
+        """Initialize EVA2CLIPModel.
+        
+        Args:
+            config: Model config object.
+        """
         super().__init__()
         vision_config = Namespace(**config.mm_related_params.config)
         self.patch_embedding = PatchEmbedding(vision_config)
-        self.transformer = Transformer(vision_config)
+        self.transformer = Transformer(vision_config, vit_trt)
         self.linear_proj = GLU(
             config,
             in_features=(
diff --git a/rtp_llm/models/falcon.py b/rtp_llm/models/falcon.py
index c36ba29bf..bdb54df18 100644
--- a/rtp_llm/models/falcon.py
+++ b/rtp_llm/models/falcon.py
@@ -2,7 +2,7 @@ import functools
 import json
 import os
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight
 from rtp_llm.model_loader.ffn_weight import FfnAtomicWeight, FfnWeight
@@ -176,31 +176,30 @@ class Falcon(BaseModel):
         return FalconWeightInfo
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config_path = os.path.join(ckpt_path, "config.json")
         with open(config_path) as f:
             config_json = json.load(f)
         head_num = config_json.get("n_head", config_json.get("num_attention_heads"))
-        config = GptInitModelParameters(
-            head_num=head_num,
-            head_num_kv=config_json.get(
-                "n_head_kv", config_json.get("num_kv_heads", 1)
-            ),
-            size_per_head=config_json["hidden_size"] // head_num,
-            inter_size=config_json["hidden_size"] * 4,
-            layer_num=config_json.get("n_layer", config_json.get("num_hidden_layers")),
-            max_seq_len=2048,
-            vocab_size=config_json["vocab_size"],
-            activation_type="gelu-none-approximate",
-            has_post_decoder_layernorm=True,
-            rotary_embedding_style=1,
-            ckpt_path=ckpt_path,
+        config = ModelConfig()
+        config.ckpt_path = ckpt_path
+        config.head_num_ = head_num
+        config.head_num_kv_ = config_json.get(
+            "n_head_kv", config_json.get("num_kv_heads", 1)
         )
+        config.size_per_head_ = config_json["hidden_size"] // head_num
+        config.inter_size = config_json["hidden_size"] * 4
+        config.num_layers = config_json.get("n_layer", config_json.get("num_hidden_layers"))
+        config.max_seq_len = 2048
+        config.vocab_size = config_json["vocab_size"]
+        config.activation_type = "gelu-none-approximate"
+        config.has_post_decoder_layernorm_ = True
+        config.rotary_embedding_style_ = 1
         config.special_tokens.bos_token_id = config_json.get("bos_token_id", -1)
         config.special_tokens.eos_token_id = config_json.get("eos_token_id", 0)
-        config.rotary_embedding_dim = config.size_per_head
-        config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
-        config.config_dtype = config_json.get("torch_dtype", None)
+        config.rotary_embedding_dim_ = config.size_per_head_
+        config.tie_word_embeddings_ = config_json.get("tie_word_embeddings", False)
+        config.config_dtype_ = config_json.get("torch_dtype", None)
         return config
 
 
diff --git a/rtp_llm/models/glm4_moe.py b/rtp_llm/models/glm4_moe.py
index 5aa473cd1..262368fca 100644
--- a/rtp_llm/models/glm4_moe.py
+++ b/rtp_llm/models/glm4_moe.py
@@ -6,7 +6,6 @@ from typing import Any, Dict, List
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight, AttnConfig
 from rtp_llm.model_loader.ffn_weight import (
@@ -397,16 +396,16 @@ class Glm4MoeWeight(ModelDeployWeightInfo):
 class Glm4Moe(DeepSeekV2):
     @classmethod
     def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0,
-            head_num_kv=0,
-            size_per_head=0,
-            layer_num=0,
-            inter_size=0,  # 13696
-            vocab_size=152064,
-            max_seq_len=8192,
-        )
-        config.rotary_embedding_style = 1
+        from rtp_llm.config.model_config import ModelConfig
+        config = ModelConfig()
+        config.head_num_ = 0
+        config.head_num_kv_ = 0
+        config.size_per_head_ = 0
+        config.num_layers = 0
+        config.inter_size = 0  # 13696
+        config.vocab_size = 152064
+        config.max_seq_len = 8192
+        config.rope_config.style = 1
         config.activation_type = "SiGLU"
         config.has_pre_decoder_layernorm = False
         config.has_post_decoder_layernorm = True
@@ -414,16 +413,16 @@ class Glm4Moe(DeepSeekV2):
 
         cls._from_hf(config, ckpt_path)
         assert (
-            config.head_num > 0
-            and config.head_num_kv > 0
-            and config.size_per_head > 0
-            and config.layer_num > 0
+            config.head_num_ > 0
+            and config.head_num_kv_ > 0
+            and config.size_per_head_ > 0
+            and config.num_layers > 0
             and config.inter_size > 0
-        ), f"error config config.head_num={config.head_num} config.head_num_kv={config.head_num_kv} config.size_per_head={config.size_per_head} config.layer_num={config.layer_num} config.inter_size={config.inter_size}"
+        ), f"error config config.head_num_={config.head_num_} config.head_num_kv_={config.head_num_kv_} config.size_per_head_={config.size_per_head_} config.num_layers={config.num_layers} config.inter_size={config.inter_size}"
         return config
 
     @classmethod
-    def _from_hf(cls, config: GptInitModelParameters, ckpt_path: str):
+    def _from_hf(cls, config: "ModelConfig", ckpt_path: str):
         config_path = os.path.join(ckpt_path, "config.json")
 
         if not os.path.exists(config_path):
@@ -438,28 +437,27 @@ class Glm4Moe(DeepSeekV2):
         return config
 
     @staticmethod
-    def _from_config_json(config: GptInitModelParameters, config_json: Dict[str, Any]):
-        config.use_mla = False
+    def _from_config_json(config: "ModelConfig", config_json: Dict[str, Any]):
         config.inter_size = config_json["intermediate_size"]
-        config.head_num = config_json["num_attention_heads"]
-        config.head_num_kv = config_json.get("num_key_value_heads", config.head_num)
-        config.size_per_head = (
+        config.head_num_ = config_json["num_attention_heads"]
+        config.head_num_kv_ = config_json.get("num_key_value_heads", config.head_num_)
+        config.size_per_head_ = (
             int(config_json.get("head_dim", 0))
             if "head_dim" in config_json
-            else config_json["hidden_size"] // config.head_num
+            else config_json["hidden_size"] // config.head_num_
         )
         if config_json.get("hidden_size") is not None:
             config.hidden_size = config_json["hidden_size"]
-        config.layer_num = config_json["num_hidden_layers"]
-        config.rotary_embedding_base = config_json.get(
-            "rope_theta", config.rotary_embedding_base
+        config.num_layers = config_json["num_hidden_layers"]
+        config.rope_config.base = config_json.get(
+            "rope_theta", config.rope_config.base
         )
         config.vocab_size = config_json["vocab_size"]
-        config.partial_rotary_factor = config_json.get("partial_rotary_factor", 1.0)
-        config.rotary_embedding_dim = int(
-            config.size_per_head * config.partial_rotary_factor
+        partial_rotary_factor = config_json.get("partial_rotary_factor", 1.0)
+        config.rope_config.dim = int(
+            config.size_per_head_ * partial_rotary_factor
         )
-        config.layernorm_eps = config_json.get("rms_norm_eps", 1e-06)
+        config.layernorm_eps_ = config_json.get("rms_norm_eps", 1e-06)
         config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
 
         config.moe_k = config_json["num_experts_per_tok"]
@@ -478,12 +476,12 @@ class Glm4Moe(DeepSeekV2):
 
         first_k_dense_replace = config_json["first_k_dense_replace"]
         config.moe_layer_index = [
-            i for i in range(config.layer_num) if i >= first_k_dense_replace
+            i for i in range(config.num_layers) if i >= first_k_dense_replace
         ]
 
         ffn_inter_size = config_json.get("intermediate_size", config.inter_size)
         layer_inter_size = []
-        for i in range(config.layer_num):
+        for i in range(config.num_layers):
             if i in config.moe_layer_index:
                 layer_inter_size.append(config.inter_size)
             else:
diff --git a/rtp_llm/models/gpt_neox.py b/rtp_llm/models/gpt_neox.py
index 2aee8662e..52de9440f 100644
--- a/rtp_llm/models/gpt_neox.py
+++ b/rtp_llm/models/gpt_neox.py
@@ -1,6 +1,6 @@
 from typing import Any, Dict
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.base_model import BaseModel
 from rtp_llm.models.gpt_neox_weight import GPTNeox13BWeight, GPTNeoxWeight
@@ -13,23 +13,23 @@ class GPTNeox(BaseModel):
         return GPTNeoxWeight
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config_dict = get_config_from_path(ckpt_path)
         if config_dict:
             config = GPTNeox.from_huggingface(config_dict)
             config.ckpt_path = ckpt_path
         else:
-            config = GptInitModelParameters(
+            config = ModelConfig(
                 head_num=40,
                 head_num_kv=40,
                 size_per_head=128,
-                layer_num=40,
+                num_layers=40,
                 max_seq_len=4096,
                 vocab_size=250752,
-                eos_token_id=2,
                 inter_size=20480,
                 inter_padding_size=20480,
             )
+            config.special_tokens.eos_token_id = 2
             config.rotary_embedding_dim = 128
             config.rotary_embedding_style = 1
             config.has_pre_decoder_layernorm = False
@@ -39,8 +39,8 @@ class GPTNeox(BaseModel):
         return config
 
     @staticmethod
-    def from_huggingface(config_json: Dict[str, Any]):
-        config = GptInitModelParameters(
+    def from_huggingface(config_json: Dict[str, Any]) -> ModelConfig:
+        config = ModelConfig(
             head_num=40,
             size_per_head=128,
             layer_num=40,
@@ -86,16 +86,16 @@ class GPTNeox13B(GPTNeox):
         return GPTNeox13BWeight
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config_dict = get_config_from_path(ckpt_path)
         if config_dict:
             config = GPTNeox13B.from_huggingface(config_dict)
         else:
-            config = GptInitModelParameters(
+            config = ModelConfig(
                 head_num=40,
                 head_num_kv=40,
                 size_per_head=128,
-                layer_num=40,
+                num_layers=40,
                 max_seq_len=4096,
                 vocab_size=250752,
                 inter_size=20480,
@@ -111,21 +111,15 @@ class GPTNeox13B(GPTNeox):
         return config
 
     @staticmethod
-    def from_huggingface(config_json: Dict[str, Any]):
-        config = GptInitModelParameters(
-            head_num=40,
-            size_per_head=128,
-            layer_num=40,
+    def from_huggingface(config_json: Dict[str, Any]) -> ModelConfig:
+        config = ModelConfig(
+            head_num=config_json["num_attention_heads"],
+            size_per_head=config_json["hidden_size"] // config_json["num_attention_heads"],
+            num_layers=config_json["num_hidden_layers"],
             max_seq_len=4096,
-            vocab_size=250752,
+            vocab_size=config_json["vocab_size"],
         )
-        config.head_num = config_json["num_attention_heads"]
         config.head_num_kv = config.head_num
-        config.size_per_head = (
-            config_json["hidden_size"] // config_json["num_attention_heads"]
-        )
-        config.layer_num = config_json["num_hidden_layers"]
-        config.vocab_size = config_json["vocab_size"]
         config.layernorm_eps = config_json["layer_norm_eps"]
         config.inter_size = config_json["intermediate_size"]
         config.inter_padding_size = config.inter_size
diff --git a/rtp_llm/models/gpt_util/prefix_encoder.py b/rtp_llm/models/gpt_util/prefix_encoder.py
index 142ad35c8..dba4f3cf4 100644
--- a/rtp_llm/models/gpt_util/prefix_encoder.py
+++ b/rtp_llm/models/gpt_util/prefix_encoder.py
@@ -1,6 +1,6 @@
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 
 
 class PrefixEncoder(torch.nn.Module):
@@ -10,23 +10,23 @@ class PrefixEncoder(torch.nn.Module):
     Output shape: (batch-size, prefix-length, 2*layers*hidden)
     """
 
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         super().__init__()
         self.config = config
-        self.prefix_projection = config.prefix_projection
-        hidden_size = config.head_num * config.size_per_head
+        self.prefix_projection = config.prefix_projection_
+        hidden_size = config.head_num_ * config.size_per_head_
         if self.prefix_projection:
             # Use a two-layer MLP to encode the prefix
             self.embedding = torch.nn.Embedding(config.pre_seq_len, hidden_size)
             self.trans = torch.nn.Sequential(
                 torch.nn.Linear(hidden_size, hidden_size),
                 torch.nn.Tanh(),
-                torch.nn.Linear(hidden_size, config.layer_num * hidden_size * 2),
+                torch.nn.Linear(hidden_size, config.layer_num_ * hidden_size * 2),
             )
         else:
             self.embedding = torch.nn.Embedding(
                 config.pre_seq_len,
-                config.layer_num * config.size_per_head * config.head_num_kv * 2,
+                config.layer_num_ * config.size_per_head_ * config.head_num_kv_ * 2,
             )
 
     # input shape: [batch_size, pre_seq_len]
@@ -41,9 +41,9 @@ class PrefixEncoder(torch.nn.Module):
         past_key_values = past_key_values.view(
             batch_size,
             self.config.pre_seq_len,
-            self.config.layer_num * 2,
-            self.config.head_num_kv,
-            self.config.size_per_head,
+            self.config.layer_num_ * 2,
+            self.config.head_num_kv_,
+            self.config.size_per_head_,
         )
         past_key_values = past_key_values.permute(0, 2, 3, 1, 4).contiguous()
         return past_key_values
diff --git a/rtp_llm/models/internvl.py b/rtp_llm/models/internvl.py
index f8c79fb7f..ff233dbd1 100644
--- a/rtp_llm/models/internvl.py
+++ b/rtp_llm/models/internvl.py
@@ -4,48 +4,48 @@ from typing import Any, Dict, List
 
 from transformers import AutoTokenizer
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.py_config_modules import VitConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.base_model import BaseModel
 from rtp_llm.models.internvl_vit import InternVLImageEmbedding
 from rtp_llm.models.internvl_weight import InternVLVitWeight, InternVLWeightInfo
 from rtp_llm.models.llama import Llama
-from rtp_llm.models.multimodal.multimodal_mixin import MultiModalMixin
+from rtp_llm.models.multimodal.multimodal_mixin import BaseVitWeights, MultiModalMixin
 from rtp_llm.models.qwen_v2 import QWenV2
 
 
 class InternVL(BaseModel, MultiModalMixin):
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = InternVLImageEmbedding(config)
-        config.mm_related_params.vit_weights = InternVLVitWeight(
+    def _init_multimodal(
+        self,
+        mm_model_config: Any,  # MMModelConfig
+        vit_config: VitConfig,
+    ):
+        mm_related_params = mm_model_config.mm_related_params if hasattr(mm_model_config, 'mm_related_params') else None
+        if mm_related_params is None:
+            raise ValueError("mm_related_params is required for InternVL")
+        self.mm_part = InternVLImageEmbedding(mm_related_params)
+        mm_model_config.mm_related_params.vit_weights = InternVLVitWeight(
             {"vision_model": self.mm_part.vision_model, "mlp1": self.mm_part.mlp1}, True
         )
-        config.mm_sep_tokens = [
-            [self.tokenizer.encode("<img>")[0], self.tokenizer.encode("</img>")[0]]
-        ]
+        # mm_sep_tokens is stored in mm_model_config or can be set directly
+        if not hasattr(mm_model_config, 'mm_sep_tokens') or mm_model_config.mm_sep_tokens is None:
+            mm_model_config.mm_sep_tokens = [
+                [self.tokenizer.encode("<img>")[0], self.tokenizer.encode("</img>")[0]]
+            ]
 
     @staticmethod
     def get_weight_cls():
         return InternVLWeightInfo
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0,
-            head_num_kv=0,
-            size_per_head=0,
-            layer_num=0,
-            inter_size=0,
-            vocab_size=0,
-            max_seq_len=0,
-            ckpt_path=ckpt_path,
-            rotary_embedding_dim=128,
-            rotary_embedding_style=1,
-            activation_type="SiGLU",
-            has_pre_decoder_layernorm=False,
-            has_post_decoder_layernorm=True,
-            norm_type="rmsnorm",
-        )
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
+        config = ModelConfig()
+        config.ckpt_path = ckpt_path
+        config.rope_config.dim = 128
+        config.rope_config.style = 1
+        config.has_pre_decoder_layernorm = False
 
         config_path = os.path.join(ckpt_path, "config.json")
         if os.path.exists(config_path):
@@ -70,14 +70,14 @@ class InternVL(BaseModel, MultiModalMixin):
             config.head_num > 0
             and config.head_num_kv > 0
             and config.size_per_head > 0
-            and config.layer_num > 0
+            and config.num_layers > 0
             and config.inter_size > 0
         ), "error config"
         config.mm_related_params.special_tokens.update({"default_mm_token": "<image>"})
         return config
 
     @staticmethod
-    def _init_vit_params(config: GptInitModelParameters, config_json: Dict[str, Any]):
+    def _init_vit_params(config: ModelConfig, config_json: Dict[str, Any]):
         config.mm_related_params.config = config_json["vision_config"]
         config.mm_related_params.config["select_layer"] = config_json["select_layer"]
         config.mm_related_params.config["llm_hidden_size"] = config_json["llm_config"][
diff --git a/rtp_llm/models/internvl_vit.py b/rtp_llm/models/internvl_vit.py
index 7801339a4..3abee5e42 100644
--- a/rtp_llm/models/internvl_vit.py
+++ b/rtp_llm/models/internvl_vit.py
@@ -27,7 +27,7 @@ from transformers.configuration_utils import PretrainedConfig
 from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling
 from transformers.modeling_utils import PreTrainedModel
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
 from rtp_llm.models.multimodal.multimodal_common import (
     MultiModalEmbeddingInterface,
     timeout_decorator,
@@ -182,9 +182,11 @@ def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=3
 
 
 class InternVLImageEmbedding(MultiModalEmbeddingInterface):
-    def __init__(self, config: GptInitModelParameters):
-        self.config = config
-        config = config.mm_related_params.config
+    def __init__(self, mm_related_params: VitParameters):
+        if mm_related_params is None or not hasattr(mm_related_params, 'config'):
+            raise ValueError("mm_related_params.config is required for InternVLImageEmbedding")
+        self.mm_related_params = mm_related_params
+        config = mm_related_params.config
         self.select_layer = config["select_layer"]
         self.vision_model = InternVisionModel(InternVisionConfig(**config))
 
@@ -247,7 +249,7 @@ class InternVLImageEmbedding(MultiModalEmbeddingInterface):
     def image_embedding(self, images: List[Image.Image], max_num):
         # hugging face default value
         device = self._device
-        config = self.config.mm_related_params.config
+        config = self.mm_related_params.config
         input_size = config["image_size"]
         transform = build_transform(input_size=config["image_size"])
         res = []
diff --git a/rtp_llm/models/internvl_weight.py b/rtp_llm/models/internvl_weight.py
index 827f7e884..aedcd95c5 100644
--- a/rtp_llm/models/internvl_weight.py
+++ b/rtp_llm/models/internvl_weight.py
@@ -66,9 +66,9 @@ class InternVLVitWeight(BaseVitWeights):
 
 
 class InternVLWeightInfo(ModelDeployWeightInfo, BaseMultiModalWeightInfo):
-    def __init__(self, config, tp_size, tp_rank):
-        ModelDeployWeightInfo.__init__(self, config, tp_size, tp_rank)
-        BaseMultiModalWeightInfo.__init__(self, config)
+    def __init__(self, vit_weights, **kwargs):
+        ModelDeployWeightInfo.__init__(self, **kwargs)
+        BaseMultiModalWeightInfo.__init__(self, vit_weights=vit_weights, **kwargs)
         self._names = None
         self._merge_qkv = None
         self._merge_qkv_b = None
@@ -196,5 +196,4 @@ class InternVLWeightInfo(ModelDeployWeightInfo, BaseMultiModalWeightInfo):
             layer_weights.append(layer_weight)
 
         model_weights = ModelWeightInfo(layer_weights=layer_weights, weights=weights)
-        model_weights = self._get_vit_info(model_weights)
         return model_weights
diff --git a/rtp_llm/models/llama.py b/rtp_llm/models/llama.py
index 796900096..e2fe4cacd 100644
--- a/rtp_llm/models/llama.py
+++ b/rtp_llm/models/llama.py
@@ -2,9 +2,11 @@ import json
 import logging
 import math
 import os
-from typing import Any, Dict, List
+from typing import Any, Dict, List, Optional
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.engine_config import EngineConfig
+from rtp_llm.config.model_config import ModelConfig as PyModelConfig
+from rtp_llm.ops import MMModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.base_model import BaseModel
 from rtp_llm.models.llama_weight import GemmaWeightInfo, LlamaWeightInfo
@@ -28,20 +30,16 @@ class Llama(BaseModel):
         return LlamaWeightInfo
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0,
-            size_per_head=0,
-            layer_num=0,
-            max_seq_len=0,
-            vocab_size=0,
-            ckpt_path=ckpt_path,
-            activation_type="SiGLU",
-            norm_type="rmsnorm",
-            rotary_embedding_dim=128,
-            rotary_embedding_style=1,
-            has_post_decoder_layernorm=True,
-        )
+    def _create_config(cls, ckpt_path: str) -> PyModelConfig:
+        # Create PyModelConfig instance (default constructor)
+        # Default values for activation_type, norm_type, has_post_decoder_layernorm
+        # are set in PyModelConfig.__init__
+        config = PyModelConfig()
+        # Initialize checkpoint path and rope config
+        config.ckpt_path = ckpt_path
+        config.rope_config.dim = 128
+        config.rope_config.style = 1
+        
         # hugggingface
         config_path = os.path.join(ckpt_path, "config.json")
         # llama-int8
@@ -64,7 +62,7 @@ class Llama(BaseModel):
         return config
 
     @staticmethod
-    def from_huggingface(config, config_json: Dict[str, Any]):
+    def from_huggingface(config: PyModelConfig, config_json: Dict[str, Any]):
         config.head_num = config_json["num_attention_heads"]
         config.head_num_kv = config_json.get("num_key_value_heads", config.head_num)
         config.hidden_size = config_json["hidden_size"]
@@ -72,64 +70,64 @@ class Llama(BaseModel):
             config_json["hidden_size"] // config_json["num_attention_heads"]
         )
         config.size_per_head = config_json.get("head_dim", config.size_per_head)
-        config.layer_num = config_json["num_hidden_layers"]
+        config.num_layers = config_json["num_hidden_layers"]
         config.max_seq_len = config_json.get("max_sequence_length", 2048)
         config.vocab_size = config_json["vocab_size"]
         config.layernorm_eps = config_json.get(
             "rms_norm_eps", config_json.get("layer_norm_eps", 1e-05)
         )
         config.inter_size = config_json["intermediate_size"]
-        config.rotary_embedding_base = config_json.get("rope_theta", 10000)
-        config.rotary_embedding_dim = config.size_per_head
+        config.rope_config.base = config_json.get("rope_theta", 10000)
+        config.rope_config.dim = config.size_per_head
         config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
         rope_scaling = config_json.get("rope_scaling")
         if rope_scaling is not None:
             rope_type = rope_scaling.get("type", rope_scaling.get("rope_type"))
             if rope_type == "linear":
-                config.rotary_embedding_scale = rope_scaling["factor"]
-                config.org_embedding_max_pos = config_json.get(
+                config.rope_config.scale = rope_scaling["factor"]
+                config.rope_config.max_pos = config_json.get(
                     "max_position_embeddings", 2048
                 )
             elif rope_type == "dynamic":
-                config.rotary_embedding_style = 3
+                config.rope_config.style = 3
             elif rope_type == "yarn":
-                config.rotary_embedding_style = 5
-                config.rotary_embedding_scale = rope_scaling["factor"]
-                config.rotary_factor1 = rope_scaling.get("beta_slow", 1)
-                config.rotary_factor2 = rope_scaling.get("beta_fast", 32)
-                config.org_embedding_max_pos = rope_scaling[
+                config.rope_config.style = 5
+                config.rope_config.scale = rope_scaling["factor"]
+                config.rope_config.factor1 = rope_scaling.get("beta_slow", 1)
+                config.rope_config.factor2 = rope_scaling.get("beta_fast", 32)
+                config.rope_config.max_pos = rope_scaling[
                     "original_max_position_embeddings"
                 ]
-                config.rotary_embedding_mscale = Llama.get_mscale(
-                    config.rotary_embedding_scale
+                config.rope_config.mscale = Llama.get_mscale(
+                    config.rope_config.scale
                 )
             elif rope_type == "llama3":
-                config.rotary_embedding_style = 6
-                config.rotary_embedding_scale = rope_scaling["factor"]
-                config.rotary_factor1 = rope_scaling["low_freq_factor"]
-                config.rotary_factor2 = rope_scaling["high_freq_factor"]
-                config.org_embedding_max_pos = rope_scaling[
+                config.rope_config.style = 6
+                config.rope_config.scale = rope_scaling["factor"]
+                config.rope_config.factor1 = rope_scaling["low_freq_factor"]
+                config.rope_config.factor2 = rope_scaling["high_freq_factor"]
+                config.rope_config.max_pos = rope_scaling[
                     "original_max_position_embeddings"
                 ]
             else:
                 raise Exception(f"unsupport rope_scaling {rope_scaling}")
         # config.activation_type = config_json.get("hidden_act", config.activation_type)
-        config.special_tokens.bos_token_id = config_json.get("bos_token_id", -1)
-        eos_token_id = config_json.get("eos_token_id", 0)
-        # openai endpoint will get corrent eos token id list from tokenizer
-        if isinstance(eos_token_id, list):
-            config.special_tokens.eos_token_id = eos_token_id[0]
-        else:
-            config.special_tokens.eos_token_id = eos_token_id
+        # Note: special_tokens is not in ModelConfig, this should be handled elsewhere
+        # config.special_tokens.bos_token_id = config_json.get("bos_token_id", -1)
+        # eos_token_id = config_json.get("eos_token_id", 0)
+        # if isinstance(eos_token_id, list):
+        #     config.special_tokens.eos_token_id = eos_token_id[0]
+        # else:
+        #     config.special_tokens.eos_token_id = eos_token_id
         config.use_logn_attn = config_json.get("use_logn_attn", False)
         config.config_dtype = config_json.get("torch_dtype", None)
 
     @staticmethod
-    def from_params(config: GptInitModelParameters, params_json: Dict[str, Any]):
+    def from_params(config: PyModelConfig, params_json: Dict[str, Any]):
         config.head_num = params_json["n_heads"]
         config.head_num_kv = params_json.get("n_kv_heads", config.head_num)
         config.size_per_head = params_json["dim"] // params_json["n_heads"]
-        config.layer_num = params_json["n_layers"]
+        config.num_layers = params_json["n_layers"]
         config.max_seq_len = 2048
         config.vocab_size = 32000
         config.layernorm_eps = params_json["norm_eps"]
@@ -138,9 +136,10 @@ class Llama(BaseModel):
             params_json.get("ffn_dim_multiplier", 1),
             params_json["multiple_of"],
         )
-        config.special_tokens.bos_token_id = 1
-        config.special_tokens.eos_token_id = 2
-        config.rotary_embedding_dim = config.size_per_head
+        # Note: special_tokens is not in ModelConfig, this should be handled elsewhere
+        # config.special_tokens.bos_token_id = 1
+        # config.special_tokens.eos_token_id = 2
+        config.rope_config.dim = config.size_per_head
         config.tie_word_embeddings = params_json.get("tie_word_embeddings", False)
         config.config_dtype = params_json.get("torch_dtype", None)
         return config
@@ -148,52 +147,66 @@ class Llama(BaseModel):
 
 class Baichuan(Llama):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> PyModelConfig:
         config = Llama._create_config(ckpt_path)
-        if config.layer_num == 40:  # 13B
-            config.rotary_embedding_style = 0
-            config.rotary_embedding_dim = 0
+        if config.num_layers == 40:  # 13B
+            config.rope_config.style = 0
+            config.rope_config.dim = 0
             config.use_attention_linear_bias = True
-        config.special_tokens.bos_token_id = -1
+        # Note: special_tokens is not in ModelConfig, this should be handled elsewhere
+        # config.special_tokens.bos_token_id = -1
         return config
 
 
 class Baichuan2(Baichuan):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> PyModelConfig:
         config = Baichuan._create_config(ckpt_path)
         config.normalize_lm_head_weight = True
         return config
 
 
 class Gemma(Llama):
-    def __init__(self, config: GptInitModelParameters):
-        if config.py_env_configs.fmha_config.enable_open_source_fmha:
+    def __init__(
+        self,
+        py_model_config: PyModelConfig,
+        mm_model_config: MMModelConfig,
+        engine_config: EngineConfig,
+        vit_config: Optional[Any] = None,
+        merge_lora: bool = False,
+    ):
+        if engine_config.fmha_config.enable_open_source_fmha:
             logging.warn(
                 "opensource fmha does not support head dim 256, thus disabled for gemma model"
             )
             os.environ["ENABLE_OPENSOURCE_FMHA"] = "OFF"
-        super().__init__(config)
+        super().__init__(
+            py_model_config=py_model_config,
+            mm_model_config=mm_model_config,
+            engine_config=engine_config,
+            vit_config=vit_config,
+            merge_lora=merge_lora,
+        )
 
     @staticmethod
     def get_weight_cls():
         return GemmaWeightInfo
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> PyModelConfig:
         config = Llama._create_config(ckpt_path)
         config.has_post_decoder_layernorm = True
         config.input_embedding_scalar = config.hidden_size**0.5
-        config.rotary_embedding_dim = config.size_per_head
+        config.rope_config.dim = config.size_per_head
         config.activation_type = "gated-gelu"
         return config
 
 
 class Cohere(Llama):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> PyModelConfig:
         config = Llama._create_config(ckpt_path)
-        config.rotary_embedding_style = 0
+        config.rope_config.style = 0
         config.norm_type = "layernorm"
         config.qk_norm = True
         return config
diff --git a/rtp_llm/models/llava.py b/rtp_llm/models/llava.py
index a9faabec0..91d8050a1 100644
--- a/rtp_llm/models/llava.py
+++ b/rtp_llm/models/llava.py
@@ -3,9 +3,10 @@ import os
 import re
 from typing import Any, Dict, List, Tuple, Union
 
-from transformers import AutoTokenizer, CLIPVisionConfig
+from transformers import CLIPVisionConfig
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.py_config_modules import VitConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.llama import Llama
 from rtp_llm.models.llava_vit import LlavaImageEmbedding
@@ -14,23 +15,29 @@ from rtp_llm.models.multimodal.multimodal_mixin import BaseVitWeights, MultiModa
 
 
 class Llava(Llama, MultiModalMixin):
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = LlavaImageEmbedding(config)
+    def _init_multimodal(
+        self,
+        mm_model_config: Any,  # MMModelConfig
+        vit_config: VitConfig,
+    ):
+        mm_related_params = mm_model_config.mm_related_params if hasattr(mm_model_config, 'mm_related_params') else None
+        if mm_related_params is None:
+            raise ValueError("mm_related_params is required for Llava")
+        self.mm_part = LlavaImageEmbedding(mm_related_params)
         vit_weight_dict: Dict[str, Any] = {"mm_projector": self.mm_part.mm_projector}
         if (
-            config.mm_related_params.config["unfreeze_mm_vision_tower"]
-            or "mm_vision_tower" in config.mm_related_params.config["mm_tunable_parts"]
+            mm_related_params.config.get("unfreeze_mm_vision_tower", False)
+            or "mm_vision_tower" in mm_related_params.config.get("mm_tunable_parts", [])
         ):
             vit_weight_dict["vision_tower"] = self.mm_part.vision_tower
-        if "unpad" in config.mm_related_params.config.get(
-            "mm_patch_merge_type", "flat"
-        ):
+        if "unpad" in mm_related_params.config.get("mm_patch_merge_type", "flat"):
             vit_weight_dict["image_newline"] = self.mm_part.image_newline
-        config.mm_related_params.vit_weights = BaseVitWeights(vit_weight_dict, True)
+        mm_model_config.mm_related_params.vit_weights = BaseVitWeights(vit_weight_dict, True)
 
     @staticmethod
-    def _create_config(ckpt_path):
-        config = GptInitModelParameters(
+    def _create_config(ckpt_path: str) -> ModelConfig:
+        from rtp_llm.config.model_config import ModelConfig
+        config = ModelConfig(
             head_num=0,
             size_per_head=0,
             layer_num=0,
@@ -61,7 +68,11 @@ class Llava(Llama, MultiModalMixin):
         return LlavaWeightInfo
 
     @staticmethod
-    def from_huggingface(config: GptInitModelParameters, config_json: Dict[str, Any]):
+    def from_huggingface(config: ModelConfig, config_json: Dict[str, Any]):
+        from rtp_llm.config.model_config import VitParameters
+        # Initialize mm_related_params if not already initialized
+        if config.mm_related_params is None:
+            config.mm_related_params = VitParameters()
         if "text_config" in config_json:
             text_config = config_json["text_config"]
             # if text_config.get("_name_or_path", "") != "":
diff --git a/rtp_llm/models/llava_vit.py b/rtp_llm/models/llava_vit.py
index 03fb79871..3156a6ade 100644
--- a/rtp_llm/models/llava_vit.py
+++ b/rtp_llm/models/llava_vit.py
@@ -5,16 +5,18 @@ import os
 import re
 from dataclasses import dataclass
 from functools import partial, reduce
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union
 
 import numpy as np
+
+if TYPE_CHECKING:
+    from rtp_llm.config.model_config import VitParameters
 import torch
 import torch.nn as nn
 import torch.utils.checkpoint
 from PIL import Image
 from transformers import CLIPImageProcessor, CLIPVisionConfig, CLIPVisionModel
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.models.llava_utils import (
     expand2square,
     get_anyres_image_grid_shape,
@@ -53,18 +55,18 @@ from transformers.utils import ModelOutput
 
 
 class LlavaImageEmbedding(MultiModalEmbeddingInterface):
-    def __init__(self, config: GptInitModelParameters):
-        self.config = config
-        if config.mm_related_params.config.get("vision_config", None) != None:
+    def __init__(self, mm_related_params: "VitParameters"):
+        if mm_related_params is None or not hasattr(mm_related_params, 'config'):
+            raise ValueError("mm_related_params.config is required for LlavaImageEmbedding")
+        self.mm_related_params = mm_related_params
+        if mm_related_params.config.get("vision_config", None) != None:
             raise Exception("llava-hf style config is not implemented yet")
         else:
-            self.vision_tower = self.build_vision_tower(config.mm_related_params.config)
-        self.mm_projector = self.build_vision_projector(config.mm_related_params.config)
-        if "unpad" in self.config.mm_related_params.config.get(
-            "mm_patch_merge_type", "flat"
-        ):
+            self.vision_tower = self.build_vision_tower(mm_related_params.config)
+        self.mm_projector = self.build_vision_projector(mm_related_params.config)
+        if "unpad" in mm_related_params.config.get("mm_patch_merge_type", "flat"):
             self.image_newline = nn.Parameter(
-                torch.empty(self.config.mm_related_params.config["hidden_size"])
+                torch.empty(mm_related_params.config["hidden_size"])
             )
 
     @torch.inference_mode()
@@ -138,7 +140,7 @@ class LlavaImageEmbedding(MultiModalEmbeddingInterface):
 
     @torch.no_grad()
     def image_embedding(self, images: List[Image.Image], mm_type=MMUrlType.IMAGE):
-        config = self.config.mm_related_params.config
+        config = self.mm_related_params.config
         image_aspect_ratio = config["image_aspect_ratio"]
         mm_patch_merge_type = config.get("mm_patch_merge_type", "flat")
         mm_newline_position = config.get("mm_newline_position", "one_token")
@@ -174,7 +176,7 @@ class LlavaImageEmbedding(MultiModalEmbeddingInterface):
                 if mm_type == MMUrlType.VIDEO:  # video operations
                     if mm_newline_position == "grid":
                         image_feature = self.add_token_per_grid(image_feature)
-                        if self.config.mm_related_params.config["add_faster_video"]:
+                        if self.mm_related_params.config.get("add_faster_video", False):
                             raise Exception("add_faster_video is not implemented")
                             # faster_video_feature = self.add_token_per_grid(all_faster_video_features[image_idx])
                             # concat_slow_fater_token = []
@@ -354,7 +356,7 @@ class LlavaImageEmbedding(MultiModalEmbeddingInterface):
             ),
             dim=-1,
         )
-        if self.config.mm_related_params.config["add_faster_video"]:
+        if self.mm_related_params.config.get("add_faster_video", False):
             image_feature = image_feature.view(feature_dim, num_frames, resize_h, -1)
             image_feature = image_feature.permute(1, 2, 3, 0).contiguous()
             image_feature = image_feature.flatten(1, 2)
@@ -368,9 +370,9 @@ class LlavaImageEmbedding(MultiModalEmbeddingInterface):
         image_feature = image_feature.view(num_frames, height, width, -1)
         image_feature = image_feature.permute(0, 3, 1, 2).contiguous()
         # image_feature = nn.functional.max_pool2d(image_feature, self.config.mm_spatial_pool_stride)
-        mm_spatial_pool_mode = self.config.mm_related_params.config[
-            "mm_spatial_pool_mode"
-        ]
+        mm_spatial_pool_mode = self.mm_related_params.config.get(
+            "mm_spatial_pool_mode", "none"
+        )
         if mm_spatial_pool_mode == "average":
             image_feature = nn.functional.avg_pool2d(image_feature, stride)
         elif mm_spatial_pool_mode == "max":
@@ -391,8 +393,8 @@ class LlavaImageEmbedding(MultiModalEmbeddingInterface):
         return image_feature
 
     def build_vision_tower(self, vision_tower_cfg: Dict[str, Any], **kwargs: Any):
-        vision_tower_name = self.config.py_env_configs.model_config.extra_data_path
-        vision_tower = self.config.py_env_configs.model_config.local_extra_data_path
+        vision_tower_name = self.py_model_config.extra_data_path
+        vision_tower = self.py_model_config.local_extra_data_path
         if vision_tower is None:
             vision_tower_name = vision_tower_cfg["vit_tower_path"]
             vision_tower = vision_tower_cfg["vit_tower_path"]
diff --git a/rtp_llm/models/llava_weight.py b/rtp_llm/models/llava_weight.py
index 9c56aa237..638368e45 100644
--- a/rtp_llm/models/llava_weight.py
+++ b/rtp_llm/models/llava_weight.py
@@ -1,17 +1,14 @@
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.model_loader.model_weight_info import ModelWeightInfo
 from rtp_llm.models.llama_weight import LlamaWeightInfo
 from rtp_llm.models.multimodal.multimodal_mixin import BaseMultiModalWeightInfo
 from rtp_llm.utils.model_weight import W
 
 
 class LlavaWeightInfo(LlamaWeightInfo, BaseMultiModalWeightInfo):
-    def __init__(self, config: GptInitModelParameters, tp_size: int, tp_rank: int):
-        LlamaWeightInfo.__init__(self, config, tp_size, tp_rank)
-        BaseMultiModalWeightInfo.__init__(self, config)
+    def __init__(self, vit_weights, **kwargs):
+        LlamaWeightInfo.__init__(self, **kwargs)
+        BaseMultiModalWeightInfo.__init__(self, vit_weights=vit_weights, **kwargs)
 
     def _get_weight_info(self):
-        llava_weight = ModelWeightInfo(layer_weights=[], weights=[])
         llava_weight = super()._get_weight_info()
 
         # for llava-next
@@ -20,5 +17,4 @@ class LlavaWeightInfo(LlamaWeightInfo, BaseMultiModalWeightInfo):
                 llava_weight.layer_weights.remove(weight)
                 break
 
-        llava_weight = self._get_vit_info(llava_weight)
         return llava_weight
diff --git a/rtp_llm/models/megatron_bert.py b/rtp_llm/models/megatron_bert.py
index 03019f99f..26f302803 100644
--- a/rtp_llm/models/megatron_bert.py
+++ b/rtp_llm/models/megatron_bert.py
@@ -1,6 +1,5 @@
 from transformers import AutoTokenizer
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.bert import Bert
 from rtp_llm.models.megatron_bert_weight import MegatronBertWeightInfo
diff --git a/rtp_llm/models/minicpmv/minicpmv.py b/rtp_llm/models/minicpmv/minicpmv.py
index 89d889a61..0ddbf3b1e 100644
--- a/rtp_llm/models/minicpmv/minicpmv.py
+++ b/rtp_llm/models/minicpmv/minicpmv.py
@@ -6,7 +6,8 @@ import torch
 from PIL import Image
 from transformers import AutoProcessor, AutoTokenizer
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.model_config import VitParameters
 from rtp_llm.distribute.worker_info import g_parallel_info
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.minicpmv.modeling_navit_siglip import (
@@ -29,6 +30,7 @@ from rtp_llm.models.qwen_v2 import QWenV2, QWenV2Weight
 # minicpmv need to calculate num of frames to renderer input prompt, it must be preprocess first in frontend
 from rtp_llm.openai.renderers.minicpmv_renderer import encode_video
 from rtp_llm.utils.multimodal_util import (
+    MMDataCache,
     MMUrlType,
     get_bytes_io_from_url,
     vit_emb_cache_,
@@ -37,9 +39,11 @@ from rtp_llm.utils.multimodal_util import (
 
 class ImageEmbeddingInterface(MultiModalEmbeddingInterface):
 
-    def __init__(self, config: GptInitModelParameters):
-        self.config = config
-        config = config.mm_related_params.config
+    def __init__(self, mm_related_params: VitParameters):
+        if mm_related_params is None or mm_related_params.config is None:
+            raise ValueError("mm_related_params.config is required for ImageEmbeddingInterface")
+        self.mm_related_params = mm_related_params
+        config = mm_related_params.config
         self.vision_config = SiglipVisionConfig(**config)
         self.processor = AutoProcessor.from_pretrained(
             config["ckpt_path"], trust_remote_code=True
@@ -65,6 +69,7 @@ class ImageEmbeddingInterface(MultiModalEmbeddingInterface):
         dtype = self._data_type
         if g_parallel_info.tp_rank > 0:
             return torch.Tensor([])
+        # Use global vit_emb_cache_ instead of parameter
         cached_res = vit_emb_cache_.check_cache(url)
         if cached_res is None:
             cached_url_res = get_bytes_io_from_url(url)
@@ -189,9 +194,9 @@ class MiniCPMVVitWeight(BaseVitWeights):
 
 class MiniCPMVWeightInfo(QWenV2Weight, BaseMultiModalWeightInfo):
 
-    def __init__(self, config, tp_size, tp_rank):
-        QWenV2Weight.__init__(self, config, tp_size, tp_rank, prefix="llm.")
-        BaseMultiModalWeightInfo.__init__(self, config)
+    def __init__(self, vit_weights, **kwargs):
+        QWenV2Weight.__init__(self, prefix="llm.", **kwargs)
+        BaseMultiModalWeightInfo.__init__(self, vit_weights=vit_weights, **kwargs)
 
     def _get_weight_info(self):
         weights = super()._get_weight_info()
@@ -201,16 +206,34 @@ class MiniCPMVWeightInfo(QWenV2Weight, BaseMultiModalWeightInfo):
 
 class MiniCPMV(QWenV2, MultiModalMixin):
 
-    def __init__(self, config: GptInitModelParameters):
-        QWenV2.__init__(self, config)
-        self.config.mm_sep_tokens = [
+    def __init__(
+        self,
+        py_model_config,
+        mm_model_config,
+        engine_config,
+        vit_config=None,
+        merge_lora=False,
+    ):
+        QWenV2.__init__(
+            self,
+            py_model_config=py_model_config,
+            mm_model_config=mm_model_config,
+            engine_config=engine_config,
+            vit_config=vit_config,
+            merge_lora=merge_lora,
+        )
+        if self.py_model_config.mm_sep_tokens is None:
+            self.py_model_config.mm_sep_tokens = []
+        self.py_model_config.mm_sep_tokens = [
             [self.tokenizer.im_start_id, self.tokenizer.im_end_id],
             [self.tokenizer.slice_start_id, self.tokenizer.slice_end_id],
         ]
 
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = ImageEmbeddingInterface(config)
-        config.mm_related_params.vit_weights = MiniCPMVVitWeight(
+    def _init_multimodal(self, mm_model_config, vit_config):
+        if mm_model_config.mm_related_params is None:
+            raise ValueError("mm_model_config.mm_related_params is required for MiniCPMV")
+        self.mm_part = ImageEmbeddingInterface(mm_model_config.mm_related_params)
+        mm_model_config.mm_related_params.vit_weights = MiniCPMVVitWeight(
             {"vpm": self.mm_part.vpm, "resampler": self.mm_part.resampler}
         )
 
@@ -220,22 +243,24 @@ class MiniCPMV(QWenV2, MultiModalMixin):
 
     @classmethod
     def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0,
-            head_num_kv=0,
-            size_per_head=0,
-            layer_num=0,
-            inter_size=0,
-            vocab_size=0,
-            max_seq_len=8192,
-            ckpt_path=ckpt_path,
-            rotary_embedding_dim=128,
-            rotary_embedding_style=1,
-            activation_type="SiGLU",
-            has_pre_decoder_layernorm=False,
-            has_post_decoder_layernorm=True,
-            norm_type="rmsnorm",
-        )
+        from rtp_llm.config.model_config import VitParameters
+        config = ModelConfig()
+        config.head_num_ = 0
+        config.head_num_kv_ = 0
+        config.size_per_head_ = 0
+        config.num_layers = 0
+        config.inter_size = 0
+        config.vocab_size = 0
+        config.max_seq_len = 8192
+        config.ckpt_path = ckpt_path
+        config.rope_config.dim = 128
+        config.rope_config.style = 1
+        config.activation_type = "SiGLU"
+        config.has_pre_decoder_layernorm = False
+        config.has_post_decoder_layernorm = True
+        config.norm_type = "rmsnorm"
+        if config.mm_related_params is None:
+            config.mm_related_params = VitParameters()
         config_path = os.path.join(ckpt_path, "config.json")
         if os.path.exists(config_path):
             with open(config_path) as reader:
@@ -248,7 +273,12 @@ class MiniCPMV(QWenV2, MultiModalMixin):
         return config
 
     @staticmethod
-    def _init_vit_params(config: GptInitModelParameters, config_json: Dict[str, Any]):
+    def _init_vit_params(config: ModelConfig, config_json: Dict[str, Any]):
+        if config.mm_related_params is None:
+            from rtp_llm.config.model_config import VitParameters
+            config.mm_related_params = VitParameters()
+        if config.mm_related_params.config is None:
+            config.mm_related_params.config = {}
         config.mm_related_params.config = config_json["vision_config"]
         config.mm_related_params.config["llm_hidden_size"] = config_json["hidden_size"]
         config.mm_related_params.config["query_num"] = config_json["query_num"]
diff --git a/rtp_llm/models/minicpmv_embedding/minicpmv_embedding.py b/rtp_llm/models/minicpmv_embedding/minicpmv_embedding.py
index bd926008d..4718a1947 100644
--- a/rtp_llm/models/minicpmv_embedding/minicpmv_embedding.py
+++ b/rtp_llm/models/minicpmv_embedding/minicpmv_embedding.py
@@ -13,7 +13,8 @@ from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
 from torchvision import transforms
 from transformers import AutoTokenizer, LlamaTokenizer
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.downstream_modules.custom_module import CustomModule
 from rtp_llm.models.downstream_modules.embedding.minicpmv_embedding_module import (
@@ -37,6 +38,7 @@ from rtp_llm.models.multimodal.multimodal_mixin import (
     MultiModalMixin,
 )
 from rtp_llm.utils.multimodal_util import (
+    MMDataCache,
     MMUrlType,
     get_bytes_io_from_url,
     vit_emb_cache_,
@@ -45,9 +47,11 @@ from rtp_llm.utils.multimodal_util import (
 
 class ImageEmbeddingInterface(MultiModalEmbeddingInterface):
 
-    def __init__(self, config: GptInitModelParameters):
-        self.config = config
-        config = config.mm_related_params.config
+    def __init__(self, mm_related_params: VitParameters):
+        if mm_related_params is None or mm_related_params.config is None:
+            raise ValueError("mm_related_params.config is required for ImageEmbeddingInterface")
+        self.mm_related_params = mm_related_params
+        config = mm_related_params.config
         self.transform = transforms.Compose(
             [
                 transforms.ToTensor(),
@@ -98,13 +102,27 @@ class ImageEmbeddingInterface(MultiModalEmbeddingInterface):
         return model
 
     @torch.inference_mode()
-    def mm_embedding(self, url: str, mm_type: MMUrlType, **kwargs):
+    def mm_embedding(
+        self, 
+        url: str, 
+        mm_type: MMUrlType, 
+        download_headers: str = "",
+        url_cache_size: int = 10,
+        mm_cache_size: int = 10,
+        url_data_cache: MMDataCache = None,
+        **kwargs
+    ):
         dtype = self._data_type
         if self.config.tp_rank > 0:
             return torch.Tensor([])
+        
+        # Use global vit_emb_cache_ instead of parameter
+        if url_data_cache is None:
+            url_data_cache = MMDataCache(url_cache_size)
+        
         cached_res = vit_emb_cache_.check_cache(url)
         if cached_res is None:
-            cached_url_res = get_bytes_io_from_url(url)
+            cached_url_res = get_bytes_io_from_url(url, download_headers=download_headers, url_cache_size=url_cache_size, url_data_cache=url_data_cache)
             cached_url_res = self._mm_preprocess(cached_url_res, mm_type)
             with mm_lock:
                 features = self.mm_process(cached_url_res, mm_type=mm_type, **kwargs)
@@ -217,20 +235,28 @@ class MiniCPMVVitWeight(BaseVitWeights):
 
 class MiniCPMVWeightInfo(LlamaWeightInfo, BaseMultiModalWeightInfo):
 
-    def __init__(self, config, tp_size, tp_rank):
-        LlamaWeightInfo.__init__(self, config, tp_size, tp_rank, prefix="llm.")
-        BaseMultiModalWeightInfo.__init__(self, config)
-
-    def _get_weight_info(self):
-        llama_vl_weight = super()._get_weight_info()
-        self._get_vit_info(llama_vl_weight)
-        return llama_vl_weight
-
+    def __init__(self, vit_weights, **kwargs):
+        LlamaWeightInfo.__init__(self, prefix="llm.", **kwargs)
+        BaseMultiModalWeightInfo.__init__(self, vit_weights=vit_weights, **kwargs)
 
 class MiniCPMVEmbedding(Llama, MultiModalMixin):
 
-    def __init__(self, config: GptInitModelParameters):
-        Llama.__init__(self, config)
+    def __init__(
+        self,
+        py_model_config,
+        mm_model_config,
+        engine_config,
+        vit_config=None,
+        merge_lora=False,
+    ):
+        Llama.__init__(
+            self,
+            py_model_config=py_model_config,
+            mm_model_config=mm_model_config,
+            engine_config=engine_config,
+            vit_config=vit_config,
+            merge_lora=merge_lora,
+        )
         self.im_start = "<image>"
         self.im_end = "</image>"
         self.slice_start = "<slice>"
@@ -245,14 +271,18 @@ class MiniCPMVEmbedding(Llama, MultiModalMixin):
         self.slice_start_id = self.tokenizer._convert_token_to_id(self.slice_start)
         self.slice_end_id = self.tokenizer._convert_token_to_id(self.slice_end)
 
-        self.config.mm_sep_tokens = [
+        if self.py_model_config.mm_sep_tokens is None:
+            self.py_model_config.mm_sep_tokens = []
+        self.py_model_config.mm_sep_tokens = [
             [self.im_start_id, self.im_end_id]
             # [self.slice_start_id, self.slice_end_id]
         ]
 
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = ImageEmbeddingInterface(config)
-        config.mm_related_params.vit_weights = MiniCPMVVitWeight(
+    def _init_multimodal(self, mm_model_config, vit_config):
+        if mm_model_config.mm_related_params is None:
+            raise ValueError("mm_model_config.mm_related_params is required for MiniCPMVEmbedding")
+        self.mm_part = ImageEmbeddingInterface(mm_model_config.mm_related_params)
+        mm_model_config.mm_related_params.vit_weights = MiniCPMVVitWeight(
             {"vpm": self.mm_part.vpm, "resampler": self.mm_part.resampler}
         )
 
@@ -262,19 +292,22 @@ class MiniCPMVEmbedding(Llama, MultiModalMixin):
 
     @classmethod
     def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0,
-            size_per_head=0,
-            layer_num=0,
-            max_seq_len=0,
-            vocab_size=0,
-            ckpt_path=ckpt_path,
-            activation_type="SiGLU",
-            norm_type="rmsnorm",
-            rotary_embedding_dim=128,
-            rotary_embedding_style=1,
-            has_post_decoder_layernorm=True,
-        )
+        from rtp_llm.config.model_config import ModelConfig
+        from rtp_llm.config.model_config import VitParameters
+        config = ModelConfig()
+        config.head_num_ = 0
+        config.size_per_head_ = 0
+        config.num_layers = 0
+        config.max_seq_len = 0
+        config.vocab_size = 0
+        config.ckpt_path = ckpt_path
+        config.activation_type = "SiGLU"
+        config.norm_type = "rmsnorm"
+        config.rope_config.dim = 128
+        config.rope_config.style = 1
+        config.has_post_decoder_layernorm = True
+        if config.mm_related_params is None:
+            config.mm_related_params = VitParameters()
         config_path = os.path.join(ckpt_path, "config.json")
         if os.path.exists(config_path):
             with open(config_path) as reader:
@@ -284,7 +317,7 @@ class MiniCPMVEmbedding(Llama, MultiModalMixin):
                 config.input_embedding_scalar = config_json.get("scale_emb", 1)
                 config.residual_scalar = config_json.get(
                     "scale_depth", 1.4
-                ) / math.sqrt(config.layer_num)
+                ) / math.sqrt(config.num_layers)
                 # config.activation_type = config_json["hidden_act"]
                 MiniCPMVEmbedding._init_vit_params(config, config_json)
         else:
@@ -292,7 +325,12 @@ class MiniCPMVEmbedding(Llama, MultiModalMixin):
         return config
 
     @staticmethod
-    def _init_vit_params(config: GptInitModelParameters, config_json: Dict[str, Any]):
+    def _init_vit_params(config: ModelConfig, config_json: Dict[str, Any]):
+        if config.mm_related_params is None:
+            from rtp_llm.config.model_config import VitParameters
+            config.mm_related_params = VitParameters()
+        if config.mm_related_params.config is None:
+            config.mm_related_params.config = {}
         # config.mm_related_params.config = config_json["vision_config"]
         config.mm_related_params.config["llm_hidden_size"] = config_json["hidden_size"]
         config.mm_related_params.config["query_num"] = config_json["query_num"]
@@ -313,7 +351,7 @@ class MiniCPMVEmbedding(Llama, MultiModalMixin):
         ]
 
     def _init_custom_module(self) -> Optional[CustomModule]:
-        return MiniCPMVModule(self.config, self.tokenizer)
+        return MiniCPMVModule(self.config, self.tokenizer, vit_config=self.vit_config)
 
 
 register_model("minicpmv_embedding", MiniCPMVEmbedding, ["MiniCPMVEmbedding"])
diff --git a/rtp_llm/models/mixtral.py b/rtp_llm/models/mixtral.py
index e2e9a29ea..4dc2d4be7 100644
--- a/rtp_llm/models/mixtral.py
+++ b/rtp_llm/models/mixtral.py
@@ -5,7 +5,8 @@ from typing import List
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight, AttnConfig
 from rtp_llm.model_loader.ffn_weight import MoeAtomicWeight, MoeConfig, MoeWeight
@@ -212,34 +213,32 @@ class Mixtral(BaseModel):
         return MixtralWeightInfo
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config_path = os.path.join(ckpt_path, "config.json")
         with open(config_path) as f:
             config_json = json.load(f)
         size_per_head = config_json["hidden_size"] // config_json["num_attention_heads"]
-        config = GptInitModelParameters(
-            head_num=config_json["num_attention_heads"],
-            size_per_head=size_per_head,
-            inter_size=config_json["intermediate_size"],
-            layer_num=config_json["num_hidden_layers"],
-            max_seq_len=config_json.get("max_sequence_length", 2048),
-            vocab_size=config_json["vocab_size"],
-            head_num_kv=config_json["num_key_value_heads"],
-            activation_type="SiGLU",
-            norm_type="rmsnorm",
-            rotary_embedding_dim=size_per_head,
-            has_moe_norm=True,
-            rotary_embedding_style=1,
-            has_post_decoder_layernorm=True,
-            rotary_embedding_base=config_json.get("rope_theta", 10000),
-            expert_num=config_json["num_local_experts"],
-            moe_k=config_json["num_experts_per_tok"],
-            moe_style=1,
-            moe_layer_index=[i for i in range(config_json["num_hidden_layers"])],
-        )
+        config = ModelConfig()
+        config.ckpt_path = ckpt_path
+        config.head_num = config_json["num_attention_heads"]
+        config.size_per_head = size_per_head
+        config.inter_size = config_json["intermediate_size"]
+        config.num_layers = config_json["num_hidden_layers"]
+        config.max_seq_len = config_json.get("max_sequence_length", 2048)
+        config.vocab_size = config_json["vocab_size"]
+        config.head_num_kv = config_json["num_key_value_heads"]
+        config.rope_config.dim = size_per_head
+        config.has_moe_norm = True
+        config.rope_config.style = 1
+        config.rope_config.base = config_json.get("rope_theta", 10000)
+        config.expert_num = config_json["num_local_experts"]
+        config.moe_k = config_json["num_experts_per_tok"]
+        config.moe_style = 1
+        config.moe_layer_index = [i for i in range(config_json["num_hidden_layers"])]
         config.special_tokens.eos_token_id = 2
         config.special_tokens.bos_token_id = 1
         config.config_dtype = config_json.get("torch_dtype", None)
+        config.mm_related_params = VitParameters()
         return config
 
 
diff --git a/rtp_llm/models/mpt.py b/rtp_llm/models/mpt.py
index 2ee582903..77ef5e586 100644
--- a/rtp_llm/models/mpt.py
+++ b/rtp_llm/models/mpt.py
@@ -4,7 +4,7 @@ import os
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight
 from rtp_llm.model_loader.ffn_weight import FfnAtomicWeight, FfnWeight
@@ -42,12 +42,12 @@ class MptWeightInfo(ModelDeployWeightInfo):
             ),
         ]
 
-        if self.config.use_attention_linear_bias:
+        if self.py_model_config.use_attention_linear_bias:
             weights.append(
                 AtomicWeight(
                     W.linear_bias_slopes,
                     [],
-                    functools.partial(slopes, n=self.config.head_num),
+                    functools.partial(slopes, n=self.py_model_config.head_num_),
                     data_type=torch.float,
                 )
             )
@@ -133,15 +133,15 @@ class Mpt(BaseModel):
         return MptWeightInfo
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config_path = os.path.join(ckpt_path, "config.json")
         with open(config_path) as f:
             config_json = json.load(f)
-        config = GptInitModelParameters(
+        config = ModelConfig(
             head_num=config_json["n_heads"],
             size_per_head=config_json["d_model"] // config_json["n_heads"],
             inter_size=config_json["d_model"] * 4,
-            layer_num=config_json["n_layers"],
+            num_layers=config_json["n_layers"],
             max_seq_len=8192,
             vocab_size=config_json["vocab_size"],
             activation_type="gelu-none-approximate",
diff --git a/rtp_llm/models/multimodal/multimodal_common.py b/rtp_llm/models/multimodal/multimodal_common.py
index e0943424e..7f056c12e 100644
--- a/rtp_llm/models/multimodal/multimodal_common.py
+++ b/rtp_llm/models/multimodal/multimodal_common.py
@@ -22,6 +22,7 @@ from torchvision import transforms
 
 from rtp_llm.distribute.worker_info import g_parallel_info
 from rtp_llm.utils.multimodal_util import (
+    MMDataCache,
     MMUrlType,
     get_bytes_io_from_url,
     get_vit_compute_dtype,
@@ -88,14 +89,28 @@ class MultiModalEmbeddingInterface:
         raise NotImplementedError
 
     @torch.inference_mode()
-    def mm_embedding(self, url: str, mm_type: MMUrlType, **kwargs: Any):
+    def mm_embedding(
+        self, 
+        url: str, 
+        mm_type: MMUrlType, 
+        download_headers: str = "",
+        url_cache_size: int = 10,
+        mm_cache_size: int = 10,
+        url_data_cache: MMDataCache = None,
+        **kwargs: Any
+    ):
         dtype = self._data_type
         if g_parallel_info.tp_rank > 0:
             return torch.Tensor([])
+        
+        # Use global vit_emb_cache_ instead of parameter
+        if url_data_cache is None:
+            url_data_cache = MMDataCache(url_cache_size)
+        
         cached_res = vit_emb_cache_.check_cache(url)
         if cached_res is not None:
             return cached_res
-        bytes_io = get_bytes_io_from_url(url)
+        bytes_io = get_bytes_io_from_url(url, download_headers=download_headers, url_cache_size=url_cache_size, url_data_cache=url_data_cache)
         mm_input = self._mm_preprocess(bytes_io, mm_type=mm_type, **kwargs)
         with mm_lock:
             features = self.mm_process(mm_input, mm_type=mm_type, **kwargs)
diff --git a/rtp_llm/models/multimodal/multimodal_mixin.py b/rtp_llm/models/multimodal/multimodal_mixin.py
index af5e55d92..e70ec58a3 100644
--- a/rtp_llm/models/multimodal/multimodal_mixin.py
+++ b/rtp_llm/models/multimodal/multimodal_mixin.py
@@ -1,28 +1,23 @@
 import gc
-import json
 import os
 import re
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import torch
 
-from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
-from rtp_llm.config.generate_config import RequestFormat
-from rtp_llm.config.gpt_init_model_parameters import (
-    GptInitModelParameters,
-    VitParameters,
-)
-from rtp_llm.config.py_config_modules import StaticConfig
-from rtp_llm.model_loader.model_weight_info import (
-    ModelDeployWeightInfo,
-    ModelWeightInfo,
-)
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.py_config_modules import VitConfig
 from rtp_llm.model_loader.weight_module import MMAtomicWeight
+from typing import TYPE_CHECKING
+
+if TYPE_CHECKING:
+    from rtp_llm.model_loader.model_weight_info import ModelWeightInfo
 from rtp_llm.models.multimodal.multimodal_common import MultiModalEmbeddingInterface
 from rtp_llm.models.multimodal.multimodal_trt_engine import MultiModalTRTEngine
 from rtp_llm.ops.comm.nccl_op import NcclOp
 from rtp_llm.utils.model_weight import CkptWeightInfo, identity, sp_id
-from rtp_llm.utils.multimodal_util import MultimodalInput, get_vit_compute_dtype
+from rtp_llm.utils.multimodal_util import get_vit_compute_dtype
 
 
 class BaseVitWeights:
@@ -63,35 +58,27 @@ class BaseVitWeights:
 
 
 class BaseMultiModalWeightInfo:
-    def __init__(self, config: GptInitModelParameters):
-        self.vit_weights: Optional[BaseVitWeights] = (
-            config.mm_related_params.vit_weights
-        )
-        self.vit_separation: int = config.vit_separation
-        self.tp_rank = config.tp_rank
-
-    def _get_vit_info(self, llm_weights: ModelDeployWeightInfo):
-        # Currently, the multimodel network isn't split between devices. Only Rank 0 loads the weights.
-        # After supporting TP mm network, we will remove the check here.
-        if self.vit_separation == 1:
-            llm_weights = ModelWeightInfo(layer_weights=[], weights=[])
-
-        if self.vit_separation != 2:
-            if self.vit_weights is not None and self.tp_rank == 0:
-                weight_names = self.vit_weights.weight_names
-                ckpt_prefix = self.vit_weights.ckpt_prefix
-
-                for w in weight_names:
-                    w_name = ckpt_prefix + w
-                    llm_weights.weights.append(
-                        MMAtomicWeight(
-                            w,
-                            [CkptWeightInfo(w_name, identity)],
-                            identity,
-                            split_func=sp_id,
-                        )
+    def __init__(
+        self,
+        vit_weights: Optional[BaseVitWeights],
+        **kwargs,
+    ):
+        self.vit_weights: Optional[BaseVitWeights] = vit_weights
+
+    def _get_vit_info(self, llm_weights: "ModelWeightInfo") -> "ModelWeightInfo":
+        if self.vit_weights is not None:
+            weight_names = self.vit_weights.weight_names
+            ckpt_prefix = self.vit_weights.ckpt_prefix
+            for w in weight_names:
+                w_name = ckpt_prefix + w
+                llm_weights.weights.append(
+                    MMAtomicWeight(
+                        w,
+                        [CkptWeightInfo(w_name, identity)],
+                        identity,
+                        split_func=sp_id,
                     )
-
+                )
         return llm_weights
 
 
@@ -101,18 +88,29 @@ class MultiModalMixin:
 
     @property
     def vit_data_type(self):
-        return get_vit_compute_dtype(self.config.data_type)
-
-    def init_multimodal(self, config: GptInitModelParameters, device: str) -> None:
-        self.vit_config = config.py_env_configs.vit_config
-        if config.vit_separation != 2:
-            with torch.device(device):
-                torch_default_dtype = torch.get_default_dtype()
-                torch.set_default_dtype(self.vit_data_type)
-                self._init_multimodal(config)
-                torch.set_default_dtype(torch_default_dtype)
-
-    def _init_multimodal(self, config: GptInitModelParameters) -> None:
+        return get_vit_compute_dtype(self.py_model_config.data_type)
+
+    def init_multimodal(
+        self,
+        mm_model_config: Any,  # MMModelConfig
+        vit_config: VitConfig,
+        device: str,
+    ) -> None:
+        self.vit_config = vit_config
+        with torch.device(device):
+            torch_default_dtype = torch.get_default_dtype()
+            torch.set_default_dtype(self.vit_data_type)
+            self._init_multimodal(
+                mm_model_config=mm_model_config,
+                vit_config=vit_config,
+            )
+            torch.set_default_dtype(torch_default_dtype)
+
+    def _init_multimodal(
+        self,
+        mm_model_config: Any,  # MMModelConfig
+        vit_config: VitConfig,
+    ) -> None:
         raise NotImplementedError
 
     def _load_mm_weight(self, vit_params: VitParameters, ctype: str, device: str):
@@ -172,7 +170,7 @@ class MultiModalMixin:
                 self._load_mm_weight(vit_params, dtype, device)
 
                 # create cached dir if not exists
-                output_dir = MultiModalTRTEngine.cache_path(model_name_path, dtype)
+                output_dir = MultiModalTRTEngine.cache_path(model_name_path, dtype, self.vit_config)
                 if not os.path.exists(output_dir):
                     os.makedirs(output_dir)
 
@@ -207,12 +205,22 @@ class MultiModalMixin:
         gc.collect()
         torch.cuda.empty_cache()
 
-    def load_mm_weight(self, ctype: str, tp_size: int, tp_rank: int, device: str):
-
-        if StaticConfig.vit_config.vit_trt == 1:
+    def load_mm_weight(
+        self,
+        py_model_config: ModelConfig,
+        mm_model_config: Any,  # MMModelConfig
+        ctype: str,
+        tp_size: int,
+        tp_rank: int,
+        device: str,
+    ):
+        vit_trt = self.vit_config.vit_trt
+        
+        if vit_trt == 1:
+            mm_related_params = mm_model_config.mm_related_params if hasattr(mm_model_config, 'mm_related_params') else None
             self.init_mm_trt(
-                self.config.ckpt_path,
-                self.config.mm_related_params,
+                py_model_config.ckpt_path,
+                mm_related_params,
                 tp_size,
                 tp_rank,
                 device,
@@ -232,4 +240,5 @@ class MultiModalMixin:
         if isinstance(self.mm_part, MultiModalTRTEngine):
             return
 
-        self._load_mm_weight(self.config.mm_related_params, ctype, device)
+        mm_related_params = mm_model_config.mm_related_params if hasattr(mm_model_config, 'mm_related_params') else None
+        self._load_mm_weight(mm_related_params, ctype, device)
diff --git a/rtp_llm/models/multimodal/multimodal_trt_engine.py b/rtp_llm/models/multimodal/multimodal_trt_engine.py
index 2ce35f571..83c5b80c8 100644
--- a/rtp_llm/models/multimodal/multimodal_trt_engine.py
+++ b/rtp_llm/models/multimodal/multimodal_trt_engine.py
@@ -11,7 +11,6 @@ import torch
 from PIL import Image
 from torch import nn
 
-from rtp_llm.config.py_config_modules import StaticConfig
 from rtp_llm.models.multimodal.multimodal_common import (
     AudioEmbeddingInterface,
     ImageEmbeddingInterface,
@@ -85,8 +84,13 @@ class MultiModalTRTEngine(nn.Module, ImageEmbeddingInterface, AudioEmbeddingInte
         return MultiModalTRTEngine.completion_file_path(model_name, dtype).exists()
 
     @staticmethod
-    def cache_path(model_name: str, dtype: torch.dtype) -> str:
-        trt_cache_path = StaticConfig.vit_config.trt_cache_path
+    def cache_path(model_name: str, dtype: torch.dtype, vit_config) -> str:
+        """Get cache path for TRT engine.
+        
+        Args:
+            model_name: Model name.
+            dtype: Torch dtype.        """
+        trt_cache_path = vit_config.trt_cache_path
         if trt_cache_path is None:
             trt_cache_path = os.path.join(os.getcwd(), "trt_cache")
         return os.path.join(
diff --git a/rtp_llm/models/phi.py b/rtp_llm/models/phi.py
index ef1d075bb..2f7233f3b 100644
--- a/rtp_llm/models/phi.py
+++ b/rtp_llm/models/phi.py
@@ -1,4 +1,4 @@
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight
 from rtp_llm.model_loader.ffn_weight import FfnAtomicWeight, FfnWeight
@@ -119,27 +119,29 @@ class Phi(BaseModel):
         return PhiWeightInfo
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config_dict = get_config_from_path(ckpt_path)
+        if config_dict is None:
+            config_dict = {}
         size_per_head = int(
             config_dict.get("n_embd", 2048) / config_dict.get("n_head", 32)
         )
-        config = GptInitModelParameters(
-            head_num=config_dict.get("n_head", 32),
-            size_per_head=size_per_head,
-            inter_size=4 * config_dict.get("n_embd", 2048),
-            layer_num=config_dict.get("n_layer", 24),
-            max_seq_len=config_dict.get("n_positions", 2048),
-            vocab_size=config_dict.get("vocab_size", 32),
-            rotary_embedding_dim=config_dict.get("rotary_dim", size_per_head),
-            rotary_embedding_style=1,
-            activation_type="gelu",
-            has_positional_encoding=False,
-            has_post_decoder_layernorm=True,
-            has_lm_head_bias=True,
-            tie_word_embeddings=config_dict.get("tie_word_embeddings", False),
-        )
-        config.head_num_kv = config.head_num
+        config = ModelConfig()
+        config.ckpt_path = ckpt_path
+        config.attn_config.head_num = config_dict.get("n_head", 32)
+        config.attn_config.size_per_head = size_per_head
+        config.inter_size = 4 * config_dict.get("n_embd", 2048)
+        config.num_layers = config_dict.get("n_layer", 24)
+        config.max_seq_len = config_dict.get("n_positions", 2048)
+        config.vocab_size = config_dict.get("vocab_size", 32)
+        config.attn_config.rope_config.dim = config_dict.get("rotary_dim", size_per_head)
+        config.attn_config.rope_config.style = 1
+        config.activation_type = "gelu"
+        config.has_positional_encoding = False
+        config.has_post_decoder_layernorm = True
+        config.has_lm_head_bias = True
+        config.tie_word_embeddings = config_dict.get("tie_word_embeddings", False)
+        config.attn_config.kv_head_num = config.attn_config.head_num
         config.config_dtype = config_dict.get("torch_dtype", None)
         return config
 
diff --git a/rtp_llm/models/qwen.py b/rtp_llm/models/qwen.py
index ed317b269..886293c3f 100644
--- a/rtp_llm/models/qwen.py
+++ b/rtp_llm/models/qwen.py
@@ -3,7 +3,8 @@ import json
 import os
 from typing import List, Optional
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight, AttnConfig
 from rtp_llm.model_loader.ffn_weight import FfnAtomicWeight, FfnConfig, FfnWeight
@@ -206,32 +207,60 @@ class QWenBase(BaseModel):
         return QWenWeight
 
     def _create_python_model(self) -> Optional[GptModelBase]:
-        if self.config.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate:
-            self.py_model = Qwen3DisaggregateModel(self.config, self.weight)
+        py_model_config = self.py_model_config
+        parallelism_config = self.engine_config.parallelism_config
+        ffn_disaggregate_config = self.engine_config.parallelism_config.ffn_disaggregate_config
+        device_resource_config = self.engine_config.device_resource_config
+        quant_config = self.py_model_config.quant_config
+        
+        vocab_size = self.py_model_config.vocab_size
+        fmha_config = self.engine_config.fmha_config
+        py_hw_kernel_config = self.engine_config.hw_kernel_config
+        
+        if ffn_disaggregate_config.enable_ffn_disaggregate:
+            self.py_model = Qwen3DisaggregateModel(
+                py_model_config, 
+                parallelism_config,
+                ffn_disaggregate_config,
+                device_resource_config,
+                self.weight,
+                quant_config,
+                vocab_size,
+                fmha_config=fmha_config,
+                py_hw_kernel_config=py_hw_kernel_config,
+            )
         else:
-            self.py_model = Qwen3Model(self.config, self.weight)
+            self.py_model = Qwen3Model(
+                py_model_config,
+                parallelism_config,
+                device_resource_config,
+                self.weight,
+                quant_config,
+                vocab_size,
+                fmha_config=fmha_config,
+                py_hw_kernel_config=py_hw_kernel_config,
+            )
 
     def support_cuda_graph(self) -> bool:
         return True
 
     @staticmethod
-    def _common_config(config, ckpt_path: str) -> GptInitModelParameters:
-        config.rotary_embedding_dim = 128
-        config.rotary_embedding_style = 1
-        config.activation_type = "SiGLU"
+    def _common_config(config: ModelConfig, ckpt_path: str) -> ModelConfig:
+        config.ckpt_path = ckpt_path
+        config.rope_config.dim = 128
+        config.rope_config.style = 1
         config.has_pre_decoder_layernorm = False
-        config.has_post_decoder_layernorm = True
-        config.norm_type = "rmsnorm"
         config.layernorm_eps = 1e-5
         config.special_tokens.bos_token_id = -1
         config.special_tokens.eos_token_id = 151643
         # <|im_start|> and <|im_end|>
         config.special_tokens.stop_words_id_list = [[151645], [151644]]
+        config.mm_related_params = VitParameters()
         QWen._from_hf(config, ckpt_path)
         return config
 
     @staticmethod
-    def _from_hf(config: GptInitModelParameters, ckpt_path: str):
+    def _from_hf(config: ModelConfig, ckpt_path: str):
         config_path = os.path.join(ckpt_path, "config.json")
         if not os.path.exists(config_path):
             return
@@ -258,43 +287,37 @@ class QWenBase(BaseModel):
         config.layernorm_eps = config_json.get(
             "layer_norm_epsilon", config.layernorm_eps
         )
-        config.layer_num = config_json.get(
-            "num_hidden_layers", config_json.get("n_layer", config.layer_num)
+        config.num_layers = config_json.get(
+            "num_hidden_layers", config_json.get("n_layer", config.num_layers)
         )
         config.vocab_size = config_json.get(
             "vocab_size", config_json.get("padded_vocab_size", config.vocab_size)
         )
-        config.rotary_embedding_base = config_json.get("rotary_emb_base", 10000)
-        config.rotary_embedding_dim = config.size_per_head
+        config.rope_config.base = config_json.get("rotary_emb_base", 10000)
+        config.rope_config.dim = config.size_per_head
         config.special_tokens.eos_token_id = config_json.get(
             "eos_token_id", config.special_tokens.eos_token_id
         )
         config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
 
         if config_json.get("use_dynamic_ntk"):
-            config.rotary_embedding_style = 4
-        config.org_embedding_max_pos = config_json.get("seq_length", 8192)
+            config.rope_config.style = 4
+        config.rope_config.max_pos = config_json.get("seq_length", 8192)
         config.use_logn_attn = config_json.get("use_logn_attn")
 
 
 class QWen(QWenBase):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0,
-            head_num_kv=0,
-            size_per_head=0,
-            layer_num=0,
-            inter_size=0,  # 13696
-            vocab_size=152064,
-            max_seq_len=8192,
-        )
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
+        config = ModelConfig()
+        config.vocab_size = 152064
+        config.max_seq_len = 8192
         QWenBase._common_config(config, ckpt_path)
         assert (
             config.head_num > 0
             and config.head_num_kv > 0
             and config.size_per_head > 0
-            and config.layer_num > 0
+            and config.num_layers > 0
             and config.inter_size > 0
         ), "error config"
         return config
@@ -302,12 +325,12 @@ class QWen(QWenBase):
 
 class QWen_7B(QWenBase):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
+        config = ModelConfig(
             head_num=32,
             head_num_kv=32,
             size_per_head=128,
-            layer_num=32,
+            num_layers=32,
             inter_size=hidden_to_inter(4096),  # 11008
             vocab_size=151936,
             max_seq_len=8192,
@@ -318,28 +341,28 @@ class QWen_7B(QWenBase):
 
 class QWen_13B(QWenBase):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
+        config = ModelConfig(
             head_num=40,
             head_num_kv=40,
             size_per_head=128,
-            layer_num=40,
+            num_layers=40,
             inter_size=hidden_to_inter(5120),  # 13696
             vocab_size=152064,
             max_seq_len=8192,
         )
-        QWen._common_config(config, ckpt_path)
+        QWenBase._common_config(config, ckpt_path)
         return config
 
 
 class QWen_1B8(QWenBase):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
+        config = ModelConfig(
             head_num=16,
             head_num_kv=16,
             size_per_head=128,
-            layer_num=24,
+            num_layers=24,
             inter_size=hidden_to_inter(2048),  # 5504
             vocab_size=151936,
             max_seq_len=2048,
diff --git a/rtp_llm/models/qwen2_5_vl/qwen2_5_vl.py b/rtp_llm/models/qwen2_5_vl/qwen2_5_vl.py
index 8d6db16b7..dd05406f9 100644
--- a/rtp_llm/models/qwen2_5_vl/qwen2_5_vl.py
+++ b/rtp_llm/models/qwen2_5_vl/qwen2_5_vl.py
@@ -1,8 +1,12 @@
+from typing import Any
+
 import torch
 from torchvision import transforms
 from torchvision.transforms import InterpolationMode
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.py_config_modules import VitConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.qwen2_vl.qwen2_vl import QWen2_VL, QwenVL2VitWeight
 
@@ -13,8 +17,6 @@ except ModuleNotFoundError:
     cpu = None
 
 import torch.library as tl
-
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.models.qwen2_5_vl.modeling_qwen2_5_vl import (
     Qwen2_5_VisionTransformerPretrainedModel,
 )
@@ -66,14 +68,16 @@ def smart_nframes(configs, total_frames, video_fps) -> int:
 
 
 class Qwen2_5_VLImageEmbedding(Qwen2VLImageEmbedding):
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, mm_related_params: VitParameters):
+        if mm_related_params is None or not hasattr(mm_related_params, 'config'):
+            raise ValueError("mm_related_params.config is required for Qwen2_5_VLImageEmbedding")
+        self.mm_related_params = mm_related_params
         self.image_processor = Qwen2VLImageProcessor.from_pretrained(
-            config.mm_related_params.config["ckpt_path"]
+            mm_related_params.config["ckpt_path"]
         )
         self.visual = Qwen2_5_VisionTransformerPretrainedModel(
-            config.mm_related_params.config
+            mm_related_params.config
         )
-        self.config = config
 
     def load_video(self, data, configs, **kwargs):
         vr = VideoReader(data, ctx=cpu(0), num_threads=1)
@@ -124,9 +128,16 @@ class Qwen2_5_VLImageEmbedding(Qwen2VLImageEmbedding):
 
 
 class QWen2_5_VL(QWen2_VL):
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = Qwen2_5_VLImageEmbedding(config)
-        config.mm_related_params.vit_weights = QwenVL2VitWeight(
+    def _init_multimodal(
+        self,
+        mm_model_config: Any,  # MMModelConfig
+        vit_config: VitConfig,
+    ):
+        mm_related_params = mm_model_config.mm_related_params if hasattr(mm_model_config, 'mm_related_params') else None
+        if mm_related_params is None:
+            raise ValueError("mm_related_params is required for QWen2_5_VL")
+        self.mm_part = Qwen2_5_VLImageEmbedding(mm_related_params)
+        mm_model_config.mm_related_params.vit_weights = QwenVL2VitWeight(
             {"vit": self.mm_part.visual}
         )
 
diff --git a/rtp_llm/models/qwen2_vl/qwen2_vl.py b/rtp_llm/models/qwen2_vl/qwen2_vl.py
index ffa00fcc4..f86c4bc51 100644
--- a/rtp_llm/models/qwen2_vl/qwen2_vl.py
+++ b/rtp_llm/models/qwen2_vl/qwen2_vl.py
@@ -5,7 +5,9 @@ from typing import Any, Dict, List, Tuple, Union
 
 from transformers import AutoTokenizer
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.py_config_modules import VitConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight, AttnConfig
 from rtp_llm.model_loader.ffn_weight import FfnAtomicWeight, FfnConfig, FfnWeight
@@ -14,7 +16,6 @@ from rtp_llm.model_loader.model_weight_info import (
     ModelWeightInfo,
 )
 from rtp_llm.model_loader.weight_module import AtomicWeight, WeightModule
-from rtp_llm.models.base_model import BaseModel, MultimodalInput
 from rtp_llm.models.multimodal.multimodal_mixin import (
     BaseMultiModalWeightInfo,
     BaseVitWeights,
@@ -47,9 +48,9 @@ class QwenVL2VitWeight(BaseVitWeights):
 
 
 class QWen2VLWeightInfo(ModelDeployWeightInfo, BaseMultiModalWeightInfo):
-    def __init__(self, config, tp_size, tp_rank):
-        ModelDeployWeightInfo.__init__(self, config, tp_size, tp_rank)
-        BaseMultiModalWeightInfo.__init__(self, config)
+    def __init__(self, vit_weights, **kwargs):
+        ModelDeployWeightInfo.__init__(self, **kwargs)
+        BaseMultiModalWeightInfo.__init__(self, vit_weights=vit_weights, **kwargs)
 
     @property
     def support_lora(self) -> bool:
@@ -57,7 +58,6 @@ class QWen2VLWeightInfo(ModelDeployWeightInfo, BaseMultiModalWeightInfo):
 
     def _get_weight_info(self):
         weights = self._get_hf_weight_info()
-        weights = self._get_vit_info(weights)
         return weights
 
     def _get_hf_weight_info(self):
@@ -229,84 +229,100 @@ class QWen2VLWeightInfo(ModelDeployWeightInfo, BaseMultiModalWeightInfo):
 
 
 class QWen2_VL(QWen_VL, MultiModalMixin):
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = Qwen2VLImageEmbedding(config)
-        config.mm_related_params.vit_weights = QwenVL2VitWeight(
+    def _init_multimodal(
+        self,
+        mm_model_config,
+        vit_config: VitConfig,
+    ):
+        if mm_model_config.mm_related_params is None:
+            mm_model_config.mm_related_params = VitParameters()
+        self.mm_part = Qwen2VLImageEmbedding(mm_model_config.mm_related_params)
+        mm_model_config.mm_related_params.vit_weights = QwenVL2VitWeight(
             {"vit": self.mm_part.visual}
         )
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0, size_per_head=0, layer_num=0, max_seq_len=0, vocab_size=0
-        )
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
+        config = ModelConfig()
+        config.ckpt_path = ckpt_path
 
         config_path = os.path.join(ckpt_path, "config.json")
         if not os.path.exists(config_path):
-            return
+            return config
         with open(config_path) as reader:
             content = reader.read()
             config_json = json.loads(content)
 
         QWen2_VL._from_hf(config, config_json)
         QWen2_VL._load_vit_param(config, config_json)
+        if config.mm_related_params is None:
+            config.mm_related_params = VitParameters()
         config.mm_related_params.config["ckpt_path"] = ckpt_path
 
         return config
 
     @staticmethod
-    def _load_vit_param(config: GptInitModelParameters, config_json: Dict[str, Any]):
+    def _load_vit_param(config: ModelConfig, config_json: Dict[str, Any]):
+        if config.mm_related_params is None:
+            config.mm_related_params = VitParameters()
         config.mm_related_params.config = config_json["vision_config"]
+        if config.mm_related_params.special_tokens is None:
+            from rtp_llm.config.model_config import SpecialTokens
+            config.mm_related_params.special_tokens = SpecialTokens()
         config.mm_related_params.special_tokens.update({"default_mm_token": "<img/>"})
         config.mm_sep_tokens = [
             [config_json["vision_start_token_id"], config_json["vision_end_token_id"]]
         ]
 
     @staticmethod
-    def _from_hf(config: GptInitModelParameters, config_json: Dict[str, Any]):
+    def _from_hf(config: ModelConfig, config_json: Dict[str, Any]):
         config.vocab_size = config_json["vocab_size"]
-        config.rotary_embedding_base = config_json["rope_theta"]
         config.max_seq_len = 10240
         config.activation_type = "SiGLU"
-        config.head_num = config_json["num_attention_heads"]
-        config.head_num_kv = config_json["num_key_value_heads"]
+        config.head_num_ = config_json["num_attention_heads"]
+        config.head_num_kv_ = config_json["num_key_value_heads"]
         config.hidden_size = config_json["hidden_size"]
-        config.size_per_head = (
+        config.size_per_head_ = (
             int(config_json.get("head_dim"))
             if "head_dim" in config_json
-            else config_json["hidden_size"] // config.head_num
+            else config_json["hidden_size"] // config.head_num_
         )
-        config.layer_num = config_json["num_hidden_layers"]
+        config.layer_num_ = config_json["num_hidden_layers"]
         config.inter_size = config_json["intermediate_size"]
         config.norm_type = "rmsnorm"
-        config.layernorm_eps = config_json["rms_norm_eps"]
-        config.has_post_decoder_layernorm = True
+        config.layernorm_eps_ = config_json["rms_norm_eps"]
+        config.has_post_decoder_layernorm_ = True
         config.special_tokens.bos_token_id = config_json.get("bos_token_id", -1)
         config.special_tokens.eos_token_id = config_json.get("eos_token_id", 0)
-        config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
-
-        config.rotary_embedding_style = 7
-        config.mrope_section = config_json["rope_scaling"].get(
-            "mrope_section", [16, 24, 24]
-        )
-        config.mm_position_ids_style = 2
-        config.position_id_len_factor = len(config.mrope_section)
-        config.rotary_embedding_dim = 128
+        config.tie_word_embeddings_ = config_json.get("tie_word_embeddings", False)
+        config.mm_model_config.mm_position_ids_style = 2
+        rope_config = config.attn_config
+        rope_config.style = 7
+        rope_config.base = int(config_json["rope_theta"])
+        # mrope_section is not available in RopeConfig, using default value
+        mrope_section = config_json["rope_scaling"].get("mrope_section", [16, 24, 24])
+        rope_config.index_factor = len(mrope_section)
+        rope_config.mrope_dim1 = mrope_section[0]
+        rope_config.mrope_dim2 = mrope_section[1]
+        rope_config.mrope_dim3 = mrope_section[2]
+        rope_config.dim = 128
 
     @staticmethod
     def get_weight_cls():
         return QWen2VLWeightInfo
 
     @staticmethod
-    def eval_model_size(config: GptInitModelParameters):
-        llm_size = BaseModel.eval_model_size(config)
+    def eval_model_size(config: ModelConfig):
+        llm_size = config.eval_model_size()
 
         data_width = 4
         llm_size += QWen2_VL.eval_vit_param_count(config) * data_width
         return llm_size
 
     @staticmethod
-    def eval_vit_param_count(config: GptInitModelParameters):
+    def eval_vit_param_count(config: ModelConfig):
+        if config.mm_related_params is None or config.mm_related_params.config is None:
+            return 0
         vit_config = config.mm_related_params.config
         embed_dim = vit_config.get("embed_dim", 1280)
         hidden_size = vit_config.get("hidden_size", 3584)
@@ -326,8 +342,8 @@ class QWen2_VL(QWen_VL, MultiModalMixin):
         return vit_size
 
     @staticmethod
-    def eval_model_param_count(config: GptInitModelParameters):
-        llm_param_count = BaseModel.eval_model_param_count(config)
+    def eval_model_param_count(config: ModelConfig):
+        llm_param_count = config.model_param_count()
         llm_param_count += QWen2_VL.eval_vit_param_count(config)
 
         return llm_param_count
diff --git a/rtp_llm/models/qwen2_vl/qwen2_vl_vit.py b/rtp_llm/models/qwen2_vl/qwen2_vl_vit.py
index 4e69f47c0..4b8e202f6 100644
--- a/rtp_llm/models/qwen2_vl/qwen2_vl_vit.py
+++ b/rtp_llm/models/qwen2_vl/qwen2_vl_vit.py
@@ -11,7 +11,7 @@ import torch
 from torchvision import transforms
 from torchvision.transforms import InterpolationMode
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
 from rtp_llm.models.multimodal.multimodal_common import (
     MultiModalEmbeddingInterface,
     timeout_decorator,
@@ -85,14 +85,14 @@ def smart_resize(
 
 
 class Qwen2VLImageEmbedding(MultiModalEmbeddingInterface):
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, mm_related_params: VitParameters):
+        self.mm_related_params = mm_related_params
         self.image_processor = Qwen2VLImageProcessor.from_pretrained(
-            config.mm_related_params.config["ckpt_path"]
+            mm_related_params.config["ckpt_path"]
         )
         self.visual = Qwen2VisionTransformerPretrainedModel(
-            config.mm_related_params.config
+            mm_related_params.config
         )
-        self.config = config
 
     @property
     def _device(self):
@@ -223,7 +223,7 @@ class Qwen2VLImageEmbedding(MultiModalEmbeddingInterface):
         return embeddings, pos_id
 
     def get_position_ids(self, grid_thw: torch.Tensor = None) -> torch.Tensor:
-        spatial_merge_size = self.config.mm_related_params.config.get(
+        spatial_merge_size = self.mm_related_params.config.get(
             "spatial_merge_size", 2
         )
 
diff --git a/rtp_llm/models/qwen3_vl_moe/qwen3_vl_moe.py b/rtp_llm/models/qwen3_vl_moe/qwen3_vl_moe.py
index 3757ee77a..cfd729cf7 100644
--- a/rtp_llm/models/qwen3_vl_moe/qwen3_vl_moe.py
+++ b/rtp_llm/models/qwen3_vl_moe/qwen3_vl_moe.py
@@ -2,7 +2,8 @@ import torch
 import torch.nn as nn
 from transformers.activations import ACT2FN
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.py_config_modules import VitConfig
+from rtp_llm.config.model_config import VitParameters
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.multimodal.multimodal_mixin import BaseMultiModalWeightInfo
 from rtp_llm.models.qwen2_5_vl.qwen2_5_vl import QWen2_5_VL, Qwen2_5_VLImageEmbedding
@@ -58,9 +59,9 @@ class Qwen3_VisionMlp(nn.Module):
 
 
 class QWenV3VLWeightInfo(QWenV3MoeWeight, BaseMultiModalWeightInfo):
-    def __init__(self, config, tp_size, tp_rank):
-        QWenV3MoeWeight.__init__(self, config, tp_size, tp_rank)
-        BaseMultiModalWeightInfo.__init__(self, config)
+    def __init__(self, vit_weights, **kwargs):
+        QWenV3MoeWeight.__init__(self, **kwargs)
+        BaseMultiModalWeightInfo.__init__(self, vit_weights=vit_weights, **kwargs)
         self.bias = False
         self.use_qk_norm = True
 
@@ -70,22 +71,27 @@ class QWenV3VLWeightInfo(QWenV3MoeWeight, BaseMultiModalWeightInfo):
 
     def _get_weight_info(self):
         weights = self._get_hf_weight_info()
-        weights = self._get_vit_info(weights)
         return weights
 
 
 class QWen3_VL_MOE(QWen2_5_VL):
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = Qwen2_5_VLImageEmbedding(config)
+    def _init_multimodal(
+        self,
+        mm_model_config,
+        vit_config: VitConfig,
+    ):
+        if mm_model_config.mm_related_params is None:
+            mm_model_config.mm_related_params = VitParameters()
+        self.mm_part = Qwen2_5_VLImageEmbedding(mm_model_config.mm_related_params)
         self.mm_part.visual = Qwen3_VL_MOEVisionTransformerPretrainedModel(
-            config.mm_related_params.config
+            mm_model_config.mm_related_params.config
         )
-        # vl_config = Qwen2_5_VLVisionConfig(**config.mm_related_params.config)
+        # vl_config = Qwen2_5_VLVisionConfig(**mm_model_config.mm_related_params.config)
 
         # for i in range(len(self.mm_part.visual.blocks)):
         #     self.mm_part.visual.blocks[i].mlp = Qwen3_VisionMlp(vl_config, bias=True)
 
-        config.mm_related_params.vit_weights = QwenVL2VitWeight(
+        mm_model_config.mm_related_params.vit_weights = QwenVL2VitWeight(
             {"vit": self.mm_part.visual}
         )
 
diff --git a/rtp_llm/models/qwen_v2.py b/rtp_llm/models/qwen_v2.py
index adb0eeefd..f708c9ab4 100644
--- a/rtp_llm/models/qwen_v2.py
+++ b/rtp_llm/models/qwen_v2.py
@@ -7,7 +7,8 @@ from typing import Any, Dict, List
 import torch
 from transformers import AutoTokenizer
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight, AttnConfig
 from rtp_llm.model_loader.ffn_weight import FfnAtomicWeight, FfnConfig, FfnWeight
@@ -46,7 +47,6 @@ class QWenV2Weight(ModelDeployWeightInfo):
         self.model_prefix: str = "model."
         self.bias = True
         self.strip_model_prefix = False
-        super().__init__(*args, **kwargs)
 
     @property
     def support_lora(self):
@@ -346,39 +346,32 @@ class QWenV2Weight(ModelDeployWeightInfo):
 
 class QWenV2(QWen):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0,
-            head_num_kv=0,
-            size_per_head=0,
-            layer_num=0,
-            inter_size=0,  # 13696
-            vocab_size=152064,
-            max_seq_len=8192,
-        )
-        config.rotary_embedding_dim = 128
-        config.rotary_embedding_style = 1
-        config.activation_type = "SiGLU"
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
+        config = ModelConfig()
+        config.ckpt_path = ckpt_path
+        config.vocab_size = 152064
+        config.max_seq_len = 8192
+        config.rope_config.dim = 128
+        config.rope_config.style = 1
         config.has_pre_decoder_layernorm = False
-        config.has_post_decoder_layernorm = True
-        config.norm_type = "rmsnorm"
         config.special_tokens.bos_token_id = -1
         config.special_tokens.eos_token_id = 151643
         # <|im_start|> and <|im_end|>
         config.special_tokens.stop_words_id_list = [[151645], [151644]]
+        config.mm_related_params = VitParameters()
 
         cls._from_hf(config, ckpt_path)
         assert (
             config.head_num > 0
             and config.head_num_kv > 0
             and config.size_per_head > 0
-            and config.layer_num > 0
+            and config.num_layers > 0
             and config.inter_size > 0
-        ), f"error config config.head_num={config.head_num} config.head_num_kv={config.head_num_kv} config.size_per_head={config.size_per_head} config.layer_num={config.layer_num} config.inter_size={config.inter_size}"
+        ), f"error config config.head_num={config.head_num} config.head_num_kv={config.head_num_kv} config.size_per_head={config.size_per_head} config.num_layers={config.num_layers} config.inter_size={config.inter_size}"
         return config
 
     @classmethod
-    def _from_hf(cls, config: GptInitModelParameters, ckpt_path: str):
+    def _from_hf(cls, config: "ModelConfig", ckpt_path: str):
         config_path = os.path.join(ckpt_path, "config.json")
 
         if not os.path.exists(config_path):
@@ -390,7 +383,7 @@ class QWenV2(QWen):
         return config
 
     @staticmethod
-    def _from_config_json(config: GptInitModelParameters, config_json: Dict[str, Any]):
+    def _from_config_json(config: "ModelConfig", config_json: Dict[str, Any]):
         # config.activation_type = config_json["hidden_act"]
         config.inter_size = config_json["intermediate_size"]
         config.head_num = config_json["num_attention_heads"]
@@ -402,12 +395,10 @@ class QWenV2(QWen):
         )
         if config_json.get("hidden_size") is not None:
             config.hidden_size = config_json["hidden_size"]
-        config.layer_num = config_json["num_hidden_layers"]
-        config.rotary_embedding_base = config_json.get(
-            "rope_theta", config.rotary_embedding_base
-        )
+        config.num_layers = config_json["num_hidden_layers"]
+        config.rope_config.base = config_json.get("rope_theta", config.rope_config.base)
         config.vocab_size = config_json["vocab_size"]
-        config.rotary_embedding_dim = config.size_per_head
+        config.rope_config.dim = config.size_per_head
         config.layernorm_eps = config_json.get("rms_norm_eps", 1e-06)
         config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
         config.config_dtype = config_json.get("torch_dtype", None)
@@ -419,15 +410,15 @@ class QWenV2(QWen):
 
 class QWenV2Embedding(QWenV2):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config = QWenV2._create_config(ckpt_path)
         config.is_causal = False
         return config
 
 
 class QwenV2MTPWeight(QWenV2Weight):
-    def __init__(self, config: GptInitModelParameters, tp_size: int, tp_rank: int):
-        super().__init__(config, tp_size, tp_rank)
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
 
     def _get_weight_info(self):
         weights = [
@@ -498,9 +489,9 @@ class QwenV2MTPWeight(QWenV2Weight):
 
 class QwenV2MTP(QWenV2):
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config = super()._create_config(ckpt_path)
-        config.moe_layer_index = [i for i in range(config.layer_num)]
+        config.moe_layer_index = [i for i in range(config.num_layers)]
         config.is_mtp = True
         return config
 
diff --git a/rtp_llm/models/qwen_v2_audio/processor.py b/rtp_llm/models/qwen_v2_audio/processor.py
index 146cc65a3..84bc786f0 100644
--- a/rtp_llm/models/qwen_v2_audio/processor.py
+++ b/rtp_llm/models/qwen_v2_audio/processor.py
@@ -7,7 +7,7 @@ from transformers.models.whisper.feature_extraction_whisper import (
     WhisperFeatureExtractor,
 )
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
 from rtp_llm.models.multimodal.multimodal_common import (
     AudioEmbeddingInterface,
     timeout_decorator,
@@ -24,9 +24,8 @@ from rtp_llm.utils.util import get_config_from_path
 
 
 class Processor(AudioEmbeddingInterface):
-    def __init__(self, config: GptInitModelParameters):
-        self.config = config
-        ckpt_path = config.ckpt_path
+    def __init__(self, mm_related_params: VitParameters, ckpt_path: str):
+        self.mm_related_params = mm_related_params
         dtype = self._data_type
         self.feature_extractor = WhisperFeatureExtractor.from_pretrained(ckpt_path)
         config_json = get_config_from_path(ckpt_path)
diff --git a/rtp_llm/models/qwen_v2_audio/qwen_v2_audio.py b/rtp_llm/models/qwen_v2_audio/qwen_v2_audio.py
index 8b826611b..1025e92a3 100644
--- a/rtp_llm/models/qwen_v2_audio/qwen_v2_audio.py
+++ b/rtp_llm/models/qwen_v2_audio/qwen_v2_audio.py
@@ -1,4 +1,6 @@
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.py_config_modules import VitConfig
+from rtp_llm.config.model_config import VitParameters
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.multimodal.multimodal_mixin import (
     BaseMultiModalWeightInfo,
@@ -11,27 +13,27 @@ from rtp_llm.utils.util import get_config_from_path
 
 
 class QWenV2AudioWeightinfo(QWenV2Weight, BaseMultiModalWeightInfo):
-    def __init__(self, config: GptInitModelParameters, tp_size: int, tp_rank: int):
-        QWenV2Weight.__init__(self, config, tp_size, tp_rank)
-        BaseMultiModalWeightInfo.__init__(self, config)
-
-    def _get_weight_info(self):
-        qwen_weight = super()._get_weight_info()
-        qwen_weight = self._get_vit_info(qwen_weight)
-        return qwen_weight
-
+    def __init__(self, vit_weights, **kwargs):
+        QWenV2Weight.__init__(self, **kwargs)
+        BaseMultiModalWeightInfo.__init__(self, vit_weights=vit_weights, **kwargs)
 
 class QWenV2Audio(QWenV2, MultiModalMixin):
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = Processor(config)
-        config.mm_related_params.vit_weights = BaseVitWeights(
+    def _init_multimodal(
+        self,
+        mm_model_config,
+        vit_config: VitConfig,
+    ):
+        if mm_model_config.mm_related_params is None:
+            mm_model_config.mm_related_params = VitParameters()
+        self.mm_part = Processor(mm_model_config.mm_related_params, self.py_model_config.ckpt_path)
+        mm_model_config.mm_related_params.vit_weights = BaseVitWeights(
             {
                 "multi_modal_projector": self.mm_part.multi_modal_projector,
                 "audio_tower": self.mm_part.audio_tower,
             },
             with_prefix=True,
         )
-        config.mm_related_params.vit_weights._ckpt_prefix = ""
+        mm_model_config.mm_related_params.vit_weights._ckpt_prefix = ""
 
     @classmethod
     def _create_config(cls, ckpt_path: str):
@@ -43,7 +45,7 @@ class QWenV2Audio(QWenV2, MultiModalMixin):
         return QWenV2AudioWeightinfo
 
     @classmethod
-    def _from_hf(cls, config: GptInitModelParameters, ckpt_path: str):
+    def _from_hf(cls, config: ModelConfig, ckpt_path: str):
         config_json = get_config_from_path(ckpt_path)
         if not config_json:
             raise Exception(f"failed to get config.json from path: {ckpt_path}")
@@ -52,20 +54,20 @@ class QWenV2Audio(QWenV2, MultiModalMixin):
 
         # config.activation_type = config_json["hidden_act"]
         config.inter_size = config_json.get("intermediate_size", 11008)
-        config.head_num = config_json.get("num_attention_heads", 32)
-        config.head_num_kv = config_json.get("num_key_value_heads", config.head_num)
-        config.size_per_head = config_json.get("hidden_size", 4096) // config.head_num
-        config.layer_num = config_json.get("num_hidden_layers", 32)
-        config.rotary_embedding_base = config_json.get(
-            "rope_theta", config.rotary_embedding_base
+        config.head_num_ = config_json.get("num_attention_heads", 32)
+        config.head_num_kv_ = config_json.get("num_key_value_heads", config.head_num_)
+        config.size_per_head_ = config_json.get("hidden_size", 4096) // config.head_num_
+        config.layer_num_ = config_json.get("num_hidden_layers", 32)
+        config.rope_config.base = config_json.get(
+            "rope_theta", config.rope_config.base
         )
         config.vocab_size = config_json["vocab_size"]
-        config.rotary_embedding_dim = config.size_per_head
-        config.layernorm_eps = config_json.get("rms_norm_eps", 1e-06)
-        config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
+        config.rope_config.dim = config.size_per_head_
+        config.layernorm_eps_ = config_json.get("rms_norm_eps", 1e-06)
+        config.tie_word_embeddings_ = config_json.get("tie_word_embeddings", False)
 
-        config.mm_sep_tokens = [[sep_token]]  # image_token_index
-        config.config_dtype = config_json.get("torch_dtype", None)
+        config.mm_sep_tokens_ = [[sep_token]]  # image_token_index
+        config.config_dtype_ = config_json.get("torch_dtype", None)
 
 
 register_model("qwen_v2_audio", QWenV2Audio)
diff --git a/rtp_llm/models/qwen_v2_moe.py b/rtp_llm/models/qwen_v2_moe.py
index 62d7466e2..5bbdcc600 100644
--- a/rtp_llm/models/qwen_v2_moe.py
+++ b/rtp_llm/models/qwen_v2_moe.py
@@ -1,7 +1,7 @@
 import json
 import os
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.ffn_weight import (
     FfnAtomicWeight,
@@ -141,7 +141,7 @@ class Qwen2Moe(QWenV2):
         return config
 
     @classmethod
-    def load_moe_config(cls, ckpt_path: str, config: GptInitModelParameters):
+    def load_moe_config(cls, ckpt_path: str, config: ModelConfig):
         config_path = os.path.join(ckpt_path, "config.json")
         if not os.path.exists(config_path):
             raise Exception("qwen2 moe should have config.json")
diff --git a/rtp_llm/models/qwen_v3.py b/rtp_llm/models/qwen_v3.py
index 10fa723c7..90fe03315 100644
--- a/rtp_llm/models/qwen_v3.py
+++ b/rtp_llm/models/qwen_v3.py
@@ -1,7 +1,7 @@
 import logging
 from typing import Optional
 
-from rtp_llm.config.task_type import TaskType
+from rtp_llm.ops import TaskType
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.models.downstream_modules.custom_module import CustomModule
 from rtp_llm.models.downstream_modules.reranker.qwen3_reranker import (
diff --git a/rtp_llm/models/qwen_v3_moe.py b/rtp_llm/models/qwen_v3_moe.py
index 75b53b53f..794e8e420 100644
--- a/rtp_llm/models/qwen_v3_moe.py
+++ b/rtp_llm/models/qwen_v3_moe.py
@@ -1,6 +1,6 @@
 from typing import Any, List, Optional
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.ffn_weight import MoeAtomicWeight, MoeConfig, MoeWeight
 from rtp_llm.model_loader.model_weight_info import ModelWeightInfo
@@ -20,8 +20,8 @@ from rtp_llm.utils.model_weight import (
 
 
 class QWenV3MoeWeight(QWenV2MoeWeight):
-    def __init__(self, *args: Any, **kwargs: Any):
-        super().__init__(*args, **kwargs)
+    def __init__(self, py_model_config, engine_config, merge_lora=False, tp_size=1, tp_rank=0, vit_config=None, prefix="", **kwargs: Any):
+        super().__init__(py_model_config, engine_config, merge_lora, tp_size, tp_rank, vit_config, prefix=prefix, **kwargs)
         self.bias = False
 
     def _get_hf_ffn_layer_weight_info(self, layer_id: int):
@@ -89,12 +89,29 @@ class Qwen3Moe(Qwen2Moe):
         return config
 
     def _create_python_model(self) -> Optional[GptModelBase]:
-        self.py_model = GenericMoeModel(self.config, self.weight)
+        model_config = self.model_config
+        parallelism_config = self.engine_config.parallelism_config
+        device_resource_config = self.engine_config.device_resource_config
+        quant_config = self.model_config.quant_config
+        vocab_size = self.model_config.vocab_size
+        fmha_config = self.engine_config.fmha_config
+        py_hw_kernel_config = self.engine_config.hw_kernel_config
+        
+        self.py_model = GenericMoeModel(
+            model_config,
+            parallelism_config,
+            device_resource_config,
+            self.weight,
+            vocab_size,
+            quant_config,
+            fmha_config=fmha_config,
+            py_hw_kernel_config=py_hw_kernel_config,
+        )
         return self.py_model
 
 
 class Qwen3MoeEagle3Weight(QWenV2Weight):
-    def __init__(self, config: GptInitModelParameters, tp_size: int, tp_rank: int):
+    def __init__(self, config: ModelConfig, tp_size: int, tp_rank: int):
         super().__init__(config, tp_size, tp_rank)
         self.bias = False
         self._use_qk_norm = True
diff --git a/rtp_llm/models/qwen_vl.py b/rtp_llm/models/qwen_vl.py
index c3338fab1..6382bfb47 100644
--- a/rtp_llm/models/qwen_vl.py
+++ b/rtp_llm/models/qwen_vl.py
@@ -3,11 +3,9 @@ import os
 from typing import Any, Dict, List, Tuple, Union
 
 import torch
-from transformers import AutoTokenizer
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
-from rtp_llm.models.base_model import BaseModel, MultimodalInput
 from rtp_llm.models.multimodal.multimodal_common import ImageEmbeddingInterface
 from rtp_llm.models.multimodal.multimodal_mixin import MultiModalMixin
 from rtp_llm.models.qwen import QWen
@@ -16,9 +14,11 @@ from rtp_llm.models.qwen_vl_weight import QwenVLVitWeight, QWenVLWeightInfo
 
 
 class QwenVLImageEmbedding(ImageEmbeddingInterface):
-    def __init__(self, config: GptInitModelParameters):
-        self.vit = QWen_VL_ViT(**config.mm_related_params.config)
-        self.config = config
+    def __init__(self, mm_related_params):
+        if mm_related_params is None or mm_related_params.config is None:
+            raise ValueError("mm_related_params.config is required for QwenVLImageEmbedding")
+        self.vit = QWen_VL_ViT(**mm_related_params.config)
+        self.mm_related_params = mm_related_params
 
     @property
     def _device(self):
@@ -31,31 +31,37 @@ class QwenVLImageEmbedding(ImageEmbeddingInterface):
 
 
 class QWen_VL(QWen, MultiModalMixin):
-    def _init_multimodal(self, config: GptInitModelParameters):
-        self.mm_part = QwenVLImageEmbedding(config)
-        config.mm_related_params.vit_weights = QwenVLVitWeight(
+    def _init_multimodal(self, mm_model_config, vit_config):
+        if mm_model_config.mm_related_params is None:
+            raise ValueError("mm_model_config.mm_related_params is required for QWen_VL")
+        self.mm_part = QwenVLImageEmbedding(mm_model_config.mm_related_params)
+        mm_model_config.mm_related_params.vit_weights = QwenVLVitWeight(
             {"vit": self.mm_part.vit}
         )
 
     @classmethod
     def _create_config(cls, ckpt_path: str):
-        config = GptInitModelParameters(
-            head_num=0, size_per_head=0, layer_num=0, max_seq_len=1024, vocab_size=0
-        )
+        from rtp_llm.config.model_config import ModelConfig
+        config = ModelConfig()
+        config.head_num_ = 0
+        config.size_per_head_ = 0
+        config.num_layers = 0
+        config.max_seq_len = 1024
+        config.vocab_size = 0
         QWen_VL._common_config(config, ckpt_path)
         return config
 
     @staticmethod
     def _common_config(
-        config: GptInitModelParameters, ckpt_path: str
-    ) -> GptInitModelParameters:
+        config: ModelConfig, ckpt_path: str
+    ) -> ModelConfig:
         QWen._common_config(config, ckpt_path)
         QWen._from_hf(config, ckpt_path)
         QWen_VL._load_vit_param(config, ckpt_path)
         return config
 
     @staticmethod
-    def _load_vit_param(config: GptInitModelParameters, ckpt_path: str):
+    def _load_vit_param(config: ModelConfig, ckpt_path: str):
         config_path = os.path.join(ckpt_path, "config.json")
         if not os.path.exists(config_path):
             return
@@ -63,8 +69,15 @@ class QWen_VL(QWen, MultiModalMixin):
             content = reader.read()
             config_json = json.loads(content)
 
+        from rtp_llm.config.model_config import VitParameters
+        if config.mm_related_params is None:
+            config.mm_related_params = VitParameters()
         vit_config = config_json["visual"]
+        if config.mm_related_params.config is None:
+            config.mm_related_params.config = {}
         config.mm_related_params.config.update(vit_config)
+        if config.mm_related_params.special_token_ids is None:
+            config.mm_related_params.special_token_ids = {}
         config.mm_related_params.special_token_ids.update(
             {
                 "image_start_id": vit_config["image_start_id"],
@@ -72,6 +85,8 @@ class QWen_VL(QWen, MultiModalMixin):
                 "image_pad_id": vit_config["image_start_id"] + 2,
             }
         )
+        if config.mm_related_params.special_tokens is None:
+            config.mm_related_params.special_tokens = {}
         config.mm_related_params.special_tokens.update({"default_mm_token": "<img/>"})
         config.mm_sep_tokens = [
             [vit_config["image_start_id"], vit_config["image_start_id"] + 1]
@@ -82,15 +97,17 @@ class QWen_VL(QWen, MultiModalMixin):
         return QWenVLWeightInfo
 
     @staticmethod
-    def eval_model_size(config: GptInitModelParameters):
-        llm_size = BaseModel.eval_model_size(config)
+    def eval_model_size(config: ModelConfig):
+        llm_size = config.eval_model_size()
 
         data_width = 4
         llm_size += QWen_VL.eval_vit_param_count(config) * data_width
         return llm_size
 
     @staticmethod
-    def eval_vit_param_count(config: GptInitModelParameters):
+    def eval_vit_param_count(config: ModelConfig):
+        if config.mm_related_params is None or config.mm_related_params.config is None:
+            return 0
         vit_config = config.mm_related_params.config
         embed_dim = vit_config["output_dim"]
         width = vit_config["width"]
@@ -112,8 +129,8 @@ class QWen_VL(QWen, MultiModalMixin):
         return llm_size
 
     @staticmethod
-    def eval_model_param_count(config: GptInitModelParameters):
-        llm_param_count = BaseModel.eval_model_param_count(config)
+    def eval_model_param_count(config: ModelConfig):
+        llm_param_count = config.model_param_count()
         llm_param_count += QWen_VL.eval_vit_param_count(config)
 
         return llm_param_count
diff --git a/rtp_llm/models/qwen_vl_vit.py b/rtp_llm/models/qwen_vl_vit.py
index 612f22267..c919a1902 100644
--- a/rtp_llm/models/qwen_vl_vit.py
+++ b/rtp_llm/models/qwen_vl_vit.py
@@ -175,15 +175,15 @@ class VisualAttention(nn.Module):
 
         # Per attention head and per partition values.
         assert embed_dim % num_heads == 0
-        self.hidden_size_per_attention_head = embed_dim // num_heads
+        self.hidden_sizeper_attention_head = embed_dim // num_heads
         self.num_attention_heads_per_partition = num_heads
-        self.hidden_size_per_partition = embed_dim
+        self.hidden_sizeper_partition = embed_dim
 
         # Strided linear layer.
         assert self._qkv_same_embed_dim, "Only Support SelfAttention Currently"
         self.in_proj = nn.Linear(embed_dim, 3 * embed_dim)
         self.out_proj = nn.Linear(embed_dim, embed_dim)
-        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)
+        self.norm_factor = math.sqrt(self.hidden_sizeper_attention_head)
 
     def forward(self, query, key, value, attn_mask=None):
         # query/key/value: [sq, b, h]
@@ -196,26 +196,26 @@ class VisualAttention(nn.Module):
         # [sq, b, (np * 3 * hn)] --> [sq, b, np, 3 * hn]
         new_tensor_shape = mixed_x_layer.size()[:-1] + (
             self.num_attention_heads_per_partition,
-            3 * self.hidden_size_per_attention_head,
+            3 * self.hidden_sizeper_attention_head,
         )
         mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)
 
         # [sq, b, np, 3 * hn] --> 3 [sq, b, np, hn]
         query_layer, key_layer, value_layer = mixed_x_layer.split(
-            self.hidden_size_per_attention_head, dim=-1
+            self.hidden_sizeper_attention_head, dim=-1
         )
 
         # [sq, b, np, hn] -> [sq, b * np, hn]
         query_layer = query_layer.view(
             sq,
             b * self.num_attention_heads_per_partition,
-            self.hidden_size_per_attention_head,
+            self.hidden_sizeper_attention_head,
         ).transpose(0, 1)
         # [sk, b, np, hn] -> [sk, b * np, hn]
         key_layer = key_layer.view(
             sk,
             b * self.num_attention_heads_per_partition,
-            self.hidden_size_per_attention_head,
+            self.hidden_sizeper_attention_head,
         ).transpose(0, 1)
 
         q_scaled = query_layer / self.norm_factor
@@ -230,7 +230,7 @@ class VisualAttention(nn.Module):
         value_layer = value_layer.view(
             sk,
             b * self.num_attention_heads_per_partition,
-            self.hidden_size_per_attention_head,
+            self.hidden_sizeper_attention_head,
         ).transpose(0, 1)
 
         # matmul: [b * np, sq, hn]
@@ -241,7 +241,7 @@ class VisualAttention(nn.Module):
             b,
             self.num_attention_heads_per_partition,
             sq,
-            self.hidden_size_per_attention_head,
+            self.hidden_sizeper_attention_head,
         )
 
         # [b, np, sq, hn] --> [sq, b, np, hn]
@@ -249,7 +249,7 @@ class VisualAttention(nn.Module):
 
         # [sq, b, np, hn] --> [sq, b, hp]
         new_context_layer_shape = context_layer.size()[:-2] + (
-            self.hidden_size_per_partition,
+            self.hidden_sizeper_partition,
         )
         context_layer = context_layer.view(*new_context_layer_shape)
 
diff --git a/rtp_llm/models/qwen_vl_weight.py b/rtp_llm/models/qwen_vl_weight.py
index b90fba600..151f129e1 100644
--- a/rtp_llm/models/qwen_vl_weight.py
+++ b/rtp_llm/models/qwen_vl_weight.py
@@ -13,11 +13,6 @@ class QwenVLVitWeight(BaseVitWeights):
 
 class QWenVLWeightInfo(QWenWeight, BaseMultiModalWeightInfo):
 
-    def __init__(self, config, tp_size, tp_rank):
-        QWenWeight.__init__(self, config, tp_size, tp_rank)
-        BaseMultiModalWeightInfo.__init__(self, config)
-
-    def _get_weight_info(self):
-        qwen_vl_weight = super()._get_weight_info()
-        qwen_vl_weight = self._get_vit_info(qwen_vl_weight)
-        return qwen_vl_weight
+    def __init__(self, vit_weights, **kwargs):
+        QWenWeight.__init__(self, **kwargs)
+        BaseMultiModalWeightInfo.__init__(self, vit_weights=vit_weights, **kwargs)
diff --git a/rtp_llm/models/rotary_embedding/deepseek_rotary_embedding.py b/rtp_llm/models/rotary_embedding/deepseek_rotary_embedding.py
index 515cfb6f7..8b08e18e5 100644
--- a/rtp_llm/models/rotary_embedding/deepseek_rotary_embedding.py
+++ b/rtp_llm/models/rotary_embedding/deepseek_rotary_embedding.py
@@ -22,12 +22,12 @@ class DeepseekV3RotaryEmbedding(nn.Module):
             device=self.inv_freq.device,
             dtype=torch.get_default_dtype(),
         )
-        self.max_seq_len_cached = None
+        self.max_seq_lencached = None
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
-        self.max_seq_len_cached = seq_len
+        self.max_seq_lencached = seq_len
         t = torch.arange(
-            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
+            self.max_seq_lencached, device=device, dtype=self.inv_freq.dtype
         )
 
         freqs = torch.outer(t, self.inv_freq.to(t.device))
@@ -38,7 +38,7 @@ class DeepseekV3RotaryEmbedding(nn.Module):
 
     def forward(self, x, seq_len=None):
         # x: [bs, num_attention_heads, seq_len, head_size]
-        if self.max_seq_len_cached is None or seq_len > self.max_seq_len_cached:
+        if self.max_seq_lencached is None or seq_len > self.max_seq_lencached:
             self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
 
         return (
@@ -63,9 +63,9 @@ class DeepseekV3LinearScalingRotaryEmbedding(DeepseekV3RotaryEmbedding):
         super().__init__(dim, max_position_embeddings, base, device)
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
-        self.max_seq_len_cached = seq_len
+        self.max_seq_lencached = seq_len
         t = torch.arange(
-            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
+            self.max_seq_lencached, device=device, dtype=self.inv_freq.dtype
         )
         t = t / self.scaling_factor
 
@@ -92,7 +92,7 @@ class DeepseekV3DynamicNTKScalingRotaryEmbedding(DeepseekV3RotaryEmbedding):
         super().__init__(dim, max_position_embeddings, base, device)
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
-        self.max_seq_len_cached = seq_len
+        self.max_seq_lencached = seq_len
 
         if seq_len > self.max_position_embeddings:
             base = self.base * (
@@ -105,7 +105,7 @@ class DeepseekV3DynamicNTKScalingRotaryEmbedding(DeepseekV3RotaryEmbedding):
             self.register_buffer("inv_freq", inv_freq, persistent=False)
 
         t = torch.arange(
-            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
+            self.max_seq_lencached, device=device, dtype=self.inv_freq.dtype
         )
 
         freqs = torch.outer(t, self.inv_freq)
@@ -176,7 +176,7 @@ class DeepseekV3YarnRotaryEmbedding(DeepseekV3RotaryEmbedding):
         super().__init__(dim, max_position_embeddings, base, device)
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
-        self.max_seq_len_cached = seq_len
+        self.max_seq_lencached = seq_len
         dim = self.dim
 
         freq_extra = 1.0 / (
diff --git a/rtp_llm/models/sgpt_bloom.py b/rtp_llm/models/sgpt_bloom.py
index 9e7d16a26..0e4b98662 100644
--- a/rtp_llm/models/sgpt_bloom.py
+++ b/rtp_llm/models/sgpt_bloom.py
@@ -2,7 +2,7 @@ import torch
 
 from rtp_llm.distribute.worker_info import g_parallel_info
 from rtp_llm.model_factory_register import register_model
-from rtp_llm.models.base_model import GenerateOutput
+from rtp_llm.utils.base_model_datatypes import GenerateOutput
 from rtp_llm.models.bloom import Bloom
 
 
diff --git a/rtp_llm/models/sgpt_bloom_vector.py b/rtp_llm/models/sgpt_bloom_vector.py
index 286f7ea9d..5ad3be7a9 100644
--- a/rtp_llm/models/sgpt_bloom_vector.py
+++ b/rtp_llm/models/sgpt_bloom_vector.py
@@ -4,7 +4,7 @@ import numpy as np
 import torch
 
 from rtp_llm.model_factory_register import register_model
-from rtp_llm.models.base_model import GenerateOutput
+from rtp_llm.utils.base_model_datatypes import GenerateOutput
 from rtp_llm.models.sgpt_bloom import SGPTBloom
 
 
diff --git a/rtp_llm/models/starcoder.py b/rtp_llm/models/starcoder.py
index aeb0f87ed..40908f226 100644
--- a/rtp_llm/models/starcoder.py
+++ b/rtp_llm/models/starcoder.py
@@ -2,7 +2,8 @@ from typing import Any, Dict, List
 
 from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight
 from rtp_llm.model_loader.ffn_weight import FfnAtomicWeight, FfnConfig, FfnWeight
@@ -183,15 +184,15 @@ class StarCoder(BaseModel):
         return StarcoderWeightInfo
 
     @staticmethod
-    def from_huggingface(ckpt_path: str, config_json: Dict[str, Any]):
+    def from_huggingface(ckpt_path: str, config_json: Dict[str, Any]) -> ModelConfig:
         model_type = config_json["model_type"]
-        config = GptInitModelParameters(
-            head_num=config_json["n_head"],
-            size_per_head=config_json["n_embd"] // config_json["n_head"],
-            layer_num=config_json["n_layer"],
-            max_seq_len=config_json.get("n_positions", 8192),
-            vocab_size=config_json["vocab_size"],
-        )
+        config = ModelConfig()
+        config.ckpt_path = ckpt_path
+        config.head_num = config_json["n_head"]
+        config.size_per_head = config_json["n_embd"] // config_json["n_head"]
+        config.num_layers = config_json["n_layer"]
+        config.max_seq_len = config_json.get("n_positions", 8192)
+        config.vocab_size = config_json["vocab_size"]
         if model_type != "gpt_bigcode":
             raise BaseException(f"model type is not starcoder: {model_type}")
         config.head_num_kv = 1
@@ -201,30 +202,29 @@ class StarCoder(BaseModel):
         config.special_tokens.bos_token_id = config_json.get("bos_token_id", -1)
         # config.activation_type = config_json['activation_function']
         config.has_positional_encoding = True
-        config.has_post_decoder_layernorm = True
         config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
         config.config_dtype = config_json.get("torch_dtype", None)
+        config.mm_related_params = VitParameters()
         return config
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config_dict = get_config_from_path(ckpt_path)
         if config_dict:
             config = StarCoder.from_huggingface(ckpt_path, config_dict)
         else:
-            config = GptInitModelParameters(
-                head_num=48,
-                head_num_kv=1,
-                size_per_head=128,
-                inter_size=4 * 6144,
-                layer_num=40,
-                max_seq_len=8192,
-                vocab_size=49152,
-                has_positional_encoding=True,
-                has_post_decoder_layernorm=True,
-            )
+            config = ModelConfig()
+            config.head_num = 48
+            config.head_num_kv = 1
+            config.size_per_head = 128
+            config.inter_size = 4 * 6144
+            config.num_layers = 40
+            config.max_seq_len = 8192
+            config.vocab_size = 49152
+            config.has_positional_encoding = True
             config.special_tokens.bos_token_id = 0
             config.special_tokens.eos_token_id = 0
+            config.mm_related_params = VitParameters()
         return config
 
 
diff --git a/rtp_llm/models/starcoder2.py b/rtp_llm/models/starcoder2.py
index 422d821a3..da07a2270 100644
--- a/rtp_llm/models/starcoder2.py
+++ b/rtp_llm/models/starcoder2.py
@@ -4,7 +4,8 @@ from typing import Any, Dict, List
 import torch
 from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import VitParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory_register import register_model
 from rtp_llm.model_loader.attn_weight import AttnAtomicWeight
 from rtp_llm.model_loader.ffn_weight import FfnAtomicWeight, FfnWeight
@@ -196,19 +197,17 @@ class StarCoder2(BaseModel):
         return Starcoder2WeightInfo
 
     @staticmethod
-    def from_huggingface(config_json: Dict[str, Any]):
+    def from_huggingface(config_json: Dict[str, Any]) -> ModelConfig:
         model_type = config_json["model_type"]
-        config = GptInitModelParameters(
-            head_num=config_json["num_attention_heads"],
-            head_num_kv=config_json["num_key_value_heads"],
-            size_per_head=config_json["hidden_size"]
-            // config_json["num_attention_heads"],
-            layer_num=config_json["num_hidden_layers"],
-            max_seq_len=config_json.get("max_position_embeddings", 8192),
-            vocab_size=config_json["vocab_size"],
-            rotary_embedding_dim=128,
-            rotary_embedding_style=1,
-        )
+        config = ModelConfig()
+        config.head_num = config_json["num_attention_heads"]
+        config.head_num_kv = config_json["num_key_value_heads"]
+        config.size_per_head = config_json["hidden_size"] // config_json["num_attention_heads"]
+        config.num_layers = config_json["num_hidden_layers"]
+        config.max_seq_len = config_json.get("max_position_embeddings", 8192)
+        config.vocab_size = config_json["vocab_size"]
+        config.rope_config.dim = 128
+        config.rope_config.style = 1
         if model_type != "starcoder2":
             raise BaseException(f"model type is not starcoder: {model_type}")
         config.layernorm_eps = config_json["layer_norm_epsilon"]
@@ -216,33 +215,32 @@ class StarCoder2(BaseModel):
         config.special_tokens.eos_token_id = config_json.get("eos_token_id", 0)
         config.special_tokens.bos_token_id = config_json.get("bos_token_id", -1)
         config.activation_type = config_json["activation_function"]
-        config.has_post_decoder_layernorm = True
-        config.rotary_embedding_base = config_json.get("rope_theta", 1000000)
-        config.rotary_embedding_dim = config.size_per_head
+        config.rope_config.base = config_json.get("rope_theta", 1000000)
+        config.rope_config.dim = config.size_per_head
         config.tie_word_embeddings = config_json.get("tie_word_embeddings", False)
         config.config_dtype = config_json.get("torch_dtype", None)
+        config.mm_related_params = VitParameters()
         return config
 
     @classmethod
-    def _create_config(cls, ckpt_path: str):
+    def _create_config(cls, ckpt_path: str) -> ModelConfig:
         config_dict = get_config_from_path(ckpt_path)
         if config_dict:
             config = StarCoder2.from_huggingface(config_dict)
         else:
-            config = GptInitModelParameters(
-                head_num=36,
-                head_num_kv=4,
-                size_per_head=128,
-                inter_size=4 * 4608,
-                layer_num=32,
-                max_seq_len=16384,
-                vocab_size=49152,
-                bos_token_id=0,
-                eos_token_id=0,
-                rotary_embedding_dim=128,
-                rotary_embedding_style=1,
-                has_post_decoder_layernorm=True,
-            )
+            config = ModelConfig()
+            config.head_num = 36
+            config.head_num_kv = 4
+            config.size_per_head = 128
+            config.inter_size = 4 * 4608
+            config.num_layers = 32
+            config.max_seq_len = 16384
+            config.vocab_size = 49152
+            config.special_tokens.bos_token_id = 0
+            config.special_tokens.eos_token_id = 0
+            config.rope_config.dim = 128
+            config.rope_config.style = 1
+            config.mm_related_params = VitParameters()
         return config
 
 
diff --git a/rtp_llm/models_py/bindings/cuda/BUILD b/rtp_llm/models_py/bindings/cuda/BUILD
index 4479b8304..499fcf005 100644
--- a/rtp_llm/models_py/bindings/cuda/BUILD
+++ b/rtp_llm/models_py/bindings/cuda/BUILD
@@ -19,6 +19,7 @@ cc_library(
         "//rtp_llm/cpp/kernels:kernels_ep_util",
         "//rtp_llm/models_py/bindings:op_defs",
         "//rtp_llm/models_py/bindings/common:common",
+        "//rtp_llm/cpp/config:model_config",
     ],
 )
 
diff --git a/rtp_llm/models_py/bindings/cuda/FMHACudaBase.h b/rtp_llm/models_py/bindings/cuda/FMHACudaBase.h
deleted file mode 100644
index d927fb59b..000000000
--- a/rtp_llm/models_py/bindings/cuda/FMHACudaBase.h
+++ /dev/null
@@ -1,21 +0,0 @@
-#pragma once
-
-#include "rtp_llm/cpp/config/GptInitParameter.h"
-#include "rtp_llm/cpp/devices/cuda_impl/CudaDevice.h"
-#include "rtp_llm/cpp/devices/DeviceFactory.h"
-
-namespace rtp_llm {
-
-class FMHACudaBase {
-public:
-    FMHACudaBase(const GptInitParameter& gpt_init_parameter):
-        attn_configs_(gpt_init_parameter.getAttentionConfigs()),
-        fmha_config_(gpt_init_parameter.fmha_config),
-        device_(dynamic_cast<CudaDevice*>(DeviceFactory::getDefaultDevice())) {}
-
-protected:
-    AttentionConfigs attn_configs_;
-    FMHAConfig       fmha_config_;
-    CudaDevice*      device_;
-};
-}  // namespace rtp_llm
diff --git a/rtp_llm/models_py/bindings/cuda/FlashInferOp.cc b/rtp_llm/models_py/bindings/cuda/FlashInferOp.cc
index ea7404071..57139b9e5 100644
--- a/rtp_llm/models_py/bindings/cuda/FlashInferOp.cc
+++ b/rtp_llm/models_py/bindings/cuda/FlashInferOp.cc
@@ -11,11 +11,12 @@ using namespace torch_ext;
 
 namespace rtp_llm {
 
-FlashInferPrefillOp::FlashInferPrefillOp(const GptInitParameter& gpt_init_parameter):
-    FMHACudaBase(gpt_init_parameter) {}
+FlashInferPrefillOp::FlashInferPrefillOp(const AttentionConfigs& attn_configs):
+    attn_configs_(attn_configs),
+    device_(dynamic_cast<CudaDevice*>(DeviceFactory::getDefaultDevice())) {}
 
 bool FlashInferPrefillOp::support(torch_ext::PyAttentionInputs attn_inputs) {
-    if (fmha_config_.disable_flash_infer || attn_configs_.kv_cache_dtype != KvCacheDataType::BASE) {
+    if (attn_configs_.kv_cache_dtype != KvCacheDataType::BASE) {
         return false;
     }
     auto     prefix_lengths_host   = torchTensor2Buffer(attn_inputs.prefix_lengths);
@@ -99,10 +100,12 @@ torch::Tensor FlashInferPrefillOp::forward(const torch::Tensor&              q,
     return output;
 }
 
-FlashInferDecodeOp::FlashInferDecodeOp(const GptInitParameter& gpt_init_parameter): FMHACudaBase(gpt_init_parameter) {}
+FlashInferDecodeOp::FlashInferDecodeOp(const AttentionConfigs& attn_configs):
+    attn_configs_(attn_configs),
+    device_(dynamic_cast<CudaDevice*>(DeviceFactory::getDefaultDevice())) {}
 
 bool FlashInferDecodeOp::support(torch_ext::PyAttentionInputs attn_inputs) {
-    if (fmha_config_.disable_flash_infer || attn_configs_.kv_cache_dtype != KvCacheDataType::BASE) {
+    if (attn_configs_.kv_cache_dtype != KvCacheDataType::BASE) {
         return false;
     }
     // FIXME: FlashInferDecodeOp causes crash in this case, temporarily bypassing it here
@@ -178,12 +181,14 @@ void registerFlashInferOp(const py::module& m) {
         m, "FlashInferAttnParams")
         .def(pybind11::init<>());
     pybind11::class_<FlashInferPrefillOp>(m, "FlashInferPrefillOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const AttentionConfigs&>(),
+             py::arg("attn_configs"))
         .def("support", &FlashInferPrefillOp::support, py::arg("attn_inputs"))
         .def("prepare", &FlashInferPrefillOp::prepare, py::arg("attn_inputs"))
         .def("forward", &FlashInferPrefillOp::forward, py::arg("q"), py::arg("kv_cache"), py::arg("params"));
     pybind11::class_<FlashInferDecodeOp>(m, "FlashInferDecodeOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const AttentionConfigs&>(),
+             py::arg("attn_configs"))
         .def("support", &FlashInferDecodeOp::support, py::arg("attn_inputs"))
         .def("prepare", &FlashInferDecodeOp::prepare, py::arg("attn_inputs"))
         .def("forward", &FlashInferDecodeOp::forward, py::arg("q"), py::arg("kv_cache"), py::arg("params"));
diff --git a/rtp_llm/models_py/bindings/cuda/FlashInferOp.h b/rtp_llm/models_py/bindings/cuda/FlashInferOp.h
index 5746b6478..2b9aa447e 100644
--- a/rtp_llm/models_py/bindings/cuda/FlashInferOp.h
+++ b/rtp_llm/models_py/bindings/cuda/FlashInferOp.h
@@ -2,15 +2,17 @@
 
 #include <memory>
 #include "rtp_llm/cpp/devices/cuda_impl/CudaFlashInfer.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "rtp_llm/models_py/bindings/OpDefs.h"
-#include "rtp_llm/models_py/bindings/cuda/FMHACudaBase.h"
+#include "rtp_llm/cpp/devices/cuda_impl/CudaDevice.h"
+#include "rtp_llm/cpp/devices/DeviceFactory.h"
+#include "rtp_llm/cpp/model_utils/AttentionConfig.h"
 
 namespace rtp_llm {
 
-class FlashInferPrefillOp: public FMHACudaBase {
+class FlashInferPrefillOp {
 public:
-    FlashInferPrefillOp(const GptInitParameter& gpt_init_parameter);
+    FlashInferPrefillOp(const AttentionConfigs& attn_configs);
 
     bool support(torch_ext::PyAttentionInputs attn_inputs);
 
@@ -18,16 +20,24 @@ public:
 
     torch::Tensor
     forward(const torch::Tensor& q, std::optional<torch_ext::KVCache> kv_cache, const FlashInferAttnParamsPtr& params);
+
+protected:
+    AttentionConfigs attn_configs_;
+    CudaDevice*      device_;
 };
 
-class FlashInferDecodeOp: public FMHACudaBase {
+class FlashInferDecodeOp {
 public:
-    FlashInferDecodeOp(const GptInitParameter& gpt_init_parameter);
+    FlashInferDecodeOp(const AttentionConfigs& attn_configs);
     bool          support(torch_ext::PyAttentionInputs attn_inputs);
     ParamsBasePtr prepare(torch_ext::PyAttentionInputs attn_inputs);
 
     torch::Tensor
     forward(const torch::Tensor& q, std::optional<torch_ext::KVCache> kv_cache, const FlashInferAttnParamsPtr& params);
+
+protected:
+    AttentionConfigs attn_configs_;
+    CudaDevice*      device_;
 };
 
 void registerFlashInferOp(const py::module& m);
diff --git a/rtp_llm/models_py/bindings/cuda/FusedMoEOp.cc b/rtp_llm/models_py/bindings/cuda/FusedMoEOp.cc
index c5558c177..aff641c0e 100644
--- a/rtp_llm/models_py/bindings/cuda/FusedMoEOp.cc
+++ b/rtp_llm/models_py/bindings/cuda/FusedMoEOp.cc
@@ -20,8 +20,16 @@ nvinfer1::DataType nvinfer1DtypeConvert(at::ScalarType dtype) {
     return nvinfer1::DataType::kFLOAT;
 }
 
-FusedMoEOp::FusedMoEOp(const GptInitParameter& gpt_init_parameter):
-    configs_(gpt_init_parameter), moe_plugin_(std::make_unique<trt_plugins::MixtureOfExpertsPlugin>()) {}
+FusedMoEOp::FusedMoEOp(const ModelConfig& model_config, const ParallelismConfig& parallelism_config):
+    expert_num_(model_config.expert_num),
+    moe_k_(model_config.moe_k),
+    moe_inter_padding_size_(model_config.moe_inter_padding_size),
+    moe_normalize_expert_scale_(model_config.moe_normalize_expert_scale),
+    has_moe_norm_(model_config.has_moe_norm),
+    activation_type_(model_config.activation_type),
+    ep_size_(parallelism_config.ep_size),
+    ep_rank_(parallelism_config.ep_rank),
+    moe_plugin_(std::make_unique<trt_plugins::MixtureOfExpertsPlugin>()) {}
 void FusedMoEOp::forward(torch::Tensor hidden_states,
                          torch::Tensor up_proj,
                          torch::Tensor down_proj,
@@ -32,13 +40,13 @@ void FusedMoEOp::forward(torch::Tensor hidden_states,
     const auto weight_type            = down_proj.scalar_type();
     const auto token_num              = hidden_states.sizes()[0];
     const auto hidden_dim             = hidden_states.sizes()[1];
-    const auto num_expert             = configs_.expert_num_;
-    const auto top_k                  = configs_.moe_k_;
-    const auto moe_inter_size         = configs_.moe_inter_padding_size_;
-    const auto normalize_expert_scale = configs_.moe_normalize_expert_scale_;
-    auto       normalization_mode     = configs_.has_moe_norm_ ?
-                                            tensorrt_llm::kernels::MOEExpertScaleNormalizationMode::RENORMALIZE :
-                                            tensorrt_llm::kernels::MOEExpertScaleNormalizationMode::NONE;
+    const auto num_expert             = expert_num_;
+    const auto top_k                  = moe_k_;
+    const auto moe_inter_size         = moe_inter_padding_size_;
+    const auto normalize_expert_scale = moe_normalize_expert_scale_;
+    auto       normalization_mode     = has_moe_norm_ ?
+                                          tensorrt_llm::kernels::MOEExpertScaleNormalizationMode::RENORMALIZE :
+                                          tensorrt_llm::kernels::MOEExpertScaleNormalizationMode::NONE;
     auto       group_size             = 0;
     // TODO group_size
     if (token_num == 0) {
@@ -49,14 +57,14 @@ void FusedMoEOp::forward(torch::Tensor hidden_states,
                       normalize_expert_scale,
                       hidden_dim,
                       moe_inter_size,
-                      configs_.activation_type_,
+                      activation_type_,
                       nvinfer1DtypeConvert(type),
                       nvinfer1DtypeConvert(weight_type),
                       group_size > 0,
                       group_size,
                       normalization_mode,
-                      configs_.ep_size_,
-                      configs_.ep_rank_);
+                      ep_size_,
+                      ep_rank_);
     const auto new_ws_size = moe_plugin_->getWorkspaceSize(token_num);
     const auto new_worksapce =
         torch::zeros({static_cast<int64_t>(new_ws_size)}, hidden_states.options().dtype(torch::kUInt8));
@@ -114,15 +122,16 @@ void FusedMoEOp::forward(torch::Tensor hidden_states,
 
 void registerFusedMoEOp(const py::module& m) {
     pybind11::class_<FusedMoEOp>(m, "FusedMoEOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const ModelConfig&, const ParallelismConfig&>(),
+             pybind11::arg("model_config"), pybind11::arg("parallelism_config"))
         .def("forward",
              &FusedMoEOp::forward,
-             py::arg("hidden_states"),
-             py::arg("up_proj"),
-             py::arg("down_proj"),
-             py::arg("expert_scales"),
-             py::arg("expert_ids"),
-             py::arg("outputs"));
+             pybind11::arg("hidden_states"),
+             pybind11::arg("up_proj"),
+             pybind11::arg("down_proj"),
+             pybind11::arg("expert_scales"),
+             pybind11::arg("expert_ids"),
+             pybind11::arg("outputs"));
 }
 
 }  // namespace rtp_llm
\ No newline at end of file
diff --git a/rtp_llm/models_py/bindings/cuda/FusedMoEOp.h b/rtp_llm/models_py/bindings/cuda/FusedMoEOp.h
index cb38933f3..2c6e39b5a 100644
--- a/rtp_llm/models_py/bindings/cuda/FusedMoEOp.h
+++ b/rtp_llm/models_py/bindings/cuda/FusedMoEOp.h
@@ -1,10 +1,13 @@
 #pragma once
 
 #include <torch/torch.h>
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include <pybind11/pybind11.h>
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/config/ModelConfig.h"
 #include "trt_plugins/mixtureOfExperts/mixtureOfExpertsPlugin.h"
 
 namespace trt_plugins = tensorrt_llm::plugins;
+namespace py = pybind11;
 
 namespace rtp_llm {
 
@@ -12,7 +15,7 @@ nvinfer1::DataType nvinfer1DtypeConvert(at::ScalarType dtype);
 
 class FusedMoEOp {
 public:
-    FusedMoEOp(const GptInitParameter& gpt_init_parameter);
+    FusedMoEOp(const ModelConfig& model_config, const ParallelismConfig& parallelism_config);
     void forward(torch::Tensor hidden_states,
                  torch::Tensor up_proj,
                  torch::Tensor down_proj,
@@ -21,9 +24,16 @@ public:
                  torch::Tensor outputs);
 
 private:
-    GptInitParameter                                     configs_;
+    int64_t                                               expert_num_;
+    int64_t                                               moe_k_;
+    int64_t                                               moe_inter_padding_size_;
+    bool                                                  moe_normalize_expert_scale_;
+    bool                                                  has_moe_norm_;
+    ActivationType                                        activation_type_;
+    int64_t                                               ep_size_;
+    int64_t                                               ep_rank_;
     std::unique_ptr<trt_plugins::MixtureOfExpertsPlugin> moe_plugin_;
 };
 
-void registerFusedMoEOp(const pybind11::module& m);
+void registerFusedMoEOp(const py::module& m);
 }  // namespace rtp_llm
\ No newline at end of file
diff --git a/rtp_llm/models_py/bindings/cuda/FusedRopeKVCacheOp.cc b/rtp_llm/models_py/bindings/cuda/FusedRopeKVCacheOp.cc
index c8901a7f1..2ff554e6d 100644
--- a/rtp_llm/models_py/bindings/cuda/FusedRopeKVCacheOp.cc
+++ b/rtp_llm/models_py/bindings/cuda/FusedRopeKVCacheOp.cc
@@ -9,8 +9,9 @@
 
 namespace rtp_llm {
 
-FusedRopeKVCachePrefillOp::FusedRopeKVCachePrefillOp(const GptInitParameter& gpt_init_parameter):
-    FMHACudaBase(gpt_init_parameter) {}
+FusedRopeKVCachePrefillOp::FusedRopeKVCachePrefillOp(const AttentionConfigs& attn_configs):
+    attn_configs_(attn_configs),
+    device_(dynamic_cast<CudaDevice*>(DeviceFactory::getDefaultDevice())) {}
 
 TRTAttnPtr FusedRopeKVCachePrefillOp::prepare(torch_ext::PyAttentionInputs attn_inputs) {
     int       batch_size = attn_inputs.input_lengths.size(0);
@@ -137,8 +138,9 @@ torch::Tensor FusedRopeKVCachePrefillOp::forward(const torch::Tensor&
     }
 }
 
-FusedRopeKVCacheDecodeOp::FusedRopeKVCacheDecodeOp(const GptInitParameter& gpt_init_parameter):
-    FMHACudaBase(gpt_init_parameter) {}
+FusedRopeKVCacheDecodeOp::FusedRopeKVCacheDecodeOp(const AttentionConfigs& attn_configs):
+    attn_configs_(attn_configs),
+    device_(dynamic_cast<CudaDevice*>(DeviceFactory::getDefaultDevice())) {}
 
 TRTAttnPtr FusedRopeKVCacheDecodeOp::prepare(torch_ext::PyAttentionInputs attn_inputs) {
     int       batch_size = attn_inputs.sequence_lengths.size(0);
@@ -224,7 +226,8 @@ void registerFusedRopeKVCacheOp(const py::module& m) {
     pybind11::class_<KVBlockArray>(m, "KVBlockArray").def(pybind11::init<>());
     pybind11::class_<TRTAttn, std::shared_ptr<TRTAttn>, rtp_llm::ParamsBase>(m, "TRTAttn").def(pybind11::init<>());
     pybind11::class_<FusedRopeKVCachePrefillOp>(m, "FusedRopeKVCachePrefillOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const AttentionConfigs&>(),
+             py::arg("attn_configs"))
         .def("prepare", &FusedRopeKVCachePrefillOp::prepare, py::arg("attn_inputs"))
         .def("forward",
              &FusedRopeKVCachePrefillOp::forward,
@@ -234,7 +237,8 @@ void registerFusedRopeKVCacheOp(const py::module& m) {
              py::arg("params"));
 
     pybind11::class_<FusedRopeKVCacheDecodeOp>(m, "FusedRopeKVCacheDecodeOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const AttentionConfigs&>(),
+             py::arg("attn_configs"))
         .def("prepare", &FusedRopeKVCacheDecodeOp::prepare, py::arg("attn_inputs"))
         .def("forward",
              &FusedRopeKVCacheDecodeOp::forward,
diff --git a/rtp_llm/models_py/bindings/cuda/FusedRopeKVCacheOp.h b/rtp_llm/models_py/bindings/cuda/FusedRopeKVCacheOp.h
index be64ecf57..95423c894 100644
--- a/rtp_llm/models_py/bindings/cuda/FusedRopeKVCacheOp.h
+++ b/rtp_llm/models_py/bindings/cuda/FusedRopeKVCacheOp.h
@@ -1,31 +1,41 @@
 #pragma once
 
-#include "rtp_llm/models_py/bindings/cuda/FMHACudaBase.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/model_utils/AttentionConfig.h"
+#include "rtp_llm/cpp/devices/cuda_impl/CudaDevice.h"
+#include "rtp_llm/cpp/devices/DeviceFactory.h"
 #include "rtp_llm/cpp/kernels/kv_cache/kv_cache_utils.h"
 #include "rtp_llm/models_py/bindings/OpDefs.h"
 #include <optional>
 
 namespace rtp_llm {
 
-class FusedRopeKVCachePrefillOp: public FMHACudaBase {
+class FusedRopeKVCachePrefillOp {
 public:
-    FusedRopeKVCachePrefillOp(const GptInitParameter& gpt_init_parameter);
+    FusedRopeKVCachePrefillOp(const AttentionConfigs& attn_configs);
     TRTAttnPtr    prepare(torch_ext::PyAttentionInputs attn_inputs);
     torch::Tensor forward(const torch::Tensor&              qkv,
                           FMHAType                          fmha_type,
                           std::optional<torch_ext::KVCache> kv_cache,
                           const TRTAttnPtr&                 params);
+
+protected:
+    AttentionConfigs attn_configs_;
+    CudaDevice*      device_;
 };
 
-class FusedRopeKVCacheDecodeOp: public FMHACudaBase {
+class FusedRopeKVCacheDecodeOp {
 public:
-    FusedRopeKVCacheDecodeOp(const GptInitParameter& gpt_init_parameter);
+    FusedRopeKVCacheDecodeOp(const AttentionConfigs& attn_configs);
     TRTAttnPtr    prepare(torch_ext::PyAttentionInputs attn_inputs);
     torch::Tensor forward(const torch::Tensor&              qkv,
                           FMHAType                          fmha_type,
                           std::optional<torch_ext::KVCache> kv_cache,
                           const TRTAttnPtr&                 params);
+
+protected:
+    AttentionConfigs attn_configs_;
+    CudaDevice*      device_;
 };
 
 void registerFusedRopeKVCacheOp(const py::module& m);
diff --git a/rtp_llm/models_py/bindings/cuda/SelectTopkOp.cc b/rtp_llm/models_py/bindings/cuda/SelectTopkOp.cc
index 9c7de1cbe..c6f5ff2e6 100644
--- a/rtp_llm/models_py/bindings/cuda/SelectTopkOp.cc
+++ b/rtp_llm/models_py/bindings/cuda/SelectTopkOp.cc
@@ -3,14 +3,17 @@
 
 namespace rtp_llm {
 
-SelectTopkOp::SelectTopkOp(const GptInitParameter& gpt_init_parameter):
-    configs_(gpt_init_parameter), moe_plugin_(std::make_unique<trt_plugins::MixtureOfExpertsPlugin>()) {}
+SelectTopkOp::SelectTopkOp(const ModelConfig& model_config):
+    expert_num_(model_config.expert_num),
+    moe_k_(model_config.moe_k),
+    has_moe_norm_(model_config.has_moe_norm),
+    moe_plugin_(std::make_unique<trt_plugins::MixtureOfExpertsPlugin>()) {}
 
 void SelectTopkOp::forward(torch::Tensor router_logits, torch::Tensor expert_ids, torch::Tensor expert_scales) {
     const auto   token_num          = router_logits.sizes()[0];
-    const auto   num_expert         = configs_.expert_num_;
-    const auto   top_k              = configs_.moe_k_;
-    auto         normalization_mode = configs_.has_moe_norm_ ?
+    const auto   num_expert         = expert_num_;
+    const auto   top_k              = moe_k_;
+    auto         normalization_mode = has_moe_norm_ ?
                                           tensorrt_llm::kernels::MOEExpertScaleNormalizationMode::RENORMALIZE :
                                           tensorrt_llm::kernels::MOEExpertScaleNormalizationMode::NONE;
     auto         topk_t             = expert_ids.dtype();
@@ -60,7 +63,7 @@ void SelectTopkOp::forward(torch::Tensor router_logits, torch::Tensor expert_ids
 
 void registerSelectTopkOp(const py::module& m) {
     pybind11::class_<SelectTopkOp>(m, "SelectTopkOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const ModelConfig&>(), py::arg("model_config"))
         .def("forward",
              &SelectTopkOp::forward,
              py::arg("router_logits"),
diff --git a/rtp_llm/models_py/bindings/cuda/SelectTopkOp.h b/rtp_llm/models_py/bindings/cuda/SelectTopkOp.h
index 0cbb0dd86..3f949e2ca 100644
--- a/rtp_llm/models_py/bindings/cuda/SelectTopkOp.h
+++ b/rtp_llm/models_py/bindings/cuda/SelectTopkOp.h
@@ -1,7 +1,7 @@
 #pragma once
 
 #include <torch/torch.h>
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
 #include "trt_plugins/mixtureOfExperts/mixtureOfExpertsPlugin.h"
 #include "rtp_llm/cpp/devices/cuda_impl/CudaDevice.h"
 #include "rtp_llm/cpp/devices/DeviceFactory.h"
@@ -12,11 +12,13 @@ namespace rtp_llm {
 
 class SelectTopkOp {
 public:
-    SelectTopkOp(const GptInitParameter& gpt_init_parameter);
+    SelectTopkOp(const ModelConfig& model_config);
     void forward(torch::Tensor router_logits, torch::Tensor expert_ids, torch::Tensor expert_scales);
 
 private:
-    GptInitParameter                                     configs_;
+    int64_t                                               expert_num_;
+    int64_t                                               moe_k_;
+    bool                                                  has_moe_norm_;
     std::unique_ptr<trt_plugins::MixtureOfExpertsPlugin> moe_plugin_;
 };
 
diff --git a/rtp_llm/models_py/bindings/cuda/TRTAttnOp.cc b/rtp_llm/models_py/bindings/cuda/TRTAttnOp.cc
index 2dea8c6ab..e79faef6d 100644
--- a/rtp_llm/models_py/bindings/cuda/TRTAttnOp.cc
+++ b/rtp_llm/models_py/bindings/cuda/TRTAttnOp.cc
@@ -9,10 +9,13 @@ using namespace torch_ext;
 
 namespace rtp_llm {
 
-TRTPrefillOp::TRTPrefillOp(const GptInitParameter& gpt_init_parameter): FMHACudaBase(gpt_init_parameter) {}
+TRTPrefillOp::TRTPrefillOp(const AttentionConfigs& attn_configs):
+    attn_configs_(attn_configs),
+    device_(dynamic_cast<CudaDevice*>(DeviceFactory::getDefaultDevice())) {}
 
 bool TRTPrefillOp::support(torch_ext::PyAttentionInputs attn_inputs) {
-    return fmha_config_.enable_paged_trt_fmha && attn_configs_.kv_cache_dtype != KvCacheDataType::INT8;
+    // FMHAConfig check will be done in Python layer
+    return attn_configs_.kv_cache_dtype != KvCacheDataType::INT8;
 }
 
 ParamsBasePtr TRTPrefillOp::prepare(torch_ext::PyAttentionInputs attn_inputs) {
@@ -176,7 +179,8 @@ torch::Tensor TRTPrefillOp::forward(const torch::Tensor&              input,
 
 void registerTRTAttnOp(const py::module& m) {
     pybind11::class_<TRTPrefillOp>(m, "TRTAttnOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const AttentionConfigs&>(),
+             py::arg("attn_configs"))
         .def("support", &TRTPrefillOp::support, py::arg("attn_inputs"))
         .def("prepare", &TRTPrefillOp::prepare, py::arg("attn_inputs"))
         .def("forward", &TRTPrefillOp::forward, py::arg("input"), py::arg("kv_cache"), py::arg("params"));
diff --git a/rtp_llm/models_py/bindings/cuda/TRTAttnOp.h b/rtp_llm/models_py/bindings/cuda/TRTAttnOp.h
index d41b9c48c..f094ff17c 100644
--- a/rtp_llm/models_py/bindings/cuda/TRTAttnOp.h
+++ b/rtp_llm/models_py/bindings/cuda/TRTAttnOp.h
@@ -1,16 +1,17 @@
 #pragma once
 
 #include <memory>
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/model_utils/AttentionConfig.h"
 #include "rtp_llm/cpp/devices/cuda_impl/CudaDevice.h"
+#include "rtp_llm/cpp/devices/DeviceFactory.h"
 #include "rtp_llm/models_py/bindings/OpDefs.h"
-#include "rtp_llm/models_py/bindings/cuda/FMHACudaBase.h"
 
 namespace rtp_llm {
 
-class TRTPrefillOp: public FMHACudaBase {
+class TRTPrefillOp {
 public:
-    TRTPrefillOp(const GptInitParameter& gpt_init_parameter);
+    TRTPrefillOp(const AttentionConfigs& attn_configs);
     bool support(torch_ext::PyAttentionInputs attn_inputs);
 
     ParamsBasePtr prepare(torch_ext::PyAttentionInputs attn_inputs);
@@ -18,6 +19,10 @@ public:
     torch::Tensor
     forward(const torch::Tensor& input, std::optional<torch_ext::KVCache> kv_cache, const TRTAttnPtr& params);
 
+protected:
+    AttentionConfigs attn_configs_;
+    CudaDevice*      device_;
+
 private:
     std::shared_ptr<cufmha> cufmha_runner_;
     torch::Tensor           static_scale_;
diff --git a/rtp_llm/models_py/bindings/cuda/XQAAttnOp.cc b/rtp_llm/models_py/bindings/cuda/XQAAttnOp.cc
index 346da5fda..cbec8f3e1 100644
--- a/rtp_llm/models_py/bindings/cuda/XQAAttnOp.cc
+++ b/rtp_llm/models_py/bindings/cuda/XQAAttnOp.cc
@@ -9,10 +9,13 @@
 
 namespace rtp_llm {
 
-XQAAttnOp::XQAAttnOp(const GptInitParameter& gpt_init_parameter): FMHACudaBase(gpt_init_parameter) {}
+XQAAttnOp::XQAAttnOp(const AttentionConfigs& attn_configs):
+    attn_configs_(attn_configs),
+    device_(dynamic_cast<CudaDevice*>(DeviceFactory::getDefaultDevice())) {}
 
 bool XQAAttnOp::support(torch_ext::PyAttentionInputs attn_inputs) {
-    return fmha_config_.enable_xqa && attn_configs_.kv_cache_dtype != KvCacheDataType::INT8
+    // FMHAConfig check will be done in Python layer
+    if (attn_configs_.kv_cache_dtype != KvCacheDataType::INT8
            && supportXqa(DataType::TYPE_BF16,
                          DataType::TYPE_BF16,
                          DataType::TYPE_FP8_E4M3,
@@ -88,7 +91,8 @@ void registerXQAAttnOp(const py::module& m) {
     pybind11::class_<XQAParams, std::shared_ptr<XQAParams>, rtp_llm::ParamsBase>(m, "XQAParams")
         .def(pybind11::init<>());
     pybind11::class_<XQAAttnOp>(m, "XQAAttnOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const AttentionConfigs&>(),
+             py::arg("attn_configs"))
         .def("support", &XQAAttnOp::support, py::arg("attn_inputs").noconvert())
         .def("prepare", &XQAAttnOp::prepare, py::arg("attn_inputs"))
         .def("forward", &XQAAttnOp::forward, py::arg("input"), py::arg("kv_cache"), py::arg("params"));
diff --git a/rtp_llm/models_py/bindings/cuda/XQAAttnOp.h b/rtp_llm/models_py/bindings/cuda/XQAAttnOp.h
index 83dec4ef8..904e2a68f 100644
--- a/rtp_llm/models_py/bindings/cuda/XQAAttnOp.h
+++ b/rtp_llm/models_py/bindings/cuda/XQAAttnOp.h
@@ -2,22 +2,28 @@
 
 #ifdef USING_CUDA12
 
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/model_utils/AttentionConfig.h"
 #include "rtp_llm/cpp/devices/cuda_impl/CudaXqa.h"
+#include "rtp_llm/cpp/devices/cuda_impl/CudaDevice.h"
+#include "rtp_llm/cpp/devices/DeviceFactory.h"
 #include "rtp_llm/models_py/bindings/OpDefs.h"
-#include "rtp_llm/models_py/bindings/cuda/FMHACudaBase.h"
 
 namespace rtp_llm {
 
-class XQAAttnOp: public FMHACudaBase {
+class XQAAttnOp {
 public:
-    XQAAttnOp(const GptInitParameter& gpt_init_parameter);
+    XQAAttnOp(const AttentionConfigs& attn_configs);
     bool support(torch_ext::PyAttentionInputs attn_inputs);
 
     ParamsBasePtr prepare(torch_ext::PyAttentionInputs attn_inputs);
 
     torch::Tensor
     forward(const torch::Tensor& input, std::optional<torch_ext::KVCache> kv_cache, const XQAParamsPtr& params);
+
+protected:
+    AttentionConfigs attn_configs_;
+    CudaDevice*      device_;
 };
 
 void registerXQAAttnOp(const py::module& m);
diff --git a/rtp_llm/models_py/bindings/rocm/FMHARocmBase.h b/rtp_llm/models_py/bindings/rocm/FMHARocmBase.h
deleted file mode 100644
index 980d336bc..000000000
--- a/rtp_llm/models_py/bindings/rocm/FMHARocmBase.h
+++ /dev/null
@@ -1,23 +0,0 @@
-#pragma once
-
-#include "rtp_llm/cpp/config/GptInitParameter.h"
-#include "rtp_llm/cpp/devices/rocm_impl/ROCmDevice.h"
-#include "rtp_llm/cpp/devices/DeviceFactory.h"
-
-namespace rtp_llm {
-
-class FMHARocmBase {
-public:
-    FMHARocmBase(const GptInitParameter& gpt_init_parameter):
-        attn_configs_(gpt_init_parameter.getAttentionConfigs()),
-        layer_num_(gpt_init_parameter.num_layers_),
-        hw_kernel_config_(gpt_init_parameter.hw_kernel_config),
-        device_(dynamic_cast<ROCmDevice*>(DeviceFactory::getDefaultDevice())) {}
-
-protected:
-    AttentionConfigs attn_configs_;
-    int              layer_num_;
-    HWKernelConfig   hw_kernel_config_;
-    ROCmDevice*      device_;
-};
-}  // namespace rtp_llm
\ No newline at end of file
diff --git a/rtp_llm/models_py/bindings/rocm/FusedRopeKVCacheOp.cc b/rtp_llm/models_py/bindings/rocm/FusedRopeKVCacheOp.cc
index e9dfcf77a..d771e1678 100644
--- a/rtp_llm/models_py/bindings/rocm/FusedRopeKVCacheOp.cc
+++ b/rtp_llm/models_py/bindings/rocm/FusedRopeKVCacheOp.cc
@@ -8,8 +8,11 @@
 
 namespace rtp_llm {
 
-FusedRopeKVCachePrefillOp::FusedRopeKVCachePrefillOp(const GptInitParameter& gpt_init_parameter):
-    FMHARocmBase(gpt_init_parameter) {}
+FusedRopeKVCachePrefillOp::FusedRopeKVCachePrefillOp(const AttentionConfigs& attn_configs, int layer_num, const HWKernelConfig& hw_kernel_config):
+    attn_configs_(attn_configs),
+    layer_num_(layer_num),
+    hw_kernel_config_(hw_kernel_config),
+    device_(dynamic_cast<ROCmDevice*>(DeviceFactory::getDefaultDevice())) {}
 
 CKAttnPtr FusedRopeKVCachePrefillOp::prepare(torch_ext::PyAttentionInputs attn_inputs) {
     int       batch_size = attn_inputs.input_lengths.size(0);
@@ -186,8 +189,11 @@ std::tuple<torch::Tensor, torch::Tensor, torch::Tensor> FusedRopeKVCachePrefillO
     return std::make_tuple(q_output, k_output, v_output);
 }
 
-FusedRopeKVCacheDecodeOp::FusedRopeKVCacheDecodeOp(const GptInitParameter& gpt_init_parameter):
-    FMHARocmBase(gpt_init_parameter) {}
+FusedRopeKVCacheDecodeOp::FusedRopeKVCacheDecodeOp(const AttentionConfigs& attn_configs, int layer_num, const HWKernelConfig& hw_kernel_config):
+    attn_configs_(attn_configs),
+    layer_num_(layer_num),
+    hw_kernel_config_(hw_kernel_config),
+    device_(dynamic_cast<ROCmDevice*>(DeviceFactory::getDefaultDevice())) {}
 
 CKAttnPtr FusedRopeKVCacheDecodeOp::prepare(torch_ext::PyAttentionInputs attn_inputs) {
     int       batch_size = attn_inputs.sequence_lengths.size(0);
@@ -317,7 +323,8 @@ void registerFusedRopeKVCacheOp(const py::module& m) {
     pybind11::class_<KVBlockArray>(m, "KVBlockArray").def(pybind11::init<>());
     pybind11::class_<CKAttn, std::shared_ptr<CKAttn>>(m, "CKAttn").def(pybind11::init<>());
     pybind11::class_<FusedRopeKVCachePrefillOp>(m, "FusedRopeKVCachePrefillOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const AttentionConfigs&, int, const HWKernelConfig&>(),
+             py::arg("attn_configs"), py::arg("layer_num"), py::arg("hw_kernel_config"))
         .def("prepare", &FusedRopeKVCachePrefillOp::prepare, py::arg("attn_inputs"))
         .def("forward",
              &FusedRopeKVCachePrefillOp::forward,
@@ -326,7 +333,8 @@ void registerFusedRopeKVCacheOp(const py::module& m) {
              py::arg("kv_cache"),
              py::arg("params"));
     pybind11::class_<FusedRopeKVCacheDecodeOp>(m, "FusedRopeKVCacheDecodeOp")
-        .def(pybind11::init<GptInitParameter>(), py::arg("gpt_init_parameter"))
+        .def(pybind11::init<const AttentionConfigs&, int, const HWKernelConfig&>(),
+             py::arg("attn_configs"), py::arg("layer_num"), py::arg("hw_kernel_config"))
         .def("prepare", &FusedRopeKVCacheDecodeOp::prepare, py::arg("attn_inputs"))
         .def("forward",
              &FusedRopeKVCacheDecodeOp::forward,
diff --git a/rtp_llm/models_py/bindings/rocm/FusedRopeKVCacheOp.h b/rtp_llm/models_py/bindings/rocm/FusedRopeKVCacheOp.h
index 2432f6ef5..0dabf1bd0 100644
--- a/rtp_llm/models_py/bindings/rocm/FusedRopeKVCacheOp.h
+++ b/rtp_llm/models_py/bindings/rocm/FusedRopeKVCacheOp.h
@@ -1,30 +1,44 @@
 #pragma once
 
-#include "rtp_llm/models_py/bindings/rocm/FMHARocmBase.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/model_utils/AttentionConfig.h"
+#include "rtp_llm/cpp/devices/rocm_impl/ROCmDevice.h"
+#include "rtp_llm/cpp/devices/DeviceFactory.h"
 #include "rtp_llm/cpp/kernels/kv_cache/kv_cache_utils.h"
 #include "rtp_llm/models_py/bindings/OpDefs.h"
 
 namespace rtp_llm {
 
-class FusedRopeKVCachePrefillOp: public FMHARocmBase {
+class FusedRopeKVCachePrefillOp {
 public:
-    FusedRopeKVCachePrefillOp(const GptInitParameter& gpt_init_parameter);
+    FusedRopeKVCachePrefillOp(const AttentionConfigs& attn_configs, int layer_num, const HWKernelConfig& hw_kernel_config);
     CKAttnPtr                                               prepare(torch_ext::PyAttentionInputs attn_inputs);
     std::tuple<torch::Tensor, torch::Tensor, torch::Tensor> forward(const torch::Tensor&              qkv,
                                                                     FMHAType                          fmha_type,
                                                                     std::optional<torch_ext::KVCache> kv_cache_base,
                                                                     const CKAttnPtr&                  params);
+
+protected:
+    AttentionConfigs attn_configs_;
+    int              layer_num_;
+    HWKernelConfig   hw_kernel_config_;
+    ROCmDevice*      device_;
 };
 
-class FusedRopeKVCacheDecodeOp: public FMHARocmBase {
+class FusedRopeKVCacheDecodeOp {
 public:
-    FusedRopeKVCacheDecodeOp(const GptInitParameter& gpt_init_parameter);
+    FusedRopeKVCacheDecodeOp(const AttentionConfigs& attn_configs, int layer_num, const HWKernelConfig& hw_kernel_config);
     CKAttnPtr     prepare(torch_ext::PyAttentionInputs attn_inputs);
     torch::Tensor forward(const torch::Tensor&              qkv,
                           FMHAType                          fmha_type,
                           std::optional<torch_ext::KVCache> kv_cache_base,
                           const CKAttnPtr&                  params);
+
+protected:
+    AttentionConfigs attn_configs_;
+    int              layer_num_;
+    HWKernelConfig   hw_kernel_config_;
+    ROCmDevice*      device_;
 };
 
 void registerFusedRopeKVCacheOp(const py::module& m);
diff --git a/rtp_llm/models_py/bindings/rocm/Gemm.cc b/rtp_llm/models_py/bindings/rocm/Gemm.cc
index 8ca4235d2..87e187ec6 100644
--- a/rtp_llm/models_py/bindings/rocm/Gemm.cc
+++ b/rtp_llm/models_py/bindings/rocm/Gemm.cc
@@ -178,9 +178,6 @@ void gemm(at::Tensor& output, at::Tensor& input, at::Tensor& weight) {
     rtp_llm::BufferPtr weight_buffer = rtp_llm::torchTensor2Buffer(weight);
     rtp_llm::BufferPtr output_buffer = rtp_llm::torchTensor2Buffer(output);
 
-    if (!rtp_llm::DeviceFactory::isAlreadyInit()) {
-        rtp_llm::DeviceFactory::initDevices(rtp_llm::GptInitParameter());
-    }
     rtp_llm::ROCmDevice* device_ = dynamic_cast<rtp_llm::ROCmDevice*>(rtp_llm::DeviceFactory::getDefaultDevice());
     device_->gemm({*input_buffer, *weight_buffer, std::nullopt, output_buffer});
 }
\ No newline at end of file
diff --git a/rtp_llm/models_py/bindings/rocm/PagedAttn.cc b/rtp_llm/models_py/bindings/rocm/PagedAttn.cc
index 344dc8bad..7a03afd64 100644
--- a/rtp_llm/models_py/bindings/rocm/PagedAttn.cc
+++ b/rtp_llm/models_py/bindings/rocm/PagedAttn.cc
@@ -8,7 +8,6 @@
 // #include "rtp_llm/cpp/core/torch_utils/BufferTorchUtils.h"
 // #include "rtp_llm/cpp/devices/rocm_impl/aiterPA.h"
 // #include "rtp_llm/models_py/bindings/rocm/PagedAttn.h"
-// #include "rtp_llm/models_py/bindings/rocm/FMHARocmBase.h"
 
 #include "rtp_llm/cpp/devices/CommonDefines.h"
 #include "rtp_llm/cpp/core/Dispatch.h"
@@ -18,13 +17,15 @@
 #include "rtp_llm/cpp/core/torch_utils/BufferTorchUtils.h"
 #include "rtp_llm/cpp/devices/rocm_impl/aiterPA.h"
 #include "rtp_llm/models_py/bindings/rocm/PagedAttn.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
-#include "rtp_llm/cpp/devices/rocm_impl/ROCmDevice.h"
+#include "rtp_llm/cpp/devices/DeviceFactory.h"
 namespace rtp_llm {
-PagedAttnDecodeOp::PagedAttnDecodeOp(const GptInitParameter& gpt_init_parameter):
-    FMHARocmBase(gpt_init_parameter),
-    kv_block_offset_(gpt_init_parameter.num_layers_ * gpt_init_parameter.block_nums_) {
-    use_aiter_pa_ = gpt_init_parameter.hw_kernel_config.use_aiter_pa;
+PagedAttnDecodeOp::PagedAttnDecodeOp(const AttentionConfigs& attn_configs, int layer_num, int64_t block_nums, const HWKernelConfig& hw_kernel_config):
+    attn_configs_(attn_configs),
+    layer_num_(layer_num),
+    hw_kernel_config_(hw_kernel_config),
+    device_(dynamic_cast<ROCmDevice*>(DeviceFactory::getDefaultDevice())),
+    kv_block_offset_(layer_num * block_nums),
+    use_aiter_pa_(hw_kernel_config.use_aiter_pa) {
 }
 
 bool PagedAttnDecodeOp::support(torch_ext::PyAttentionInputs attn_inputs) {
@@ -150,7 +151,8 @@ forward_param PagedAttnDecodeOp::forward(const torch::Tensor&              qkv,
 
 void registerPagedAttnDecodeOp(py::module& m) {
     py::class_<PagedAttnDecodeOp>(m, "PagedAttnDecodeOp")
-        .def(py::init<GptInitParameter>(), py::arg("device_init_params"))
+        .def(py::init<const AttentionConfigs&, int, int64_t, const HWKernelConfig&>(),
+             py::arg("attn_configs"), py::arg("layer_num"), py::arg("block_nums"), py::arg("hw_kernel_config"))
         .def("support", &PagedAttnDecodeOp::support, py::arg("attn_inputs"))
 
         .def("prepare",
diff --git a/rtp_llm/models_py/bindings/rocm/PagedAttn.h b/rtp_llm/models_py/bindings/rocm/PagedAttn.h
index ddc26b40a..8ba9a5a90 100644
--- a/rtp_llm/models_py/bindings/rocm/PagedAttn.h
+++ b/rtp_llm/models_py/bindings/rocm/PagedAttn.h
@@ -3,11 +3,11 @@
 #include "rtp_llm/cpp/model_utils/RopeConfig.h"
 #include "rtp_llm/models_py/bindings/rocm/FusedRopeKVCacheOp.h"
 #include "rtp_llm/cpp/kernels/kv_cache/kv_cache_utils.h"
-#include "rtp_llm/models_py/bindings/rocm/FMHARocmBase.h"
-#include "rtp_llm/cpp/config/GptInitParameter.h"
-#include "rtp_llm/cpp/kernels/kv_cache/kv_cache_utils.h"
-#include "rtp_llm/models_py/bindings/OpDefs.h"
+#include "rtp_llm/cpp/config/ConfigModules.h"
+#include "rtp_llm/cpp/model_utils/AttentionConfig.h"
 #include "rtp_llm/cpp/devices/rocm_impl/ROCmDevice.h"
+#include "rtp_llm/cpp/devices/DeviceFactory.h"
+#include "rtp_llm/models_py/bindings/OpDefs.h"
 #include <pybind11/pybind11.h>
 
 #include "rtp_llm/cpp/devices/DeviceData.h"
@@ -29,11 +29,9 @@ struct forward_param {
     int64_t       max_seq_len;     // Maximum sequence length
     int64_t       partition_size;  // Size of partitions for paged attention
 };
-class PagedAttnDecodeOp: public rtp_llm::FMHARocmBase {
+class PagedAttnDecodeOp {
 public:
-    PagedAttnDecodeOp(const GptInitParameter& gpt_init_parameter
-                      // const DeviceInitParams& device_init_params
-    );
+    PagedAttnDecodeOp(const AttentionConfigs& attn_configs, int layer_num, int64_t block_nums, const HWKernelConfig& hw_kernel_config);
     bool support(torch_ext::PyAttentionInputs attn_inputs);
 
     CKAttnPtr     prepare(torch_ext::PyAttentionInputs attn_inputs);
@@ -43,6 +41,10 @@ public:
                           const CKAttnPtr&                  params);
 
 private:
+    AttentionConfigs attn_configs_;
+    int              layer_num_;
+    HWKernelConfig   hw_kernel_config_;
+    ROCmDevice*      device_;
     // Offset for KV cache blocks, calculated as num_layers * block_nums
     size_t kv_block_offset_;
     // Flag to control whether to use AITER paged attention, controlled by USE_AITER_PA env var
diff --git a/rtp_llm/models_py/distributed/deepep_wrapper.py b/rtp_llm/models_py/distributed/deepep_wrapper.py
index d1ac223ee..b4b7f8b7a 100644
--- a/rtp_llm/models_py/distributed/deepep_wrapper.py
+++ b/rtp_llm/models_py/distributed/deepep_wrapper.py
@@ -7,7 +7,10 @@ from deep_ep import Buffer as DeepEPBuffer
 from deep_ep import Config as DeepEPConfig
 from torch.distributed import ProcessGroup
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from typing import Optional
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
+from rtp_llm.ops import MoeConfig, RuntimeConfig, FfnDisAggregateConfig
 
 __all__ = [
     "DeepEPBuffer",
@@ -44,15 +47,28 @@ class DeepEPWrapper:
     _use_accl_ep: bool = True
     _mode: DeepEPMode = DeepEPMode.NORMAL
 
-    def __init__(self, group: ProcessGroup, params: GptInitModelParameters) -> None:
-        self._ep_rank = params.ep_rank
-        self._ep_size = params.ep_size
-        self._hidden_size = params.hidden_size
-        self._num_experts = params.expert_num
-        self._num_topk = params.moe_k
-        self._num_sms = params.moe_config.deep_ep_num_sm
+    def __init__(
+        self,
+        group: ProcessGroup,
+        py_model_config: ModelConfig,
+        parallelism_config,
+        moe_config: MoeConfig,
+        runtime_config: RuntimeConfig,
+        ffn_disaggregate_config: Optional[FfnDisAggregateConfig] = None,
+    ) -> None:
+        self._ep_rank = parallelism_config.ep_rank
+        self._ep_size = parallelism_config.ep_size
+        self._hidden_size = py_model_config.hidden_size
+        self._num_experts = py_model_config.expert_num
+        self._num_topk = py_model_config.moe_k
+        self._num_sms = moe_config.deep_ep_num_sm
         self._use_accl_ep = True
-        self._mode, self._buffer = self._init_deepep_buffer(group, params)
+        self._py_model_config = py_model_config
+        self._parallelism_config = parallelism_config
+        self._moe_config = moe_config
+        self._runtime_config = runtime_config
+        self._ffn_disaggregate_config = ffn_disaggregate_config
+        self._mode, self._buffer = self._init_deepep_buffer(group)
 
     @property
     def buffer(self) -> DeepEPBuffer:
@@ -96,19 +112,19 @@ class DeepEPWrapper:
         return self._use_accl_ep
 
     def _init_deepep_buffer(
-        self, group: ProcessGroup, params: GptInitModelParameters
+        self, group: ProcessGroup
     ) -> Tuple[DeepEPMode, DeepEPBuffer]:
         # init deep_ep buffer
-        ep_rank = params.ep_rank
-        use_deepep_low_latency: bool = params.moe_config.use_deepep_low_latency
+        ep_rank = self._ep_rank
+        use_deepep_low_latency: bool = self._moe_config.use_deepep_low_latency
         enable_ffn_disaggregate: bool = (
-            params.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate
+            self._ffn_disaggregate_config.enable_ffn_disaggregate
+            if self._ffn_disaggregate_config
+            else False
         )
         if use_deepep_low_latency and enable_ffn_disaggregate:
             if self._use_accl_ep:
-                return DeepEPMode.LOW_LATENCY_M2N, self._init_low_latency_m2n_buffer(
-                    group, params
-                )
+                return DeepEPMode.LOW_LATENCY_M2N, self._init_low_latency_m2n_buffer(group)
             else:
                 raise RuntimeError(
                     f"[rank: {ep_rank}] init deep_ep buffer failed, current deep_ep provider "
@@ -116,9 +132,9 @@ class DeepEPWrapper:
                     f"and enable_ffn_disaggregate: {enable_ffn_disaggregate}"
                 )
         elif use_deepep_low_latency and not enable_ffn_disaggregate:
-            return DeepEPMode.LOW_LATENCY, self._init_low_latency_buffer(group, params)
+            return DeepEPMode.LOW_LATENCY, self._init_low_latency_buffer(group)
         elif not use_deepep_low_latency and not enable_ffn_disaggregate:
-            return DeepEPMode.NORMAL, self._init_normal_buffer(group, params)
+            return DeepEPMode.NORMAL, self._init_normal_buffer(group)
         else:
             raise RuntimeError(
                 f"[rank: {ep_rank}] init deep_ep buffer failed, unsupported "
@@ -158,14 +174,14 @@ class DeepEPWrapper:
         return 128
 
     def _init_normal_buffer(
-        self, group: ProcessGroup, params: GptInitModelParameters
+        self, group: ProcessGroup
     ) -> DeepEPBuffer:
         num_nvl_bytes = 0
         num_rdma_bytes = 0
         num_qps_per_rank = 1
-        use_deepep_internode: bool = params.moe_config.use_deepep_internode
-        num_experts: int = params.expert_num
-        ep_size: int = params.ep_size
+        use_deepep_internode: bool = self._moe_config.use_deepep_internode
+        num_experts: int = self._num_experts
+        ep_size: int = self._ep_size
         assert num_experts > 0 and ep_size > 0, "num_experts and ep_size must be set"
         # normal-kernel internode
         if use_deepep_internode:
@@ -196,10 +212,10 @@ class DeepEPWrapper:
         return DeepEPBuffer(**init_kwargs)  # type: ignore
 
     def _init_low_latency_buffer(
-        self, group: ProcessGroup, params: GptInitModelParameters
+        self, group: ProcessGroup
     ) -> DeepEPBuffer:
-        max_generate_batch_size: int = params.max_generate_batch_size
-        tp_size: int = params.tp_size
+        max_generate_batch_size: int = self._runtime_config.max_generate_batch_size
+        tp_size: int = self._parallelism_config.tp_size
         assert (
             max_generate_batch_size > 0 and tp_size > 0
         ), "max_generate_batch_size and tp_size must be set"
@@ -211,9 +227,9 @@ class DeepEPWrapper:
         num_nvl_bytes = 0
         num_rdma_bytes = 0
         num_qps_per_rank = 1
-        hidden_size: int = params.hidden_size
-        ep_size: int = params.ep_size
-        num_experts: int = params.expert_num
+        hidden_size: int = self._hidden_size
+        ep_size: int = self._ep_size
+        num_experts: int = self._num_experts
         assert (
             hidden_size > 0 and ep_size > 0 and num_experts > 0
         ), "hidden_size, ep_size and num_experts must be set"
@@ -223,7 +239,7 @@ class DeepEPWrapper:
             ep_size,
             num_experts,
         )
-        local_rank: int = params.local_rank
+        local_rank: int = self._parallelism_config.local_rank
         if local_rank == 0:
             print(
                 f"Allocating buffer size: {num_rdma_bytes / 1e6} MB, "
@@ -248,12 +264,13 @@ class DeepEPWrapper:
         return DeepEPBuffer(**init_kwargs)  # type: ignore
 
     def _init_low_latency_m2n_buffer(
-        self, group: ProcessGroup, params: GptInitModelParameters
+        self, group: ProcessGroup
     ) -> DeepEPBuffer:
-        max_generate_batch_size: int = params.max_generate_batch_size
-        attention_tp_size: int = (
-            params.gpt_init_params.ffn_disaggregate_config.attention_tp_size
-        )
+        if self._ffn_disaggregate_config is None:
+            raise RuntimeError("ffn_disaggregate_config is required for low-latency m2n mode")
+        
+        max_generate_batch_size: int = self._runtime_config.max_generate_batch_size
+        attention_tp_size: int = self._ffn_disaggregate_config.attention_tp_size
         assert (
             max_generate_batch_size > 0 and attention_tp_size > 0
         ), "max_generate_batch_size and attention_tp_size must be set"
@@ -261,11 +278,9 @@ class DeepEPWrapper:
             max_generate_batch_size, attention_tp_size
         )
 
-        attention_dp_size: int = (
-            params.gpt_init_params.ffn_disaggregate_config.attention_dp_size
-        )
-        ffn_dp_size: int = params.gpt_init_params.ffn_disaggregate_config.ffn_dp_size
-        ffn_tp_size: int = params.gpt_init_params.ffn_disaggregate_config.ffn_tp_size
+        attention_dp_size: int = self._ffn_disaggregate_config.attention_dp_size
+        ffn_dp_size: int = self._ffn_disaggregate_config.ffn_dp_size
+        ffn_tp_size: int = self._ffn_disaggregate_config.ffn_tp_size
         assert (
             attention_dp_size > 0 and ffn_dp_size > 0 and ffn_tp_size > 0
         ), "attention_dp_size, ffn_dp_size and ffn_tp_size must be set"
@@ -279,8 +294,8 @@ class DeepEPWrapper:
             raise RuntimeError(
                 "current deep_ep provider does not support low-latency m2n"
             )
-        hidden_size: int = params.hidden_size
-        num_experts: int = params.expert_num
+        hidden_size: int = self._hidden_size
+        num_experts: int = self._num_experts
         assert (
             hidden_size > 0 and num_experts > 0
         ), "hidden_size and num_experts must be set"
@@ -291,7 +306,7 @@ class DeepEPWrapper:
             num_experts,
             num_m,
         )
-        local_rank: int = params.local_rank
+        local_rank: int = self._parallelism_config.local_rank
         if local_rank == 0:
             print(
                 f"Allocating buffer size: {num_rdma_bytes / 1e6} MB, "
@@ -331,10 +346,17 @@ def get_deepep_wrapper() -> DeepEPWrapper:
     return _DEEP_EP
 
 
-def init_deepep_wrapper(group: ProcessGroup, params: GptInitModelParameters) -> None:
+def init_deepep_wrapper(
+    group: ProcessGroup,
+    py_model_config: ModelConfig,
+    parallelism_config,
+    moe_config: MoeConfig,
+    runtime_config: RuntimeConfig,
+    ffn_disaggregate_config: Optional[FfnDisAggregateConfig] = None,
+) -> None:
     global _DEEP_EP
     _DEEP_EP = DeepEPWrapper(
-        group, params
+        group, py_model_config, parallelism_config, moe_config, runtime_config, ffn_disaggregate_config
     )  # pyright: ignore[reportConstantRedefinition]
 
 
diff --git a/rtp_llm/models_py/distributed/process_group_state.py b/rtp_llm/models_py/distributed/process_group_state.py
index b77462b1b..7c1f7e3be 100644
--- a/rtp_llm/models_py/distributed/process_group_state.py
+++ b/rtp_llm/models_py/distributed/process_group_state.py
@@ -11,7 +11,7 @@ import torch
 import torch.distributed
 from torch.distributed import Backend, ProcessGroup
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.ops import ParallelismConfig
 
 __all__ = [
     "ProcessGroupState",
@@ -121,16 +121,28 @@ def _destroy_process_group_state(process_group_state: ProcessGroupState) -> None
 
 
 def init_distributed_environment(
-    params: GptInitModelParameters,
+    parallelism_config: ParallelismConfig,
+    nccl_ip: str,
+    th_nccl_port: int,
     backend: str = "nccl",
     timeout: Optional[int] = None,
 ):
+    """
+    Initialize distributed environment.
+    
+    Args:
+        parallelism_config: Parallelism configuration containing rank/size info
+        nccl_ip: NCCL master IP address
+        th_nccl_port: NCCL master port
+        backend: Backend to use (default: "nccl")
+        timeout: Timeout in seconds (optional)
+    """
     assert backend in ["nccl"], "backend current only supports nccl"
-    ip = params.nccl_ip
-    port = params.th_nccl_port
-    rank = params.dp_rank * params.tp_size + params.tp_rank
-    world_size = params.world_size
-    local_rank = params.local_rank
+    ip = nccl_ip
+    port = th_nccl_port
+    rank = parallelism_config.dp_rank * parallelism_config.tp_size + parallelism_config.tp_rank
+    world_size = parallelism_config.world_size
+    local_rank = parallelism_config.local_rank if hasattr(parallelism_config, 'local_rank') else parallelism_config.world_rank % parallelism_config.local_world_size
     os.environ["TORCH_DIST_INIT_BARRIER"] = "1"
     if not torch.distributed.is_initialized():
         print(
@@ -152,7 +164,7 @@ def init_distributed_environment(
             device_id=torch.device(f"cuda:{local_rank}"),
             timeout=timeout,  # pyright: ignore[reportArgumentType]
         )
-    initialize_expert_parallel(params, backend)
+    initialize_expert_parallel(parallelism_config, backend)
 
 
 _EP: Optional[ProcessGroupState] = None
@@ -164,7 +176,7 @@ def get_ep_group() -> ProcessGroupState:
 
 
 def initialize_expert_parallel(
-    params: GptInitModelParameters,
+    parallelism_config: ParallelismConfig,
     backend: str,
 ) -> None:
     """
@@ -172,11 +184,12 @@ def initialize_expert_parallel(
     """
     global _EP
     assert _EP is None, "expert parallel group is already initialized"
-    group_ranks = list(range(params.ep_size))
+    group_ranks = list(range(parallelism_config.ep_size))
+    local_rank = parallelism_config.local_rank if hasattr(parallelism_config, 'local_rank') else parallelism_config.world_rank % parallelism_config.local_world_size
 
     _EP = _init_process_group_state(  # pyright: ignore[reportConstantRedefinition]
         group_ranks=[group_ranks],
-        local_rank=params.local_rank,
+        local_rank=local_rank,
         torch_distributed_backend=backend,
         group_name="ep",
     )
diff --git a/rtp_llm/models_py/distributed/test/deepep_test.py b/rtp_llm/models_py/distributed/test/deepep_test.py
index ba01256a7..b04caef3f 100644
--- a/rtp_llm/models_py/distributed/test/deepep_test.py
+++ b/rtp_llm/models_py/distributed/test/deepep_test.py
@@ -12,7 +12,7 @@ import torch.distributed as dist
 import torch.multiprocessing as mp
 from torch.distributed import ProcessGroup
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.distributed.deepep_wrapper import (
     DeepEPBuffer,
     DeepEPConfig,
@@ -25,6 +25,8 @@ from rtp_llm.models_py.distributed.process_group_state import (
     get_ep_group,
     init_distributed_environment,
 )
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
+from rtp_llm.ops import FfnDisAggregateConfig, MoeConfig, ParallelismConfig, RuntimeConfig
 from rtp_llm.models_py.modules.utils import align
 from rtp_llm.test.utils.bench_util import bench, bench_kineto, calc_diff, hash_tensor
 from rtp_llm.test.utils.port_util import PortsContext
@@ -1534,41 +1536,86 @@ class DeepEPTest(TestCase):
                 )
         return hash_value
 
+    @staticmethod
+    def _create_deepep_config(
+        rank: int,
+        num_ranks: int,
+        args: Dict[str, Any],
+        use_deepep_low_latency: bool = False,
+        enable_ffn_disaggregate: bool = False,
+        deep_ep_num_sm: int = 24,
+    ):
+        """Helper function to create configuration objects for DeepEP tests."""
+        py_model_config = ModelConfig()
+        py_model_config.head_num = 2
+        py_model_config.size_per_head = 128
+        py_model_config.num_layers = 2
+        py_model_config.max_seq_len = 2048
+        py_model_config.vocab_size = 500000
+        if args:
+            py_model_config.moe_k = args.get("moe_k", 2)
+            py_model_config.expert_num = args.get("expert_num", 4)
+            py_model_config.hidden_size = args.get("hidden_size", 128)
+        
+        parallelism_config = ParallelismConfig()
+        parallelism_config.nccl_ip = "127.0.0.1"
+        parallelism_config.tp_nccl_port = int(os.getenv("MASTER_PORT", "8376"))
+        parallelism_config.dp_rank = rank
+        parallelism_config.dp_size = num_ranks
+        parallelism_config.tp_rank = 0
+        parallelism_config.tp_size = 1
+        parallelism_config.ep_size = num_ranks
+        parallelism_config.ep_rank = rank
+        parallelism_config.world_size = num_ranks
+        parallelism_config.local_rank = rank
+        parallelism_config.world_rank = rank
+        parallelism_config.local_world_size = num_ranks
+        
+        moe_config = MoeConfig()
+        moe_config.use_deepep_low_latency = use_deepep_low_latency
+        moe_config.use_deepep_internode = False
+        moe_config.deep_ep_num_sm = deep_ep_num_sm
+        
+        runtime_config = RuntimeConfig()
+        if args:
+            runtime_config.max_generate_batch_size = args.get("max_generate_batch_size", 32)
+        
+        ffn_disaggregate_config = FfnDisAggregateConfig()
+        ffn_disaggregate_config.enable_ffn_disaggregate = enable_ffn_disaggregate
+        if enable_ffn_disaggregate and args:
+            ffn_disaggregate_config.attention_dp_size = num_ranks // 2
+            ffn_disaggregate_config.attention_tp_size = 1
+            ffn_disaggregate_config.ffn_dp_size = num_ranks // 2
+            ffn_disaggregate_config.ffn_tp_size = 1
+        
+        return py_model_config, parallelism_config, moe_config, runtime_config, ffn_disaggregate_config
+
     @staticmethod
     def _run_deepep_intranode_test(rank: int, num_ranks: int, args: Dict[str, Any]):
         # set env
         os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(str(i) for i in range(num_ranks))
         # init params
-        params = GptInitModelParameters(
-            head_num=2,
-            size_per_head=128,
-            layer_num=2,
-            max_seq_len=2048,
-            vocab_size=500000,
-        )
-        params.nccl_ip = "127.0.0.1"
-        params.th_nccl_port = int(os.getenv("MASTER_PORT", "8376"))
-        params.dp_rank = rank
-        params.dp_size = num_ranks
-        params.tp_rank = 0
-        params.tp_size = 1
-        params.ep_size = num_ranks
-        params.ep_rank = rank
-        params.world_size = num_ranks
-        params.local_rank = rank
-        params.moe_config.use_deepep_low_latency = False
-        params.moe_config.use_deepep_internode = False
-        params.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate = False
-        params.moe_config.deep_ep_num_sm = 24
-        params.moe_k = args["moe_k"]
-        params.expert_num = args["expert_num"]
-        params.hidden_size = args["hidden_size"]
+        py_model_config, parallelism_config, moe_config, runtime_config, ffn_disaggregate_config = \
+            DeepEPTest._create_deepep_config(rank, num_ranks, args, use_deepep_low_latency=False, enable_ffn_disaggregate=False)
         # init distributed environment
-        torch.cuda.set_device(params.local_rank)
-        torch.set_default_device(f"cuda:{params.local_rank}")
-        init_distributed_environment(params=params, backend="nccl", timeout=60)
+        torch.cuda.set_device(parallelism_config.local_rank)
+        torch.set_default_device(f"cuda:{parallelism_config.local_rank}")
+        init_distributed_environment(
+            parallelism_config=parallelism_config,
+            nccl_ip=parallelism_config.nccl_ip,
+            th_nccl_port=parallelism_config.tp_nccl_port,
+            backend="nccl",
+            timeout=60
+        )
         group = get_ep_group().device_group
-        init_deepep_wrapper(group=group, params=params)
+        init_deepep_wrapper(
+            group=group,
+            py_model_config=py_model_config,
+            parallelism_config=parallelism_config,
+            moe_config=moe_config,
+            runtime_config=runtime_config,
+            ffn_disaggregate_config=ffn_disaggregate_config,
+        )
         deep_ep_wrapper = get_deepep_wrapper()
         buffer = deep_ep_wrapper.buffer
         # run test
@@ -1597,41 +1644,32 @@ class DeepEPTest(TestCase):
         os.environ["ACCL_TOPO_FIX"] = "1"
         os.environ["ACCL_LOAD_BALANCE"] = "1"
         # init params
-        params = GptInitModelParameters(
-            head_num=2,
-            size_per_head=128,
-            layer_num=2,
-            max_seq_len=2048,
-            vocab_size=500000,
-        )
-        params.nccl_ip = "127.0.0.1"
-        params.th_nccl_port = int(os.getenv("MASTER_PORT", "8376"))
-        params.dp_rank = rank
-        params.dp_size = num_ranks
-        params.tp_rank = 0
-        params.tp_size = 1
-        params.ep_size = num_ranks
-        params.ep_rank = rank
-        params.world_size = num_ranks
-        params.local_rank = rank
-        params.moe_config.use_deepep_low_latency = True
-        params.moe_config.use_deepep_internode = False
-        params.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate = False
-        params.moe_k = args["moe_k"]
-        params.expert_num = args["expert_num"]
-        params.hidden_size = args["hidden_size"]
-        params.max_generate_batch_size = args["max_generate_batch_size"]
+        py_model_config, parallelism_config, moe_config, runtime_config, ffn_disaggregate_config = \
+            DeepEPTest._create_deepep_config(rank, num_ranks, args, use_deepep_low_latency=True, enable_ffn_disaggregate=False)
         # init distributed environment
-        torch.cuda.set_device(params.local_rank)
-        torch.set_default_device(f"cuda:{params.local_rank}")
-        init_distributed_environment(params=params, backend="nccl", timeout=60)
+        torch.cuda.set_device(parallelism_config.local_rank)
+        torch.set_default_device(f"cuda:{parallelism_config.local_rank}")
+        init_distributed_environment(
+            parallelism_config=parallelism_config,
+            nccl_ip=parallelism_config.nccl_ip,
+            th_nccl_port=parallelism_config.tp_nccl_port,
+            backend="nccl",
+            timeout=60
+        )
         group = get_ep_group().device_group
-        init_deepep_wrapper(group=group, params=params)
+        init_deepep_wrapper(
+            group=group,
+            py_model_config=py_model_config,
+            parallelism_config=parallelism_config,
+            moe_config=moe_config,
+            runtime_config=runtime_config,
+            ffn_disaggregate_config=ffn_disaggregate_config,
+        )
         deep_ep_wrapper = get_deepep_wrapper()
         buffer = deep_ep_wrapper.buffer
         # run test
         DeepEPTest._test_low_latency_main(
-            (args["max_generate_batch_size"] + params.tp_size - 1) // params.tp_size,
+            (args["max_generate_batch_size"] + parallelism_config.tp_size - 1) // parallelism_config.tp_size,
             deep_ep_wrapper.hidden_size,
             deep_ep_wrapper.num_experts,
             deep_ep_wrapper.num_topk,
@@ -1654,50 +1692,85 @@ class DeepEPTest(TestCase):
         os.environ["ACCL_TOPO_FIX"] = "1"
         os.environ["ACCL_LOAD_BALANCE"] = "1"
         # init params
-        params = GptInitModelParameters(
-            head_num=2,
-            size_per_head=128,
-            layer_num=2,
-            max_seq_len=2048,
-            vocab_size=500000,
+        py_model_config, parallelism_config, moe_config, runtime_config, ffn_disaggregate_config = \
+            DeepEPTest._create_deepep_config(rank, num_ranks, args, use_deepep_low_latency=True, enable_ffn_disaggregate=True)
+        # init distributed environment
+        torch.cuda.set_device(parallelism_config.local_rank)
+        torch.set_default_device(f"cuda:{parallelism_config.local_rank}")
+        init_distributed_environment(
+            parallelism_config=parallelism_config,
+            nccl_ip=parallelism_config.nccl_ip,
+            th_nccl_port=parallelism_config.tp_nccl_port,
+            backend="nccl",
+            timeout=60
+        )
+        group = get_ep_group().device_group
+        init_deepep_wrapper(
+            group=group,
+            py_model_config=py_model_config,
+            parallelism_config=parallelism_config,
+            moe_config=moe_config,
+            runtime_config=runtime_config,
+            ffn_disaggregate_config=ffn_disaggregate_config,
+        )
+        deep_ep_wrapper = get_deepep_wrapper()
+        buffer = deep_ep_wrapper.buffer
+        # run test
+        DeepEPTest._test_low_latency_main(
+            (args["max_generate_batch_size"] + parallelism_config.tp_size - 1) // parallelism_config.tp_size,
+            deep_ep_wrapper.hidden_size,
+            deep_ep_wrapper.num_experts,
+            deep_ep_wrapper.num_topk,
+            deep_ep_wrapper.ep_rank,
+            deep_ep_wrapper.ep_size,
+            group,
+            buffer,
+            seed=1,
         )
-        params.nccl_ip = "127.0.0.1"
-        params.th_nccl_port = int(os.getenv("MASTER_PORT", "8376"))
-        params.dp_rank = rank
-        params.dp_size = num_ranks
-        params.tp_rank = 0
-        params.tp_size = 1
-        params.ep_size = num_ranks
-        params.ep_rank = rank
-        params.world_size = num_ranks
-        params.local_rank = rank
-        params.moe_config.use_deepep_low_latency = True
-        params.moe_config.use_deepep_internode = False
-        params.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate = True
-        params.moe_k = args["moe_k"]
-        params.expert_num = args["expert_num"]
-        params.hidden_size = args["hidden_size"]
-        params.max_generate_batch_size = args["max_generate_batch_size"]
-        params.gpt_init_params.ffn_disaggregate_config.attention_dp_size = num_ranks // 2
-        params.gpt_init_params.ffn_disaggregate_config.attention_tp_size = 1
-        params.gpt_init_params.ffn_disaggregate_config.ffn_dp_size = num_ranks // 2
-        params.gpt_init_params.ffn_disaggregate_config.ffn_tp_size = 1
+        destroy_deepep_wrapper()
+        destroy_distributed_environment()
+
+    @staticmethod
+    def _run_deepep_low_latency_m2n_test(rank: int, num_ranks: int, args: Dict[str, Any]):
+        # set env
+        os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(str(i) for i in range(num_ranks))
+        os.environ["ACCL_DISPATCH_NUM_WARP_GROUPS"] = "4"
+        os.environ["ACCL_COMBINE_NUM_WARP_GROUPS"] = "4"
+        os.environ["ACCL_LOW_LATENCY_OPTIMIZE"] = "1"
+        os.environ["ACCL_TOPO_FIX"] = "1"
+        os.environ["ACCL_LOAD_BALANCE"] = "1"
+        # init params
+        py_model_config, parallelism_config, moe_config, runtime_config, ffn_disaggregate_config = \
+            DeepEPTest._create_deepep_config(rank, num_ranks, args, use_deepep_low_latency=True, enable_ffn_disaggregate=True)
         # init distributed environment
-        torch.cuda.set_device(params.local_rank)
-        torch.set_default_device(f"cuda:{params.local_rank}")
-        init_distributed_environment(params=params, backend="nccl", timeout=60)
+        torch.cuda.set_device(parallelism_config.local_rank)
+        torch.set_default_device(f"cuda:{parallelism_config.local_rank}")
+        init_distributed_environment(
+            parallelism_config=parallelism_config,
+            nccl_ip=parallelism_config.nccl_ip,
+            th_nccl_port=parallelism_config.tp_nccl_port,
+            backend="nccl",
+            timeout=60
+        )
         group = get_ep_group().device_group
-        init_deepep_wrapper(group=group, params=params)
+        init_deepep_wrapper(
+            group=group,
+            py_model_config=py_model_config,
+            parallelism_config=parallelism_config,
+            moe_config=moe_config,
+            runtime_config=runtime_config,
+            ffn_disaggregate_config=ffn_disaggregate_config,
+        )
         deep_ep_wrapper = get_deepep_wrapper()
         buffer = deep_ep_wrapper.buffer
         # run test
         num_m = (
-            params.gpt_init_params.ffn_disaggregate_config.attention_dp_size
-            * params.gpt_init_params.ffn_disaggregate_config.attention_tp_size
+            ffn_disaggregate_config.attention_dp_size
+            * ffn_disaggregate_config.attention_tp_size
         )
         num_n = (
-            params.gpt_init_params.ffn_disaggregate_config.ffn_dp_size
-            * params.gpt_init_params.ffn_disaggregate_config.ffn_tp_size
+            ffn_disaggregate_config.ffn_dp_size
+            * ffn_disaggregate_config.ffn_tp_size
         )
         scale = num_ranks / num_n
         logical_num_experts = deep_ep_wrapper.num_experts * num_ranks // num_n
@@ -1705,7 +1778,7 @@ class DeepEPTest(TestCase):
             scale,
             num_m,
             num_m,
-            (args["max_generate_batch_size"] + params.tp_size - 1) // params.tp_size,
+            (args["max_generate_batch_size"] + parallelism_config.tp_size - 1) // parallelism_config.tp_size,
             deep_ep_wrapper.hidden_size,
             logical_num_experts,
             deep_ep_wrapper.num_topk,
@@ -1723,36 +1796,28 @@ class DeepEPTest(TestCase):
         # set env
         os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(str(i) for i in range(num_ranks))
         # init params
-        params = GptInitModelParameters(
-            head_num=2,
-            size_per_head=128,
-            layer_num=2,
-            max_seq_len=2048,
-            vocab_size=500000,
-        )
-        params.nccl_ip = "127.0.0.1"
-        params.th_nccl_port = int(os.getenv("MASTER_PORT", "8376"))
-        params.dp_rank = rank
-        params.dp_size = num_ranks
-        params.tp_rank = 0
-        params.tp_size = 1
-        params.ep_size = num_ranks
-        params.ep_rank = rank
-        params.world_size = num_ranks
-        params.local_rank = rank
-        params.moe_config.use_deepep_low_latency = False
-        params.moe_config.use_deepep_internode = False
-        params.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate = False
-        params.moe_config.deep_ep_num_sm = 24
-        params.moe_k = 2
-        params.expert_num = 4
-        params.hidden_size = 128
+        args = {"moe_k": 2, "expert_num": 4, "hidden_size": 128}
+        py_model_config, parallelism_config, moe_config, runtime_config, ffn_disaggregate_config = \
+            DeepEPTest._create_deepep_config(rank, num_ranks, args, use_deepep_low_latency=False, enable_ffn_disaggregate=False)
         # init distributed environment
-        torch.cuda.set_device(params.local_rank)
-        torch.set_default_device(f"cuda:{params.local_rank}")
-        init_distributed_environment(params=params, backend="nccl", timeout=60)
+        torch.cuda.set_device(parallelism_config.local_rank)
+        torch.set_default_device(f"cuda:{parallelism_config.local_rank}")
+        init_distributed_environment(
+            parallelism_config=parallelism_config,
+            nccl_ip=parallelism_config.nccl_ip,
+            th_nccl_port=parallelism_config.tp_nccl_port,
+            backend="nccl",
+            timeout=60
+        )
         group = get_ep_group().device_group
-        init_deepep_wrapper(group=group, params=params)
+        init_deepep_wrapper(
+            group=group,
+            py_model_config=py_model_config,
+            parallelism_config=parallelism_config,
+            moe_config=moe_config,
+            runtime_config=runtime_config,
+            ffn_disaggregate_config=ffn_disaggregate_config,
+        )
         deep_ep_wrapper = get_deepep_wrapper()
         buffer = deep_ep_wrapper.buffer
         # run test
@@ -1780,41 +1845,32 @@ class DeepEPTest(TestCase):
         os.environ["ACCL_TOPO_FIX"] = "1"
         os.environ["ACCL_LOAD_BALANCE"] = "1"
         # init params
-        params = GptInitModelParameters(
-            head_num=2,
-            size_per_head=128,
-            layer_num=2,
-            max_seq_len=2048,
-            vocab_size=500000,
-        )
-        params.nccl_ip = "127.0.0.1"
-        params.th_nccl_port = int(os.getenv("MASTER_PORT", "8376"))
-        params.dp_rank = rank
-        params.dp_size = num_ranks
-        params.tp_rank = 0
-        params.tp_size = 1
-        params.ep_size = num_ranks
-        params.ep_rank = rank
-        params.world_size = num_ranks
-        params.local_rank = rank
-        params.moe_config.use_deepep_low_latency = True
-        params.moe_config.use_deepep_internode = False
-        params.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate = False
-        params.moe_k = args["moe_k"]
-        params.expert_num = args["expert_num"]
-        params.hidden_size = args["hidden_size"]
-        params.max_generate_batch_size = args["max_generate_batch_size"]
+        py_model_config, parallelism_config, moe_config, runtime_config, ffn_disaggregate_config = \
+            DeepEPTest._create_deepep_config(rank, num_ranks, args, use_deepep_low_latency=True, enable_ffn_disaggregate=False)
         # init distributed environment
-        torch.cuda.set_device(params.local_rank)
-        torch.set_default_device(f"cuda:{params.local_rank}")
-        init_distributed_environment(params=params, backend="nccl", timeout=60)
+        torch.cuda.set_device(parallelism_config.local_rank)
+        torch.set_default_device(f"cuda:{parallelism_config.local_rank}")
+        init_distributed_environment(
+            parallelism_config=parallelism_config,
+            nccl_ip=parallelism_config.nccl_ip,
+            th_nccl_port=parallelism_config.tp_nccl_port,
+            backend="nccl",
+            timeout=60
+        )
         group = get_ep_group().device_group
-        init_deepep_wrapper(group=group, params=params)
+        init_deepep_wrapper(
+            group=group,
+            py_model_config=py_model_config,
+            parallelism_config=parallelism_config,
+            moe_config=moe_config,
+            runtime_config=runtime_config,
+            ffn_disaggregate_config=ffn_disaggregate_config,
+        )
         deep_ep_wrapper = get_deepep_wrapper()
         buffer = deep_ep_wrapper.buffer
         # run test
         DeepEPTest._test_low_latency_per_token_quant_main(
-            (args["max_generate_batch_size"] + params.tp_size - 1) // params.tp_size,
+            (args["max_generate_batch_size"] + parallelism_config.tp_size - 1) // parallelism_config.tp_size,
             deep_ep_wrapper.hidden_size,
             deep_ep_wrapper.num_experts,
             deep_ep_wrapper.num_topk,
diff --git a/rtp_llm/models_py/model_desc/bert.py b/rtp_llm/models_py/model_desc/bert.py
index 74bed8581..f1d870659 100644
--- a/rtp_llm/models_py/model_desc/bert.py
+++ b/rtp_llm/models_py/model_desc/bert.py
@@ -3,7 +3,7 @@ from typing import Dict, Optional
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_loader.model_weight_info import ModelWeights
 from rtp_llm.models_py.model_desc.module_base import GptModelBase
 from rtp_llm.models_py.modules import RMSNorm
@@ -12,17 +12,17 @@ from rtp_llm.models_py.modules.embedding import EmbeddingBert
 from rtp_llm.models_py.modules.fmha import FMHAImplBase
 from rtp_llm.models_py.modules.mlp import BertGeluActDenseMLP
 from rtp_llm.models_py.modules.norm import LayerNorm
-from rtp_llm.ops import KVCache, PyAttentionInputs, PyModelInputs, PyModelOutputs
+from rtp_llm.ops import KVCache, ParallelismConfig, PyAttentionInputs, PyModelInputs, PyModelOutputs
 from rtp_llm.utils.model_weight import W
 
 
 class BertDecoderLayer(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, parallelism_config: ParallelismConfig, weights: Dict[str, torch.Tensor], quant_config: Optional[object] = None
     ):
         super().__init__()
-        self.self_attn = CausalAttention(config, weights)
-        self.mlp = BertGeluActDenseMLP(config, weights)
+        self.self_attn = CausalAttention(config, parallelism_config, weights, quant_config)
+        self.mlp = BertGeluActDenseMLP(config, parallelism_config, weights, quant_config)
         self.input_layernorm = LayerNorm(
             weights[W.post_ln_gamma],
             beta=weights[W.post_ln_beta],
@@ -57,10 +57,20 @@ class BertDecoderLayer(nn.Module):
 
 
 class BertModel(GptModelBase):
-    def __init__(self, config: GptInitModelParameters, weights: ModelWeights):
-        super().__init__(config, weights)
+    def __init__(
+        self, 
+        config: ModelConfig, 
+        parallelism_config: ParallelismConfig,
+        device_resource_config,
+        weights: ModelWeights, 
+        quant_config: Optional[object] = None,
+        vocab_size: int = 0,
+        fmha_config=None,
+        py_hw_kernel_config=None,
+    ):
+        super().__init__(config, parallelism_config, device_resource_config, weights, vocab_size, fmha_config=fmha_config, py_hw_kernel_config=py_hw_kernel_config)
         self.embed_tokens = EmbeddingBert(
-            config, weights.get_global_weight(W.embedding)
+            config, parallelism_config, weights.get_global_weight(W.embedding)
         )
         self.pre_decoder_layernorm = LayerNorm(
             weight=weights.get_global_weight(W.pre_decoder_ln_gamma),
@@ -69,7 +79,7 @@ class BertModel(GptModelBase):
         )
         self.layers = nn.ModuleList(
             [
-                BertDecoderLayer(config, weights.weights[idx])
+                BertDecoderLayer(config, parallelism_config, weights.weights[idx], quant_config)
                 for idx in range(self.layer_num)
             ]
         )
diff --git a/rtp_llm/models_py/model_desc/deepseek_v2.py b/rtp_llm/models_py/model_desc/deepseek_v2.py
index c52cdc5ff..309e34aad 100644
--- a/rtp_llm/models_py/model_desc/deepseek_v2.py
+++ b/rtp_llm/models_py/model_desc/deepseek_v2.py
@@ -3,7 +3,8 @@ from typing import Dict, Optional
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.quant_config import QuantizationConfig
 from rtp_llm.model_loader.model_weight_info import ModelWeights
 from rtp_llm.models_py.model_desc.module_base import GptModelBase
 from rtp_llm.models_py.modules.embedding import Embedding
@@ -13,23 +14,29 @@ from rtp_llm.models_py.modules.linear_factory import LinearFactory
 from rtp_llm.models_py.modules.mla import DeepSeekV2Attention
 from rtp_llm.models_py.modules.mlp import FusedSiluActDenseMLP
 from rtp_llm.models_py.modules.moe import FusedMoe
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 from rtp_llm.models_py.modules.moe.fused_moe_factory import FusedMoeFactory
 from rtp_llm.models_py.modules.norm import RMSNorm
-from rtp_llm.ops import KVCache, PyAttentionInputs, PyModelInputs, PyModelOutputs
+from rtp_llm.ops import KVCache, MoeConfig, ParallelismConfig, PyAttentionInputs, PyModelInputs, PyModelOutputs, RuntimeConfig
 from rtp_llm.utils.model_weight import W
 
 
 class DeepSeekV2NormalMoeLayer(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
+        weights: Dict[str, torch.Tensor],
+        quant_config: Optional[QuantizationConfig] = None,
     ):
         super().__init__()
         self.config = config
         self.top_k = config.moe_k
         self.gate = LinearFactory.create_linear_from_weights(
-            weights, W.moe_gate, None, None, config
+            weights, W.moe_gate, None, None,
+            py_model_config=config, quant_config=quant_config
         )
-        self.fused_moe = FusedMoE(config, weights, layer_id=0)
+        self.fused_moe = FusedMoE(config, parallelism_config, weights, layer_id=0)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         router_logits = self.gate(hidden_states)
@@ -41,13 +48,17 @@ class DeepSeekV2NormalMoeLayer(nn.Module):
 
 class DeepSeekV2MoeLayer(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
+        weights: Dict[str, torch.Tensor],
+        quant_config: Optional[QuantizationConfig] = None,
     ):
         super().__init__()
         self.config = config
         self.top_k = config.moe_k
         # Create gate layer
-        use_fp8_path = self._should_use_fp8_linear(config, weights)
+        use_fp8_path = self._should_use_fp8_linear(quant_config, weights)
         if use_fp8_path:
             gate_scale_key = "partial_moe_weights.gate.weight_only_quant_scale"
             has_gate_scale = gate_scale_key in weights
@@ -57,22 +68,34 @@ class DeepSeekV2MoeLayer(nn.Module):
                     weight=weights[W.moe_gate],
                     weight_scales=weights[gate_scale_key],
                     bias=None,
-                    config=config,
+                    py_model_config=config,
+                    quant_config=quant_config,
                     force_fp8=True,
                 )
             else:
                 # Create regular gate layer
                 self.gate = LinearFactory.create_linear_from_weights(
-                    weights, W.moe_gate, None, None, config
+                    weights, W.moe_gate, None, None,
+                    py_model_config=config, quant_config=quant_config
                 )
         else:
             # Create regular gate layer
             self.gate = LinearFactory.create_linear_from_weights(
-                weights, W.moe_gate, None, None, config
+                weights, W.moe_gate, None, None,
+                py_model_config=config, quant_config=quant_config
             )
 
-        # Always use FusedMoeFactory.create_fused_moe
-        self.fused_moe: FusedMoe = FusedMoeFactory.create_fused_moe(config, weights)
+        # Create adapter and use FusedMoeFactory.create_fused_moe
+        moe_config = MoeConfig()
+        runtime_config = RuntimeConfig()
+        config_adapter = MoEConfigAdapter(
+            py_model_config=config,
+            parallelism_config=parallelism_config,
+            moe_config=moe_config,
+            runtime_config=runtime_config,
+            quant_config=quant_config,
+        )
+        self.fused_moe: FusedMoe = FusedMoeFactory.create_fused_moe(config_adapter, weights)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         """Forward pass for FusedMoE implementation."""
@@ -99,10 +122,10 @@ class DeepSeekV2MoeLayer(nn.Module):
         )
 
     def _should_use_fp8_linear(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, quant_config: Optional[QuantizationConfig], weights: Dict[str, torch.Tensor]
     ) -> bool:
         """Check if FP8 linear layers should be used."""
-        if not hasattr(config, "quant_config"):
+        if quant_config is None:
             return False
 
         # Check if any MoE weights are FP8
@@ -121,21 +144,25 @@ class DeepSeekV2MoeLayer(nn.Module):
 class DeepSeekV2DecoderLayer(nn.Module):
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
         weights: Dict[str, torch.Tensor],
         layer_idx: int,
+        quant_config: Optional[QuantizationConfig] = None,
     ):
         super().__init__()
 
         self.layer_idx = layer_idx
-        self.self_attn = DeepSeekV2Attention(config, weights, layer_idx)
+        self.self_attn = DeepSeekV2Attention(config, parallelism_config, weights, layer_idx, quant_config)
 
-        if len(config.moe_layer_index) > 0 and layer_idx < config.moe_layer_index[0]:
+        moe_layer_index = config.moe_layer_index
+        if len(moe_layer_index) > 0 and layer_idx < moe_layer_index[0]:
             self.is_dense_layer = True
         else:
             self.is_dense_layer = False
-            self.moe_mlp = DeepSeekV2NormalMoeLayer(config, weights)
-        self.add_shared_expert = config.moe_style == 2
+            self.moe_mlp = DeepSeekV2NormalMoeLayer(config, parallelism_config, weights, quant_config)
+        moe_style = config.moe_style
+        self.add_shared_expert = moe_style == 2
 
         if self.add_shared_expert:
             self.shared_mlp = FusedSiluActDenseMLP(config, weights)
@@ -179,19 +206,32 @@ class DeepSeekV2DecoderLayer(nn.Module):
 
 
 class DeepSeekV2Model(GptModelBase):
-    def __init__(self, config: GptInitModelParameters, weights: ModelWeights):
-        config.head_num = config.head_num // config.tp_size
-        super().__init__(config, weights)
-        self.layer_num = config.layer_num
-        self.vocab_size = config.vocab_size
-        self.embed_tokens = Embedding(config, weights.get_global_weight(W.embedding))
+    def __init__(
+        self,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
+        device_resource_config,
+        weights: ModelWeights,
+        vocab_size: int,
+        quant_config: Optional[QuantizationConfig] = None,
+        fmha_config=None,
+        py_hw_kernel_config=None,
+    ):
+        # Adjust head_num for tensor parallelism
+        config.head_num = config.head_num // parallelism_config.tp_size
+        super().__init__(config, parallelism_config, device_resource_config, weights, vocab_size, fmha_config=fmha_config, py_hw_kernel_config=py_hw_kernel_config)
+        self.layer_num = config.num_layers
+        self.vocab_size = vocab_size
+        self.embed_tokens = Embedding(config, parallelism_config, weights.get_global_weight(W.embedding))
 
         self.layers = nn.ModuleList(
             [
                 DeepSeekV2DecoderLayer(
                     config,
+                    parallelism_config,
                     weights.weights[idx],
                     idx,
+                    quant_config,
                 )
                 for idx in range(self.layer_num)
             ]
diff --git a/rtp_llm/models_py/model_desc/disaggregate_qwen3.py b/rtp_llm/models_py/model_desc/disaggregate_qwen3.py
index b34fb28fd..61d3b93c9 100644
--- a/rtp_llm/models_py/model_desc/disaggregate_qwen3.py
+++ b/rtp_llm/models_py/model_desc/disaggregate_qwen3.py
@@ -1,10 +1,11 @@
 from dataclasses import dataclass
-from typing import List, Tuple
+from typing import List, Optional, Tuple
 
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.ops import ParallelismConfig, DeviceResourceConfig, FfnDisAggregateConfig
 from rtp_llm.distribute.collective import Group, recv, send
 from rtp_llm.distribute.worker_info import g_parallel_info
 from rtp_llm.model_loader.model_weight_info import ModelWeights
@@ -32,28 +33,36 @@ class BatchSplitInfo:
 
 
 class DisaggregateModelBase(GptModelBase):
-    def __init__(self, config: GptInitModelParameters, weights: ModelWeights):
-        super().__init__(config, weights)
+    def __init__(
+        self, 
+        config: ModelConfig, 
+        parallelism_config: ParallelismConfig,
+        ffn_disaggregate_config: FfnDisAggregateConfig,
+        device_resource_config: DeviceResourceConfig,
+        weights: ModelWeights,
+        vocab_size: int
+    ):
+        super().__init__(config, parallelism_config, device_resource_config, weights, vocab_size)
         check_with_info(
-            self.config.gpt_init_params.ffn_disaggregate_config.attention_tp_size == 1,
+            ffn_disaggregate_config.attention_tp_size == 1,
             "attention_tp_size must be 1",
         )
         check_with_info(
-            self.config.gpt_init_params.ffn_disaggregate_config.ffn_tp_size == 1,
+            ffn_disaggregate_config.ffn_tp_size == 1,
             "ffn_tp_size must be 1",
         )
         check_with_info(
-            self.config.gpt_init_params.ffn_disaggregate_config.ffn_dp_size == 1,
+            ffn_disaggregate_config.ffn_dp_size == 1,
             "ffn_dp_size must be 1",
         )
         self.attn_dp_rank: List[int] = [
             i
             for i in range(
-                self.config.gpt_init_params.ffn_disaggregate_config.attention_dp_size
+                ffn_disaggregate_config.attention_dp_size
             )
         ]
         self.attn_world_size = (
-            self.config.gpt_init_params.ffn_disaggregate_config.attention_dp_size
+            ffn_disaggregate_config.attention_dp_size
         )
         self.device = g_parallel_info.device
 
@@ -61,13 +70,16 @@ class DisaggregateModelBase(GptModelBase):
 class Qwen3GemmLayer(nn.Module):
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
         weights: ModelWeights,
         layer_idx: int,
         is_last_layer: bool,
+        quant_config: Optional[object] = None,
     ):
         super().__init__()
         self.config = config
+        self.parallelism_config = parallelism_config
         self.weights = weights
         self.layer_idx = layer_idx
         self.is_last_layer = is_last_layer
@@ -84,7 +96,7 @@ class Qwen3GemmLayer(nn.Module):
         self.post_attention_layernorm = RMSNorm(
             curent_layer_weights[W.post_ln_gamma], eps=config.layernorm_eps
         )
-        self.mlp = FusedSiluActDenseMLP(config, curent_layer_weights)
+        self.mlp = FusedSiluActDenseMLP(config, parallelism_config, curent_layer_weights, quant_config)
 
         # if last layer, then all weights are setted to None
         self.qkv_proj = None
@@ -134,10 +146,11 @@ class Qwen3GemmLayer(nn.Module):
 
 
 class Qwen3GemmPreLayer(nn.Module):
-    def __init__(self, config: GptInitModelParameters, weights: ModelWeights):
+    def __init__(self, config: ModelConfig, parallelism_config: ParallelismConfig, weights: ModelWeights):
         super().__init__()
         self.config = config
-        self.embed_tokens = Embedding(config, weights.get_global_weight(W.embedding))
+        self.parallelism_config = parallelism_config
+        self.embed_tokens = Embedding(config, parallelism_config, weights.get_global_weight(W.embedding))
         self.input_layernorm = RMSNorm(
             weights.weights[0][W.pre_ln_gamma], eps=config.layernorm_eps
         )
@@ -164,19 +177,28 @@ class Qwen3GemmPreLayer(nn.Module):
 
 
 class Qwen3GemmModel(DisaggregateModelBase):
-    def __init__(self, config: GptInitModelParameters, weights: ModelWeights):
-        super().__init__(config, weights)
+    def __init__(
+        self, 
+        config: ModelConfig, 
+        parallelism_config: ParallelismConfig,
+        ffn_disaggregate_config: FfnDisAggregateConfig,
+        device_resource_config: DeviceResourceConfig,
+        weights: ModelWeights, 
+        quant_config: Optional[object] = None,
+        vocab_size: int = 0
+    ):
+        super().__init__(config, parallelism_config, ffn_disaggregate_config, device_resource_config, weights, vocab_size)
         self.layers = nn.ModuleList(
             [
-                Qwen3GemmLayer(config, weights, idx, idx == self.layer_num - 1)
+                Qwen3GemmLayer(config, parallelism_config, weights, idx, idx == self.layer_num - 1, quant_config)
                 for idx in range(self.layer_num)
             ]
         )
-        self.pre_layer = Qwen3GemmPreLayer(config, weights)
+        self.pre_layer = Qwen3GemmPreLayer(config, parallelism_config, weights)
         self.dp_rank = [
-            self.config.gpt_init_params.ffn_disaggregate_config.attention_tp_size * i
+            ffn_disaggregate_config.attention_tp_size * i
             for i in range(
-                self.config.gpt_init_params.ffn_disaggregate_config.attention_dp_size
+                ffn_disaggregate_config.attention_dp_size
             )
         ]
 
@@ -230,8 +252,8 @@ class Qwen3GemmModel(DisaggregateModelBase):
         t = torch.empty(
             [
                 total_token_num,
-                self.config.gpt_init_params.head_num
-                * self.config.gpt_init_params.size_per_head,
+                self.config.head_num
+                * self.config.size_per_head,
             ],
             device=self.device,
             dtype=torch.half,
@@ -274,17 +296,25 @@ class Qwen3GemmModel(DisaggregateModelBase):
 
 
 class Qwen3AttnModel(DisaggregateModelBase):
-    def __init__(self, config: GptInitModelParameters, weights: ModelWeights):
-        super().__init__(config, weights)
+    def __init__(
+        self, 
+        config: ModelConfig, 
+        parallelism_config: ParallelismConfig,
+        ffn_disaggregate_config: FfnDisAggregateConfig,
+        device_resource_config: DeviceResourceConfig,
+        weights: ModelWeights,
+        vocab_size: int = 0
+    ):
+        super().__init__(config, parallelism_config, ffn_disaggregate_config, device_resource_config, weights, vocab_size)
         self.attention_layers = nn.ModuleList(
             [
-                CausalAttentionPure(config, weights.weights[idx])
+                CausalAttentionPure(config, parallelism_config, weights.weights[idx])
                 for idx in range(self.layer_num)
             ]
         )
         self.ffn_service_rank = (
-            config.gpt_init_params.ffn_disaggregate_config.attention_dp_size
-            * config.gpt_init_params.ffn_disaggregate_config.attention_tp_size
+            ffn_disaggregate_config.attention_dp_size
+            * ffn_disaggregate_config.attention_tp_size
         )
         self.norm = RMSNorm(
             weights.get_global_weight(W.final_ln_gamma), eps=config.layernorm_eps
@@ -306,10 +336,10 @@ class Qwen3AttnModel(DisaggregateModelBase):
             [
                 token_num,
                 (
-                    self.config.gpt_init_params.head_num
-                    + self.config.gpt_init_params.head_num_kv * 2
+                    self.config.head_num
+                    + self.config.head_num_kv * 2
                 )
-                * self.config.gpt_init_params.size_per_head,
+                * self.config.size_per_head,
             ],
             device=self.device,
             dtype=torch.half,
@@ -321,7 +351,7 @@ class Qwen3AttnModel(DisaggregateModelBase):
         t = torch.empty(
             [
                 token_num,
-                self.config.gpt_init_params.hidden_size,
+                self.config.hidden_size,
             ],
             device=self.device,
             dtype=torch.half,
@@ -360,15 +390,30 @@ class Qwen3AttnModel(DisaggregateModelBase):
 
 
 class Qwen3DisaggregateModel(GptModelBase):
-    def __init__(self, config: GptInitModelParameters, weights: ModelWeights):
-        super().__init__(config, weights)
+    def __init__(
+        self, 
+        config: ModelConfig, 
+        parallelism_config: ParallelismConfig,
+        ffn_disaggregate_config: FfnDisAggregateConfig,
+        device_resource_config: DeviceResourceConfig,
+        weights: ModelWeights, 
+        quant_config: Optional[object] = None,
+        vocab_size: int = 0,
+        fmha_config=None,
+        py_hw_kernel_config=None,
+    ):
+        super().__init__(config, parallelism_config, device_resource_config, weights, vocab_size, fmha_config=fmha_config, py_hw_kernel_config=py_hw_kernel_config)
         self.is_ffn_model = (
-            config.gpt_init_params.ffn_disaggregate_config.is_ffn_service()
+            ffn_disaggregate_config.is_ffn_service()
         )
         if self.is_ffn_model:
-            self.model = Qwen3GemmModel(config, weights)
+            self.model = Qwen3GemmModel(
+                config, parallelism_config, ffn_disaggregate_config, device_resource_config, weights, quant_config, vocab_size
+            )
         else:
-            self.model = Qwen3AttnModel(config, weights)
+            self.model = Qwen3AttnModel(
+                config, parallelism_config, ffn_disaggregate_config, device_resource_config, weights, vocab_size
+            )
 
         self.norm = RMSNorm(
             weights.get_global_weight(W.final_ln_gamma), eps=config.layernorm_eps
diff --git a/rtp_llm/models_py/model_desc/generic_moe.py b/rtp_llm/models_py/model_desc/generic_moe.py
index 689320204..997db87ad 100644
--- a/rtp_llm/models_py/model_desc/generic_moe.py
+++ b/rtp_llm/models_py/model_desc/generic_moe.py
@@ -4,7 +4,8 @@ from typing import Dict, Optional
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.ops import ParallelismConfig, MoeConfig, RuntimeConfig
 from rtp_llm.model_loader.model_weight_info import ModelWeights
 from rtp_llm.models_py.model_desc.module_base import GptModelBase
 from rtp_llm.models_py.modules.attention import CausalAttention
@@ -13,6 +14,7 @@ from rtp_llm.models_py.modules.fmha import FMHAImplBase
 from rtp_llm.models_py.modules.linear import Linear
 from rtp_llm.models_py.modules.mlp import FusedSiluActDenseMLP
 from rtp_llm.models_py.modules.moe import FusedMoe
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 from rtp_llm.models_py.modules.moe.fused_moe_factory import FusedMoeFactory
 from rtp_llm.models_py.modules.norm import RMSNorm
 from rtp_llm.ops import KVCache, PyAttentionInputs, PyModelInputs, PyModelOutputs
@@ -28,10 +30,11 @@ class GenericMoeLayer(nn.Module):
     """Generic MoE layer supporting both Qwen3 and internal model."""
 
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, parallelism_config: ParallelismConfig, weights: Dict[str, torch.Tensor], quant_config: Optional[object] = None
     ):
         super().__init__()
         self.config = config
+        self.parallelism_config = parallelism_config
 
         self.hidden_dim = config.hidden_size
         self.ffn_dim = config.moe_inter_padding_size
@@ -39,8 +42,22 @@ class GenericMoeLayer(nn.Module):
         self.top_k = config.moe_k
 
         self.gate = Linear(weights[W.moe_gate], None)
+        # ModelConfig inherits from CppModelConfig (ModelConfig), so can be passed directly
         self.select_topk_op = SelectTopkOp(config)
-        self.fused_moe: FusedMoe = FusedMoeFactory.create_fused_moe(config, weights)
+        
+                # Use MoEConfigAdapter to provide unified interface
+        # Get MoeConfig and RuntimeConfig from ops (they have defaults)
+        moe_config = MoeConfig()
+        runtime_config = RuntimeConfig()
+        # Create adapter that provides shortcut access to config fields
+        config_adapter = MoEConfigAdapter(
+            py_model_config=config,
+            parallelism_config=parallelism_config,
+            moe_config=moe_config,
+            runtime_config=runtime_config,
+            quant_config=quant_config,
+        )
+        self.fused_moe: FusedMoe = FusedMoeFactory.create_fused_moe(config_adapter, weights)
 
         self.w1 = weights.get(W.moe_w1, None)
         self.w2 = weights.get(W.moe_w2, None)
@@ -55,7 +72,7 @@ class GenericMoeLayer(nn.Module):
         num_local_experts = self.num_local_experts
         global_num_experts = self.num_experts
         expert_map = torch.full((global_num_experts,), fill_value=-1, dtype=torch.int32)
-        start_id = self.config.ep_rank * num_local_experts
+        start_id = self.parallelism_config.ep_rank * num_local_experts
         end_id = start_id + num_local_experts
         expert_map[start_id:end_id] = torch.tensor(list(range(num_local_experts)))
         return expert_map.to(device=torch.cuda.current_device(), dtype=torch.int32)
@@ -92,28 +109,30 @@ class GenericMoeDecoderLayer(nn.Module):
 
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
         weights: Dict[str, torch.Tensor],
         layer_idx: int,
+        quant_config: Optional[object] = None,
     ):
         super().__init__()
         self.layer_idx = layer_idx
-        self.self_attn = CausalAttention(config, weights)
+        self.self_attn = CausalAttention(config, parallelism_config, weights, quant_config)
 
         # Determine if this is a Dense layer (before first MoE layer or dense only)
         if layer_idx not in config.moe_layer_index:
             self.is_dense_layer = True
         else:
             self.is_dense_layer = False
-            self.moe_mlp = GenericMoeLayer(config, weights)
+            self.moe_mlp = GenericMoeLayer(config, parallelism_config, weights, quant_config)
 
-        self.add_shared_expert = getattr(config, "moe_style", 1) == 2
+        self.add_shared_expert = config.moe_style == 2
 
         # Try to create shared_mlp and catch errors if weights don't exist
         self.shared_mlp = None
         if self.is_dense_layer or self.add_shared_expert:
             try:
-                self.shared_mlp = FusedSiluActDenseMLP(config, weights)
+                self.shared_mlp = FusedSiluActDenseMLP(config, parallelism_config, weights, quant_config)
             except (KeyError, AssertionError) as e:
                 # If weights don't exist, shared_mlp remains None
                 logging.warning(
@@ -168,17 +187,27 @@ class GenericMoeDecoderLayer(nn.Module):
 class GenericMoeModel(GptModelBase):
     """Generic MoE model supporting Qwen3-MoE, internal model, and other MoE architectures."""
 
-    def __init__(self, config: GptInitModelParameters, weights: ModelWeights):
-        super().__init__(config, weights)
-        self.embed_tokens = Embedding(config, weights.get_global_weight(W.embedding))
+    def __init__(
+        self,
+        py_model_config: ModelConfig,
+        parallelism_config: ParallelismConfig,
+        device_resource_config,
+        weights: ModelWeights,
+        vocab_size: int,
+        quant_config: Optional[object] = None,
+        fmha_config=None,
+        py_hw_kernel_config=None,
+    ):
+        super().__init__(py_model_config, parallelism_config, device_resource_config, weights, vocab_size, fmha_config=fmha_config, py_hw_kernel_config=py_hw_kernel_config)
+        self.embed_tokens = Embedding(py_model_config, parallelism_config, weights.get_global_weight(W.embedding))
         self.layers = nn.ModuleList(
             [
-                GenericMoeDecoderLayer(config, weights.weights[idx], idx)
+                GenericMoeDecoderLayer(py_model_config, parallelism_config, weights.weights[idx], idx, quant_config)
                 for idx in range(self.layer_num)
             ]
         )
         self.norm = RMSNorm(
-            weights.get_global_weight(W.final_ln_gamma), eps=config.layernorm_eps
+            weights.get_global_weight(W.final_ln_gamma), eps=py_model_config.layernorm_eps
         )
 
     def forward(self, inputs: PyModelInputs) -> PyModelOutputs:
diff --git a/rtp_llm/models_py/model_desc/module_base.py b/rtp_llm/models_py/model_desc/module_base.py
index 8e4ced635..3f2b9397b 100644
--- a/rtp_llm/models_py/model_desc/module_base.py
+++ b/rtp_llm/models_py/model_desc/module_base.py
@@ -3,11 +3,12 @@ from typing import Any, Optional
 
 from torch import Tensor, nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_loader.model_weight_info import ModelWeights
 from rtp_llm.models_py.modules import DECODE_MHA_IMPS, PREFILL_MHA_IMPS, FMHAImplBase
 from rtp_llm.models_py.modules.fmha import DECODE_MLA_IMPS, PREFILL_MLA_IMPS
 from rtp_llm.ops import (
+    DeviceResourceConfig,
     DeviceType,
     KVCache,
     PyAttentionInputs,
@@ -20,20 +21,30 @@ from rtp_llm.utils.model_weight import W
 
 
 class GptModelBase(nn.Module):
-    def __init__(self, config: GptInitModelParameters, weight: ModelWeights) -> None:
+    def __init__(
+        self, 
+        config: ModelConfig, 
+        parallelism_config,
+        device_resource_config: DeviceResourceConfig,
+        weight: ModelWeights,
+        vocab_size: int,
+        fmha_config=None,  # Optional FMHAConfig
+        py_hw_kernel_config=None,  # Optional HWKernelConfig
+    ) -> None:
         super().__init__()
         self.config = config
+        self.parallelism_config = parallelism_config
+        self.device_resource_config = device_resource_config
         self.weight = weight
+        self.fmha_config = fmha_config
+        self.py_hw_kernel_config = py_hw_kernel_config
 
-        self.layer_num: int = config.layer_num
-        self.vocab_size: int = config.vocab_size
+        self.layer_num: int = config.num_layers
+        self.vocab_size: int = vocab_size
 
         self.kv_cache: Optional[KVCache] = None
         self.device_type: DeviceType = get_device().get_device_type()
 
-        self.micro_batch_size: int = (
-            1 if config.device_resource_config.enable_layer_micro_batch == 0 else 2
-        )
         ## (batch_size -> fmha_params)
         self.params_dict: dict[int, Any] = {}
 
@@ -84,6 +95,7 @@ class GptModelBase(nn.Module):
             cos_sin_cache = self.weight.get_global_weight(W.rope_cos_sin_cache)
             impl = fmha_impl(
                 self.config,
+                self.parallelism_config,
                 attn_inputs,
                 self.weight.weights,
                 cos_sin_cache,
@@ -93,9 +105,36 @@ class GptModelBase(nn.Module):
         raise Exception(f"can not find fmha type")
 
     def get_fmha_impl(self, attn_inputs: PyAttentionInputs) -> FMHAImplBase:
+        from rtp_llm.ops import FMHAType
         mha_impls = PREFILL_MHA_IMPS if attn_inputs.is_prefill else DECODE_MHA_IMPS
         for fmha_impl in mha_impls:
-            impl = fmha_impl(self.config, attn_inputs)
+            # Check config at runtime to filter implementations
+            if self.fmha_config is not None:
+                fmha_type = fmha_impl.fmha_type()
+                # Skip FlashInfer if disabled
+                if fmha_type == FMHAType.FLASH_INFER and self.fmha_config.disable_flash_infer:
+                    continue
+                # Skip TRT FMHA if not enabled
+                if fmha_type == FMHAType.TRT_V2 and not self.fmha_config.enable_paged_trt_fmha:
+                    continue
+                # Skip XQA if not enabled
+                if fmha_type == FMHAType.XQA and not self.fmha_config.enable_xqa:
+                    continue
+            
+            # Pass config objects to implementation if it accepts them
+            # Some implementations may not accept these parameters yet
+            try:
+                # Try with config parameters first
+                impl = fmha_impl(
+                    self.config, 
+                    self.parallelism_config, 
+                    attn_inputs,
+                    fmha_config=self.fmha_config,
+                    py_hw_kernel_config=self.py_hw_kernel_config,
+                )
+            except TypeError:
+                # Fallback to old signature if implementation doesn't accept config parameters yet
+                impl = fmha_impl(self.config, self.parallelism_config, attn_inputs)
             if impl.support():
                 return impl
         raise Exception(f"can not find fmha type")
diff --git a/rtp_llm/models_py/model_desc/qwen3.py b/rtp_llm/models_py/model_desc/qwen3.py
index 24d106724..f097cd3df 100644
--- a/rtp_llm/models_py/model_desc/qwen3.py
+++ b/rtp_llm/models_py/model_desc/qwen3.py
@@ -3,24 +3,24 @@ from typing import Dict, Optional
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_loader.model_weight_info import ModelWeights
 from rtp_llm.models_py.model_desc.module_base import GptModelBase
 from rtp_llm.models_py.modules import FusedSiluActDenseMLP, RMSNorm
 from rtp_llm.models_py.modules.attention import CausalAttention
 from rtp_llm.models_py.modules.embedding import Embedding
 from rtp_llm.models_py.modules.fmha import FMHAImplBase
-from rtp_llm.ops import KVCache, PyAttentionInputs, PyModelInputs, PyModelOutputs
+from rtp_llm.ops import KVCache, ParallelismConfig, PyAttentionInputs, PyModelInputs, PyModelOutputs
 from rtp_llm.utils.model_weight import W
 
 
 class Qwen3DecoderLayer(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, parallelism_config: ParallelismConfig, weights: Dict[str, torch.Tensor], quant_config: Optional[object] = None
     ):
         super().__init__()
-        self.self_attn = CausalAttention(config, weights)
-        self.mlp = FusedSiluActDenseMLP(config, weights)
+        self.self_attn = CausalAttention(config, parallelism_config, weights, quant_config)
+        self.mlp = FusedSiluActDenseMLP(config, parallelism_config, weights, quant_config)
         self.input_layernorm = RMSNorm(
             weights[W.pre_ln_gamma], eps=config.layernorm_eps
         )
@@ -53,13 +53,23 @@ class Qwen3DecoderLayer(nn.Module):
 
 
 class Qwen3Model(GptModelBase):
-    def __init__(self, config: GptInitModelParameters, weights: ModelWeights):
-        super().__init__(config, weights)
+    def __init__(
+        self, 
+        config: ModelConfig, 
+        parallelism_config: ParallelismConfig,
+        device_resource_config,
+        weights: ModelWeights, 
+        quant_config: Optional[object] = None,
+        vocab_size: int = 0,
+        fmha_config=None,
+        py_hw_kernel_config=None,
+    ):
+        super().__init__(config, parallelism_config, device_resource_config, weights, vocab_size, fmha_config=fmha_config, py_hw_kernel_config=py_hw_kernel_config)
 
-        self.embed_tokens = Embedding(config, weights.get_global_weight(W.embedding))
+        self.embed_tokens = Embedding(config, parallelism_config, weights.get_global_weight(W.embedding))
         self.layers = nn.ModuleList(
             [
-                Qwen3DecoderLayer(config, weights.weights[idx])
+                Qwen3DecoderLayer(config, parallelism_config, weights.weights[idx], quant_config)
                 for idx in range(self.layer_num)
             ]
         )
diff --git a/rtp_llm/models_py/modules/attention.py b/rtp_llm/models_py/modules/attention.py
index c2faba809..a32b26175 100644
--- a/rtp_llm/models_py/modules/attention.py
+++ b/rtp_llm/models_py/modules/attention.py
@@ -3,41 +3,45 @@ from typing import Dict, Optional
 import torch
 import torch.nn as nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.distribute.collective import Group, all_reduce
 from rtp_llm.models_py.modules import FusedQKRMSNorm
 from rtp_llm.models_py.modules.fmha import FMHAImplBase
 from rtp_llm.models_py.modules.linear_factory import LinearFactory
-from rtp_llm.ops import KVCache
+from rtp_llm.ops import KVCache, ParallelismConfig
 from rtp_llm.utils.model_weight import W
 
 
 class CausalAttention(nn.Module):
 
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, parallelism_config: ParallelismConfig, weights: Dict[str, torch.Tensor], quant_config: Optional[object] = None
     ):
         super().__init__()
         self.config = config
+        self.parallelism_config = parallelism_config
         self.head_dim = config.hidden_size // config.head_num
         self.head_num = config.head_num
         self.num_key_value_groups = config.head_num // config.head_num_kv
         self.q_size = config.head_num * self.head_dim
 
         # Create linear layers using LinearFactory
+        # Note: quant_config needs to be passed separately if available
         self.qkv_proj = LinearFactory.create_linear_from_weights(
-            weights, W.attn_qkv_w, W.attn_qkv_s, W.attn_qkv_b, config
+            weights, W.attn_qkv_w, W.attn_qkv_s, W.attn_qkv_b, 
+            py_model_config=config, gpt_init_params=None, quant_config=quant_config
         )
         self.o_proj = LinearFactory.create_linear_from_weights(
-            weights, W.attn_o_w, W.attn_o_s, W.attn_o_b, config
+            weights, W.attn_o_w, W.attn_o_s, W.attn_o_b,
+            py_model_config=config, gpt_init_params=None, quant_config=quant_config
         )
         self.qk_fuse_norm = None
         if W.q_ln_gamma in weights and W.k_ln_gamma in weights:
             self.qk_fuse_norm = FusedQKRMSNorm(
                 weights[W.q_ln_gamma],
                 weights[W.k_ln_gamma],
-                config.head_num // config.tp_size,
-                config.head_num_kv // config.tp_size,
+                config.head_num // parallelism_config.tp_size,
+                config.head_num_kv // parallelism_config.tp_size,
                 config.size_per_head,
                 config.layernorm_eps,
             )
@@ -56,6 +60,6 @@ class CausalAttention(nn.Module):
         attn_output = attn_output.reshape(*input_shape, -1).contiguous()
 
         output = self.o_proj(attn_output)
-        if self.config.tp_size > 1:
+        if self.parallelism_config.tp_size > 1:
             output = all_reduce(output, group=Group.TP)
         return output
diff --git a/rtp_llm/models_py/modules/attention_pure.py b/rtp_llm/models_py/modules/attention_pure.py
index 4ece81bdd..9fbc3b147 100644
--- a/rtp_llm/models_py/modules/attention_pure.py
+++ b/rtp_llm/models_py/modules/attention_pure.py
@@ -3,17 +3,18 @@ from typing import Dict, Optional
 import torch
 import torch.nn as nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.modules.fmha import FMHAImplBase
-from rtp_llm.ops import KVCache
+from rtp_llm.ops import KVCache, ParallelismConfig
 
 
 class CausalAttentionPure(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, parallelism_config: ParallelismConfig, weights: Dict[str, torch.Tensor]
     ):
         super().__init__()
         self.config = config
+        self.parallelism_config = parallelism_config
         self.head_dim = config.hidden_size // config.head_num
         self.head_num = config.head_num
         self.num_key_value_groups = config.head_num // config.head_num_kv
diff --git a/rtp_llm/models_py/modules/embedding.py b/rtp_llm/models_py/modules/embedding.py
index 97f6a30ff..6cb8efdae 100755
--- a/rtp_llm/models_py/modules/embedding.py
+++ b/rtp_llm/models_py/modules/embedding.py
@@ -2,9 +2,9 @@ import torch
 from torch import nn
 from torch.nn import functional as F
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.distribute.collective import Group, all_gather
-from rtp_llm.ops import rtp_llm_ops
+from rtp_llm.ops import ParallelismConfig, rtp_llm_ops
 
 
 class EmbeddingTorch(nn.Module):
@@ -17,10 +17,11 @@ class EmbeddingTorch(nn.Module):
 
 
 class Embedding(nn.Module):
-    def __init__(self, config: GptInitModelParameters, weight: torch.Tensor):
+    def __init__(self, config: ModelConfig, parallelism_config: ParallelismConfig, weight: torch.Tensor):
         super().__init__()
         self.weight = weight
         self.config = config
+        self.parallelism_config = parallelism_config
 
     def forward(self, input: torch.Tensor) -> torch.Tensor:
         tokens = input.size(0)
@@ -29,11 +30,11 @@ class Embedding(nn.Module):
             (tokens, hidden_size), dtype=self.weight.dtype, device=input.device
         )
         rtp_llm_ops.embedding(output, input, self.weight.data)
-        if self.config.tp_size > 1:
+        if self.parallelism_config.tp_size > 1:
             m, n = output.shape
             output = all_gather(output, group=Group.TP)
             output = (
-                output.reshape(self.config.tp_size, m, n)
+                output.reshape(self.parallelism_config.tp_size, m, n)
                 .transpose(0, 1)
                 .contiguous()
                 .reshape(m, -1)
@@ -42,10 +43,11 @@ class Embedding(nn.Module):
 
 
 class EmbeddingBert(nn.Module):
-    def __init__(self, config: GptInitModelParameters, weight: torch.Tensor):
+    def __init__(self, config: ModelConfig, parallelism_config: ParallelismConfig, weight: torch.Tensor):
         super().__init__()
         self.weight = weight
         self.config = config
+        self.parallelism_config = parallelism_config
 
     def forward(
         self,
@@ -72,11 +74,11 @@ class EmbeddingBert(nn.Module):
             token_type_embedding,
             input_embedding_scalar,
         )
-        if self.config.tp_size > 1:
+        if self.parallelism_config.tp_size > 1:
             m, n = output.shape
             output = all_gather(output, group=Group.TP)
             output = (
-                output.reshape(self.config.tp_size, m, n)
+                output.reshape(self.parallelism_config.tp_size, m, n)
                 .transpose(0, 1)
                 .contiguous()
                 .reshape(m, -1)
diff --git a/rtp_llm/models_py/modules/ep/ep_moe.py b/rtp_llm/models_py/modules/ep/ep_moe.py
index 2236be8a5..8c0bf49a5 100644
--- a/rtp_llm/models_py/modules/ep/ep_moe.py
+++ b/rtp_llm/models_py/modules/ep/ep_moe.py
@@ -8,19 +8,20 @@ from typing import Any, Dict, Optional
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.modules.moe.fused_moe import (
     ExpertForwardPayload,
     FusedMoeDataRouter,
     FusedMoeExpertExecutor,
     TopKWeightAndReduce,
 )
+from rtp_llm.ops import ParallelismConfig
 
 
 class EPDataRouter(FusedMoeDataRouter):
     """EP implementation of data router for expert selection and data dispatch."""
 
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         self.config = config
         self.top_k = config.moe_k
 
@@ -59,13 +60,21 @@ class EPDataRouter(FusedMoeDataRouter):
 class EPExpertExecutor(FusedMoeExpertExecutor):
     """EP implementation of expert executor."""
 
-    def __init__(self, config: GptInitModelParameters, weights, layer_idx: int):
+    def __init__(
+        self,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
+        weights,
+        layer_idx: int,
+        quant_config=None,
+    ):
         super().__init__(None)
         self.config = config
+        self.parallelism_config = parallelism_config
         # Use the proven legacy implementation for computation
         from rtp_llm.models_py.modules.ep.layers import LegacyEPMoE
 
-        self.legacy_ep_moe = LegacyEPMoE(config, weights, layer_idx)
+        self.legacy_ep_moe = LegacyEPMoE(config, parallelism_config, weights, layer_idx, quant_config)
 
     @property
     def local_num_experts(self) -> int:
@@ -152,19 +161,25 @@ class EPMoE(nn.Module):
 
 
 def create_ep_moe_instance(
-    config: GptInitModelParameters, weights, layer_idx: int
+    config: ModelConfig,
+    parallelism_config: ParallelismConfig,
+    weights,
+    layer_idx: int,
+    quant_config: Optional[object] = None,
 ) -> EPMoE:
     """
     Factory function for creating EP MoE instances.
 
     Args:
-        config: Model configuration parameters
+        config: Model configuration (ModelConfig)
+        parallelism_config: Parallelism configuration
         weights: Model weights
         layer_idx: Layer index
+        quant_config: Optional quantization configuration
 
     Returns:
         EPMoE instance with router and executor
     """
     router = EPDataRouter(config)
-    executor = EPExpertExecutor(config, weights, layer_idx)
+    executor = EPExpertExecutor(config, parallelism_config, weights, layer_idx, quant_config)
     return EPMoE(router=router, executor=executor)
diff --git a/rtp_llm/models_py/modules/ep/layers.py b/rtp_llm/models_py/modules/ep/layers.py
index cfcd4a6f5..03e2ec9df 100644
--- a/rtp_llm/models_py/modules/ep/layers.py
+++ b/rtp_llm/models_py/modules/ep/layers.py
@@ -17,9 +17,11 @@ else:
     rtp_llm_ops = None
     trt_fp8_quantize_128 = None
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.quant_config import QuantizationConfig
 from rtp_llm.model_loader.model_weight_info import ModelWeights
 from rtp_llm.models_py.modules.utils import ceil_div, dispose_tensor
+from rtp_llm.ops import ParallelismConfig
 from rtp_llm.utils.model_weight import W
 
 if utils.is_cuda():
@@ -94,13 +96,15 @@ class GroupedGemmRunner(torch.nn.Module):
 class FusedMoE(torch.nn.Module):
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
         weights: Dict[str, torch.Tensor],
         layer_id: int,
     ):
         super().__init__()
 
         self.config = config
+        self.parallelism_config = parallelism_config
         self.layer_id = layer_id
         self.hidden_dim = config.hidden_size
         self.ffn_dim = config.moe_inter_padding_size
@@ -109,7 +113,7 @@ class FusedMoE(torch.nn.Module):
         self.up_proj = weights.get(W.moe_w1, None)
         self.down_proj = weights.get(W.moe_w2, None)
         self.select_topk_op = SelectTopkOp(config)
-        self.fused_moe_op = FusedMoEOp(config)
+        self.fused_moe_op = FusedMoEOp(config, parallelism_config)
 
     def forward(
         self,
@@ -155,17 +159,20 @@ class LegacyEPMoE(torch.nn.Module):
 
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
         weights: Dict[str, torch.Tensor],
         layer_id: int,
+        quant_config: Optional[QuantizationConfig] = None,
     ):
         super().__init__()
 
         self.config = config
+        self.parallelism_config = parallelism_config
         self.layer_id = layer_id
 
-        self.tp_size = config.tp_size
-        self.tp_rank = config.tp_rank
+        self.tp_size = parallelism_config.tp_size
+        self.tp_rank = parallelism_config.tp_rank
 
         self.num_experts = config.expert_num
         assert self.num_experts % self.tp_size == 0
@@ -179,10 +186,10 @@ class LegacyEPMoE(torch.nn.Module):
         self.renormalize = True
 
         # Check if FP8 quantization should be used
-        if self.config.quant_config:
+        if quant_config:
             self.use_fp8_w8a8 = True
-            self.use_block_quant = self.config.quant_config.group_size() > 0
-            group_size = self.config.quant_config.group_size()
+            self.use_block_quant = quant_config.group_size() > 0
+            group_size = quant_config.group_size()
             self.block_shape = [group_size, group_size] if group_size > 0 else None
             self.fp8_dtype = torch.float8_e4m3fn
             self.activation_scheme = "dynamic"  # Default to dynamic activation scheme
diff --git a/rtp_llm/models_py/modules/fmha.py b/rtp_llm/models_py/modules/fmha.py
index 3614ed352..015f78fd6 100644
--- a/rtp_llm/models_py/modules/fmha.py
+++ b/rtp_llm/models_py/modules/fmha.py
@@ -20,9 +20,10 @@ try:
 except ImportError:
     logging.info("rope kv cache not available, skipped.")
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.model_config import ModelConfig as PyModelConfig
 from rtp_llm.models_py.modules.kvcache_store import WriteCacheStoreOp
-from rtp_llm.ops import FMHAType, KVCache, ParamsBase, PyAttentionInputs
+from rtp_llm.ops import FMHAType, KVCache, ParamsBase, ParallelismConfig, PyAttentionInputs
 
 
 class FMHAImplBase(object):
@@ -124,11 +125,13 @@ try:
     class FlashInferPrefillImpl(FMHAPrefillImplBase):
 
         def __init__(
-            self, config: GptInitModelParameters, attn_inputs: PyAttentionInputs
+            self, config: PyModelConfig, parallelism_config: ParallelismConfig, attn_inputs: PyAttentionInputs
         ) -> None:
+            # PyModelConfig inherits from CppModelConfig (ModelConfig), so can be passed directly
+            attn_configs = config.getAttentionConfigs(parallelism_config.tp_size)
             super().__init__(
-                FlashInferPrefillOp(config.gpt_init_params),
-                FusedRopeKVCachePrefillOp(config.gpt_init_params),
+                FlashInferPrefillOp(attn_configs),
+                FusedRopeKVCachePrefillOp(attn_configs),
                 attn_inputs,
             )
             self.support_ = self.support_ and (config.use_mla == False)
@@ -140,13 +143,15 @@ try:
         def support_cuda_graph(self) -> bool:
             return True
 
+    # Always append FlashInferPrefillImpl, check config at runtime
     PREFILL_MHA_IMPS.append(FlashInferPrefillImpl)
 
     class MlaFlashInferPrefillImpl(FMHAPrefillImplBase):
 
         def __init__(
             self,
-            config: GptInitModelParameters,
+            config: PyModelConfig,
+            parallelism_config: ParallelismConfig,
             attn_inputs: PyAttentionInputs,
             weights: List[Dict[str, torch.Tensor]],
             cos_sin_cache: torch.Tensor,
@@ -155,6 +160,7 @@ try:
             super().__init__(
                 MlaFlashInferPrefillOp(
                     config,
+                    parallelism_config,
                     config.head_num,
                     config.kv_lora_rank,
                     config.rope_head_dim,
@@ -163,6 +169,7 @@ try:
                     config.softmax_extra_scale,
                     config.use_mla,
                     weights,
+                    quant_config=None,  # TODO: pass quant_config if available
                 ),
                 # TrtV2PrefillAttentionOp(
                 #     config,
@@ -226,11 +233,13 @@ try:
     class FlashInferDecodeImpl(FMHADecodeImplBase):
 
         def __init__(
-            self, config: GptInitModelParameters, attn_inputs: PyAttentionInputs
+            self, config: PyModelConfig, parallelism_config: ParallelismConfig, attn_inputs: PyAttentionInputs
         ) -> None:
+            # PyModelConfig inherits from CppModelConfig (ModelConfig), so can be passed directly
+            attn_configs = config.getAttentionConfigs(parallelism_config.tp_size)
             super().__init__(
-                FlashInferDecodeOp(config.gpt_init_params),
-                FusedRopeKVCacheDecodeOp(config.gpt_init_params),
+                FlashInferDecodeOp(attn_configs),
+                FusedRopeKVCacheDecodeOp(attn_configs),
                 attn_inputs,
             )
             self.support_ = self.support_ and (config.use_mla == False)
@@ -242,13 +251,15 @@ try:
         def support_cuda_graph(self) -> bool:
             return True
 
+    # Always append FlashInferDecodeImpl, check config at runtime
     DECODE_MHA_IMPS.append(FlashInferDecodeImpl)
 
     class MlaFlashInferDecodeImpl(FMHADecodeImplBase):
 
         def __init__(
             self,
-            config: GptInitModelParameters,
+            config: PyModelConfig,
+            parallelism_config: ParallelismConfig,
             attn_inputs: PyAttentionInputs,
             weights: List[Dict[str, torch.Tensor]],
             cos_sin_cache: torch.Tensor,
@@ -321,11 +332,13 @@ try:
     class TRTMHAImpl(FMHAPrefillImplBase):
 
         def __init__(
-            self, config: GptInitModelParameters, attn_inputs: PyAttentionInputs
+            self, config: PyModelConfig, parallelism_config: ParallelismConfig, attn_inputs: PyAttentionInputs
         ) -> None:
+            # PyModelConfig inherits from CppModelConfig (ModelConfig), so can be passed directly
+            attn_configs = config.getAttentionConfigs(parallelism_config.tp_size)
             super().__init__(
-                TRTAttnOp(config.gpt_init_params),
-                FusedRopeKVCachePrefillOp(config.gpt_init_params),
+                TRTAttnOp(attn_configs),
+                FusedRopeKVCachePrefillOp(attn_configs),
                 attn_inputs,
             )
 
@@ -336,8 +349,8 @@ try:
         def support_cuda_graph(self) -> bool:
             return True
 
+    # Always append TRTMHAImpl, check config at runtime
     PREFILL_MHA_IMPS.append(TRTMHAImpl)
-    # PREFILL_MHA_IMPS.insert(0, TRTMHAImpl)
 
 except ImportError:
     logging.info("TRTMHAImpl not available, skipped.")
@@ -349,11 +362,13 @@ try:
     class XQAImpl(FMHADecodeImplBase):
 
         def __init__(
-            self, config: GptInitModelParameters, attn_inputs: PyAttentionInputs
+            self, config: PyModelConfig, parallelism_config: ParallelismConfig, attn_inputs: PyAttentionInputs
         ) -> None:
+            # PyModelConfig inherits from CppModelConfig (ModelConfig), so can be passed directly
+            attn_configs = config.getAttentionConfigs(parallelism_config.tp_size)
             super().__init__(
-                XQAAttnOp(config.gpt_init_params),
-                FusedRopeKVCacheDecodeOp(config.gpt_init_params),
+                XQAAttnOp(attn_configs),
+                FusedRopeKVCacheDecodeOp(attn_configs),
                 attn_inputs,
             )
 
@@ -364,6 +379,7 @@ try:
         def support_cuda_graph(self) -> bool:
             return True
 
+    # Always append XQAImpl, check config at runtime
     DECODE_MHA_IMPS.append(XQAImpl)
 except ImportError:
     logging.info("XQAAttnOp not available, skipped.")
diff --git a/rtp_llm/models_py/modules/linear_factory.py b/rtp_llm/models_py/modules/linear_factory.py
index 99a77ea2a..1d1803f4f 100644
--- a/rtp_llm/models_py/modules/linear_factory.py
+++ b/rtp_llm/models_py/modules/linear_factory.py
@@ -7,7 +7,7 @@ from typing import Dict, Optional
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.modules import utils
 from rtp_llm.models_py.modules.linear import Linear
 
@@ -31,7 +31,7 @@ class LinearFactory:
 
     @staticmethod
     def should_use_fp8_linear(
-        config: GptInitModelParameters,
+        quant_config: Optional[object],
         weights: Dict[str, torch.Tensor],
         weight_key: str,
     ) -> bool:
@@ -39,12 +39,12 @@ class LinearFactory:
         if not FP8_LINEAR_AVAILABLE:
             return False
 
-        if not hasattr(config, "quant_config") or config.quant_config is None:
+        if quant_config is None:
             return False
 
         # Check quantization method if available
-        if hasattr(config.quant_config, "get_method"):
-            quant_method = config.quant_config.get_method()
+        if hasattr(quant_config, "get_method"):
+            quant_method = quant_config.get_method()
             fp8_methods = [
                 "FP8",
                 "FP8_PER_BLOCK",
@@ -67,7 +67,7 @@ class LinearFactory:
         weight_scales: Optional[torch.Tensor] = None,
         input_scales: Optional[torch.Tensor] = None,
         bias: Optional[torch.Tensor] = None,
-        config: Optional[GptInitModelParameters] = None,
+        quant_config: Optional[object] = None,
         force_fp8: bool = False,
     ) -> nn.Module:
         """Create Linear layer (FP8 or regular)."""
@@ -76,23 +76,22 @@ class LinearFactory:
         ):
             if weight_scales is None:
                 raise ValueError("FP8 linear layer requires weight_scales")
-            if config is None:
-                raise ValueError("FP8 linear layer requires config")
+            if quant_config is None:
+                raise ValueError("FP8 linear layer requires quant_config")
             else:
-                quant_config = config.quant_config
                 if quant_config.get_method() in [
                     "FP8_PER_TENSOR_COMPRESSED",
                     "FP8_DYNAMIC_PER_TENSOR",
                 ]:
                     return Fp8PerTensorLinear(
-                        config.quant_config, weight, weight_scales, input_scales, bias
+                        quant_config, weight, weight_scales, input_scales, bias
                     )
                 else:
                     if not FP8_LINEAR_AVAILABLE:
                         raise RuntimeError(
                             "FP8 DeepGEMMLinear layer requested but not available"
                         )
-                    return Fp8DeepGEMMLinear(weight, weight_scales, bias, config)
+                    return Fp8DeepGEMMLinear(weight, weight_scales, bias)
         else:
             return Linear(weight, bias)
 
@@ -102,7 +101,8 @@ class LinearFactory:
         weight_key: str,
         scale_key: Optional[str] = None,
         bias_key: Optional[str] = None,
-        config: Optional[GptInitModelParameters] = None,
+        py_model_config: Optional[ModelConfig] = None,
+        quant_config: Optional[object] = None,  # QuantizationConfig for quantization settings
     ) -> nn.Module:
         """Create Linear layer from weight dictionary."""
         weight = weights[weight_key]
@@ -110,13 +110,13 @@ class LinearFactory:
         bias = weights.get(bias_key)
 
         # Auto-detect FP8 usage
-        use_fp8 = LinearFactory.should_use_fp8_linear(config, weights, weight_key)
+        use_fp8 = LinearFactory.should_use_fp8_linear(quant_config, weights, weight_key)
 
         return LinearFactory.create_linear(
             weight=weight,
             weight_scales=weight_scales,
             bias=bias,
-            config=config,
+            quant_config=quant_config,
             force_fp8=use_fp8,
         )
 
@@ -126,12 +126,13 @@ class LinearFactory:
         weight_keys: list,
         scale_keys: Optional[list] = None,
         bias_keys: Optional[list] = None,
-        config: Optional[GptInitModelParameters] = None,
+        py_model_config: Optional[ModelConfig] = None,
+        quant_config: Optional[object] = None,
         dim: int = -1,
     ) -> nn.Module:
         """Create merged Linear layer (e.g., gate_up_proj)."""
         # Check FP8 usage based on first weight
-        use_fp8 = LinearFactory.should_use_fp8_linear(config, weights, weight_keys[0])
+        use_fp8 = LinearFactory.should_use_fp8_linear(quant_config, weights, weight_keys[0])
 
         # Merge weights
         weight_tensors = [weights[key] for key in weight_keys]
@@ -159,6 +160,6 @@ class LinearFactory:
             weight=merged_weight,
             weight_scales=merged_scales,
             bias=merged_bias,
-            config=config,
+            quant_config=quant_config,
             force_fp8=use_fp8,
         )
diff --git a/rtp_llm/models_py/modules/mla/flashinfer_mla.py b/rtp_llm/models_py/modules/mla/flashinfer_mla.py
index dcc501257..b4fefdd83 100644
--- a/rtp_llm/models_py/modules/mla/flashinfer_mla.py
+++ b/rtp_llm/models_py/modules/mla/flashinfer_mla.py
@@ -38,11 +38,9 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.modules.linear_factory import LinearFactory
-
-# from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.ops import KVCache, PyAttentionInputs, rtp_llm_ops
+from rtp_llm.ops import KVCache, ParallelismConfig, PyAttentionInputs, rtp_llm_ops
 from rtp_llm.utils.model_weight import W
 
 
@@ -64,7 +62,8 @@ def check_attention_inputs(attention_inputs: PyAttentionInputs) -> None:
 class MlaFlashInferPrefillOp(object):
     def __init__(
         self,
-        config: GptInitModelParameters,  # for LinearFactory
+        config: ModelConfig,
+        parallelism_config,
         num_heads: int,
         kv_lora_rank: int,
         qk_rope_head_dim: int,
@@ -73,11 +72,14 @@ class MlaFlashInferPrefillOp(object):
         softmax_extra_scale: float,
         use_mla: bool,
         weights: List[Dict[str, torch.Tensor]] | None,
+        quant_config: Optional[object] = None,
     ):
         super().__init__()
         if weights is None:
             raise Exception(f"MlaAbsorbAttention need weights but got none")
         self.config = config
+        self.parallelism_config = parallelism_config
+        self.quant_config = quant_config
         self.num_heads = num_heads
         self.kv_lora_rank = kv_lora_rank
         self.qk_rope_head_dim = qk_rope_head_dim
@@ -122,11 +124,13 @@ class MlaFlashInferPrefillOp(object):
 
         k_pe = k_pe.view(-1, 1, self.qk_rope_head_dim)
         self.k_nope_proj = LinearFactory.create_linear_from_weights(
-            self.weights[layer_id], W.mla_k_nope_w, W.mla_k_nope_s, None, self.config
+            self.weights[layer_id], W.mla_k_nope_w, W.mla_k_nope_s, None,
+            py_model_config=self.config, gpt_init_params=None, quant_config=self.quant_config
         )
 
         self.v_proj = LinearFactory.create_linear_from_weights(
-            self.weights[layer_id], W.mla_v_w, W.mla_v_s, None, self.config
+            self.weights[layer_id], W.mla_v_w, W.mla_v_s, None,
+            py_model_config=self.config, gpt_init_params=None, quant_config=self.quant_config
         )
 
         k_nope = self.k_nope_proj(compressed_kv)
@@ -277,13 +281,16 @@ class MlaFlashInferDecodeOp(object):
 class TrtV2PrefillAttentionOp(object):
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: ModelConfig,
+        parallelism_config,
         num_heads: int,
         kv_lora_rank: int,
         qk_rope_head_dim: int,
         qk_nope_head_dim: int,
         use_mla: bool,
         weights: List[Dict[str, torch.Tensor]] | None,
+        fmha_config,
+        quant_config: Optional[object] = None,
     ):
         super().__init__()
         self.num_heads = num_heads
@@ -292,13 +299,21 @@ class TrtV2PrefillAttentionOp(object):
         self.qk_nope_head_dim = qk_nope_head_dim
         self.scale = (self.qk_nope_head_dim + self.qk_rope_head_dim) ** -0.5
         self.config = config
+        self.parallelism_config = parallelism_config
+        self.quant_config = quant_config
         self.weights = weights
-        self.use_mla = use_mla
+        self.use_mla = use_mla   
+        # Get FMHAConfig - will check in support() method
+        self.fmha_config = fmha_config
+        
         from libth_transformer.rtp_llm_ops import TRTAttnOp
-
-        self.fmha_impl = TRTAttnOp(self.config)
+        # ModelConfig inherits from CppModelConfig (ModelConfig), so can be passed directly
+        self.fmha_impl = TRTAttnOp(config, parallelism_config)
 
     def support(self, attention_inputs: PyAttentionInputs):
+        # Check if TRT FMHA is enabled
+        if not self.fmha_config.enable_paged_trt_fmha:
+            return False
         return (
             self.use_mla
             and attention_inputs.is_prefill
@@ -319,11 +334,13 @@ class TrtV2PrefillAttentionOp(object):
 
         k_pe = k_pe.view(-1, 1, self.qk_rope_head_dim)
         self.k_nope_proj = LinearFactory.create_linear_from_weights(
-            self.weights[layer_id], W.mla_k_nope_w, W.mla_k_nope_s, None, self.config
+            self.weights[layer_id], W.mla_k_nope_w, W.mla_k_nope_s, None,
+            py_model_config=self.config, gpt_init_params=None, quant_config=self.quant_config
         )
 
         self.v_proj = LinearFactory.create_linear_from_weights(
-            self.weights[layer_id], W.mla_v_w, W.mla_v_s, None, self.config
+            self.weights[layer_id], W.mla_v_w, W.mla_v_s, None,
+            py_model_config=self.config, gpt_init_params=None, quant_config=self.quant_config
         )
 
         k_nope = self.k_nope_proj(compressed_kv)
@@ -363,7 +380,8 @@ class TrtV2PrefillAttentionOp(object):
 class TrtV2PrefillAttention(nn.Module):
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config,
+        parallelism_config,
         num_heads: int,
         kv_lora_rank: int,
         qk_rope_head_dim: int,
@@ -384,6 +402,7 @@ class TrtV2PrefillAttention(nn.Module):
         self.v_weight = v_weight
         self.k_nope_weight = k_nope_weight
         self.config = config
+        self.parallelism_config = parallelism_config
     def forward(
         self,
         q_nope: torch.Tensor,
@@ -409,7 +428,7 @@ class TrtV2PrefillAttention(nn.Module):
         pad_len = self.qk_rope_head_dim
         value_states = F.pad(value_states, (0, pad_len))
         from libth_transformer.rtp_llm_ops import TRTAttnOp
-        self.fmha_impl = TRTAttnOp(self.config)
+        self.fmha_impl = TRTAttnOp(self.config, self.parallelism_config)
         self.support_: bool = self.fmha_impl.support(attention_inputs)
         if self.support_:
             self.fmha_params = self.fmha_impl.prepare(attention_inputs)
diff --git a/rtp_llm/models_py/modules/mla/mla_attention.py b/rtp_llm/models_py/modules/mla/mla_attention.py
index e67d928a7..63f36e2a6 100644
--- a/rtp_llm/models_py/modules/mla/mla_attention.py
+++ b/rtp_llm/models_py/modules/mla/mla_attention.py
@@ -3,11 +3,11 @@ from typing import Any, Dict, Optional
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.distribute.collective import Group, all_reduce
 from rtp_llm.models_py.modules.linear_factory import LinearFactory
 from rtp_llm.models_py.modules.norm import RMSNorm
-from rtp_llm.ops import KVCache
+from rtp_llm.ops import KVCache, ParallelismConfig
 from rtp_llm.utils.model_weight import W
 
 
@@ -16,32 +16,37 @@ class DeepSeekV2Attention(nn.Module):
 
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
         weights: Dict[str, torch.Tensor],
         layer_idx: int,
+        quant_config: Optional[object] = None,
     ):
         super().__init__()
         self.config = config
-        self.num_heads = self.config.head_num
-        self.qk_nope_head_dim = self.config.nope_head_dim
-        self.qk_rope_head_dim = self.config.rope_head_dim
+        self.parallelism_config = parallelism_config
+        self.num_heads = config.head_num
+        self.qk_nope_head_dim = config.nope_head_dim
+        self.qk_rope_head_dim = config.rope_head_dim
         self.q_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim
-        self.kv_lora_rank = self.config.kv_lora_rank
-        self.v_head_dim = self.config.v_head_dim
-        self.q_lora_rank = self.config.q_lora_rank
+        self.kv_lora_rank = config.kv_lora_rank
+        self.v_head_dim = config.v_head_dim
+        self.q_lora_rank = config.q_lora_rank
         self.softmax_scale = self.q_head_dim ** (-0.5)
         self.layer_idx = layer_idx
-        self.token_per_block = self.config.seq_size_per_block
+        self.token_per_block = config.seq_size_per_block
 
         if self.q_lora_rank > 0:
             self.fused_qkv_a_proj = LinearFactory.create_linear_from_weights(
-                weights, W.mla_fusedqkrope_w, W.mla_fusedqkrope_s, None, config
+                weights, W.mla_fusedqkrope_w, W.mla_fusedqkrope_s, None,
+                py_model_config=config, quant_config=quant_config
             )
             self.q_a_layernorm = RMSNorm(
                 weights.get(W.mla_q_a_ln_gamma, None), eps=config.layernorm_eps
             )
             self.q_b_proj = LinearFactory.create_linear_from_weights(
-                weights, W.mla_q_b_w, W.mla_q_b_s, None, config
+                weights, W.mla_q_b_w, W.mla_q_b_s, None,
+                py_model_config=config, quant_config=quant_config
             )
         else:
             self.fused_qkv_proj = LinearFactory.create_linear_from_weights(
@@ -49,7 +54,7 @@ class DeepSeekV2Attention(nn.Module):
                 W.mla_fusedqkrope_no_lora_w,
                 W.mla_fusedqkrope_no_lora_s,
                 None,
-                config,
+                py_model_config=config, quant_config=quant_config
             )
 
         self.kv_a_layernorm = RMSNorm(
@@ -57,7 +62,8 @@ class DeepSeekV2Attention(nn.Module):
         )
 
         self.o_proj = LinearFactory.create_linear_from_weights(
-            weights, W.attn_o_w, W.attn_o_s, W.attn_o_b, config
+            weights, W.attn_o_w, W.attn_o_s, W.attn_o_b,
+            py_model_config=config, quant_config=quant_config
         )
 
     def forward(
@@ -69,7 +75,7 @@ class DeepSeekV2Attention(nn.Module):
         input_shape = hidden_states.shape[:-1]
         if self.q_lora_rank > 0:
             fused_qkv = self.fused_qkv_a_proj(hidden_states)
-            kv_offset = self.config.q_lora_rank
+            kv_offset = self.q_lora_rank
             q, compressed_kv = torch.split(
                 fused_qkv,
                 [
@@ -105,6 +111,6 @@ class DeepSeekV2Attention(nn.Module):
 
         attn_output = attn_output.reshape(*input_shape, -1).contiguous()
         attn_output = self.o_proj(attn_output)
-        if self.config.tp_size > 1:
+        if self.parallelism_config.tp_size > 1:
             attn_output = all_reduce(attn_output, group=Group.TP)
         return attn_output
diff --git a/rtp_llm/models_py/modules/mla/mla_attention_ref.py b/rtp_llm/models_py/modules/mla/mla_attention_ref.py
index 90bca793b..280ae23a4 100644
--- a/rtp_llm/models_py/modules/mla/mla_attention_ref.py
+++ b/rtp_llm/models_py/modules/mla/mla_attention_ref.py
@@ -5,7 +5,8 @@ import torch
 import torch.nn.functional as F
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.quant_config import QuantizationConfig
 from rtp_llm.models_py.modules.linear_factory import LinearFactory
 from rtp_llm.models_py.modules.norm import RMSNormTorch
 from rtp_llm.utils.model_weight import W
@@ -29,12 +30,12 @@ class DeepseekV3RotaryEmbedding(nn.Module):
             device=self.inv_freq.device,
             dtype=torch.get_default_dtype(),
         )
-        self.max_seq_len_cached = None
+        self.max_seq_lencached = None
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
-        self.max_seq_len_cached = seq_len
+        self.max_seq_lencached = seq_len
         t = torch.arange(
-            self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype
+            self.max_seq_lencached, device=device, dtype=self.inv_freq.dtype
         )
 
         freqs = torch.outer(t, self.inv_freq.to(t.device))
@@ -45,7 +46,7 @@ class DeepseekV3RotaryEmbedding(nn.Module):
 
     def forward(self, x, seq_len=None):
         # x: [bs, num_attention_heads, seq_len, head_size]
-        if self.max_seq_len_cached is None or seq_len > self.max_seq_len_cached:
+        if self.max_seq_lencached is None or seq_len > self.max_seq_lencached:
             self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)
 
         return (
@@ -115,7 +116,7 @@ class DeepseekV3YarnRotaryEmbedding(DeepseekV3RotaryEmbedding):
         super().__init__(dim, max_position_embeddings, base, device)
 
     def _set_cos_sin_cache(self, seq_len, device, dtype):
-        self.max_seq_len_cached = seq_len
+        self.max_seq_lencached = seq_len
         dim = self.dim
 
         freq_extra = 1.0 / (
@@ -251,31 +252,37 @@ class DeepseekV2AttentionRef(nn.Module):
     """Multi-headed attention from 'Attention Is All You Need' paper"""
 
     def __init__(
-        self, config: GptInitModelParameters, weights, layer_idx: Optional[int] = None
+        self,
+        config: ModelConfig,
+        weights,
+        layer_idx: Optional[int] = None,
+        quant_config: Optional[QuantizationConfig] = None,
     ):
         super().__init__()
         self.config = config
         self.layer_idx = layer_idx
         self.weights = weights
-        self.num_heads = self.config.head_num
-        self.qk_nope_head_dim = self.config.nope_head_dim
-        self.qk_rope_head_dim = self.config.rope_head_dim
+        self.num_heads = config.head_num
+        self.qk_nope_head_dim = config.nope_head_dim
+        self.qk_rope_head_dim = config.rope_head_dim
         self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim
-        self.kv_lora_rank = self.config.kv_lora_rank
-        self.v_head_dim = self.config.v_head_dim
-        self.q_lora_rank = self.config.q_lora_rank
+        self.kv_lora_rank = config.kv_lora_rank
+        self.v_head_dim = config.v_head_dim
+        self.q_lora_rank = config.q_lora_rank
         self.softmax_scale = self.qk_head_dim ** (-0.5)
-        self.token_per_block = self.config.seq_size_per_block
+        self.token_per_block = config.seq_size_per_block
 
         if self.q_lora_rank > 0:
             self.fused_qkv_a_proj = LinearFactory.create_linear_from_weights(
-                weights, W.mla_fusedqkrope_w, W.mla_fusedqkrope_s, None, config
+                weights, W.mla_fusedqkrope_w, W.mla_fusedqkrope_s, None,
+                py_model_config=config, quant_config=quant_config
             )
             self.q_a_layernorm = RMSNormTorch(
                 weights.get(W.mla_q_a_ln_gamma, None), eps=config.layernorm_eps
             )
             self.q_b_proj = LinearFactory.create_linear_from_weights(
-                weights, W.mla_q_b_w, W.mla_q_b_s, None, config
+                weights, W.mla_q_b_w, W.mla_q_b_s, None,
+                py_model_config=config, quant_config=quant_config
             )
         else:
             self.fused_qkv_proj = LinearFactory.create_linear_from_weights(
@@ -283,7 +290,7 @@ class DeepseekV2AttentionRef(nn.Module):
                 W.mla_fusedqkrope_no_lora_w,
                 W.mla_fusedqkrope_no_lora_s,
                 None,
-                config,
+                py_model_config=config, quant_config=quant_config
             )
 
         self.kv_a_layernorm = RMSNormTorch(
@@ -291,7 +298,8 @@ class DeepseekV2AttentionRef(nn.Module):
         )
 
         self.o_proj = LinearFactory.create_linear_from_weights(
-            weights, W.attn_o_w, W.attn_o_s, W.attn_o_b, config
+            weights, W.attn_o_w, W.attn_o_s, W.attn_o_b,
+            py_model_config=config, quant_config=quant_config
         )
 
         self.rotary_emb = DeepseekV3YarnRotaryEmbedding(
@@ -328,7 +336,7 @@ class DeepseekV2AttentionRef(nn.Module):
             q = self.q_b_proj(fused_qkv)
         else:
             fused_qkv = self.fused_qkv_proj(hidden_states)
-            kv_offset = self.config.head_num * self.config.size_per_head
+            kv_offset = self.num_heads * self.config.size_per_head
             q, compressed_kv = torch.split(
                 fused_qkv,
                 [
diff --git a/rtp_llm/models_py/modules/mlp.py b/rtp_llm/models_py/modules/mlp.py
index 36f242ba7..a1e212c1f 100644
--- a/rtp_llm/models_py/modules/mlp.py
+++ b/rtp_llm/models_py/modules/mlp.py
@@ -1,30 +1,30 @@
-from typing import Dict
+from typing import Dict, Optional
 
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.distribute.collective import Group, all_reduce
 from rtp_llm.models_py.modules.linear_factory import LinearFactory
-from rtp_llm.ops import rtp_llm_ops
+from rtp_llm.ops import ParallelismConfig, rtp_llm_ops
 from rtp_llm.utils.model_weight import W
 
 
 class DenseMLP(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, parallelism_config: ParallelismConfig, weights: Dict[str, torch.Tensor], quant_config: Optional[object] = None
     ):
         super().__init__()
 
         # Create linear layers using LinearFactory
         self.gate_proj = LinearFactory.create_linear_from_weights(
-            weights, W.ffn_w1, W.ffn_s1, W.ffn_b1, config
+            weights, W.ffn_w1, W.ffn_s1, W.ffn_b1, py_model_config=config, gpt_init_params=None, quant_config=quant_config
         )
         self.up_proj = LinearFactory.create_linear_from_weights(
-            weights, W.ffn_w3, W.ffn_s3, W.ffn_b3, config
+            weights, W.ffn_w3, W.ffn_s3, W.ffn_b3, py_model_config=config, gpt_init_params=None, quant_config=quant_config
         )
         self.down_proj = LinearFactory.create_linear_from_weights(
-            weights, W.ffn_w2, W.ffn_s2, W.ffn_b2, config
+            weights, W.ffn_w2, W.ffn_s2, W.ffn_b2, py_model_config=config, gpt_init_params=None, quant_config=quant_config
         )
 
         if config.activation_type == "SiGLU":
@@ -43,7 +43,7 @@ class DenseMLP(nn.Module):
 
 class FusedSiluActDenseMLP(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, parallelism_config: ParallelismConfig, weights: Dict[str, torch.Tensor], quant_config: Optional[object] = None
     ):
         super().__init__()
 
@@ -51,14 +51,15 @@ class FusedSiluActDenseMLP(nn.Module):
             config.activation_type == "SiGLU"
         ), "FusedSiluActDenseMLP only supports SiGLU activation"
         self.config = config
+        self.parallelism_config = parallelism_config
 
         # Handle merged or separate weights
         if W.ffn_w13 in weights:
             self.gate_up_proj = LinearFactory.create_linear_from_weights(
-                weights, W.ffn_w13, W.ffn_s13, W.ffn_b13, config
+                weights, W.ffn_w13, W.ffn_s13, W.ffn_b13, py_model_config=config, gpt_init_params=None, quant_config=quant_config
             )
             self.down_proj = LinearFactory.create_linear_from_weights(
-                weights, W.ffn_w2, W.ffn_s2, W.ffn_b2, config
+                weights, W.ffn_w2, W.ffn_s2, W.ffn_b2, py_model_config=config, gpt_init_params=None, quant_config=quant_config
             )
         else:
             self.gate_up_proj = LinearFactory.create_merged_linear(
@@ -66,11 +67,13 @@ class FusedSiluActDenseMLP(nn.Module):
                 weight_keys=[W.ffn_w1, W.ffn_w3],
                 scale_keys=[W.ffn_s1, W.ffn_s3],
                 bias_keys=[W.ffn_b1, W.ffn_b3],
-                config=config,
+                py_model_config=config,
+                gpt_init_params=None,
+                quant_config=quant_config,
                 dim=-1,
             )
             self.down_proj = LinearFactory.create_linear_from_weights(
-                weights, W.ffn_w2, W.ffn_s2, W.ffn_b2, config
+                weights, W.ffn_w2, W.ffn_s2, W.ffn_b2, py_model_config=config, gpt_init_params=None, quant_config=quant_config
             )
 
     def forward(self, x: torch.Tensor):
@@ -82,24 +85,24 @@ class FusedSiluActDenseMLP(nn.Module):
         stream_id = torch.cuda.current_stream().cuda_stream
         rtp_llm_ops.silu_and_mul(output, gate_up, stream_id)
         down_proj = self.down_proj(output)
-        if self.config.tp_size > 1:
+        if self.parallelism_config.tp_size > 1:
             down_proj = all_reduce(down_proj, group=Group.TP)
         return down_proj
 
 
 class BertGeluActDenseMLP(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, parallelism_config: ParallelismConfig, weights: Dict[str, torch.Tensor], quant_config: Optional[object] = None
     ):
         super().__init__()
 
         # For BERT model, use traditional FFN structure with GeLU activation
         # BERT uses: intermediate_weight3 -> GeLU -> intermediate_weight2
         self.intermediate_proj = LinearFactory.create_linear_from_weights(
-            weights, W.ffn_w3, W.ffn_s3, W.ffn_b3, config
+            weights, W.ffn_w3, W.ffn_s3, W.ffn_b3, py_model_config=config, gpt_init_params=None, quant_config=quant_config
         )
         self.output_proj = LinearFactory.create_linear_from_weights(
-            weights, W.ffn_w2, W.ffn_s2, W.ffn_b2, config
+            weights, W.ffn_w2, W.ffn_s2, W.ffn_b2, py_model_config=config, gpt_init_params=None, quant_config=quant_config
         )
 
         # Use GeLU activation
diff --git a/rtp_llm/models_py/modules/moe/config_adapter.py b/rtp_llm/models_py/modules/moe/config_adapter.py
new file mode 100644
index 000000000..d5c844d53
--- /dev/null
+++ b/rtp_llm/models_py/modules/moe/config_adapter.py
@@ -0,0 +1,59 @@
+"""
+Adapter to provide a unified interface from individual config objects.
+This allows Router and Executor classes to work with specific config objects.
+"""
+
+from typing import Optional
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.quant_config import QuantizationConfig
+from rtp_llm.ops import ParallelismConfig, MoeConfig, RuntimeConfig
+
+
+class MoEConfigAdapter:
+    """
+    Adapter class that provides a unified interface
+    from individual configuration objects.
+    """
+    
+    def __init__(
+        self,
+        py_model_config: ModelConfig,
+        parallelism_config: ParallelismConfig,
+        moe_config: Optional[MoeConfig] = None,
+        runtime_config: Optional[RuntimeConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+    ):
+        self.py_model_config = py_model_config
+        self.parallelism_config = parallelism_config
+        self.moe_config = moe_config or MoeConfig()
+        self.runtime_config = runtime_config or RuntimeConfig()
+        self.quant_config = quant_config
+        
+        # Provide shortcut access to commonly used attributes
+        self.ep_size = parallelism_config.ep_size
+        self.ep_rank = parallelism_config.ep_rank
+        self.tp_size = parallelism_config.tp_size
+        self.tp_rank = parallelism_config.tp_rank
+        self.dp_size = parallelism_config.dp_size
+        self.dp_rank = parallelism_config.dp_rank
+        self.world_size = parallelism_config.world_size
+        # Calculate local_rank from world_rank and local_world_size
+        self.local_rank = parallelism_config.local_rank
+        
+        self.expert_num = py_model_config.expert_num
+        self.moe_k = py_model_config.moe_k
+        self.moe_topk_group = py_model_config.moe_topk_group
+        self.moe_inter_padding_size = py_model_config.moe_inter_padding_size
+        self.activation_type = py_model_config.activation_type
+        self.hidden_size = py_model_config.hidden_size
+        
+        self.max_generate_batch_size = runtime_config.max_generate_batch_size if runtime_config else 0
+        
+        # For compatibility with distributed init, provide nccl_ip and th_nccl_port
+        # These are typically in parallelism_config
+        self.nccl_ip = parallelism_config.nccl_ip
+        # Note: th_nccl_port may not be in ParallelismConfig, but tp_nccl_port is
+        # For now, use tp_nccl_port as fallback, but this should be passed separately if needed
+        self.th_nccl_port = parallelism_config.tp_nccl_port
+        
+
diff --git a/rtp_llm/models_py/modules/moe/executors/deepep_normal_executor.py b/rtp_llm/models_py/modules/moe/executors/deepep_normal_executor.py
index 0f82c2a41..c5e394a9b 100644
--- a/rtp_llm/models_py/modules/moe/executors/deepep_normal_executor.py
+++ b/rtp_llm/models_py/modules/moe/executors/deepep_normal_executor.py
@@ -7,7 +7,7 @@ from typing import Any, Dict, Optional
 import torch
 from librtp_compute_ops.rtp_llm_ops import trt_fp8_quantize_128
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 from rtp_llm.models_py.modules.ep.kernels import (
     ep_gather,
     ep_scatter,
@@ -41,7 +41,7 @@ def align_up_math(n: int, alignment: int = 128) -> int:
 class DeepGemmContinousExecutor(FusedMoeExpertExecutor):
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: MoEConfigAdapter,
         weights: Dict[str, torch.Tensor],
     ):
         super().__init__(FusedMoEQuantConfig())
diff --git a/rtp_llm/models_py/modules/moe/executors/deepgemm_masked_executor.py b/rtp_llm/models_py/modules/moe/executors/deepgemm_masked_executor.py
index fba364abe..f516dd6c3 100644
--- a/rtp_llm/models_py/modules/moe/executors/deepgemm_masked_executor.py
+++ b/rtp_llm/models_py/modules/moe/executors/deepgemm_masked_executor.py
@@ -2,7 +2,7 @@ from typing import Any, Dict, Optional
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 from rtp_llm.models_py.modules.moe.fused_moe import (
     ExpertForwardPayload,
     FusedMoeExpertExecutor,
@@ -31,7 +31,7 @@ class DeepGemmMaskedExecutor(FusedMoeExpertExecutor):
 
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: MoEConfigAdapter,
         weights: Dict[str, torch.Tensor],
         quant_config: FusedMoEQuantConfig,
     ):
diff --git a/rtp_llm/models_py/modules/moe/executors/test/deepgemm_masked_executor_test.py b/rtp_llm/models_py/modules/moe/executors/test/deepgemm_masked_executor_test.py
index eb50a8bdc..7c7b87db4 100644
--- a/rtp_llm/models_py/modules/moe/executors/test/deepgemm_masked_executor_test.py
+++ b/rtp_llm/models_py/modules/moe/executors/test/deepgemm_masked_executor_test.py
@@ -3,10 +3,12 @@ from typing import Dict, Tuple
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 from rtp_llm.models_py.modules.moe.executors.deepgemm_masked_executor import (
     DeepGemmMaskedExecutor,
 )
+from rtp_llm.ops import MoeConfig, ParallelismConfig, RuntimeConfig
 from rtp_llm.models_py.modules.moe.fused_moe import (
     ExpertForwardPayload,
     ExpertTokensMetadata,
@@ -28,30 +30,43 @@ K = HIDDEN_SIZE
 N = MOE_INTERMEDIATE_SIZE * 2
 
 
-def _generate_config() -> GptInitModelParameters:
-    config = GptInitModelParameters(
-        head_num=2,
-        size_per_head=128,
-        layer_num=2,
-        max_seq_len=2048,
-        vocab_size=500000,
+def _generate_config() -> MoEConfigAdapter:
+    py_model_config = ModelConfig()
+    py_model_config.head_num = 2
+    py_model_config.size_per_head = 128
+    py_model_config.num_layers = 2
+    py_model_config.max_seq_len = 2048
+    py_model_config.vocab_size = 500000
+    py_model_config.expert_num = NUM_EXPERTS
+    py_model_config.hidden_size = HIDDEN_SIZE
+    py_model_config.moe_inter_padding_size = MOE_INTERMEDIATE_SIZE
+    py_model_config.moe_k = TOPK if 'TOPK' in globals() else 8
+    
+    parallelism_config = ParallelismConfig()
+    parallelism_config.world_size = DP_SIZE * EP_SIZE
+    parallelism_config.dp_size = DP_SIZE
+    parallelism_config.tp_size = TP_SIZE
+    parallelism_config.ep_size = EP_SIZE
+    parallelism_config.dp_rank = 0
+    parallelism_config.tp_rank = 0
+    parallelism_config.ep_rank = 0
+    parallelism_config.world_rank = 0
+    parallelism_config.local_world_size = 1
+    
+    moe_config = MoeConfig()
+    runtime_config = RuntimeConfig()
+    runtime_config.max_generate_batch_size = MAX_GENERATE_BATCH_SIZE
+    
+    return MoEConfigAdapter(
+        py_model_config=py_model_config,
+        parallelism_config=parallelism_config,
+        moe_config=moe_config,
+        runtime_config=runtime_config,
     )
-    config.world_size = DP_SIZE * EP_SIZE
-    config.dp_size = DP_SIZE
-    config.tp_size = TP_SIZE
-    config.ep_size = EP_SIZE
-    config.dp_rank = 0
-    config.tp_rank = 0
-    config.ep_rank = 0
-    config.expert_num = NUM_EXPERTS
-    config.hidden_size = HIDDEN_SIZE
-    config.max_generate_batch_size = MAX_GENERATE_BATCH_SIZE
-    config.moe_inter_padding_size = MOE_INTERMEDIATE_SIZE
-    return config
 
 
 def _generate_payload_and_weights(
-    config: GptInitModelParameters,
+    config: MoEConfigAdapter,
     use_fp8: bool,
 ) -> Tuple[ExpertForwardPayload, Dict[str, torch.Tensor]]:
     torch_dtype = torch.float8_e4m3fn if use_fp8 else torch.bfloat16
diff --git a/rtp_llm/models_py/modules/moe/fused_moe_factory.py b/rtp_llm/models_py/modules/moe/fused_moe_factory.py
index 9abf9d01e..a340c36bd 100644
--- a/rtp_llm/models_py/modules/moe/fused_moe_factory.py
+++ b/rtp_llm/models_py/modules/moe/fused_moe_factory.py
@@ -1,10 +1,12 @@
-from typing import Dict
+from typing import Dict, Optional
 
 import torch
 
 import rtp_llm.models_py.modules.utils as utils
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.config.quant_config import Fp8PerTensorCompressedQuantConfig
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
+from rtp_llm.ops import MoeConfig, ParallelismConfig, RuntimeConfig
 from rtp_llm.models_py.modules.moe import (
     BatchedDataRouter,
     DataRouterNoEPStandard,
@@ -17,7 +19,7 @@ from rtp_llm.utils.model_weight import W
 initialized = False
 
 
-def init_deepep_env_once(config: GptInitModelParameters):
+def init_deepep_env_once(config: MoEConfigAdapter):
     global initialized
     if initialized:
         return
@@ -27,17 +29,40 @@ def init_deepep_env_once(config: GptInitModelParameters):
         get_ep_group,
         init_distributed_environment,
     )
-
-    init_distributed_environment(params=config, backend="nccl", timeout=None)
+    from rtp_llm.ops import FfnDisAggregateConfig
+    
+    # Extract config parameters from adapter
+    py_model_config = config.py_model_config
+    parallelism_config = config.parallelism_config
+    moe_config = config.moe_config
+    runtime_config = config.runtime_config
+    nccl_ip = config.nccl_ip
+    th_nccl_port = config.th_nccl_port
+    ffn_disaggregate_config = parallelism_config.ffn_disaggregate_config
+
+    init_distributed_environment(
+        parallelism_config=parallelism_config,
+        nccl_ip=nccl_ip,
+        th_nccl_port=th_nccl_port,
+        backend="nccl",
+        timeout=None
+    )
     ep_group = get_ep_group()
     assert ep_group.device_group is not None, "ep group device group is not initialized"
-    init_deepep_wrapper(group=ep_group.device_group, params=config)
+    init_deepep_wrapper(
+        group=ep_group.device_group,
+        py_model_config=py_model_config,
+        parallelism_config=parallelism_config,
+        moe_config=moe_config,
+        runtime_config=runtime_config,
+        ffn_disaggregate_config=ffn_disaggregate_config,
+    )
 
 
 class FusedMoeFactory(object):
     @staticmethod
     def _create_fp8_per_block_fused_moe(
-        config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        config: MoEConfigAdapter, weights: Dict[str, torch.Tensor]
     ):
         assert utils.is_cuda(), "FP8_PER_BLOCK only supports cuda"
         # single gpu
@@ -92,7 +117,7 @@ class FusedMoeFactory(object):
 
     @staticmethod
     def _create_fp8_per_tensor_fused_moe(
-        config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        config: MoEConfigAdapter, weights: Dict[str, torch.Tensor]
     ):
         assert utils.is_cuda(), "FP8_PER_TENSOR only supports cuda"
         from rtp_llm.models_py.modules.moe.executors.cutlass_moe import (
@@ -152,7 +177,7 @@ class FusedMoeFactory(object):
 
     @staticmethod
     def create_fused_moe(
-        config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        config: MoEConfigAdapter, weights: Dict[str, torch.Tensor]
     ) -> FusedMoe:
         # TODO get_method should return enu class other than string
         if config.quant_config is None:
@@ -199,14 +224,18 @@ class FusedMoeFactory(object):
                     w2=weights[W.moe_w2],
                 )
                 return FusedMoe(router, experts, expert_num=config.expert_num)
-        elif config.quant_config.get_method() == "FP8_PER_BLOCK":
+        quant_config = config.quant_config
+        if quant_config and quant_config.get_method() == "FP8_PER_BLOCK":
             return FusedMoeFactory._create_fp8_per_block_fused_moe(config, weights)
-        elif config.quant_config.get_method() in [
+        elif quant_config and quant_config.get_method() in [
             "FP8_PER_TENSOR_COMPRESSED",
             "FP8_DYNAMIC_PER_TENSOR",
         ]:
             return FusedMoeFactory._create_fp8_per_tensor_fused_moe(config, weights)
         else:
-            raise ValueError(
-                f"Unsupported quantization method: {config.quant_config.get_method()}"
-            )
+            if quant_config:
+                raise ValueError(
+                    f"Unsupported quantization method: {quant_config.get_method()}"
+                )
+            else:
+                raise ValueError("quant_config is None but quantization path was selected")
diff --git a/rtp_llm/models_py/modules/moe/naive_data_router.py b/rtp_llm/models_py/modules/moe/naive_data_router.py
index 8c6502208..b07ceb704 100644
--- a/rtp_llm/models_py/modules/moe/naive_data_router.py
+++ b/rtp_llm/models_py/modules/moe/naive_data_router.py
@@ -5,7 +5,6 @@ from typing import Any, Optional
 import torch
 
 import rtp_llm.models_py.modules.moe.fused_moe as mm
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.models_py.modules.moe.topk_weight_and_reduce import (
     TopKWeightAndReduceContiguous,
     TopKWeightAndReduceDelegate,
diff --git a/rtp_llm/models_py/modules/moe/routers/deepep_low_latency_router.py b/rtp_llm/models_py/modules/moe/routers/deepep_low_latency_router.py
index e97ea9a46..1238a42f9 100644
--- a/rtp_llm/models_py/modules/moe/routers/deepep_low_latency_router.py
+++ b/rtp_llm/models_py/modules/moe/routers/deepep_low_latency_router.py
@@ -3,7 +3,7 @@ from typing import Any, Optional, Tuple
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 from rtp_llm.models_py.distributed.deepep_wrapper import get_deepep_wrapper
 from rtp_llm.models_py.modules.moe.fused_moe import (
     ExpertForwardPayload,
@@ -31,7 +31,7 @@ class DeepEpLowLatencyRouter(FusedMoeDataRouter):
 
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: MoEConfigAdapter,
         use_fp8_dispatch: bool = True,
         zero_copy: bool = False,
         async_finish: bool = False,
diff --git a/rtp_llm/models_py/modules/moe/routers/deepep_normal_router.py b/rtp_llm/models_py/modules/moe/routers/deepep_normal_router.py
index e4c7b5ce3..00dacd96f 100644
--- a/rtp_llm/models_py/modules/moe/routers/deepep_normal_router.py
+++ b/rtp_llm/models_py/modules/moe/routers/deepep_normal_router.py
@@ -3,7 +3,6 @@ from typing import Any, Dict, Optional
 import torch
 from librtp_compute_ops.rtp_llm_ops import trt_fp8_quantize_128
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.models_py.distributed.deepep_wrapper import get_deepep_wrapper
 from rtp_llm.models_py.modules.fp8_kernel import scaled_fp8_per_token_quant
 from rtp_llm.models_py.modules.moe import (
@@ -14,12 +13,13 @@ from rtp_llm.models_py.modules.moe import (
     TopKWeightAndReduceContiguous,
     TopKWeightAndReduceDelegate,
 )
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 
 
 class DeepepNormalRouter(FusedMoeDataRouter):
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: MoEConfigAdapter,
         use_fp8: bool = True,
         async_mode: bool = False,
         expert_alignment: int = 128,
diff --git a/rtp_llm/models_py/modules/moe/routers/deepgeemm_coutinous_router.py b/rtp_llm/models_py/modules/moe/routers/deepgeemm_coutinous_router.py
index 7f58a4c7b..7ebcec586 100644
--- a/rtp_llm/models_py/modules/moe/routers/deepgeemm_coutinous_router.py
+++ b/rtp_llm/models_py/modules/moe/routers/deepgeemm_coutinous_router.py
@@ -3,7 +3,6 @@ from typing import Any, Optional
 import torch
 from librtp_compute_ops.rtp_llm_ops import trt_fp8_quantize_128
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.distribute.collective import Group, all_reduce
 from rtp_llm.models_py.modules.ep.kernels import recompute_topk_ids_sum_expert_count
 from rtp_llm.models_py.modules.moe import (
@@ -12,12 +11,13 @@ from rtp_llm.models_py.modules.moe import (
     FusedMoeDataRouter,
     FusedMoEQuantConfig,
 )
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 
 
 class DeepGemmCountinousRouter(FusedMoeDataRouter):
     def __init__(
         self,
-        config: GptInitModelParameters,
+        config: MoEConfigAdapter,
         use_fp8: bool = True,
         async_mode: bool = False,
         expert_alignment: int = 128,
diff --git a/rtp_llm/models_py/modules/moe/routers/test/deepep_low_latency_router_test.py b/rtp_llm/models_py/modules/moe/routers/test/deepep_low_latency_router_test.py
index d5b3ee6d7..3ed0fc1b2 100644
--- a/rtp_llm/models_py/modules/moe/routers/test/deepep_low_latency_router_test.py
+++ b/rtp_llm/models_py/modules/moe/routers/test/deepep_low_latency_router_test.py
@@ -4,7 +4,7 @@ import os
 import torch
 import torch.multiprocessing as mp
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.distributed.deepep_wrapper import (
     destroy_deepep_wrapper,
     init_deepep_wrapper,
@@ -14,9 +14,11 @@ from rtp_llm.models_py.distributed.process_group_state import (
     get_ep_group,
     init_distributed_environment,
 )
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 from rtp_llm.models_py.modules.moe.routers.deepep_low_latency_router import (
     DeepEpLowLatencyRouter,
 )
+from rtp_llm.ops import FfnDisAggregateConfig, MoeConfig, ParallelismConfig, RuntimeConfig
 from rtp_llm.models_py.modules.moe.topk_weight_and_reduce import (
     TopKWeightAndReduceDelegate,
 )
@@ -57,34 +59,65 @@ def _init_router(rank: int, use_fp8: bool):
     os.environ["ACCL_TOPO_FIX"] = "1"
     os.environ["ACCL_LOAD_BALANCE"] = "1"
     # init params
-    config = GptInitModelParameters(
-        head_num=2,
-        size_per_head=128,
-        layer_num=2,
-        max_seq_len=2048,
-        vocab_size=500000,
+    py_model_config = ModelConfig()
+    py_model_config.head_num = 2
+    py_model_config.size_per_head = 128
+    py_model_config.num_layers = 2
+    py_model_config.max_seq_len = 2048
+    py_model_config.vocab_size = 500000
+    py_model_config.moe_k = TOPK
+    py_model_config.expert_num = NUM_EXPERTS
+    py_model_config.hidden_size = HIDDEN_SIZE
+    
+    parallelism_config = ParallelismConfig()
+    parallelism_config.nccl_ip = "127.0.0.1"
+    parallelism_config.tp_nccl_port = int(os.getenv("MASTER_PORT", "8376"))
+    parallelism_config.dp_rank = rank
+    parallelism_config.dp_size = WORLD_SIZE
+    parallelism_config.tp_rank = 0
+    parallelism_config.tp_size = 1
+    parallelism_config.ep_rank = rank
+    parallelism_config.ep_size = WORLD_SIZE
+    parallelism_config.local_rank = rank
+    parallelism_config.world_size = WORLD_SIZE
+    parallelism_config.world_rank = rank
+    parallelism_config.local_world_size = WORLD_SIZE
+    
+    moe_config = MoeConfig()
+    moe_config.use_deepep_low_latency = True
+    moe_config.use_deepep_internode = False
+    
+    runtime_config = RuntimeConfig()
+    runtime_config.max_generate_batch_size = NUM_TOKEN_PER_RANK
+    
+    ffn_disaggregate_config = FfnDisAggregateConfig()
+    ffn_disaggregate_config.enable_ffn_disaggregate = False
+    
+    config = MoEConfigAdapter(
+        py_model_config=py_model_config,
+        parallelism_config=parallelism_config,
+        moe_config=moe_config,
+        runtime_config=runtime_config,
+    )
+    config._ffn_disaggregate_config = ffn_disaggregate_config
+    
+    torch.cuda.set_device(parallelism_config.local_rank)
+    torch.set_default_device(f"cuda:{parallelism_config.local_rank}")
+    init_distributed_environment(
+        parallelism_config=parallelism_config,
+        nccl_ip=parallelism_config.nccl_ip,
+        th_nccl_port=parallelism_config.tp_nccl_port,
+        backend="nccl",
+        timeout=60
+    )
+    init_deepep_wrapper(
+        group=get_ep_group().device_group,
+        py_model_config=py_model_config,
+        parallelism_config=parallelism_config,
+        moe_config=moe_config,
+        runtime_config=runtime_config,
+        ffn_disaggregate_config=ffn_disaggregate_config,
     )
-    config.nccl_ip = "127.0.0.1"
-    config.th_nccl_port = int(os.getenv("MASTER_PORT", "8376"))
-    config.dp_rank = rank
-    config.dp_size = WORLD_SIZE
-    config.tp_rank = 0
-    config.tp_size = 1
-    config.ep_rank = rank
-    config.ep_size = WORLD_SIZE
-    config.local_rank = rank
-    config.world_size = WORLD_SIZE
-    config.moe_config.use_deepep_low_latency = True
-    config.moe_config.use_deepep_internode = False
-    config.gpt_init_params.ffn_disaggregate_config.enable_ffn_disaggregate = False
-    config.moe_k = TOPK
-    config.expert_num = NUM_EXPERTS
-    config.hidden_size = HIDDEN_SIZE
-    config.max_generate_batch_size = NUM_TOKEN_PER_RANK
-    torch.cuda.set_device(config.local_rank)
-    torch.set_default_device(f"cuda:{config.local_rank}")
-    init_distributed_environment(config, backend="nccl", timeout=60)
-    init_deepep_wrapper(group=get_ep_group().device_group, params=config)
     router = DeepEpLowLatencyRouter(
         config,
         use_fp8_dispatch=use_fp8,
diff --git a/rtp_llm/models_py/modules/moe/routers/test/deepep_normal_router_test.py b/rtp_llm/models_py/modules/moe/routers/test/deepep_normal_router_test.py
index 5c712a213..1a53a2c06 100644
--- a/rtp_llm/models_py/modules/moe/routers/test/deepep_normal_router_test.py
+++ b/rtp_llm/models_py/modules/moe/routers/test/deepep_normal_router_test.py
@@ -5,7 +5,7 @@ from typing import List
 
 import torch
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.distribute.worker_info import g_parallel_info, update_master_info
 from rtp_llm.models_py.distributed.deepep_wrapper import init_deepep_wrapper
 from rtp_llm.models_py.distributed.process_group_state import (
@@ -13,9 +13,11 @@ from rtp_llm.models_py.distributed.process_group_state import (
     get_ep_group,
     init_distributed_environment,
 )
+from rtp_llm.models_py.modules.moe.config_adapter import MoEConfigAdapter
 from rtp_llm.models_py.modules.moe.routers.deepep_normal_router import (
     DeepepNormalRouter,
 )
+from rtp_llm.ops import MoeConfig, ParallelismConfig, RuntimeConfig
 from rtp_llm.models_py.modules.moe.utils import FusedMoEQuantConfig
 from rtp_llm.test.utils.port_util import PortsContext
 
@@ -24,24 +26,60 @@ from librtp_compute_ops.rtp_llm_ops import trt_fp8_quantize_128  # isort:skip
 
 
 def init_router(rank: int, use_fp8: bool):
-    g_parallel_info.reload()
+    from rtp_llm.config.py_config_modules import MIN_WORKER_INFO_PORT_NUM
+    worker_info_port_num = int(os.environ.get("WORKER_INFO_PORT_NUM", str(MIN_WORKER_INFO_PORT_NUM)))
+    g_parallel_info.reload(worker_info_port_num)
     update_master_info(f"0.0.0.0", int(os.environ["START_PORT"]))
     print(f"rank {rank}, {g_parallel_info}")
-    config = GptInitModelParameters(0, 0, 0, 0, 0)
-    config.moe_config.use_deepep_low_latency = False
-    config.expert_num = 16
-    config.hidden_size = 1024
-    config.tp_size = g_parallel_info.tp_size
-    config.tp_rank = g_parallel_info.tp_rank
-    config.ep_size = g_parallel_info.ep_size
-    config.ep_rank = g_parallel_info.ep_rank
-    config.dp_size = g_parallel_info.dp_size
-    config.dp_rank = g_parallel_info.dp_rank
-    config.ffn_tp_rank = g_parallel_info.ffn_tp_rank
-    config.ffn_tp_size = g_parallel_info.ffn_tp_size
-    config.local_rank = rank
-    init_distributed_environment(config, backend="nccl", timeout=60)
-    init_deepep_wrapper(group=get_ep_group().device_group, params=config)
+    
+    py_model_config = ModelConfig()
+    py_model_config.expert_num = 16
+    py_model_config.hidden_size = 1024
+    py_model_config.moe_k = 16
+    
+    parallelism_config = ParallelismConfig()
+    parallelism_config.tp_size = g_parallel_info.tp_size
+    parallelism_config.tp_rank = g_parallel_info.tp_rank
+    parallelism_config.ep_size = g_parallel_info.ep_size
+    parallelism_config.ep_rank = g_parallel_info.ep_rank
+    parallelism_config.dp_size = g_parallel_info.dp_size
+    parallelism_config.dp_rank = g_parallel_info.dp_rank
+    parallelism_config.ffn_tp_rank = g_parallel_info.ffn_tp_rank
+    parallelism_config.ffn_tp_size = g_parallel_info.ffn_tp_size
+    parallelism_config.world_size = g_parallel_info.world_size
+    parallelism_config.world_rank = g_parallel_info.world_rank
+    parallelism_config.local_world_size = g_parallel_info.local_world_size
+    parallelism_config.local_rank = rank
+    parallelism_config.nccl_ip = "0.0.0.0"
+    parallelism_config.tp_nccl_port = int(os.environ["START_PORT"])
+    
+    moe_config = MoeConfig()
+    moe_config.use_deepep_low_latency = False
+    
+    runtime_config = RuntimeConfig()
+    
+    config = MoEConfigAdapter(
+        py_model_config=py_model_config,
+        parallelism_config=parallelism_config,
+        moe_config=moe_config,
+        runtime_config=runtime_config,
+    )
+    
+    init_distributed_environment(
+        parallelism_config=parallelism_config,
+        nccl_ip=parallelism_config.nccl_ip,
+        th_nccl_port=parallelism_config.tp_nccl_port,
+        backend="nccl",
+        timeout=60
+    )
+    init_deepep_wrapper(
+        group=get_ep_group().device_group,
+        py_model_config=py_model_config,
+        parallelism_config=parallelism_config,
+        moe_config=moe_config,
+        runtime_config=runtime_config,
+        ffn_disaggregate_config=None,
+    )
     router = DeepepNormalRouter(config, use_fp8, expert_alignment=1)
     return config, router
 
diff --git a/rtp_llm/models_py/modules/rocm/fmha.py b/rtp_llm/models_py/modules/rocm/fmha.py
index 52932d9d5..1ece79175 100644
--- a/rtp_llm/models_py/modules/rocm/fmha.py
+++ b/rtp_llm/models_py/modules/rocm/fmha.py
@@ -10,9 +10,9 @@ from librtp_compute_ops.rtp_llm_ops import (
     FusedRopeKVCachePrefillOp,
 )
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.modules.fmha import FMHAImplBase
-from rtp_llm.ops import FMHAType, KVCache, PyAttentionInputs
+from rtp_llm.ops import FMHAType, KVCache, ParallelismConfig, PyAttentionInputs
 
 
 # Simple data structure for fmha_params
@@ -40,10 +40,14 @@ class FMHAPrefillImplBase(FMHAImplBase):
         self,
         fmha_impl: Any,
         attn_inputs: PyAttentionInputs,
-        config: GptInitModelParameters,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
+        py_hw_kernel_config=None,
     ) -> None:
+        attn_configs = config.getAttentionConfigs(parallelism_config.tp_size)
+        layer_num = 0  # Default layer_num, should be passed from caller if needed
         super().__init__(
-            fmha_impl, FusedRopeKVCachePrefillOp(config.gpt_init_params), attn_inputs
+            fmha_impl, FusedRopeKVCachePrefillOp(attn_configs, layer_num, py_hw_kernel_config), attn_inputs
         )
 
 
@@ -53,10 +57,14 @@ class FMHADecodeImplBase(FMHAImplBase):
         self,
         fmha_impl: Any,
         attn_inputs: PyAttentionInputs,
-        config: GptInitModelParameters,
+        config: ModelConfig,
+        parallelism_config: ParallelismConfig,
+        py_hw_kernel_config=None,
     ) -> None:
+        attn_configs = config.getAttentionConfigs(parallelism_config.tp_size)
+        layer_num = 0  # Default layer_num, should be passed from caller if needed
         super().__init__(
-            fmha_impl, FusedRopeKVCacheDecodeOp(config.gpt_init_params), attn_inputs
+            fmha_impl, FusedRopeKVCacheDecodeOp(attn_configs, layer_num, py_hw_kernel_config), attn_inputs
         )
 
 
@@ -68,9 +76,11 @@ try:
 
     class AiterPrefillImpl(FMHAPrefillImplBase):
         def __init__(
-            self, config: GptInitModelParameters, attn_inputs: PyAttentionInputs
+            self, config: ModelConfig, parallelism_config: ParallelismConfig, attn_inputs: PyAttentionInputs,
+            py_hw_kernel_config=None, **kwargs  # Accept py_hw_kernel_config and ignore other kwargs
         ) -> None:
-            super().__init__(AiterPrefillAttnOp(config), attn_inputs, config)
+            # py_hw_kernel_config is not used by AiterPrefillAttnOp, but we accept it for consistency
+            super().__init__(AiterPrefillAttnOp(config), attn_inputs, config, parallelism_config, py_hw_kernel_config)
 
         @staticmethod
         def fmha_type() -> FMHAType:
@@ -82,7 +92,7 @@ except ImportError:
 
 
 class AiterPrefillAttnOp:
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig):
         self.head_num = config.head_num
         self.head_dim = config.hidden_size // config.head_num
         self.head_num_kv = config.head_num_kv
@@ -147,9 +157,10 @@ try:
 
     class AiterDecodeImpl(FMHADecodeImplBase):
         def __init__(
-            self, config: GptInitModelParameters, attn_inputs: PyAttentionInputs
+            self, config: ModelConfig, parallelism_config: ParallelismConfig, attn_inputs: PyAttentionInputs,
+            py_hw_kernel_config, **kwargs
         ) -> None:
-            super().__init__(AiterDecodeAttnOp(config), attn_inputs, config)
+            super().__init__(AiterDecodeAttnOp(config, py_hw_kernel_config), attn_inputs, config, parallelism_config, py_hw_kernel_config)
 
     DECODE_MHA_IMPS.append(AiterDecodeImpl)
 except ImportError:
@@ -157,12 +168,12 @@ except ImportError:
 
 
 class AiterDecodeAttnOp:
-    def __init__(self, config: GptInitModelParameters):
+    def __init__(self, config: ModelConfig, py_hw_kernel_config):
         self.head_num = config.head_num
         self.head_dim = config.hidden_size // config.head_num
         self.head_num_kv = config.head_num_kv
         self.kv_cache_data_type = config.kv_cache_data_type
-        self.use_asm_pa = config.hw_kernel_config.use_asm_pa
+        self.use_asm_pa = py_hw_kernel_config.use_asm_pa
 
     def support(self, attn_inputs: PyAttentionInputs) -> bool:
         return True
diff --git a/rtp_llm/models_py/modules/rocm/mlp.py b/rtp_llm/models_py/modules/rocm/mlp.py
index ede65e9a4..880010245 100644
--- a/rtp_llm/models_py/modules/rocm/mlp.py
+++ b/rtp_llm/models_py/modules/rocm/mlp.py
@@ -4,7 +4,7 @@ import aiter
 import torch
 from torch import nn
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.modules import Linear
 from rtp_llm.ops import rtp_llm_ops
 from rtp_llm.utils.model_weight import W
@@ -12,7 +12,7 @@ from rtp_llm.utils.model_weight import W
 
 class DenseMLP(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, weights: Dict[str, torch.Tensor]
     ):
         super().__init__()
 
@@ -44,7 +44,7 @@ class DenseMLP(nn.Module):
 
 class FusedSiluActDenseMLP(nn.Module):
     def __init__(
-        self, config: GptInitModelParameters, weights: Dict[str, torch.Tensor]
+        self, config: ModelConfig, weights: Dict[str, torch.Tensor]
     ):
         super().__init__()
         assert (
diff --git a/rtp_llm/models_py/standalone/test/qwen3_test.py b/rtp_llm/models_py/standalone/test/qwen3_test.py
index 872150675..a289fc593 100644
--- a/rtp_llm/models_py/standalone/test/qwen3_test.py
+++ b/rtp_llm/models_py/standalone/test/qwen3_test.py
@@ -1,45 +1,113 @@
+"""
+Test script for Qwen3 model.
+
+This script demonstrates how to create a Qwen3 model with config and weight
+from a HuggingFace checkpoint path
+"""
+
 import os
 import sys
 
 os.environ["LOAD_PYTHON_MODEL"] = "1"
 
-sys.path.append("/home/wangyin.yx/workspace/FasterTransformer")
+# Optional: Add FasterTransformer path if needed
+# sys.path.append("/home/wangyin.yx/workspace/FasterTransformer")
 
+import torch
 import rtp_llm.models
-from rtp_llm.config.py_config_modules import StaticConfig
-from rtp_llm.distribute.worker_info import g_worker_info, update_master_info
+from rtp_llm.config.task_type import TaskType
+from rtp_llm.distribute.worker_info import ParallelInfo
 from rtp_llm.model_factory import ModelFactory
+from rtp_llm.model_loader.loader import ModelLoader
 from rtp_llm.models_py.model_desc.qwen3 import Qwen3Model
-from rtp_llm.openai.api_datatype import ChatCompletionRequest, ChatMessage, RoleEnum
-from rtp_llm.openai.openai_endpoint import OpenaiEndpoint
-from rtp_llm.pipeline import Pipeline
-from rtp_llm.test.utils.port_util import PortsContext
+from rtp_llm.ops import (
+    DeviceResourceConfig,
+    ParallelismConfig,
+    PyAttentionInputs,
+    PyModelInputs,
+)
+from rtp_llm.utils.database import CkptDatabase
 
-# import pdb
-# pdb.set_trace()
 
-# from librtp_compute_ops.rtp_llm_ops import ParamsBase
+class SimpleModelWrapper:
+    """Simple wrapper to provide config and weight attributes."""
+    def __init__(self, config, weight):
+        self.config = config
+        self.weight = weight
 
 
-start_port = 23345
-StaticConfig.server_config.start_port = start_port
-update_master_info("127.0.0.1", start_port)
-g_worker_info.reload()
-StaticConfig.model_config.model_type = "qwen_3"
-StaticConfig.model_config.checkpoint_path = "Qwen/Qwen3-0.6B"
+# Configuration parameters
+MODEL_TYPE = "qwen_3"
+CHECKPOINT_PATH = "Qwen/Qwen3-0.6B"
 os.environ["DEVICE_RESERVE_MEMORY_BYTES"] = str(3 * 1024 * 1024 * 1024)
-model_config = ModelFactory.create_normal_model_config()
-model = ModelFactory.creat_standalone_py_model_from_huggingface(
-    model_config.ckpt_path, model_config=model_config
+
+# Create model configuration from checkpoint path
+model_cls = ModelFactory.get_model_cls(MODEL_TYPE)
+py_model_config = model_cls._create_config(CHECKPOINT_PATH)
+
+# Create ModelConfig for ModelLoader
+from rtp_llm.model_factory import ModelConfig as ModelConfigType
+model_config_for_loader = ModelConfigType(
+    model_type=MODEL_TYPE,
+    ckpt_path=CHECKPOINT_PATH,
+    tokenizer_path=CHECKPOINT_PATH,
+    max_seq_len=py_model_config.max_seq_len,
+    seq_size_per_block=1,
+    gen_num_per_circle=1,
+    quantization="",
+    act_type="fp16",
 )
 
+# Create parallelism info
+env_params = {
+    "TP_SIZE": "1",
+    "TP_RANK": "0",
+    "DP_SIZE": "1",
+    "DP_RANK": "0",
+    "WORLD_SIZE": "1",
+    "WORLD_RANK": "0",
+    "LOCAL_WORLD_SIZE": "1",
+}
+parallel_info = ParallelInfo.from_params(env_params)
+
+# Create config using model's create_config method
+config = model_cls.create_config(model_config_for_loader, parallel_info)
 
-from rtp_llm.ops import KVCache, PyAttentionInputs, PyModelInputs, PyModelOutputs
+# Load weights using ModelLoader
+weights_info = ModelFactory.get_weight_cls(MODEL_TYPE)(config, tp_size=1, tp_rank=0)
+database = CkptDatabase(CHECKPOINT_PATH, None)
+model_weights_loader = ModelLoader(
+    TaskType.LANGUAGE_MODEL, weights_info, [], torch.float16, database
+)
+weights = model_weights_loader.load_weights(device="cpu")
+
+# Create model wrapper with config and weight attributes
+model = SimpleModelWrapper(config, weights)
+
+# Create required configs for Qwen3Model
+parallelism_config = ParallelismConfig()
+parallelism_config.tp_size = 1
+parallelism_config.tp_rank = 0
+
+device_resource_config = DeviceResourceConfig()
+
+# Get vocab_size from config
+vocab_size = config.vocab_size
+
+# Now we can create Qwen3Model using model.config and model.weight
+qwen3_model = Qwen3Model(
+    model.config,
+    parallelism_config,
+    device_resource_config,
+    model.weight,
+    vocab_size=vocab_size,
+)
 
+# Create test input
 attention_inputs = PyAttentionInputs()
 inputs = PyModelInputs(
     input_ids=torch.randint(0, 10, (1, 10)), attention_inputs=attention_inputs
 )
 
-qwen3_model = Qwen3Model(model.config, model.weight)
-qwen3_model.forward(model.input)
+# Test forward pass (if model is properly initialized)
+# qwen3_model.forward(inputs)
diff --git a/rtp_llm/models_py/test/embeding_test.py b/rtp_llm/models_py/test/embeding_test.py
index 119f526eb..2ad372c8d 100644
--- a/rtp_llm/models_py/test/embeding_test.py
+++ b/rtp_llm/models_py/test/embeding_test.py
@@ -4,8 +4,9 @@ from unittest import SkipTest, TestCase, main
 import torch
 from torch import dtype as _dtype
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.modules.embedding import Embedding, EmbeddingTorch
+from rtp_llm.ops import ParallelismConfig
 
 
 class EmbedingTest(TestCase):
@@ -21,12 +22,18 @@ class EmbedingTest(TestCase):
     def _run_embeding_test(self, num_tokens: int, hidden_size: int, dtype: _dtype):
         torch.manual_seed(0)
         w = torch.randn(131072, hidden_size, dtype=dtype)
-        embeding = Embedding(
-            GptInitModelParameters(
-                head_num=1, size_per_head=1, layer_num=1, max_seq_len=1, vocab_size=1
-            ),
-            w,
-        )
+        py_model_config = ModelConfig()
+        py_model_config.head_num = 1
+        py_model_config.size_per_head = 1
+        py_model_config.num_layers = 1
+        py_model_config.max_seq_len = 1
+        py_model_config.vocab_size = 1
+        
+        parallelism_config = ParallelismConfig()
+        parallelism_config.tp_size = 1
+        parallelism_config.tp_rank = 0
+        
+        embeding = Embedding(py_model_config, parallelism_config, w)
         embeding_torch = EmbeddingTorch(w)
         x = torch.randint(0, hidden_size, (num_tokens,), dtype=torch.int32)
         # with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:
diff --git a/rtp_llm/models_py/test/fused_moe_op_test.py b/rtp_llm/models_py/test/fused_moe_op_test.py
index de480a561..0c7db479c 100644
--- a/rtp_llm/models_py/test/fused_moe_op_test.py
+++ b/rtp_llm/models_py/test/fused_moe_op_test.py
@@ -7,7 +7,8 @@ from torch import dtype as _dtype
 from torch import nn
 from torch.profiler import ProfilerActivity, profile
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.ops import ParallelismConfig
 from rtp_llm.models_py.modules.linear import Linear
 
 from librtp_compute_ops.rtp_llm_ops import FusedMoEOp  # isort:skip
@@ -97,17 +98,25 @@ class FusedMoEOpTest(TestCase):
         inter_dim: int,
     ):
         torch.manual_seed(0)
-        model_param = GptInitModelParameters(1, 128, 1, 1, 5120)
-        model_param.expert_num = num_expert
-        model_param.moe_k = top_k
-        model_param.has_moe_norm = True
-        model_param.hidden_size = hidden_dim
-        model_param.moe_inter_padding_size = inter_dim
-        model_param.moe_normalize_expert_scale = 0
-        model_param.activation_type = "SiGLU"
-        model_param.ep_size = 1
-        model_param.ep_rank = 0
-        fused_moe_op = FusedMoEOp(model_param)
+        py_model_config = ModelConfig()
+        py_model_config.head_num = 1
+        py_model_config.size_per_head = 128
+        py_model_config.num_layers = 1
+        py_model_config.max_seq_len = 1
+        py_model_config.vocab_size = 5120
+        py_model_config.expert_num = num_expert
+        py_model_config.moe_k = top_k
+        py_model_config.has_moe_norm = True
+        py_model_config.hidden_size = hidden_dim
+        py_model_config.moe_inter_padding_size = inter_dim
+        py_model_config.moe_normalize_expert_scale = 0
+        py_model_config.activation_type = "SiGLU"
+        
+        parallelism_config = ParallelismConfig()
+        parallelism_config.ep_size = 1
+        parallelism_config.ep_rank = 0
+        
+        fused_moe_op = FusedMoEOp(py_model_config, parallelism_config)
 
         hidden_states = (
             torch.rand(num_tokens, hidden_dim, dtype=dtype).to("cuda") * 2 - 1
diff --git a/rtp_llm/models_py/test/fused_moe_test.py b/rtp_llm/models_py/test/fused_moe_test.py
index de84afc85..aa2b81460 100644
--- a/rtp_llm/models_py/test/fused_moe_test.py
+++ b/rtp_llm/models_py/test/fused_moe_test.py
@@ -5,9 +5,10 @@ import torch
 import torch.nn.functional as F
 from torch import dtype as _dtype
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.modules.moe import BatchedDataRouter, FusedMoe
 from rtp_llm.models_py.modules.moe.fused_batched_moe import BatchedTritonExperts
+from rtp_llm.ops import MoeConfig, ParallelismConfig, RuntimeConfig
 
 
 def torch_sparse_block_forward(
@@ -83,23 +84,34 @@ class FusedMoeBatchedTest(TestCase):
     ):
         torch.manual_seed(0)
 
-        # Model configuration
-        model_param = GptInitModelParameters(
-            head_num=4,
-            size_per_head=64,
-            layer_num=2,
-            max_seq_len=2048,
-            vocab_size=32000,
-            hidden_size=hidden_size,
-            max_generate_batch_size=num_tokens,
-        )
-        model_param.expert_num = num_experts
-        model_param.moe_k = top_k
-        model_param.moe_inter_padding_size = inter_size
-        model_param.has_moe_norm = True
-        model_param.activation_type = "SiGLU"
-        model_param.ep_size = 1
-        model_param.ep_rank = 0
+        # Create model configuration objects
+        py_model_config = ModelConfig()
+        py_model_config.head_num = 4
+        py_model_config.size_per_head = 64
+        py_model_config.num_layers = 2
+        py_model_config.max_seq_len = 2048
+        py_model_config.vocab_size = 32000
+        py_model_config.hidden_size = hidden_size
+        py_model_config.expert_num = num_experts
+        py_model_config.moe_k = top_k
+        py_model_config.moe_inter_padding_size = inter_size
+        py_model_config.has_moe_norm = True
+        py_model_config.activation_type = "SiGLU"
+        
+        parallelism_config = ParallelismConfig()
+        parallelism_config.ep_size = 1
+        parallelism_config.ep_rank = 0
+        parallelism_config.tp_size = 1
+        parallelism_config.tp_rank = 0
+        parallelism_config.dp_size = 1
+        parallelism_config.dp_rank = 0
+        parallelism_config.world_size = 1
+        parallelism_config.world_rank = 0
+        parallelism_config.local_world_size = 1
+        
+        moe_config = MoeConfig()
+        runtime_config = RuntimeConfig()
+        runtime_config.max_generate_batch_size = num_tokens
 
         # Create router and experts
         router = BatchedDataRouter(
diff --git a/rtp_llm/models_py/test/mla_attention_test.py b/rtp_llm/models_py/test/mla_attention_test.py
index 6196fc3a3..2b0fa5d0d 100644
--- a/rtp_llm/models_py/test/mla_attention_test.py
+++ b/rtp_llm/models_py/test/mla_attention_test.py
@@ -12,7 +12,8 @@ import torch
 # sys.path.append(os.path.join(str(CUR_PATH), "../../../"))
 device = torch.device(f"cuda")
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.ops import ParallelismConfig
 from rtp_llm.models.rotary_embedding.deepseek_rotary_embedding import (
     DeepseekV3YarnRotaryEmbedding,
 )
@@ -108,8 +109,12 @@ class MLATest(TestCase):
             )
             bias += seq_page_sizes[i]
 
-        self.config = GptInitModelParameters(128, 16, 27, 1024, 102400)
+        self.config = ModelConfig()
         self.config.head_num = 16
+        self.config.num_layers = 27
+        self.config.max_seq_len = 1024
+        self.config.vocab_size = 102400
+        self.config.size_per_head = 128
         self.config.hidden_size = hidden_size
         self.config.nope_head_dim = 128
         self.config.rope_head_dim = 64
@@ -120,6 +125,10 @@ class MLATest(TestCase):
         self.config.softmax_extra_scale = 1.0
         self.config.use_mla = True
         self.config.size_per_head = 192
+        
+        self.parallelism_config = ParallelismConfig()
+        self.parallelism_config.tp_size = 1
+        self.parallelism_config.tp_rank = 0
 
         torch.manual_seed(0)
         sequence_lengths_mius_1 = [x - 1 for x in sequence_lengths]
@@ -194,11 +203,11 @@ class MLATest(TestCase):
         layer_weights.append(weights)
 
         fmha_impl = MlaFlashInferPrefillImpl(
-            self.config, attn_inputs, layer_weights, create_cos_sin_cache()
+            self.config, self.parallelism_config, attn_inputs, layer_weights, create_cos_sin_cache()
         )
-        deepseekv2_mla = DeepSeekV2Attention(self.config, weights, 0)
+        deepseekv2_mla = DeepSeekV2Attention(self.config, self.parallelism_config, weights, 0)
         kv_cache: Optional[KVCache] = None
-        deepseekv2_mla_ref = DeepseekV2AttentionRef(self.config, weights, 0)
+        deepseekv2_mla_ref = DeepseekV2AttentionRef(self.config, weights, 0, quant_config=None)
 
         hidden = torch.randn(
             [num_tokens, self.config.hidden_size],
diff --git a/rtp_llm/models_py/test/mlp_test.py b/rtp_llm/models_py/test/mlp_test.py
index 9f6efd6b2..2dbafe970 100644
--- a/rtp_llm/models_py/test/mlp_test.py
+++ b/rtp_llm/models_py/test/mlp_test.py
@@ -4,8 +4,9 @@ from unittest import SkipTest, TestCase, main
 import torch
 from torch import dtype as _dtype
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.models_py.modules.mlp import DenseMLP, FusedSiluActDenseMLP
+from rtp_llm.ops import ParallelismConfig
 from rtp_llm.utils.model_weight import W
 
 
@@ -27,8 +28,18 @@ class MLPTest(TestCase):
 
     def _run_mlp_test(self, num_tokens: int, hidden_size: int, dtype: _dtype):
         torch.manual_seed(0)
-        model_param = GptInitModelParameters(1, 128, 1, 1, 5120)
-        model_param.activation_type = "SiGLU"
+        py_model_config = ModelConfig()
+        py_model_config.head_num = 1
+        py_model_config.size_per_head = 128
+        py_model_config.num_layers = 1
+        py_model_config.max_seq_len = 1
+        py_model_config.vocab_size = 5120
+        py_model_config.activation_type = "SiGLU"
+        
+        parallelism_config = ParallelismConfig()
+        parallelism_config.tp_size = 1
+        parallelism_config.tp_rank = 0
+        
         weights = {}
         weights[W.ffn_w1] = torch.randn(hidden_size, 4 * hidden_size, dtype=dtype)
         torch.nn.init.xavier_uniform_(weights[W.ffn_w1])
@@ -37,8 +48,8 @@ class MLPTest(TestCase):
         weights[W.ffn_w2] = torch.randn(4 * hidden_size, hidden_size, dtype=dtype)
         torch.nn.init.xavier_uniform_(weights[W.ffn_w2])
 
-        qwen3_mlp = DenseMLP(model_param, weights)
-        qwen3_mlp_fused = FusedSiluActDenseMLP(model_param, weights)
+        qwen3_mlp = DenseMLP(py_model_config, parallelism_config, weights)
+        qwen3_mlp_fused = FusedSiluActDenseMLP(py_model_config, parallelism_config, weights)
 
         x = torch.randn(num_tokens, hidden_size, dtype=dtype)
 
diff --git a/rtp_llm/models_py/test/moe_select_topk_test.py b/rtp_llm/models_py/test/moe_select_topk_test.py
index ae1885669..e539f5867 100644
--- a/rtp_llm/models_py/test/moe_select_topk_test.py
+++ b/rtp_llm/models_py/test/moe_select_topk_test.py
@@ -1,7 +1,6 @@
 import torch
 import itertools
 from unittest import TestCase, main, SkipTest
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.utils.model_weight import W
 from rtp_llm.models_py.modules.ep.topk import select_experts
 from torch import dtype as _dtype
diff --git a/rtp_llm/models_py/test/rocm_embedding_test.py b/rtp_llm/models_py/test/rocm_embedding_test.py
index 119f526eb..b188935e3 100644
--- a/rtp_llm/models_py/test/rocm_embedding_test.py
+++ b/rtp_llm/models_py/test/rocm_embedding_test.py
@@ -4,7 +4,8 @@ from unittest import SkipTest, TestCase, main
 import torch
 from torch import dtype as _dtype
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.ops import ParallelismConfig
 from rtp_llm.models_py.modules.embedding import Embedding, EmbeddingTorch
 
 
@@ -21,12 +22,18 @@ class EmbedingTest(TestCase):
     def _run_embeding_test(self, num_tokens: int, hidden_size: int, dtype: _dtype):
         torch.manual_seed(0)
         w = torch.randn(131072, hidden_size, dtype=dtype)
-        embeding = Embedding(
-            GptInitModelParameters(
-                head_num=1, size_per_head=1, layer_num=1, max_seq_len=1, vocab_size=1
-            ),
-            w,
-        )
+        py_model_config = ModelConfig()
+        py_model_config.head_num = 1
+        py_model_config.size_per_head = 1
+        py_model_config.num_layers = 1
+        py_model_config.max_seq_len = 1
+        py_model_config.vocab_size = 1
+        
+        parallelism_config = ParallelismConfig()
+        parallelism_config.tp_size = 1
+        parallelism_config.tp_rank = 0
+        
+        embeding = Embedding(py_model_config, parallelism_config, w)
         embeding_torch = EmbeddingTorch(w)
         x = torch.randint(0, hidden_size, (num_tokens,), dtype=torch.int32)
         # with profile(activities=[ProfilerActivity.CUDA], record_shapes=True) as prof:
diff --git a/rtp_llm/models_py/test/select_topk_op_test.py b/rtp_llm/models_py/test/select_topk_op_test.py
index 4c205f352..07c620288 100644
--- a/rtp_llm/models_py/test/select_topk_op_test.py
+++ b/rtp_llm/models_py/test/select_topk_op_test.py
@@ -6,7 +6,7 @@ import torch.nn.functional as F
 from torch import dtype as _dtype
 from torch.profiler import ProfilerActivity, profile
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 
 from librtp_compute_ops.rtp_llm_ops import SelectTopkOp  # isort:skip
 
@@ -31,11 +31,16 @@ class SelectTopkOpTest(TestCase):
         self, num_tokens: int, num_expert: int, top_k: int, dtype: _dtype
     ):
         torch.manual_seed(1)
-        model_param = GptInitModelParameters(1, 128, 1, 1, 5120)
-        model_param.expert_num = num_expert
-        model_param.moe_k = top_k
-        model_param.has_moe_norm = True
-        select_topk_op = SelectTopkOp(model_param)
+        py_model_config = ModelConfig()
+        py_model_config.head_num = 1
+        py_model_config.size_per_head = 128
+        py_model_config.num_layers = 1
+        py_model_config.max_seq_len = 1
+        py_model_config.vocab_size = 5120
+        py_model_config.expert_num = num_expert
+        py_model_config.moe_k = top_k
+        py_model_config.has_moe_norm = True
+        select_topk_op = SelectTopkOp(py_model_config)
 
         router_logits = torch.randn(num_tokens, num_expert, dtype=dtype).to("cuda")
         router_logits_fp32 = router_logits.float()
diff --git a/rtp_llm/openai/openai_endpoint.py b/rtp_llm/openai/openai_endpoint.py
index 2b1ba642b..f28af13a2 100644
--- a/rtp_llm/openai/openai_endpoint.py
+++ b/rtp_llm/openai/openai_endpoint.py
@@ -1,12 +1,19 @@
 import json
 import logging
 from functools import partial
-from typing import AsyncGenerator, List, Optional
+from typing import Any, AsyncGenerator, List, Optional
 
 from fastapi import Request
 
 from rtp_llm.config.generate_config import GenerateConfig
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_args import ModelArgs
+from rtp_llm.config.py_config_modules import (
+    GenerateEnvConfig,
+    RenderConfig,
+    PyMiscellaneousConfig,
+    VitConfig,
+)
+from rtp_llm.ops import SpecialTokens
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.openai.api_datatype import (
     ChatCompletionRequest,
@@ -39,12 +46,23 @@ from rtp_llm.utils.complete_response_async_generator import (
 class OpenaiEndpoint(object):
     def __init__(
         self,
-        model_config: GptInitModelParameters,
+        model_args: ModelArgs,
+        generate_env_config: GenerateEnvConfig,
+        render_config: RenderConfig,
+        misc_config: PyMiscellaneousConfig,
+        vit_config: VitConfig,
+        special_tokens: SpecialTokens,
+        max_seq_len: int,
+        template_type: Optional[Any],
+        model_name: str,
+        ckpt_path: str,
         tokenizer: BaseTokenizer,
         backend_rpc_server_visitor: BackendRPCServerVisitor,
     ):
-        self.model_config = model_config
-        self.max_seq_len = self.model_config.max_seq_len
+        self.generate_env_config = generate_env_config
+        self.max_seq_len = max_seq_len
+        self.model_name = model_name
+        self.special_tokens = special_tokens
 
         if tokenizer == None:
             raise AttributeError(f"tokenizer is none!")
@@ -53,27 +71,27 @@ class OpenaiEndpoint(object):
 
         self.eos_token_id = tokenizer.eos_token_id
         if self.eos_token_id == None:
-            self.eos_token_id = self.model_config.special_tokens.eos_token_id
+            self.eos_token_id = special_tokens.eos_token_id
 
-        self.stop_words_id_list = self.model_config.special_tokens.stop_words_id_list
+        self.stop_words_id_list = special_tokens.stop_words_id_list
 
         render_params = RendererParams(
-            model_type=model_config.py_env_configs.model_config.model_type,
+            model_type=model_args.model_type,
             max_seq_len=self.max_seq_len,
             eos_token_id=self.eos_token_id,
             stop_word_ids_list=self.stop_words_id_list,
-            template_type=self.model_config.template_type,
-            ckpt_path=self.model_config.ckpt_path,
+            template_type=template_type,
+            ckpt_path=ckpt_path,
         )
 
         self.chat_renderer: CustomChatRenderer = ChatRendererFactory.get_renderer(
-            self.tokenizer, render_params
+            self.tokenizer, render_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config
         )
         logging.info(f"Finally openai endpoint uses renderer: {self.chat_renderer} ")
         self.template_renderer: CustomChatRenderer = (
             self.chat_renderer
             if isinstance(self.chat_renderer, BasicRenderer)
-            else BasicRenderer(self.tokenizer, render_params)
+            else BasicRenderer(self.tokenizer, render_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         )
         logging.info(f"chat_renderer [{self.chat_renderer}] is created.")
         extra_stop_word_ids_list = self.chat_renderer.get_all_extra_stop_word_ids_list()
@@ -84,21 +102,15 @@ class OpenaiEndpoint(object):
             if len(word):
                 self.stop_words_str_list.append(word)
 
-        env_stop_words_str = (
-            self.model_config.py_env_configs.generate_env_config.stop_words_str
-        )
-        env_stop_words_id = (
-            self.model_config.py_env_configs.generate_env_config.stop_words_list
-        )
+        env_stop_words_str = generate_env_config.stop_words_str
+        env_stop_words_id = generate_env_config.stop_words_list
         env_stop_words_str_list = (
             json.loads(env_stop_words_str) if env_stop_words_str else []
         )
         env_stop_words_id_list = (
             json.loads(env_stop_words_id) if env_stop_words_id else []
         )
-        env_force_stop = (
-            self.model_config.py_env_configs.generate_env_config.force_stop_words
-        )
+        env_force_stop = generate_env_config.force_stop_words
         if env_force_stop:
             self.stop_words_str_list = env_stop_words_str_list
             self.stop_words_id_list = env_stop_words_id_list
@@ -115,7 +127,7 @@ class OpenaiEndpoint(object):
 
     async def list_models(self):
         global model_args
-        model_card = ModelCard(id=self.model_config.model_name)
+        model_card = ModelCard(id=self.model_name)
         return ModelList(data=[model_card])
 
     def _extract_generation_config(
@@ -151,7 +163,7 @@ class OpenaiEndpoint(object):
             config.return_all_probs = request.logprobs
         if request.logprobs or request.functions:
             config.is_streaming = True
-        config.add_special_tokens(self.model_config.special_tokens)
+        config.add_special_tokens(self.special_tokens)
         config.convert_select_tokens(self.tokenizer.vocab_size, self.tokenizer)
         if (
             request.extra_configs
@@ -159,7 +171,11 @@ class OpenaiEndpoint(object):
             and isinstance(request.extra_configs.max_thinking_tokens, int)
         ):
             config.max_thinking_tokens = request.extra_configs.max_thinking_tokens
-        config.add_thinking_params(self.tokenizer)
+        # add_thinking_params now accepts generate_env_config parameter
+        config.add_thinking_params(
+            self.tokenizer, 
+            self.generate_env_config
+        )
         return config
 
     def _merge_tool_calls(
@@ -317,7 +333,7 @@ class OpenaiEndpoint(object):
             choices=all_choices,
             usage=usage,
             aux_info=aux_info,
-            model=self.model_config.model_name,
+            model=self.model_name,
             debug_info=debug_info,
             extra_outputs=extra_outputs,
         )
diff --git a/rtp_llm/openai/renderer_factory.py b/rtp_llm/openai/renderer_factory.py
index 3ed788606..bc32caa84 100644
--- a/rtp_llm/openai/renderer_factory.py
+++ b/rtp_llm/openai/renderer_factory.py
@@ -1,8 +1,8 @@
 import copy
 import logging
-from typing import Optional
+from typing import Any, Optional
 
-from rtp_llm.config.py_config_modules import StaticConfig
+from rtp_llm.config.py_config_modules import GenerateEnvConfig, RenderConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.openai.renderer_factory_register import _renderer_factory
 from rtp_llm.openai.renderers.basic_renderer import BasicRenderer
@@ -19,14 +19,19 @@ class ChatRendererFactory:
     def try_get_imported_renderer(
         tokenizer: BaseTokenizer,
         params: RendererParams,
+        generate_env_config: GenerateEnvConfig,
+        render_config: Optional[RenderConfig] = None,
+        ckpt_path: Optional[str] = None,
+        misc_config: Optional[Any] = None,
+        vit_config: Optional[Any] = None,
     ) -> Optional[CustomChatRenderer]:
         try:
-            return FastChatRenderer(tokenizer, params)
+            return FastChatRenderer(tokenizer, params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         except KeyError:
             pass
 
         try:
-            return LlamaTemplateRenderer(tokenizer, params)
+            return LlamaTemplateRenderer(tokenizer, params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         except AssertionError as e:  # assertion at llama_template.py:229
             pass
         return None
@@ -35,14 +40,30 @@ class ChatRendererFactory:
     def get_renderer(
         tokenizer: BaseTokenizer,
         params: RendererParams,
+        generate_env_config: GenerateEnvConfig,
+        render_config: Optional[RenderConfig] = None,
+        ckpt_path: Optional[str] = None,
+        misc_config: Optional[Any] = None,
+        vit_config: Optional[Any] = None,
     ) -> CustomChatRenderer:
+        """Get renderer for tokenizer and params.
+        
+        Args:
+            tokenizer: BaseTokenizer instance.
+            params: RendererParams object.
+            generate_env_config: GenerateEnvConfig object.
+            render_config: RenderConfig object.
+            ckpt_path: Checkpoint path string.
+            misc_config: MiscellaneousConfig object.
+            vit_config: VitConfig object.
+        """
         # renderer priority:  `MODEL_TEMPLATE_TYPE` env for llama template or fastchat conversation
         #                    > tokenizer.chat_template
         #                    > model customized renderer (e.g. Qwen, which implemented function call)
         #                    > try get template from `MODEL_TYPE`
         #                    > transformers default chat template
 
-        model_template_type = StaticConfig.render_config.model_template_type
+        model_template_type = render_config.model_template_type
         if model_template_type:
             new_params = copy.deepcopy(params)
             new_params.model_type = model_template_type
@@ -50,7 +71,7 @@ class ChatRendererFactory:
                 f"Renderer factory try found MODEL_TEMPLATE_TYPE: {model_template_type}, try get predefined renderer."
             )
             renderer = ChatRendererFactory.try_get_imported_renderer(
-                tokenizer, new_params
+                tokenizer, new_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config
             )
             if renderer:
                 return renderer
@@ -66,14 +87,14 @@ class ChatRendererFactory:
             logging.info(
                 f"Renderer factory found model type [{params.model_type}] has dedicated renderer, use this."
             )
-            return _renderer_factory[params.model_type](tokenizer, params)
+            return _renderer_factory[params.model_type](tokenizer, params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
 
         try:
             if tokenizer.chat_template != None:
                 logging.info(
                     f"Renderer factory found tokenizer has chat_template [{tokenizer.chat_template}], use it."
                 )
-                return BasicRenderer(tokenizer, params)
+                return BasicRenderer(tokenizer, params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
             else:
                 pass
         except AttributeError:
@@ -83,7 +104,7 @@ class ChatRendererFactory:
             f"Renderer factory try get predefined renderer via model type [{params.model_type}]"
         )
         imported_template_renderer = ChatRendererFactory.try_get_imported_renderer(
-            tokenizer, params
+            tokenizer, params, generate_env_config, render_config, ckpt_path, misc_config, vit_config
         )
         if imported_template_renderer:
             logging.info(
@@ -94,4 +115,4 @@ class ChatRendererFactory:
         logging.warn(
             f"Renderer factory found model [{params.model_type}] falls back to basic renderer, this is typically unwanted."
         )
-        return BasicRenderer(tokenizer, params)
+        return BasicRenderer(tokenizer, params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
diff --git a/rtp_llm/openai/renderers/basic_renderer.py b/rtp_llm/openai/renderers/basic_renderer.py
index 4df50e374..c564ca64e 100644
--- a/rtp_llm/openai/renderers/basic_renderer.py
+++ b/rtp_llm/openai/renderers/basic_renderer.py
@@ -9,6 +9,9 @@ from jinja2.exceptions import TemplateError
 from jinja2.sandbox import ImmutableSandboxedEnvironment
 from packaging import version
 
+from typing import Any, Optional
+
+from rtp_llm.config.py_config_modules import GenerateEnvConfig, RenderConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.openai.api_datatype import ChatCompletionRequest
 from rtp_llm.openai.renderers.custom_renderer import (
@@ -46,8 +49,13 @@ class BasicRenderer(CustomChatRenderer):
         self,
         tokenizer: BaseTokenizer,
         renderer_params: RendererParams,
+        generate_env_config: GenerateEnvConfig,
+        render_config: Optional[RenderConfig] = None,
+        ckpt_path: Optional[str] = None,
+        misc_config: Optional[Any] = None,
+        vit_config: Optional[Any] = None,
     ):
-        super().__init__(tokenizer, renderer_params)
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
 
         if version.parse(jinja2.__version__) <= version.parse("3.0.0"):
             raise ImportError(
@@ -58,7 +66,7 @@ class BasicRenderer(CustomChatRenderer):
 
         self.add_generation_prompt = True
         self.chat_template = None
-        self.special_tokens_map = {}
+        self.special_tokensmap = {}
         try:
             self._setup_chat_template()
             assert self.chat_template != None
@@ -75,9 +83,9 @@ class BasicRenderer(CustomChatRenderer):
                 self.add_extra_stop_words(["<|im_end|>"])
 
         try:
-            if tokenizer.special_tokens_map != None:
-                self.special_tokens_map = tokenizer.special_tokens_map
-                for k, v in self.special_tokens_map.items():
+            if tokenizer.special_tokensmap != None:
+                self.special_tokensmap = tokenizer.special_tokensmap
+                for k, v in self.special_tokensmap.items():
                     logging.info(f"special token [{v}]({k}) added as stop words.")
                     if isinstance(v, str):
                         self.add_extra_stop_words([v])
@@ -106,10 +114,10 @@ class BasicRenderer(CustomChatRenderer):
 
         logging.info(f"found chat template to use: {self.chat_template}")
         self.default_template_key = (
-            self.py_env_configs.render_config.default_chat_template_key
+            self.render_config.default_chat_template_key
         )
         self.default_tool_use_template_key = (
-            self.py_env_configs.render_config.default_tool_use_template_key
+            self.render_config.default_tool_use_template_key
         )
         self.compiled_template_map: Dict[str, jinja2.Template] = {}
 
@@ -175,7 +183,7 @@ class BasicRenderer(CustomChatRenderer):
             "json": json,
             "add_generation_prompt": self.add_generation_prompt,
         }
-        render_args.update(self.special_tokens_map)
+        render_args.update(self.special_tokensmap)
         # functions with none value may occur exception in llama3 template
         if request_dict.get("functions", None):
             render_args["functions"] = request_dict["functions"]
diff --git a/rtp_llm/openai/renderers/chatglm4_renderer.py b/rtp_llm/openai/renderers/chatglm4_renderer.py
index 21becd4af..4a2d3b3a1 100644
--- a/rtp_llm/openai/renderers/chatglm4_renderer.py
+++ b/rtp_llm/openai/renderers/chatglm4_renderer.py
@@ -17,8 +17,17 @@ from rtp_llm.openai.renderers.custom_renderer import (
 
 
 class ChatGlm4Renderer(CustomChatRenderer):
-    def __init__(self, tokenizer: BaseTokenizer, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: BaseTokenizer, 
+        renderer_params: RendererParams,
+        generate_env_config,
+        render_config=None,
+        ckpt_path=None,
+        misc_config=None,
+        vit_config=None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
 
     def get_renderer_info(self) -> RendererInfo:
         renderer_info = super().get_renderer_info()
diff --git a/rtp_llm/openai/renderers/custom_renderer.py b/rtp_llm/openai/renderers/custom_renderer.py
index 5406ce0bd..41563880c 100644
--- a/rtp_llm/openai/renderers/custom_renderer.py
+++ b/rtp_llm/openai/renderers/custom_renderer.py
@@ -4,13 +4,13 @@ import json
 import logging
 import os
 from dataclasses import dataclass, field
-from typing import AsyncGenerator, List, Optional, Union
+from enum import Enum
+from typing import Any, AsyncGenerator, List, Optional, Union
 
 import torch
 
 from rtp_llm.config.generate_config import GenerateConfig
-from rtp_llm.config.gpt_init_model_parameters import TemplateType
-from rtp_llm.config.py_config_modules import PyEnvConfigs, StaticConfig
+from rtp_llm.config.py_config_modules import GenerateEnvConfig, RenderConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.openai.api_datatype import (
     ChatCompletionExtraOutputs,
@@ -46,14 +46,19 @@ from rtp_llm.utils.word_util import (
     truncate_response_with_stop_words,
 )
 
-THINK_MODE = StaticConfig.generate_env_config.think_mode
-
-THINK_START_TAG = StaticConfig.generate_env_config.think_start_tag.encode(
-    "utf-8"
-).decode("unicode_escape")
-THINK_END_TAG = StaticConfig.generate_env_config.think_end_tag.encode("utf-8").decode(
-    "unicode_escape"
-)
+def _get_think_config(generate_env_config):
+    """Get thinking configuration from generate_env_config.
+    
+    Args:
+        generate_env_config: GenerateEnvConfig object.
+        
+    Returns:
+        Tuple of (think_mode, think_start_tag, think_end_tag)
+    """
+    think_mode = generate_env_config.think_mode
+    think_start_tag = generate_env_config.think_start_tag.encode("utf-8").decode("unicode_escape")
+    think_end_tag = generate_env_config.think_end_tag.encode("utf-8").decode("unicode_escape")
+    return think_mode, think_start_tag, think_end_tag
 
 
 class StreamStatus:
@@ -188,6 +193,11 @@ class ResponseObject:
     usage: Optional[UsageInfo] = None
     aux_info: Optional[AuxInfo] = None
 
+class TemplateType(Enum):
+    """Template type for different model types."""
+    chat = "chat"
+    vqa = "vqa"
+    base = "image"
 
 @dataclass
 class RendererParams:
@@ -257,9 +267,33 @@ class CustomChatRenderer:
         self,
         tokenizer: BaseTokenizer,
         renderer_params: RendererParams,
+        generate_env_config: GenerateEnvConfig,
+        render_config: Optional[RenderConfig] = None,
+        ckpt_path: Optional[str] = None,
+        misc_config: Optional[Any] = None,
+        vit_config: Optional[Any] = None,
     ):
-        self.py_env_configs = PyEnvConfigs()
-        self.py_env_configs.update_from_env()
+        # Get think config from generate_env_config
+        self.think_mode, self.think_start_tag, self.think_end_tag = _get_think_config(
+            generate_env_config
+        )
+        
+        # Store configs for subclasses
+        self.ckpt_path = ckpt_path
+        self.misc_config = misc_config
+        self.vit_config = vit_config
+        self.render_config = render_config
+        
+        # Create a minimal model_config-like object for renderers that access self.model_config.checkpoint_path
+        # This is only for backward compatibility with existing renderer code that accesses model_config attributes
+        class MinimalModelConfig:
+            def __init__(self, ckpt_path: str, misc_config: Any, vit_config: Any):
+                self.ckpt_path = ckpt_path
+                self.checkpoint_path = ckpt_path
+                self.misc_config = misc_config
+                self.vit_config = vit_config
+        self.model_config = MinimalModelConfig(ckpt_path or "", misc_config, vit_config)
+        
         self.tokenizer = tokenizer
         self.model_type = renderer_params.model_type
         self.max_seq_len = renderer_params.max_seq_len
@@ -615,20 +649,20 @@ class CustomChatRenderer:
             while processing_index < output_len:
                 if think_status.in_think_mode:
                     think_status.think_buffer += item.output_str[processing_index]
-                    if think_status.think_buffer.startswith(THINK_START_TAG):
+                    if think_status.think_buffer.startswith(self.think_start_tag):
                         think_status.think_buffer = think_status.think_buffer[
-                            len(THINK_START_TAG) :
+                            len(self.think_start_tag) :
                         ]
 
-                    if think_status.think_buffer.endswith(THINK_END_TAG):
+                    if think_status.think_buffer.endswith(self.think_end_tag):
                         reasoning_text = think_status.think_buffer[
-                            : -len(THINK_END_TAG)
+                            : -len(self.think_end_tag)
                         ]
                         think_status.think_buffer = ""
                         think_status.in_think_mode = False
                     elif has_overlap_kmp(
-                        think_status.think_buffer, THINK_END_TAG
-                    ) or THINK_START_TAG.startswith(think_status.think_buffer):
+                        think_status.think_buffer, self.think_end_tag
+                    ) or self.think_start_tag.startswith(think_status.think_buffer):
                         pass
                     else:
                         reasoning_text = think_status.think_buffer
@@ -639,8 +673,8 @@ class CustomChatRenderer:
 
             if think_status.in_think_mode:
                 if has_overlap_kmp(
-                    think_status.think_buffer, THINK_END_TAG
-                ) or THINK_START_TAG.startswith(think_status.think_buffer):
+                    think_status.think_buffer, self.think_end_tag
+                ) or self.think_start_tag.startswith(think_status.think_buffer):
                     reasoning_text = ""
                 else:
                     think_status.think_buffer = ""
@@ -823,8 +857,7 @@ class CustomChatRenderer:
         return [StreamStatus(request) for _ in range(n)]
 
     def in_think_mode(self, request: ChatCompletionRequest):
-        global THINK_MODE
-        return THINK_MODE
+        return self.think_mode
 
     def should_process_think(self, request: ChatCompletionRequest):
         # 留出方法给子类重写, 避免重复的think处理
@@ -1321,7 +1354,7 @@ class CustomChatRenderer:
         def split_think_tag(text: Optional[str]):
             if text is None:
                 return None, None
-            text_results = text.split(THINK_END_TAG, 1)
+            text_results = text.split(self.think_end_tag, 1)
             reasoning_content = text_results[0] if len(text_results) == 2 else None
             content = text_results[1] if len(text_results) == 2 else text
             return content, reasoning_content
diff --git a/rtp_llm/openai/renderers/fast_chat_renderer.py b/rtp_llm/openai/renderers/fast_chat_renderer.py
index 4754457f2..8095976ab 100644
--- a/rtp_llm/openai/renderers/fast_chat_renderer.py
+++ b/rtp_llm/openai/renderers/fast_chat_renderer.py
@@ -1,3 +1,6 @@
+from typing import Any, Optional
+
+from rtp_llm.config.py_config_modules import GenerateEnvConfig, RenderConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.openai.api_datatype import ChatCompletionRequest, RendererInfo, RoleEnum
 from rtp_llm.openai.renderers.custom_renderer import (
@@ -10,8 +13,17 @@ from .conversation import get_conv_template
 
 
 class FastChatRenderer(CustomChatRenderer):
-    def __init__(self, tokenizer: BaseTokenizer, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: BaseTokenizer, 
+        renderer_params: RendererParams,
+        generate_env_config: GenerateEnvConfig,
+        render_config: Optional[RenderConfig] = None,
+        ckpt_path: Optional[str] = None,
+        misc_config: Optional[Any] = None,
+        vit_config: Optional[Any] = None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         self.conv_template = get_conv_template(renderer_params.model_type)
         self.roles_map = {
             RoleEnum.user: self.conv_template.roles[0],
diff --git a/rtp_llm/openai/renderers/internvl_renderer.py b/rtp_llm/openai/renderers/internvl_renderer.py
index 27dc60f74..cc3376a97 100644
--- a/rtp_llm/openai/renderers/internvl_renderer.py
+++ b/rtp_llm/openai/renderers/internvl_renderer.py
@@ -4,7 +4,6 @@ import json
 import os
 from typing import List
 
-from rtp_llm.config.py_config_modules import StaticConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.openai.api_datatype import (
     ChatCompletionRequest,
@@ -117,14 +116,28 @@ conv_templates = {
 }
 
 
+from typing import Any, Optional
+
+from rtp_llm.config.py_config_modules import GenerateEnvConfig, RenderConfig
+
 class InternVLRenderer(CustomChatRenderer):
-    def __init__(self, tokenizer: BaseTokenizer, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: BaseTokenizer, 
+        renderer_params: RendererParams,
+        generate_env_config: GenerateEnvConfig,
+        render_config: Optional[RenderConfig] = None,
+        ckpt_path: Optional[str] = None,
+        misc_config: Optional[Any] = None,
+        vit_config: Optional[Any] = None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         self.roles = {RoleEnum.user: "USER", RoleEnum.assistant: "ASSISTANT"}
         self.video_frame_num = 8
 
     def _render_messages(self, messages: List[ChatMessage]) -> PromptWithMMInput:
-        ckpt_path: str = StaticConfig.model_config.checkpoint_path
+        # Use checkpoint path from model_config
+        ckpt_path: str = self.model_config.checkpoint_path
         config_path = os.path.join(fetch_remote_file_to_local(ckpt_path), "config.json")
         if os.path.exists(config_path):
             with open(config_path) as reader:
diff --git a/rtp_llm/openai/renderers/llama_template_renderer.py b/rtp_llm/openai/renderers/llama_template_renderer.py
index c42edbdbb..9f38dd006 100644
--- a/rtp_llm/openai/renderers/llama_template_renderer.py
+++ b/rtp_llm/openai/renderers/llama_template_renderer.py
@@ -1,6 +1,9 @@
 from dataclasses import dataclass
 from typing import List, Optional, Tuple
 
+from typing import Any, Optional
+
+from rtp_llm.config.py_config_modules import GenerateEnvConfig, RenderConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.openai.api_datatype import (
     ChatCompletionRequest,
@@ -25,8 +28,17 @@ class LlamaTemplateArgs:
 
 
 class LlamaTemplateRenderer(CustomChatRenderer):
-    def __init__(self, tokenizer: BaseTokenizer, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: BaseTokenizer, 
+        renderer_params: RendererParams,
+        generate_env_config: GenerateEnvConfig,
+        render_config: Optional[RenderConfig] = None,
+        ckpt_path: Optional[str] = None,
+        misc_config: Optional[Any] = None,
+        vit_config: Optional[Any] = None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         model_name = renderer_params.model_type
         self.template = get_template_and_fix_tokenizer(model_name, tokenizer)
         self.add_extra_stop_words(self.template.stop_words)
diff --git a/rtp_llm/openai/renderers/llava_renderer.py b/rtp_llm/openai/renderers/llava_renderer.py
index 2e984d9c6..a9158ab4a 100644
--- a/rtp_llm/openai/renderers/llava_renderer.py
+++ b/rtp_llm/openai/renderers/llava_renderer.py
@@ -1,9 +1,8 @@
 import copy
 from dataclasses import dataclass
 from enum import Enum, auto
-from typing import Dict, List
+from typing import Any, Dict, List, Optional
 
-from rtp_llm.config.py_config_modules import StaticConfig
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.openai.api_datatype import (
     ChatCompletionRequest,
@@ -180,9 +179,20 @@ conv_templates = {
 }
 
 
+from rtp_llm.config.py_config_modules import GenerateEnvConfig, RenderConfig
+
 class LlavaRenderer(CustomChatRenderer):
-    def __init__(self, tokenizer: BaseTokenizer, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: BaseTokenizer, 
+        renderer_params: RendererParams,
+        generate_env_config: GenerateEnvConfig,
+        render_config: Optional[RenderConfig] = None,
+        ckpt_path: Optional[str] = None,
+        misc_config: Optional[Any] = None,
+        vit_config: Optional[Any] = None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
 
     def _get_conv_template(self, model_name: str) -> Conversation:
         if "v1" in model_name.lower():
@@ -196,10 +206,11 @@ class LlavaRenderer(CustomChatRenderer):
         return conv_templates[conv_mode]
 
     def _render_messages(self, messages: List[ChatMessage]) -> PromptWithMMInput:
-        ckpt_path: str = StaticConfig.model_config.checkpoint_path
+        # Use checkpoint path from model_config
+        ckpt_path: str = self.model_config.checkpoint_path
         model_name: str = ckpt_path.split("?")[0]  # oss style path
         model_name = model_name.strip("/").split("/")[-1]
-        llava_template_env: str = self.py_env_configs.render_config.llava_chat_template
+        llava_template_env: str = self.render_config.llava_chat_template
         conv_template = (
             self._get_conv_template(model_name)
             if llava_template_env == ""
diff --git a/rtp_llm/openai/renderers/minicpmv_renderer.py b/rtp_llm/openai/renderers/minicpmv_renderer.py
index bb3b68bf0..93ba0b0f7 100644
--- a/rtp_llm/openai/renderers/minicpmv_renderer.py
+++ b/rtp_llm/openai/renderers/minicpmv_renderer.py
@@ -70,7 +70,7 @@ class MiniCPMVConversation:
                     elif content_part.type == ContentPartTypeEnum.image_url:
                         assert content_part.image_url != None
                         urls.append(content_part.image_url.url)
-                        data = get_bytes_io_from_url(content_part.image_url.url)
+                        data = get_bytes_io_from_url(content_part.image_url.url, download_headers=self.download_headers, url_cache_size=self.url_cache_size)
                         data = Image.open(data).convert("RGB")
                         images.append(data)
                         cur_msgs.append("(<image>./</image>)")
@@ -78,7 +78,7 @@ class MiniCPMVConversation:
                     elif content_part.type == ContentPartTypeEnum.video_url:
                         assert content_part.video_url != None
                         urls.append(content_part.video_url.url)
-                        data = get_bytes_io_from_url(content_part.video_url.url)
+                        data = get_bytes_io_from_url(content_part.video_url.url, download_headers=self.download_headers, url_cache_size=self.url_cache_size)
                         data = encode_video(data)
                         images.extend(data)
                         cur_msgs.extend(
@@ -91,12 +91,27 @@ class MiniCPMVConversation:
 
 class MiniCPMVRenderer(CustomChatRenderer):
 
-    def __init__(self, tokenizer: BaseTokenizer, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: BaseTokenizer, 
+        renderer_params: RendererParams,
+        generate_env_config,
+        render_config=None,
+        ckpt_path=None,
+        misc_config=None,
+        vit_config=None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         self.processor = AutoProcessor.from_pretrained(
             self.ckpt_path, trust_remote_code=True
         )
-        self.conv_template = MiniCPMVConversation()
+        # Get vit_config if available
+        download_headers = ""
+        url_cache_size = 10
+        if vit_config is not None:
+            download_headers = vit_config.download_headers
+            url_cache_size = vit_config.url_cache_item_num
+        self.conv_template = MiniCPMVConversation(download_headers=download_headers, url_cache_size=url_cache_size)
 
     def _render_messages(self, messages: List[ChatMessage]) -> RenderedInputs:
         msgs, urls, types, images = self.conv_template.render_messages(messages)
diff --git a/rtp_llm/openai/renderers/qwen_agent/llm/oai.py b/rtp_llm/openai/renderers/qwen_agent/llm/oai.py
index 70e7ebb22..bae44a516 100644
--- a/rtp_llm/openai/renderers/qwen_agent/llm/oai.py
+++ b/rtp_llm/openai/renderers/qwen_agent/llm/oai.py
@@ -6,8 +6,6 @@ import sys
 from pprint import pformat
 from typing import Dict, Iterator, List, Optional
 
-from rtp_llm.config.py_config_modules import StaticConfig
-
 path_index_list = -1
 path_remove = ""
 for idx, path in enumerate(sys.path):
@@ -52,8 +50,11 @@ class TextChatAtOAI(BaseTextChatModel):
 
         api_key = cfg.get("api_key", "")
         if not api_key:
-            api_key = StaticConfig.model_config.openai_api_key
-        api_key = api_key.strip()
+            # Try to get from misc_config if available in cfg
+            misc_config = cfg.get("misc_config")
+            if misc_config:
+                api_key = misc_config.openai_api_key
+        api_key = api_key.strip() if api_key else ""
 
         if openai.__version__.startswith("0."):
             if api_base:
diff --git a/rtp_llm/openai/renderers/qwen_agent/llm/qwen_dashscope.py b/rtp_llm/openai/renderers/qwen_agent/llm/qwen_dashscope.py
index 286fdc3fe..2f77ac3fd 100644
--- a/rtp_llm/openai/renderers/qwen_agent/llm/qwen_dashscope.py
+++ b/rtp_llm/openai/renderers/qwen_agent/llm/qwen_dashscope.py
@@ -22,8 +22,6 @@ from qwen_agent.log import logger
 from qwen_agent.settings import DEFAULT_MAX_INPUT_TOKENS
 from qwen_agent.utils.utils import has_chinese_messages, merge_generate_cfgs
 
-from rtp_llm.config.py_config_modules import StaticConfig
-
 # end region
 
 
@@ -236,20 +234,29 @@ class QwenChatAtDS(BaseTextChatModel):
 
 
 def initialize_dashscope(cfg: Optional[Dict] = None) -> None:
+    """Initialize dashscope with configuration from cfg.
+    
+    Args:
+        cfg: Configuration dictionary. Should contain either:
+            - Direct keys: "api_key", "base_http_api_url", "base_websocket_api_url"
+            - Or "misc_config" object with: dashscope_api_key, dashscope_http_url, dashscope_websocket_url
+    """
     cfg = cfg or {}
 
     api_key = cfg.get("api_key", "")
     base_http_api_url = cfg.get("base_http_api_url", None)
     base_websocket_api_url = cfg.get("base_websocket_api_url", None)
 
-    if not api_key:
-        api_key = StaticConfig.model_config.dashscope_api_key
-    if not base_http_api_url:
-        base_http_api_url = StaticConfig.model_config.dashscope_http_url
-    if not base_websocket_api_url:
-        base_websocket_api_url = StaticConfig.model_config.dashscope_websocket_url
+    # Try to get from misc_config if available in cfg
+    misc_config = cfg.get("misc_config")
+    if not api_key and misc_config:
+        api_key = misc_config.dashscope_api_key
+    if not base_http_api_url and misc_config:
+        base_http_api_url = misc_config.dashscope_http_url
+    if not base_websocket_api_url and misc_config:
+        base_websocket_api_url = misc_config.dashscope_websocket_url
 
-    api_key = api_key.strip()
+    api_key = api_key.strip() if api_key else ""
     dashscope.api_key = api_key
     if base_http_api_url is not None:
         dashscope.base_http_api_url = base_http_api_url.strip()
diff --git a/rtp_llm/openai/renderers/qwen_agent/log.py b/rtp_llm/openai/renderers/qwen_agent/log.py
index 2bb5b13eb..df7b28039 100644
--- a/rtp_llm/openai/renderers/qwen_agent/log.py
+++ b/rtp_llm/openai/renderers/qwen_agent/log.py
@@ -1,15 +1,8 @@
 import logging
 
-from rtp_llm.config.py_config_modules import StaticConfig
-
-
 def setup_logger(level=None):
-
     if level is None:
-        if StaticConfig.profiling_debug_config.qwen_agent_debug:
-            level = logging.DEBUG
-        else:
-            level = logging.INFO
+        level = logging.INFO
 
     logger = logging.getLogger("qwen_agent_logger")
     logger.setLevel(level)
diff --git a/rtp_llm/openai/renderers/qwen_agent_renderer.py b/rtp_llm/openai/renderers/qwen_agent_renderer.py
index da26ac2a9..840ff3a53 100644
--- a/rtp_llm/openai/renderers/qwen_agent_renderer.py
+++ b/rtp_llm/openai/renderers/qwen_agent_renderer.py
@@ -51,8 +51,17 @@ class ProcessedOutput:
 
 
 class QwenAgentRenderer(CustomChatRenderer):
-    def __init__(self, tokenizer: QwenTokenizerTypes, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: QwenTokenizerTypes, 
+        renderer_params: RendererParams,
+        generate_env_config,
+        render_config=None,
+        ckpt_path=None,
+        misc_config=None,
+        vit_config=None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         self.add_extra_stop_words(["✿RESULT✿", "✿RETURN✿"])
 
         self.template_chat_renderer: Optional[BasicRenderer] = None
@@ -62,13 +71,22 @@ class QwenAgentRenderer(CustomChatRenderer):
                     f"qwen model has chat_template [{tokenizer.chat_template}], "
                     "which will be used for non-function call dialogue."
                 )
-                self.template_chat_renderer = BasicRenderer(tokenizer, renderer_params)
+                self.template_chat_renderer = BasicRenderer(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         except AttributeError:
             pass
 
         llm_cfg = {
             "model": "qwen"
         }  # model 设置以qwen开头，且没设置server，会直接路由初始化 qwen_dashcope 这个类，我们要用里面的处理逻辑
+        
+        # Get misc_config if available
+        if misc_config is not None:
+            # misc_config might be PyMiscellaneousConfig, extract the actual misc_config
+            if hasattr(misc_config, 'misc_config'):
+                llm_cfg["misc_config"] = misc_config.misc_config
+            else:
+                llm_cfg["misc_config"] = misc_config
+        
         self.qwen_llm = get_chat_model(llm_cfg)
 
     def render_chat(self, request: ChatCompletionRequest) -> RenderedInputs:
diff --git a/rtp_llm/openai/renderers/qwen_reasoning_tool_renderer.py b/rtp_llm/openai/renderers/qwen_reasoning_tool_renderer.py
index f908970a7..0e8b634d7 100644
--- a/rtp_llm/openai/renderers/qwen_reasoning_tool_renderer.py
+++ b/rtp_llm/openai/renderers/qwen_reasoning_tool_renderer.py
@@ -4,7 +4,6 @@ from typing_extensions import override
 
 from rtp_llm.openai.api_datatype import ChatCompletionRequest
 from rtp_llm.openai.renderer_factory_register import register_renderer
-from rtp_llm.openai.renderers.custom_renderer import THINK_START_TAG
 from rtp_llm.openai.renderers.reasoning_tool_base_renderer import (
     ReasoningToolBaseRenderer,
 )
@@ -33,7 +32,7 @@ class QwenReasoningToolRenderer(ReasoningToolBaseRenderer):
             # 对于qwen3-thinking的模型，注意到tool_call_separator需要设置为"\n\n"
             try:
                 rendered_result = self.render_chat(request)
-                if rendered_result.rendered_prompt.endswith(THINK_START_TAG):
+                if rendered_result.rendered_prompt.endswith(self.think_start_tag):
                     detector.tool_call_separator = "\n\n"
             except Exception:
                 pass
@@ -52,7 +51,7 @@ class QwenReasoningToolRenderer(ReasoningToolBaseRenderer):
         model_type = "qwen3"
         try:
             rendered_result = self.render_chat(request)
-            if rendered_result.rendered_prompt.endswith(THINK_START_TAG):
+            if rendered_result.rendered_prompt.endswith(self.think_start_tag):
                 model_type = "qwen3-thinking"
         except Exception:
             pass
diff --git a/rtp_llm/openai/renderers/qwen_renderer.py b/rtp_llm/openai/renderers/qwen_renderer.py
index c3f43dee9..bcd8b85f5 100644
--- a/rtp_llm/openai/renderers/qwen_renderer.py
+++ b/rtp_llm/openai/renderers/qwen_renderer.py
@@ -3,7 +3,7 @@ import functools
 import json
 import logging
 from dataclasses import dataclass
-from typing import List, Optional, Tuple, Union
+from typing import List, Optional, Tuple
 
 import torch
 from typing_extensions import override
@@ -186,12 +186,21 @@ def make_context(
 
 
 class QwenRenderer(CustomChatRenderer):
-    def __init__(self, tokenizer: BaseTokenizer, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: BaseTokenizer, 
+        renderer_params: RendererParams,
+        generate_env_config,
+        render_config=None,
+        ckpt_path=None,
+        misc_config=None,
+        vit_config=None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         self.add_extra_stop_word_ids([[37763, 367, 25], [151643]])  # Observation:
 
         self.qwen_reasoning_tool_renderer = QwenReasoningToolRenderer(
-            tokenizer, renderer_params
+            tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config
         )
 
         self.template_chat_renderer: Optional[BasicRenderer] = None
@@ -201,7 +210,7 @@ class QwenRenderer(CustomChatRenderer):
                     f"qwen model has chat_template [{tokenizer.chat_template}], "
                     "which will be used for non-function call dialogue."
                 )
-                self.template_chat_renderer = BasicRenderer(tokenizer, renderer_params)
+                self.template_chat_renderer = BasicRenderer(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         except AttributeError:
             pass
 
diff --git a/rtp_llm/openai/renderers/qwen_v2_audio_renderer.py b/rtp_llm/openai/renderers/qwen_v2_audio_renderer.py
index 4618148e8..e27b1f2dd 100644
--- a/rtp_llm/openai/renderers/qwen_v2_audio_renderer.py
+++ b/rtp_llm/openai/renderers/qwen_v2_audio_renderer.py
@@ -14,7 +14,8 @@ from rtp_llm.utils.util import check_with_info
 
 class QwenV2AudioRenderer(BasicRenderer):
     def __init__(self, *args: Any, **kwargs: Any):
-        CustomChatRenderer.__init__(self, *args, **kwargs)
+        # BasicRenderer.__init__ will call CustomChatRenderer.__init__ with all args
+        super().__init__(*args, **kwargs)
         self.chat_template = self._create_chat_template()
 
     def _create_chat_template(self):
diff --git a/rtp_llm/openai/renderers/qwen_vl_renderer.py b/rtp_llm/openai/renderers/qwen_vl_renderer.py
index d4720aab9..59ff036e9 100644
--- a/rtp_llm/openai/renderers/qwen_vl_renderer.py
+++ b/rtp_llm/openai/renderers/qwen_vl_renderer.py
@@ -19,8 +19,17 @@ from rtp_llm.utils.multimodal_util import MMPreprocessConfig, MMUrlType
 
 
 class QwenVLRenderer(CustomChatRenderer):
-    def __init__(self, tokenizer: BaseTokenizer, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: BaseTokenizer, 
+        renderer_params: RendererParams,
+        generate_env_config,
+        render_config=None,
+        ckpt_path=None,
+        misc_config=None,
+        vit_config=None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
 
     def _render_messages(self, messages: List[ChatMessage]) -> PromptWithMMInput:
         prompt = ""
@@ -62,8 +71,17 @@ class QwenVLRenderer(CustomChatRenderer):
 
 
 class Qwen2VLRenderer(CustomChatRenderer):
-    def __init__(self, tokenizer: BaseTokenizer, renderer_params: RendererParams):
-        super().__init__(tokenizer, renderer_params)
+    def __init__(
+        self, 
+        tokenizer: BaseTokenizer, 
+        renderer_params: RendererParams,
+        generate_env_config,
+        render_config=None,
+        ckpt_path=None,
+        misc_config=None,
+        vit_config=None,
+    ):
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
 
     def _render_messages(
         self, messages: List[ChatMessage], add_vision_id: bool
diff --git a/rtp_llm/openai/renderers/reasoning_tool_base_renderer.py b/rtp_llm/openai/renderers/reasoning_tool_base_renderer.py
index c3c4d973b..5ad5ca114 100644
--- a/rtp_llm/openai/renderers/reasoning_tool_base_renderer.py
+++ b/rtp_llm/openai/renderers/reasoning_tool_base_renderer.py
@@ -63,8 +63,13 @@ class ReasoningToolBaseRenderer(CustomChatRenderer, ABC):
         self,
         tokenizer: BaseTokenizer,
         renderer_params: RendererParams,
+        generate_env_config,
+        render_config=None,
+        ckpt_path=None,
+        misc_config=None,
+        vit_config=None,
     ):
-        super().__init__(tokenizer, renderer_params)
+        super().__init__(tokenizer, renderer_params, generate_env_config, render_config, ckpt_path, misc_config, vit_config)
         self._setup_stop_words()
         self._setup_chat_template()
         # 避免短期内多次encode prompt的开销
diff --git a/rtp_llm/ops/__init__.py b/rtp_llm/ops/__init__.py
index f70998b5e..3e9edd7e0 100644
--- a/rtp_llm/ops/__init__.py
+++ b/rtp_llm/ops/__init__.py
@@ -114,28 +114,37 @@ try:
         CacheStoreConfig,
         ConcurrencyConfig,
         DeviceResourceConfig,
-        EplbConfig,
         EplbMode,
         FfnDisAggregateConfig,
         FIFOSchedulerConfig,
         FMHAConfig,
         FMHAType,
-        GptInitParameter,
         HWKernelConfig,
         KVCacheConfig,
         MiscellaneousConfig,
         MlaOpsType,
+        ModelConfig,
         ModelSpecificConfig,
+        MMModelConfig,
         MoeConfig,
-        ParallelismDistributedConfig,
+        PDSepConfig,
+        ParallelismConfig,
         ProfilingDebugLoggingConfig,
+        TaskType,
+        VitConfig,
+        VitSeparation,
+    )
+    # Alias for backward compatibility
+    from libth_transformer_config import (
         QuantAlgo,
         RoleType,
-        SchedulerConfig,
-        ServiceDiscoveryConfig,
+        RuntimeConfig,
         SpecialTokens,
         SpeculativeExecutionConfig,
+        EPLBConfig,
     )
+    # Alias for backward compatibility
+    EplbConfig = EPLBConfig
     from libth_transformer_config import (
         get_block_cache_keys as cpp_get_block_cache_keys,
     )
diff --git a/rtp_llm/ops/libth_transformer.pyi b/rtp_llm/ops/libth_transformer.pyi
index 6327d7028..968ceedda 100644
--- a/rtp_llm/ops/libth_transformer.pyi
+++ b/rtp_llm/ops/libth_transformer.pyi
@@ -21,7 +21,6 @@ __all__ = [
     "FMHAConfig",
     "FlashInferOp",
     "FfnDisAggregateConfig",
-    "GptInitParameter",
     "HWKernelConfig",
     "Host",
     "KVCacheConfig",
@@ -32,15 +31,14 @@ __all__ = [
     "ModelSpecificConfig",
     "MoeConfig",
     "MultimodalInput",
-    "ParallelismDistributedConfig",
+    "ParallelismConfig",
     "ProfilingDebugLoggingConfig",
     "QuantAlgo",
     "RoleSpecialTokens",
     "RoleType",
     "RtpEmbeddingOp",
     "RtpLLMOp",
-    "SchedulerConfig",
-    "ServiceDiscoveryConfig",
+    "RuntimeConfig",
     "SpecialTokens",
     "SpeculativeExecutionConfig",
     "get_block_cache_keys",
@@ -69,7 +67,6 @@ class BatchDecodeSchedulerConfig:
         batch_decode_scheduler_warmup_type: int = 0,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class CacheStoreConfig:
     cache_store_rdma_mode: bool
@@ -93,7 +90,6 @@ class CacheStoreConfig:
         messager_worker_thread_count: int = 16,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class ConcurrencyConfig:
     concurrency_limit: int
@@ -103,7 +99,6 @@ class ConcurrencyConfig:
         self, concurrency_with_block: bool = False, concurrency_limit: int = 32
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class DeviceExporter:
     def get_device_id(self) -> int: ...
@@ -144,7 +139,6 @@ class DeviceResourceConfig:
         not_use_default_stream: bool = False,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class DeviceType:
     """
@@ -263,7 +257,6 @@ class FIFOSchedulerConfig:
         fast_gen_context_budget: int = -1,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class FMHAConfig:
     disable_flash_infer: bool
@@ -291,10 +284,9 @@ class FMHAConfig:
         enable_xqa: bool = True,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class FlashInferOp:
-    def __init__(self, gpt_init_parameter: GptInitParameter) -> None: ...
+    def __init__(self, attn_configs: typing.Any) -> None: ...
     def forward(
         self,
         input: torch.Tensor,
@@ -323,7 +315,6 @@ class FfnDisAggregateConfig:
     ) -> None: ...
     def is_ffn_service(self) -> bool: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class FMHAType:
     """
@@ -381,196 +372,6 @@ class FMHAType:
     @property
     def value(self) -> int: ...
 
-class GptInitParameter:
-    activation_type: str
-    add_bias_linear: bool
-    batch_decode_scheduler_config: BatchDecodeSchedulerConfig
-    block_nums: int
-    cache_store_config: CacheStoreConfig
-    cache_store_connect_port: int
-    cache_store_listen_port: int
-    cache_store_rdma_connect_port: int
-    cache_store_rdma_listen_port: int
-    cache_store_rdma_mode: bool
-    ckpt_path: str
-    concurrency_config: ConcurrencyConfig
-    cross_attn_input_len: int
-    data_type: str
-    decode_polling_kv_cache_step_ms: int
-    decode_retry_timeout_ms: int
-    decode_retry_times: int
-    deepseek_mscale_all_dim: float
-    deepseek_rope_mscale: float
-    device_resource_config: DeviceResourceConfig
-    dp_rank: int
-    dp_size: int
-    dp_tp_nccl_port: int
-    embedding_size: int
-    enable_3fs: bool
-    enable_eplb: bool
-    enable_fast_gen: bool
-    enable_partial_fallback: bool
-    enable_sp: bool
-    enable_speculative_decoding: bool
-    ep_rank: int
-    ep_size: int
-    eplb_mode: EplbMode
-    eplb_update_time: int
-    expert_num: int
-    fast_gen_max_context_len: int
-    ffn_disaggregate_config: FfnDisAggregateConfig
-    ffn_tp_nccl_port: int
-    ffn_tp_rank: int
-    ffn_tp_size: int
-    fifo_scheduler_config: FIFOSchedulerConfig
-    fmha_config: FMHAConfig
-    gen_num_per_circle: int
-    has_lm_head: bool
-    has_moe_norm: bool
-    has_positional_encoding: bool
-    has_post_decoder_layernorm: bool
-    has_pre_decoder_layernorm: bool
-    head_num: int
-    head_num_kv: int
-    hidden_size: int
-    http_port: int
-    hw_kernel_config: HWKernelConfig
-    include_sep_tokens: bool
-    input_embedding_scalar: float
-    input_vocab_size: int
-    inter_padding_size: int
-    inter_size: int
-    is_causal: bool
-    is_multimodal: bool
-    is_sparse_head: bool
-    kv_cache_config: KVCacheConfig
-    kv_cache_data_type: str
-    kv_cache_mem_mb: int
-    kv_lora_rank: int
-    layer_head_num: list[int]
-    layer_head_num_kv: list[int]
-    layer_inter_padding_size: list[int]
-    layer_inter_size: list[int]
-    layer_num: int
-    layernorm_eps: float
-    layernorm_type: str
-    load_cache_timeout_ms: int
-    local_rank: int
-    logit_scale: float
-    max_context_batch_size: int
-    max_generate_batch_size: int
-    max_rpc_timeout_ms: int
-    max_seq_len: int
-    misc_config: MiscellaneousConfig
-    mla_ops_type: MlaOpsType
-    mm_position_ids_style: int
-    mm_sep_tokens: list[list[int]]
-    model_name: str
-    model_rpc_port: int
-    model_specific_config: ModelSpecificConfig
-    moe_config: MoeConfig
-    moe_inter_padding_size: int
-    moe_k: int
-    moe_layer_index: list[int]
-    moe_n_group: int
-    moe_normalize_expert_scale: bool
-    moe_style: int
-    moe_topk_group: int
-    routed_scaling_factor: float
-    mrope_section: list[int]
-    nccl_ip: str
-    nope_head_dim: int
-    norm_type: str
-    num_layers: int
-    num_valid_layer: int
-    org_embedding_max_pos: int
-    parallelism_distributed_config: ParallelismDistributedConfig
-    phy_exp_num: int
-    position_id_len_factor: int
-    position_ids_style: int
-    pre_allocate_op_mem: bool
-    pre_seq_len: int
-    prefill_max_wait_timeout_ms: int
-    prefill_retry_timeout_ms: int
-    prefill_retry_times: int
-    prefix_projection: bool
-    profiling_debug_logging_config: ProfilingDebugLoggingConfig
-    py_eplb: typing.Any
-    q_lora_rank: int
-    q_scaling: float
-    qk_norm: bool
-    quant_algo: QuantAlgo
-    rdma_connect_retry_times: int
-    remote_rpc_server_port: int
-    reserve_runtime_mem_mb: int
-    residual_scalar: float
-    reuse_cache: bool
-    reverse_e_h_norm: bool
-    role_type: RoleType
-    rope_head_dim: int
-    rotary_embedding_base: float
-    rotary_embedding_dim: int
-    rotary_embedding_mscale: float
-    rotary_embedding_offset: int
-    rotary_embedding_scale: float
-    rotary_embedding_style: int
-    rotary_factor1: float
-    rotary_factor2: float
-    partial_rotary_factor: float
-    scheduler_config: SchedulerConfig
-    scheduler_reserve_resource_ratio: int
-    scoring_func: int
-    seq_size_per_block: int
-    service_discovery_config: ServiceDiscoveryConfig
-    size_per_head: int
-    softmax_extra_scale: float
-    sp_config: SpeculativeExecutionConfig
-    special_tokens: SpecialTokens
-    tokenizer_path: str
-    tp_nccl_port: int
-    tp_rank: int
-    tp_size: int
-    type_vocab_size: int
-    use_all_gather: bool
-    use_attention_linear_bias: bool
-    use_cross_attn: bool
-    use_fp32_to_compute_logit: bool
-    use_kvcache: bool
-    use_logn_attn: bool
-    use_mla: bool
-    use_norm_attn_out_residual: bool
-    use_norm_input_residual: bool
-    using_hf_sampling: bool
-    v_head_dim: int
-    vit_separation: int
-    vocab_size: int
-    warm_up: bool
-    warm_up_with_loss: bool
-    worker_addrs: list[str]
-    worker_grpc_addrs: list[str]
-    worker_port_offset: int
-    world_size: int
-
-    def __init__(
-        self,
-        head_num: int,
-        size_per_head: int,
-        num_layers: int,
-        max_seq_len: int,
-        vocab_size: int,
-        hidden_size: int,
-    ) -> None: ...
-    def insertMultiTaskPromptTokens(
-        self, task_id: str, tokens_id: list[int]
-    ) -> None: ...
-    def isGatedActivation(self) -> bool: ...
-    def isKvCacheQuant(self) -> bool: ...
-    def setActivationType(self) -> None: ...
-    def setKvCacheDataType(self) -> None: ...
-    def setLayerNormType(self) -> None: ...
-    def setNormType(self) -> None: ...
-    def setTaskType(self, task: str) -> None: ...
-    def showDebugInfo(self) -> None: ...
 
 class HWKernelConfig:
     arm_gemm_use_kai: bool
@@ -603,7 +404,6 @@ class HWKernelConfig:
         num_native_cuda_graph: int = 200,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class Host:
     http_port: int
@@ -642,7 +442,6 @@ class KVCacheConfig:
         threefs_write_iov_size: int = 1 << 32,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class KVCacheInfo:
     available_kv_cache: int
@@ -673,7 +472,6 @@ class MiscellaneousConfig:
         aux_string: str = "",
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class MlaOpsType:
     """
@@ -718,7 +516,6 @@ class ModelSpecificConfig:
         self, max_lora_model_size: int = -1, load_python_model: bool = False
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class MoeConfig:
     deep_ep_num_sm: int
@@ -748,7 +545,6 @@ class MoeConfig:
         max_moe_normal_masked_token_num: int = 1024,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class MultimodalInput:
     mm_type: int
@@ -756,7 +552,7 @@ class MultimodalInput:
     url: str
     def __init__(self, url: str, tensor: torch.Tensor, mm_type: int) -> None: ...
 
-class ParallelismDistributedConfig:
+class ParallelismConfig:
     dp_size: int
     ep_size: int
     ffn_sp_size: int
@@ -778,7 +574,6 @@ class ParallelismDistributedConfig:
         ffn_sp_size: int = 1,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class ProfilingDebugLoggingConfig:
     trace_memory: bool
@@ -822,7 +617,6 @@ class ProfilingDebugLoggingConfig:
         enable_detail_log: bool = False,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 class QuantAlgo:
     def __getstate__(self) -> tuple: ...
@@ -926,30 +720,6 @@ class RtpLLMOp:
     def update_eplb_config(self, config: EplbConfig) -> bool: ...
     def update_scheduler_info(self, arg0: str) -> None: ...
 
-class SchedulerConfig:
-    use_batch_decode_scheduler: bool
-    def __init__(self, use_batch_decode_scheduler: bool = False) -> None: ...
-    def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
-
-class ServiceDiscoveryConfig:
-    decode_cm2_config: str
-    multimodal_part_cm2_config: str
-    remote_rpc_server_ip: str
-    remote_vit_server_ip: str
-    use_local: bool
-
-    def __init__(
-        self,
-        use_local: bool = False,
-        remote_rpc_server_ip: str = "",
-        decode_cm2_config: str = "",
-        remote_vit_server_ip: str = "",
-        multimodal_part_cm2_config: str = "",
-    ) -> None: ...
-    def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
-
 class SpecialTokens:
     assistant: RoleSpecialTokens
     bos_token_id: int
@@ -982,7 +752,6 @@ class SpeculativeExecutionConfig:
         force_score_context_attention: bool = True,
     ) -> None: ...
     def to_string(self) -> str: ...
-    def update_from_env(self) -> None: ...
 
 def get_block_cache_keys(token_ids_list: list[list[int]]) -> list[int]: ...
 def get_device() -> DeviceExporter: ...
diff --git a/rtp_llm/ops/rtp_llm_ops.pyi b/rtp_llm/ops/rtp_llm_ops.pyi
index 44ea011f0..8daacffd2 100644
--- a/rtp_llm/ops/rtp_llm_ops.pyi
+++ b/rtp_llm/ops/rtp_llm_ops.pyi
@@ -39,7 +39,7 @@ class FlashInferMlaAttnParams:
 
 class FlashInferOp:
     def __init__(
-        self, gpt_init_parameter: libth_transformer.GptInitParameter
+        self, attn_configs: typing.Any
     ) -> None: ...
     def forward(
         self,
@@ -54,7 +54,7 @@ class FlashInferOp:
 
 class FusedMoEOp:
     def __init__(
-        self, gpt_init_parameter: libth_transformer.GptInitParameter
+        self, attn_configs: typing.Any
     ) -> None: ...
     def forward(
         self,
@@ -68,7 +68,7 @@ class FusedMoEOp:
 
 class SelectTopkOp:
     def __init__(
-        self, gpt_init_parameter: libth_transformer.GptInitParameter
+        self, attn_configs: typing.Any
     ) -> None: ...
     def forward(
         self,
@@ -82,7 +82,7 @@ class TRTAttn:
 
 class TRTAttnOp:
     def __init__(
-        self, gpt_init_parameter: libth_transformer.GptInitParameter
+        self, attn_configs: typing.Any
     ) -> None: ...
     def forward(
         self,
@@ -95,7 +95,7 @@ class TRTAttnOp:
 
 class XQAAttnOp:
     def __init__(
-        self, gpt_init_parameter: libth_transformer.GptInitParameter
+        self, attn_configs: typing.Any
     ) -> None: ...
     def forward(
         self,
diff --git a/rtp_llm/pipeline/pipeline.py b/rtp_llm/pipeline/pipeline.py
index d31b9d6a8..5bf143361 100644
--- a/rtp_llm/pipeline/pipeline.py
+++ b/rtp_llm/pipeline/pipeline.py
@@ -8,7 +8,6 @@ import torch
 
 from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
 from rtp_llm.config.generate_config import GenerateConfig
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
 from rtp_llm.frontend.tokenizer_factory.tokenizer_utils import (
     DecodingState,
     IncrementDecodingUtils,
@@ -16,6 +15,7 @@ from rtp_llm.frontend.tokenizer_factory.tokenizer_utils import (
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.metrics import GaugeMetrics, kmonitor
 from rtp_llm.server.backend_rpc_server_visitor import BackendRPCServerVisitor
+from rtp_llm.ops import FfnDisAggregateConfig, SpecialTokens, SpeculativeExecutionConfig
 from rtp_llm.utils.base_model_datatypes import (
     GenerateInput,
     GenerateOutput,
@@ -39,18 +39,38 @@ request_counter = AtomicCounter()
 class Pipeline(object):
     def __init__(
         self,
-        model_config: GptInitModelParameters,
+        special_tokens: SpecialTokens,  # SpecialTokens from ModelConfig
+        pd_sep_config,  # PDSepConfig from ops
+        runtime_config,  # RuntimeConfig from ops
+        ffn_disaggregate_config: FfnDisAggregateConfig,
+        max_seq_len: int,  # max_seq_len_ from ModelConfig
+        seq_size_per_block: int,  # seq_size_per_block_ from ModelConfig
         tokenizer: Optional[BaseTokenizer],
+        sp_config: Optional[SpeculativeExecutionConfig] = None,
         separated_frontend: bool = False,
+        mm_related_params: Optional[Any] = None,  # mm_related_params from ModelConfig (optional)
+        gang_info=None,
     ):
-        self.model_config = model_config
+        self.pd_sep_config = pd_sep_config
+        self.runtime_config = runtime_config
+        self.ffn_disaggregate_config = ffn_disaggregate_config
         self.tokenizer = tokenizer
-        self._special_tokens: int = self.model_config.special_tokens
-        self._mm_token: str = self.model_config.mm_related_params.special_tokens.get(
-            "default_mm_token", ""
-        )
+        self._special_tokens: SpecialTokens = special_tokens
+        self._mm_token: str = ""
+        if mm_related_params:
+            self._mm_token = mm_related_params.special_tokens.get("default_mm_token", "")
+        
         self.backend_rpc_server_visitor = BackendRPCServerVisitor(
-            model_config, separated_frontend
+            max_seq_len=max_seq_len,
+            seq_size_per_block=seq_size_per_block,
+            pd_sep_config=pd_sep_config,
+            runtime_config=runtime_config,
+            ffn_disaggregate_config=ffn_disaggregate_config,
+            sp_config=sp_config,
+            max_rpc_timeout_ms=pd_sep_config.max_rpc_timeout_ms,
+            decode_entrance=pd_sep_config.decode_entrance,
+            separated_frontend=separated_frontend,
+            gang_info=gang_info,
         )
 
     def encode(self, prompt: str):
@@ -67,6 +87,7 @@ class Pipeline(object):
         vocab_size: int,
         special_tokens: Any,
         tokenizer: BaseTokenizer,
+        generate_env_config,
         **kwargs: Any
     ) -> GenerateConfig:
         if isinstance(generate_config, dict):
@@ -76,7 +97,7 @@ class Pipeline(object):
             config = generate_config
         config.add_special_tokens(special_tokens)
         config.convert_select_tokens(vocab_size, tokenizer)
-        config.add_thinking_params(tokenizer)
+        config.add_thinking_params(tokenizer, generate_env_config)
         config.add_stop_ids_from_str(tokenizer)
         return config
 
@@ -148,7 +169,7 @@ class Pipeline(object):
         generate_config = self.create_generate_config(
             generate_config_json,
             self.tokenizer.vocab_size,
-            self.model_config.special_tokens,
+            self._special_tokens,
             self.tokenizer,
             **kwargs
         )
diff --git a/rtp_llm/server/backend_app.py b/rtp_llm/server/backend_app.py
index b52b94c49..c6c2f920c 100644
--- a/rtp_llm/server/backend_app.py
+++ b/rtp_llm/server/backend_app.py
@@ -17,7 +17,7 @@ from typing_extensions import override
 from uvicorn import Config, Server
 from uvicorn.loops.auto import auto_loop_setup
 
-from rtp_llm.config.py_config_modules import PyEnvConfigs, StaticConfig
+from rtp_llm.config.py_config_modules import PyEnvConfigs
 from rtp_llm.config.uvicorn_config import UVICORN_LOGGING_CONFIG
 from rtp_llm.distribute.worker_info import WorkerInfo
 from rtp_llm.embedding.backend_embedding_app import register_backend_embedding_api
@@ -54,7 +54,7 @@ class GracefulShutdownServer(Server):
 
 
 class BackendApp(object):
-    def __init__(self, py_env_configs: PyEnvConfigs = StaticConfig):
+    def __init__(self, py_env_configs: PyEnvConfigs):
         self.py_env_configs = py_env_configs
         self.backend_server = BackendServer(py_env_configs)
 
diff --git a/rtp_llm/server/backend_rpc_server_visitor.py b/rtp_llm/server/backend_rpc_server_visitor.py
index 574818376..0e901e500 100644
--- a/rtp_llm/server/backend_rpc_server_visitor.py
+++ b/rtp_llm/server/backend_rpc_server_visitor.py
@@ -1,13 +1,13 @@
 import logging
-from typing import AsyncGenerator, List
+from typing import AsyncGenerator, List, Optional
 
 import torch
 
 from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
 from rtp_llm.config.generate_config import RoleAddr, RoleType
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.config.py_config_modules import StaticConfig
+from rtp_llm.config.model_config import ModelConfig as PyModelConfig
 from rtp_llm.cpp.model_rpc.model_rpc_client import ModelRpcClient
+from rtp_llm.ops import FfnDisAggregateConfig, SpeculativeExecutionConfig
 from rtp_llm.metrics import kmonitor
 from rtp_llm.metrics.kmonitor_metric_reporter import AccMetrics, GaugeMetrics
 from rtp_llm.ops import get_block_cache_keys
@@ -22,36 +22,58 @@ route_logger = logging.getLogger("route_logger")
 
 class BackendRPCServerVisitor:
     def __init__(
-        self, model_config: GptInitModelParameters, separated_frontend: bool = False
+        self,
+        max_seq_len: int,  # max_seq_len_ from ModelConfig
+        seq_size_per_block: int,  # seq_size_per_block_ from ModelConfig
+        pd_sep_config,  # PDSepConfig from ops
+        runtime_config,  # RuntimeConfig from ops
+        ffn_disaggregate_config: FfnDisAggregateConfig,
+        sp_config: Optional[SpeculativeExecutionConfig] = None,
+        max_rpc_timeout_ms: int = 0,
+        decode_entrance: bool = False,
+        separated_frontend: bool = False,
+        gang_info=None,
     ) -> None:
-        self.config = model_config
-        assert self.config.max_seq_len > 0
-        self.model_rpc_client = ModelRpcClient(self.config)
+        self.max_seq_len = max_seq_len
+        self.seq_size_per_block = seq_size_per_block
+        self.pd_sep_config = pd_sep_config
+        self.runtime_config = runtime_config
+        self.ffn_disaggregate_config = ffn_disaggregate_config
+        self.sp_config = sp_config
+        self.max_rpc_timeout_ms = max_rpc_timeout_ms
+        self.decode_entrance = decode_entrance
+        assert self.max_seq_len > 0
+        
+        self.model_rpc_client = ModelRpcClient(
+            self.ffn_disaggregate_config,
+            max_rpc_timeout_ms=self.max_rpc_timeout_ms,
+            decode_entrance=self.decode_entrance,
+            gang_info=gang_info,
+        )
+        
         host_args = HostServiceArgs.create_from_env()
-        self.backend_role_list = self.get_backend_role_list(self.config, host_args)
+        self.backend_role_list = self.get_backend_role_list(
+            self.pd_sep_config, host_args
+        )
         self.host_service = HostService(host_args)
         self.master_client = MasterClient()
         self.separated_frontend = separated_frontend
 
     @staticmethod
     def get_backend_role_list(
-        config: GptInitModelParameters, host_args: HostServiceArgs
+        pd_sep_config, host_args: HostServiceArgs
     ) -> List[RoleType]:
         role_list: List[RoleType] = []
 
         # Convert config.role_type to the correct enum if needed
-        config_role_type = config.role_type
-        if hasattr(config.role_type, "value"):
-            config_role_type = config.role_type.value
-
-        if config.vit_separation == 2 and host_args.vit_domain:
-            role_list.append(RoleType.VIT)
-            logging.info("Added VIT role")
+        config_role_type = pd_sep_config.role_type
+        if hasattr(config_role_type, "value"):
+            config_role_type = config_role_type.value
 
-        if config_role_type == RoleType.PREFILL.value and not config.decode_entrance:
+        if config_role_type == RoleType.PREFILL.value and not pd_sep_config.decode_entrance:
             role_list.append(RoleType.DECODE)
             logging.info("Added DECODE role for PREFILL type")
-        elif config_role_type == RoleType.DECODE.value and config.decode_entrance:
+        elif config_role_type == RoleType.DECODE.value and pd_sep_config.decode_entrance:
             role_list.append(RoleType.PREFILL)
             logging.info("Added PREFILL role for DECODE type")
         elif config_role_type == RoleType.FRONTEND.value:
@@ -78,7 +100,7 @@ class BackendRPCServerVisitor:
         else:
             token_ids: List[int] = input.token_ids.tolist()  # type: ignore
         block_cache_keys = get_block_cache_keys(
-            token_ids, self.config.seq_size_per_block
+            token_ids, self.seq_size_per_block
         )
 
         try:
@@ -181,7 +203,7 @@ class BackendRPCServerVisitor:
             )
 
     def check_sp_supported(self, input: GenerateInput):
-        if not StaticConfig.py_speculative_execution_config.sp_model_type:
+        if not self.sp_config or not self.sp_config.sp_model_type:
             return
         if input.generate_config.force_disable_sp_run:
             return
@@ -221,13 +243,13 @@ class BackendRPCServerVisitor:
         self.check_sp_supported(input)
 
         max_new_tokens = min(
-            self.config.max_seq_len - input.prompt_length,
+            self.max_seq_len - input.prompt_length,
             input.generate_config.max_new_tokens,
         )
         if max_new_tokens <= 0:
             raise FtRuntimeException(
                 ExceptionType.LONG_PROMPT_ERROR,
-                f"model max tokens is {self.config.max_seq_len}, "
+                f"model max tokens is {self.max_seq_len}, "
                 f"request length is {input.prompt_length}, max_new_tokens is {max_new_tokens}",
             )
 
diff --git a/rtp_llm/server/backend_server.py b/rtp_llm/server/backend_server.py
index edde18173..3309e6d6e 100644
--- a/rtp_llm/server/backend_server.py
+++ b/rtp_llm/server/backend_server.py
@@ -14,8 +14,9 @@ from pydantic import BaseModel
 
 from rtp_llm.access_logger.access_logger import AccessLogger
 from rtp_llm.async_decoder_engine.async_model import AsyncModel
-from rtp_llm.config.py_config_modules import PyEnvConfigs, StaticConfig
-from rtp_llm.config.task_type import TaskType
+from rtp_llm.config.engine_config import EngineConfig
+from rtp_llm.config.py_config_modules import PyEnvConfigs
+from rtp_llm.ops import TaskType
 from rtp_llm.distribute.gang_server import GangServer
 from rtp_llm.distribute.worker_info import g_parallel_info
 from rtp_llm.embedding.embedding_endpoint import EmbeddingEndpoint
@@ -23,7 +24,7 @@ from rtp_llm.lora.lora_manager import LoraManager
 from rtp_llm.metrics import AccMetrics, GaugeMetrics, kmonitor
 from rtp_llm.model_factory import ModelFactory
 from rtp_llm.openai.openai_endpoint import OpenaiEndpoint
-from rtp_llm.ops import EngineScheduleInfo, KVCacheInfo, WorkerStatusInfo
+from rtp_llm.ops import EngineScheduleInfo, KVCacheInfo, MMModelConfig, WorkerStatusInfo
 from rtp_llm.server.backend_rpc_server_visitor import BackendRPCServerVisitor
 from rtp_llm.server.misc import format_exception
 from rtp_llm.model_loader.weight_manager import WeightManager
@@ -53,8 +54,8 @@ class BackendServer(object):
                 os.environ["NCCL_P2P_DISABLE"] = "1"
         else:
             os.environ["NCCL_P2P_DISABLE"] = "1"
-        self._access_logger = AccessLogger()
-        self._gang_server = GangServer(py_env_configs)
+        self._access_logger = AccessLogger(py_env_configs.profiling_debug_config.log_path, py_env_configs.profiling_debug_config.log_file_backup_count)
+        self._gang_server = GangServer(py_env_configs.gang_config, py_env_configs.server_config)
         self._openai_endpoint = None
         self._lora_manager = None
         self.thread_lock_ = threading.Lock()
@@ -65,8 +66,6 @@ class BackendServer(object):
         self.model = None
         self._openai_endpoint = None
         self._embedding_endpoint = None
-        self.py_env_configs = py_env_configs
-        self.dp_rank = g_parallel_info.dp_rank
         self.dp_size = g_parallel_info.dp_size
         self.tp_size = g_parallel_info.tp_size
         self._weight_manager = None
@@ -77,31 +76,88 @@ class BackendServer(object):
             # for debug online
             logging.info("DEBUG_START_FAKE_PROCESS is set, start fake backend server")
         else:
-            self.model: AsyncModel = ModelFactory.create_from_env()
+            # Get gang_info from gang_server after start()
+            gang_info = self._gang_server._gang_info
+            
+            # Create and fully initialize EngineConfig from py_env_configs
+            engine_config = EngineConfig.create(py_env_configs, gang_info=gang_info)
+            
+            # Create model configs (ModelConfig construction is handled in ModelFactory)
+            py_model_config, propose_py_model_config = ModelFactory.create_model_configs(
+                engine_config=engine_config,
+                model_args=py_env_configs.model_args,
+                lora_config=py_env_configs.lora_config,
+                generate_env_config=py_env_configs.generate_env_config,
+                embedding_config=py_env_configs.embedding_config,
+            )
+            
+            # All model metadata (lora_infos, multi_task_prompt, model_name, template_type)
+            # is now set in py_model_config by create_model_configs()
+            mm_model_config = MMModelConfig()
+            
+            # Create model using new API
+            # All metadata is already in py_model_config
+            # vit_config is needed for multimodal models
+            self.model: AsyncModel = ModelFactory.from_model_configs(
+                model_config=py_model_config,
+                mm_model_config=mm_model_config,
+                engine_config=engine_config,
+                gang_info=gang_info,
+                vit_config=py_env_configs.vit_config,
+                propose_model_config=propose_py_model_config,
+            )
+            
+            # Load default generate config if needed
+            if py_env_configs.generate_env_config:
+                ModelFactory.load_default_generate_config(self.model, py_env_configs.generate_env_config)
+             
             if (
                 self.model is not None
-                and self.model.task_type != TaskType.LANGUAGE_MODEL
+                and self.model.model.py_model_config.task_type != TaskType.LANGUAGE_MODEL
             ):
                 self._embedding_endpoint = EmbeddingEndpoint(self.model)
             else:
                 self.backend_rpc_server_visitor = BackendRPCServerVisitor(
-                    self.model.config
+                    max_seq_len=py_model_config.max_seq_len,
+                    seq_size_per_block=engine_config.kv_cache_config.seq_size_per_block,
+                    pd_sep_config=engine_config.pd_sep_config,
+                    runtime_config=engine_config.runtime_config,
+                    ffn_disaggregate_config=engine_config.parallelism_config.ffn_disaggregate_config,
+                    sp_config=engine_config.sp_config,
+                    max_rpc_timeout_ms=engine_config.pd_sep_config.max_rpc_timeout_ms,
+                    decode_entrance=engine_config.pd_sep_config.decode_entrance,
+                    gang_info=gang_info,
                 )
+                # Get values from py_model_config for OpenaiEndpoint
+                template_type = py_model_config.template_type
+                model_name = py_model_config.model_name
+                ckpt_path = py_model_config.ckpt_path or ""
+
                 self._openai_endpoint = OpenaiEndpoint(
-                    self.model.config,
-                    self.model.tokenizer,
-                    self.backend_rpc_server_visitor,
+                    model_args=py_env_configs.model_args,
+                    generate_env_config=py_env_configs.generate_env_config,
+                    render_config=py_env_configs.render_config,
+                    misc_config=py_env_configs.misc_config,
+                    vit_config=py_env_configs.vit_config,
+                    special_tokens=py_model_config.special_tokens,
+                    max_seq_len=py_model_config.max_seq_len,
+                    template_type=template_type,
+                    model_name=model_name,
+                    ckpt_path=ckpt_path,
+                    tokenizer=self.model.tokenizer,
+                    backend_rpc_server_visitor=self.backend_rpc_server_visitor,
                 )
                 if isinstance(self.model, AsyncModel):
                     # uply hack :(
                     self.model.decoder_engine_.rtp_llm_op_.ft_op.start_http_server(
                         self.model.model.model_weights_loader,
-                        self.model.model.config.lora_infos,
+                        py_model_config.lora_infos,
                         self._gang_server._gang_info,
                         self._openai_endpoint.tokenizer,
                         self._openai_endpoint.chat_renderer,
                     )
-                    self._lora_manager = LoraManager(self.model)
+                    max_lora_model_size = self.model.config.model_specific_config.max_lora_model_size
+                    self._lora_manager = LoraManager(self.model, max_lora_model_size=max_lora_model_size)
                     self._weight_manager = WeightManager(self.model)
 
     def model_runtime_meta(self) -> str:
diff --git a/rtp_llm/server/server_args/batch_decode_scheduler_group_args.py b/rtp_llm/server/server_args/batch_decode_scheduler_group_args.py
index beefa90bb..d223326f7 100644
--- a/rtp_llm/server/server_args/batch_decode_scheduler_group_args.py
+++ b/rtp_llm/server/server_args/batch_decode_scheduler_group_args.py
@@ -1,12 +1,26 @@
-def init_batch_decode_scheduler_group_args(parser):
+from rtp_llm.server.server_args.util import str2bool
+
+
+def init_batch_decode_scheduler_group_args(parser, batch_decode_scheduler_config):
     ##############################################################################################################
-    # BatchDecode 调度器配置
+    # Batch Decode Scheduler 配置
     ##############################################################################################################
-    batch_decode_scheduler_group = parser.add_argument_group("BatchDecode Scheduler")
+    batch_decode_scheduler_group = parser.add_argument_group("Batch Decode Scheduler")
+    
     batch_decode_scheduler_group.add_argument(
         "--batch_decode_scheduler_batch_size",
         env_name="BATCH_DECODE_SCHEDULER_BATCH_SIZE",
+        bind_to=[(batch_decode_scheduler_config, 'batch_decode_scheduler_batch_size')],
         type=int,
         default=1,
-        help="当 USE_BATCH_DECODE_SCHEDULER 为 True 时，decode 阶段单次处理迭代中将组合在一起的请求数量。增加此值可以提高系统的整体 throughput，但代价是单个请求的 latency 可能会增加。减小此值可以降低 latency，但可能无法充分利用硬件资源。约束：整数 > 0。仅当 USE_BATCH_DECODE_SCHEDULER 为 True 时有效。",
+        help="Batch decode scheduler 的 batch size",
+    )
+    
+    batch_decode_scheduler_group.add_argument(
+        "--batch_decode_scheduler_warmup_type",
+        env_name="BATCH_DECODE_SCHEDULER_WARMUP_TYPE",
+        bind_to=[(batch_decode_scheduler_config, 'batch_decode_scheduler_warmup_type')],
+        type=int,
+        default=0,
+        help="Batch decode scheduler 的 warmup 类型：0 表示使用 decode warmup，其他值表示使用 prefill warmup",
     )
diff --git a/rtp_llm/server/server_args/cache_store_group_args.py b/rtp_llm/server/server_args/cache_store_group_args.py
index d1e96c7b8..96c97cabc 100644
--- a/rtp_llm/server/server_args/cache_store_group_args.py
+++ b/rtp_llm/server/server_args/cache_store_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_cache_store_group_args(parser):
+def init_cache_store_group_args(parser, cache_store_config):
     ##############################################################################################################
     # Cache Store 配置
     ##############################################################################################################
@@ -9,6 +9,7 @@ def init_cache_store_group_args(parser):
     cache_store_group.add_argument(
         "--cache_store_rdma_mode",
         env_name="CACHE_STORE_RDMA_MODE",
+        bind_to=(cache_store_config, 'cache_store_rdma_mode'),
         type=str2bool,
         default=False,
         help="控制 cache store 是否使用 RDMA 模式。",
@@ -17,6 +18,7 @@ def init_cache_store_group_args(parser):
     cache_store_group.add_argument(
         "--wrr_available_ratio",
         env_name="WRR_AVAILABLE_RATIO",
+        bind_to=(cache_store_config, 'wrr_available_ratio'),
         type=int,
         default=80,
         help="为 WRR (Weighted Round Robin) 负载均衡器设置的可用性阈值百分比 (0-100)，数值越低越容易启用动态权重分配，但可能降低全局负载均衡准确性。",
@@ -25,6 +27,7 @@ def init_cache_store_group_args(parser):
     cache_store_group.add_argument(
         "--rank_factor",
         env_name="RANK_FACTOR",
+        bind_to=(cache_store_config, 'rank_factor'),
         type=int,
         default=0,
         choices=[0, 1],
@@ -34,6 +37,7 @@ def init_cache_store_group_args(parser):
     cache_store_group.add_argument(
         "--cache_store_thread_count",
         env_name="CACHE_STORE_THREAD_COUNT",
+        bind_to=(cache_store_config, 'thread_count'),
         type=int,
         default=16,
         help="为 cache store 线程池设置线程数量。",
@@ -42,6 +46,7 @@ def init_cache_store_group_args(parser):
     cache_store_group.add_argument(
         "--cache_store_rdma_connect_timeout_ms",
         env_name="CACHE_STORE_RDMA_CONNECT_TIMEOUT_MS",
+        bind_to=(cache_store_config, 'rdma_connect_timeout_ms'),
         type=int,
         default=250,
         help="为 cache store RDMA 连接设置超时时间，单位为毫秒。",
@@ -50,6 +55,7 @@ def init_cache_store_group_args(parser):
     cache_store_group.add_argument(
         "--cache_store_rdma_qp_count_per_connection",
         env_name="CACHE_STORE_RDMA_QP_COUNT_PER_CONNECTION",
+        bind_to=(cache_store_config, 'rdma_qp_count_per_connection'),
         type=int,
         default=2,
         help="为 cache store RDMA 连接设置每个连接的底层QP数量。",
diff --git a/rtp_llm/server/server_args/concurrent_group_args.py b/rtp_llm/server/server_args/concurrent_group_args.py
index d29b8eba7..dd30e09cb 100644
--- a/rtp_llm/server/server_args/concurrent_group_args.py
+++ b/rtp_llm/server/server_args/concurrent_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_concurrent_group_args(parser):
+def init_concurrent_group_args(parser, concurrency_config):
     ##############################################################################################################
     # Concurrency 控制
     ##############################################################################################################
@@ -9,6 +9,7 @@ def init_concurrent_group_args(parser):
     concurrent_group.add_argument(
         "--concurrency_with_block",
         env_name="CONCURRENCY_WITH_BLOCK",
+        bind_to=(concurrency_config, 'concurrency_with_block'),
         type=str2bool,
         default=False,
         help="控制并发请求的阻塞行为。通常设置为 '1' (启用阻塞) 或 '0' (禁用阻塞)。",
@@ -16,6 +17,7 @@ def init_concurrent_group_args(parser):
     concurrent_group.add_argument(
         "--concurrency_limit",
         env_name="CONCURRENCY_LIMIT",
+        bind_to=(concurrency_config, 'concurrency_limit'),
         type=int,
         default=32,
         help="设置系统允许的最大并发请求数量。",
diff --git a/rtp_llm/server/server_args/device_resource_group_args.py b/rtp_llm/server/server_args/device_resource_group_args.py
index f6985baee..02c670755 100644
--- a/rtp_llm/server/server_args/device_resource_group_args.py
+++ b/rtp_llm/server/server_args/device_resource_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_device_resource_group_args(parser):
+def init_device_resource_group_args(parser, device_resource_config, runtime_config):
     ##############################################################################################################
     # 设备和资源管理
     ##############################################################################################################
@@ -10,6 +10,7 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--device_reserve_memory_bytes",
         env_name="DEVICE_RESERVE_MEMORY_BYTES",
+        bind_to=(device_resource_config, 'device_reserve_memory_bytes'),
         type=int,
         default=0,
         help="指定在GPU设备上预留的内存量（单位：字节）。此内存不会被常规操作使用，可用于应对突发需求或特定驱动/内核开销。",
@@ -18,6 +19,7 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--host_reserve_memory_bytes",
         env_name="HOST_RESERVE_MEMORY_BYTES",
+        bind_to=(device_resource_config, 'host_reserve_memory_bytes'),
         type=int,
         default=4 * 1024 * 1024 * 1024,  # 4GB
         help="指定在主机（CPU）上预留的内存量（单位：字节）。此内存不会被常规操作使用。默认为 4GB。",
@@ -26,6 +28,7 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--overlap_math_sm_count",
         env_name="OVERLAP_MATH_SM_COUNT",
+        bind_to=(device_resource_config, 'overlap_math_sm_count'),
         type=int,
         default=0,
         help="指定用于计算与通信重叠优化的 SM 数量。",
@@ -34,6 +37,7 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--overlap_comm_type",
         env_name="OVERLAP_COMM_TYPE",
+        bind_to=(device_resource_config, 'overlap_comm_type'),
         type=int,
         default=0,
         help="指定计算与通信重叠的策略类型。0: 禁止重叠，串行执行；1: 轻量级重叠，平衡性能与复杂度；2: 深度优化，通过多流和事件管理实现更大吞吐量。",
@@ -42,6 +46,7 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--m_split",
         env_name="M_SPLIT",
+        bind_to=(device_resource_config, 'm_split'),
         type=int,
         default=0,
         help="为特定设备操作设置 M_SPLIT 参数值。`0` 通常表示使用默认或不拆分。",
@@ -50,6 +55,7 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--enable_comm_overlap",
         env_name="ENABLE_COMM_OVERLAP",
+        bind_to=(device_resource_config, 'enable_comm_overlap'),
         type=str2bool,
         default=None,
         help="设置为 `True` 以启用计算与通信之间的重叠执行，旨在提高设备利用率和吞吐量。",
@@ -58,6 +64,7 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--enable_layer_micro_batch",
         env_name="ENABLE_LAYER_MICRO_BATCH",
+        bind_to=(device_resource_config, 'enable_layer_micro_batch'),
         type=int,
         default=0,
         help="控制是否启用层级的 micro-batching。",
@@ -66,14 +73,17 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--not_use_default_stream",
         env_name="NOT_USE_DEFAULT_STREAM",
+        bind_to=(device_resource_config, 'not_use_default_stream'),
         type=str2bool,
         default=False,
         help="控制 PyTorch 操作不使用标准的默认 CUDA 流。",
     )
 
+    # Fields merged from PyDeviceResourceConfig to RuntimeConfig
     device_resource_group.add_argument(
         "--reserver_runtime_mem_mb",
         env_name="RESERVER_RUNTIME_MEM_MB",
+        bind_to=(runtime_config, 'reserve_runtime_mem_mb'),  # Note: spelling difference (reserver -> reserve)
         type=int,
         default=1024,
         help="设备保留的运行时显存大小",
@@ -81,6 +91,7 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--specify_gpu_arch",
         env_name="SPECIFY_GPU_ARCH",
+        bind_to=(runtime_config, 'specify_gpu_arch'),
         type=str,
         default="",
         help="测试时使用的指定GPU架构",
@@ -88,7 +99,8 @@ def init_device_resource_group_args(parser):
     device_resource_group.add_argument(
         "--acext_gemm_config_dir",
         env_name="ACEXT_GEMM_CONFIG_DIR",
+        bind_to=(runtime_config, 'acext_gemm_config_dir'),
         type=str,
-        default=None,
+        default="",
         help="ACEXT GEMM配置目录",
     )
diff --git a/rtp_llm/server/server_args/embedding_group_args.py b/rtp_llm/server/server_args/embedding_group_args.py
index 92fbb6b51..3eefddbff 100644
--- a/rtp_llm/server/server_args/embedding_group_args.py
+++ b/rtp_llm/server/server_args/embedding_group_args.py
@@ -1,4 +1,4 @@
-def init_embedding_group_args(parser):
+def init_embedding_group_args(parser, embedding_config):
     ##############################################################################################################
     # Embedding Configuration
     ##############################################################################################################
@@ -6,6 +6,7 @@ def init_embedding_group_args(parser):
     embedding_group.add_argument(
         "--embedding_model",
         env_name="EMBEDDING_MODEL",
+        bind_to=(embedding_config, 'embedding_model'),
         type=int,
         default=0,
         help="嵌入模型的具体类型",
@@ -14,6 +15,7 @@ def init_embedding_group_args(parser):
     embedding_group.add_argument(
         "--extra_input_in_mm_embedding",
         env_name="EXTRA_INPUT_IN_MM_EMBEDDING",
+        bind_to=(embedding_config, 'extra_input_in_mm_embedding'),
         type=str,
         default=None,
         help='在多模态嵌入中使用额外的输入，可选值"INDEX"',
diff --git a/rtp_llm/server/server_args/engine_group_args.py b/rtp_llm/server/server_args/engine_group_args.py
index 2b4e89237..f8191deca 100644
--- a/rtp_llm/server/server_args/engine_group_args.py
+++ b/rtp_llm/server/server_args/engine_group_args.py
@@ -1,26 +1,25 @@
-def init_engine_group_args(parser):
+from rtp_llm.server.server_args.util import str2bool
+
+
+def init_engine_group_args(parser, runtime_config):
     ##############################################################################################################
     # Engine Configuration
+    # Fields merged from EngineConfig to RuntimeConfig (warm_up, warm_up_with_loss)
     ##############################################################################################################
     engine_group = parser.add_argument_group("Engine Configuration")
     engine_group.add_argument(
         "--warm_up",
         env_name="WARM_UP",
-        type=int,
-        default=1,
+        bind_to=(runtime_config, 'warm_up'),
+        type=str2bool,
+        default=True,
         help="在服务启动时是否开启预热",
     )
     engine_group.add_argument(
         "--warm_up_with_loss",
         env_name="WARM_UP_WITH_LOSS",
-        type=int,
-        default=0,
+        bind_to=(runtime_config, 'warm_up_with_loss'),
+        type=str2bool,
+        default=False,
         help="在服务启动时是否开启损失去预热",
     )
-    engine_group.add_argument(
-        "--max_seq_len",
-        env_name="MAX_SEQ_LEN",
-        type=int,
-        default=0,
-        help="输入输出的最大长度",
-    )
diff --git a/rtp_llm/server/server_args/fifo_scheduler_group_args.py b/rtp_llm/server/server_args/fifo_scheduler_group_args.py
index fe844e2fa..e798d765d 100644
--- a/rtp_llm/server/server_args/fifo_scheduler_group_args.py
+++ b/rtp_llm/server/server_args/fifo_scheduler_group_args.py
@@ -1,14 +1,16 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_fifo_scheduler_group_args(parser):
+def init_fifo_scheduler_group_args(parser, fifo_scheduler_config):
     ##############################################################################################################
     # FIFO 调度器配置
     ##############################################################################################################
     fifo_scheduler_group = parser.add_argument_group("FIFO Scheduler")
+    
     fifo_scheduler_group.add_argument(
         "--max_context_batch_size",
         env_name="MAX_CONTEXT_BATCH_SIZE",
+        bind_to=[(fifo_scheduler_config, 'max_context_batch_size')],
         type=int,
         default=1,
         help="（设备参数）为设备参数设置的最大 context batch size，影响默认调度器的凑批决策。",
@@ -16,6 +18,7 @@ def init_fifo_scheduler_group_args(parser):
     fifo_scheduler_group.add_argument(
         "--scheduler_reserve_resource_ratio",
         env_name="SCHEDULER_RESERVE_RESOURCE_RATIO",
+        bind_to=[(fifo_scheduler_config, 'scheduler_reserve_resource_ratio')],
         type=int,
         default=5,
         help="默认调度器将尝试保留的 KV cache blocks 的最小百分比。这有助于应对突发请求模式，为高优先级请求预留空间，或防止系统性能颠簸。",
@@ -24,6 +27,7 @@ def init_fifo_scheduler_group_args(parser):
     fifo_scheduler_group.add_argument(
         "--enable_fast_gen",
         env_name="ENABLE_FAST_GEN",
+        bind_to=[(fifo_scheduler_config, 'enable_fast_gen')],
         type=str2bool,
         default=False,
         help="若为 True，长请求会被拆分为chunks并分步处理。这主要用于提高长序列或流式输入的处理效率，并能改善并发场景下其他请求的交互性。注意：仅在使用默认调度器时有效。",
@@ -32,13 +36,23 @@ def init_fifo_scheduler_group_args(parser):
     fifo_scheduler_group.add_argument(
         "--fast_gen_context_budget",
         env_name="FAST_GEN_MAX_CONTEXT_LEN",  # 和参数名不一致
+        bind_to=[(fifo_scheduler_config, 'fast_gen_context_budget')],
         type=int,
         help="当 ENABLE_FAST_GEN 启用时，拆分成的chunk的大小。注意：仅当 ENABLE_FAST_GEN 为 True 且使用默认调度器时有效。",
     )
     fifo_scheduler_group.add_argument(
         "--enable_partial_fallback",
         env_name="ENABLE_PARTIAL_FALLBACK",
+        bind_to=[(fifo_scheduler_config, 'enable_partial_fallback')],
         type=str2bool,
         default=False,
         help="若为 True，则允许默认调度器在系统内存不足以满足活动请求时，从某些请求中回收一部分 KV cache blocks。这可以在高负载下提高系统利用率，但可能会影响那些资源被回收的请求的公平性。注意：在使用默认调度器时有效。",
     )
+    fifo_scheduler_group.add_argument(
+        "--max_batch_tokens_size",
+        env_name="MAX_BATCH_TOKENS_SIZE",
+        bind_to=[(fifo_scheduler_config, 'max_batch_tokens_size')],
+        type=int,
+        default=0,
+        help="最大 batch tokens 大小。",
+    )
diff --git a/rtp_llm/server/server_args/fmha_group_args.py b/rtp_llm/server/server_args/fmha_group_args.py
index f211d3ced..a61deba94 100644
--- a/rtp_llm/server/server_args/fmha_group_args.py
+++ b/rtp_llm/server/server_args/fmha_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_fmha_group_args(parser):
+def init_fmha_group_args(parser, fmha_config):
     ##############################################################################################################
     # FMHA
     ##############################################################################################################
@@ -9,6 +9,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--enable_fmha",
         env_name="ENABLE_FMHA",
+        bind_to=(fmha_config, 'enable_fmha'),
         type=str2bool,
         default=True,
         help="控制是否启用Fused Multi-Head Attention (FMHA) 功能。可选值: True (启用), False (禁用)。",
@@ -16,6 +17,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--enable_trt_fmha",
         env_name="ENABLE_TRT_FMHA",
+        bind_to=(fmha_config, 'enable_trt_fmha'),
         type=str2bool,
         default=True,
         help="控制是否启用经TensorRT(V2版本)优化的FMHA功能。可选值: True (启用), False (禁用)。",
@@ -23,6 +25,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--enable_paged_trt_fmha",
         env_name="ENABLE_PAGED_TRT_FMHA",
+        bind_to=(fmha_config, 'enable_paged_trt_fmha'),
         type=str2bool,
         default=True,
         help="控制是否启用Paged TensorRT FMHA功能。可选值: True (启用), False (禁用)。",
@@ -30,6 +33,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--enable_open_source_fmha",
         env_name="ENABLE_OPENSOURCE_FMHA",
+        bind_to=(fmha_config, 'enable_open_source_fmha'),
         type=str2bool,
         default=True,
         help="控制是否启用开源版本的FMHA实现。可选值: True (启用), False (禁用)。",
@@ -37,6 +41,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--enable_paged_open_source_fmha",
         env_name="ENABLE_PAGED_OPEN_SOURCE_FMHA",
+        bind_to=(fmha_config, 'enable_paged_open_source_fmha'),
         type=str2bool,
         default=True,
         help="控制是否启用Paged开源版本的FMHA实现。可选值: True (启用), False (禁用)。",
@@ -44,6 +49,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--enable_trtv1_fmha",
         env_name="ENABLE_TRTV1_FMHA",
+        bind_to=(fmha_config, 'enable_trtv1_fmha'),
         type=str2bool,
         default=True,
         help="控制是否启用TRTv1风格的FMHA功能。可选值: True (启用), False (禁用)。",
@@ -51,6 +57,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--fmha_perf_instrument",
         env_name="FMHA_PERF_INSTRUMENT",
+        bind_to=(fmha_config, 'fmha_perf_instrument'),
         type=str2bool,
         default=False,
         help="控制是否为FMHA启用NVTX性能分析。设置为 True 启用, False 禁用。",
@@ -58,6 +65,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--fmha_show_params",
         env_name="FMHA_SHOW_PARAMS",
+        bind_to=(fmha_config, 'fmha_show_params'),
         type=str2bool,
         default=False,
         help="控制是否显示FMHA的参数信息。设置为 True 启用, False 禁用。",
@@ -65,6 +73,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--disable_flash_infer",
         env_name="DISABLE_FLASH_INFER",
+        bind_to=(fmha_config, 'disable_flash_infer'),
         type=str2bool,
         default=False,
         help="控制是否禁用FlashInfer Attention机制。设置为 True 启用, False 禁用。",
@@ -72,6 +81,7 @@ def init_fmha_group_args(parser):
     fmha_group.add_argument(
         "--enable_xqa",
         env_name="ENABLE_XQA",
+        bind_to=(fmha_config, 'enable_xqa'),
         type=str2bool,
         default=True,
         help="控制是否开启 xqa 的功能，此功能需要 SM 90 (Hopper) 或更新的 GPU 架构。可选值: True (启用), False (禁用)。",
diff --git a/rtp_llm/server/server_args/gang_group_args.py b/rtp_llm/server/server_args/gang_group_args.py
index 31784d047..b187aacf8 100644
--- a/rtp_llm/server/server_args/gang_group_args.py
+++ b/rtp_llm/server/server_args/gang_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_gang_group_args(parser):
+def init_gang_group_args(parser, gang_config):
     ##############################################################################################################
     # Gang Configuration
     ##############################################################################################################
@@ -9,6 +9,7 @@ def init_gang_group_args(parser):
     gang_group.add_argument(
         "--fake_gang_env",
         env_name="FAKE_GANG_ENV",
+        bind_to=(gang_config, 'fake_gang_env'),
         type=str2bool,
         default=False,
         help="在多机启动时的fake行为",
@@ -16,6 +17,7 @@ def init_gang_group_args(parser):
     gang_group.add_argument(
         "--gang_annocation_path",
         env_name="GANG_ANNOCATION_PATH",
+        bind_to=(gang_config, 'gang_annocation_path'),
         type=str,
         default="/etc/podinfo/annotations",
         help="GANG信息的路径",
@@ -23,16 +25,23 @@ def init_gang_group_args(parser):
     gang_group.add_argument(
         "--gang_config_string",
         env_name="GANG_CONFIG_STRING",
+        bind_to=(gang_config, 'gang_config_string'),
         type=str,
         default=None,
         help="GAG信息的字符串表达",
     )
     gang_group.add_argument(
-        "--zone_name", env_name="ZONE_NAME", type=str, default="", help="角色名"
+        "--zone_name",
+        env_name="ZONE_NAME",
+        bind_to=(gang_config, 'zone_name'),
+        type=str,
+        default="",
+        help="角色名"
     )
     gang_group.add_argument(
         "--distribute_config_file",
         env_name="DISTRIBUTE_CONFIG_FILE",
+        bind_to=(gang_config, 'distribute_config_file'),
         type=str,
         default=None,
         help="分布式的配置文件路径",
@@ -40,6 +49,7 @@ def init_gang_group_args(parser):
     gang_group.add_argument(
         "--dist_barrier_timeout",
         env_name="DIST_BARRIER_TIMEOUT",
+        bind_to=(gang_config, 'dist_barrier_timeout'),
         type=int,
         default=45,
         help="心跳检测的超时时间",
@@ -47,6 +57,7 @@ def init_gang_group_args(parser):
     gang_group.add_argument(
         "--gang_sleep_time",
         env_name="GANG_SLEEP_TIME",
+        bind_to=(gang_config, 'gang_sleep_time'),
         type=int,
         default=10,
         help="心跳检测的间隔时间",
@@ -54,6 +65,7 @@ def init_gang_group_args(parser):
     gang_group.add_argument(
         "--gang_timeout_min",
         env_name="GANG_TIMEOUT_MIN",
+        bind_to=(gang_config, 'gang_timeout_min'),
         type=int,
         default=30,
         help="心跳超时的最小时间",
diff --git a/rtp_llm/server/server_args/generate_group_args.py b/rtp_llm/server/server_args/generate_group_args.py
index a09c484c9..2a1d9ea26 100644
--- a/rtp_llm/server/server_args/generate_group_args.py
+++ b/rtp_llm/server/server_args/generate_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_generate_group_args(parser):
+def init_generate_group_args(parser, generate_env_config):
     ##############################################################################################################
     # Generate Configuration
     ##############################################################################################################
@@ -9,6 +9,7 @@ def init_generate_group_args(parser):
     generate_group.add_argument(
         "--think_end_tag",
         env_name="THINK_END_TAG",
+        bind_to=(generate_env_config, 'think_end_tag'),
         type=str,
         default="</think>\n\n",
         help="深度思考模式的结束标签",
@@ -16,6 +17,7 @@ def init_generate_group_args(parser):
     generate_group.add_argument(
         "--think_end_token_id",
         env_name="THINK_END_TOKEN_ID",
+        bind_to=(generate_env_config, 'think_end_token_id'),
         type=int,
         default=-1,
         help="深度思考模式的结束标签的 TOKEN_ID",
@@ -23,6 +25,7 @@ def init_generate_group_args(parser):
     generate_group.add_argument(
         "--think_mode",
         env_name="THINK_MODE",
+        bind_to=(generate_env_config, 'think_mode'),
         type=int,
         default=0,
         help="深度思考模式是否开启",
@@ -30,6 +33,7 @@ def init_generate_group_args(parser):
     generate_group.add_argument(
         "--force_stop_words",
         env_name="FORCE_STOP_WORDS",
+        bind_to=(generate_env_config, 'force_stop_words'),
         type=str2bool,
         default=False,
         help="是否开启使用环境变量强制指定模型的STOP WORDS",
@@ -37,6 +41,7 @@ def init_generate_group_args(parser):
     generate_group.add_argument(
         "--stop_words_list",
         env_name="STOP_WORDS_LIST",
+        bind_to=(generate_env_config, 'stop_words_list'),
         type=str,
         default=None,
         help="STOP_WORDS的TokenID列表",
@@ -44,6 +49,7 @@ def init_generate_group_args(parser):
     generate_group.add_argument(
         "--stop_words_str",
         env_name="STOP_WORDS_STR",
+        bind_to=(generate_env_config, 'stop_words_str'),
         type=str,
         default=None,
         help="STOP_WORDS的string明文",
@@ -51,6 +57,7 @@ def init_generate_group_args(parser):
     generate_group.add_argument(
         "--think_start_tag",
         env_name="THINK_START_TAG",
+        bind_to=(generate_env_config, 'think_start_tag'),
         type=str,
         default="<think>\\n",
         help="深度思考模式的起始标签",
@@ -58,6 +65,7 @@ def init_generate_group_args(parser):
     generate_group.add_argument(
         "--generation_config_path",
         env_name="GENERATION_CONFIG_PATH",
+        bind_to=(generate_env_config, 'generation_config_path'),
         type=str,
         default=None,
         help="生成配置路径",
diff --git a/rtp_llm/server/server_args/hw_kernel_group_args.py b/rtp_llm/server/server_args/hw_kernel_group_args.py
index affb27151..2a001be18 100644
--- a/rtp_llm/server/server_args/hw_kernel_group_args.py
+++ b/rtp_llm/server/server_args/hw_kernel_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_hw_kernel_group_args(parser):
+def init_hw_kernel_group_args(parser, hw_kernel_config):
     ##############################################################################################################
     # 硬件/Kernel 特定优化
     ##############################################################################################################
@@ -10,6 +10,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--enable_cuda_graph",
         env_name="ENABLE_CUDA_GRAPH",
+        bind_to=(hw_kernel_config, 'enable_cuda_graph'),
         type=str2bool,
         default=False,
         help="系统是否允许使用Cuda Graph",
@@ -18,6 +19,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--enable_cuda_graph_debug_mode",
         env_name="ENABLE_CUDA_GRAPH_DEBUG_MODE",
+        bind_to=(hw_kernel_config, 'enable_cuda_graph_debug_mode'),
         type=str2bool,
         default=False,
         help="系统是否允许使用Cuda Graph开启Debug模式来生成可视化文件",
@@ -26,6 +28,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--enable_native_cuda_graph",
         env_name="ENABLE_NATIVE_CUDA_GRAPH",
+        bind_to=(hw_kernel_config, 'enable_native_cuda_graph'),
         type=str2bool,
         default=False,
         help="系统是否允许在C++后端使用Cuda Graph",
@@ -34,6 +37,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--num_native_cuda_graph",
         env_name="NUM_NATIVE_CUDA_GRAPH",
+        bind_to=(hw_kernel_config, 'num_native_cuda_graph'),
         type=int,
         default=200,
         help="C++后端缓存Cuda Graph数量",
@@ -42,6 +46,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--deep_gemm_num_sm",
         env_name="DEEP_GEMM_NUM_SM",
+        bind_to=(hw_kernel_config, 'deep_gemm_num_sm'),
         type=int,
         default=None,
         help="指定 DeepGEMM 使用的 SM (Streaming Multiprocessor) 数量。如果设置，此值将覆盖自动检测的数量。",
@@ -50,6 +55,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--arm_gemm_use_kai",
         env_name="ARM_GEMM_USE_KAI",
+        bind_to=(hw_kernel_config, 'arm_gemm_use_kai'),
         type=str2bool,
         default=False,
         help="设置为 `True` 时，为 ARM GEMM 操作启用 KleidiAI 支持。这可能影响权重处理和计算性能。",
@@ -58,6 +64,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--enable_stable_scatter_add",
         env_name="ENABLE_STABLE_SCATTER_ADD",
+        bind_to=(hw_kernel_config, 'enable_stable_scatter_add'),
         type=str2bool,
         default=False,
         help="控制是否启用稳定的 scatter add 操作。",
@@ -66,6 +73,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--enable_multi_block_mode",
         env_name="ENABLE_MULTI_BLOCK_MODE",
+        bind_to=(hw_kernel_config, 'enable_multi_block_mode'),
         type=str2bool,
         default=True,
         help="控制是否为 Multi-Head Attention (MMHA) 启用 multi-block 模式。设置为 'ON' 启用，'OFF' 禁用。",
@@ -74,6 +82,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--rocm_hipblaslt_config",
         env_name="ROCM_HIPBLASLT_CONFIG",
+        bind_to=(hw_kernel_config, 'rocm_hipblaslt_config'),
         type=str,
         default="gemm_config.csv",
         help="指定 hipBLASLt GEMM 配置文件的路径。此文件用于优化 ROCm平台上的 GEMM 操作。",
@@ -82,6 +91,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--ft_disable_custom_ar",
         env_name="FT_DISABLE_CUSTOM_AR",
+        bind_to=(hw_kernel_config, 'ft_disable_custom_ar'),
         type=str2bool,
         default=None,
         help="设置为 `True` 时，禁用自定义的 AllReduce (AR) 实现，可能回退到标准库（如 NCCL）的 AllReduce。",
@@ -90,6 +100,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--use_aiter_pa",
         env_name="USE_AITER_PA",
+        bind_to=(hw_kernel_config, 'use_aiter_pa'),
         type=str2bool,
         default=True,
         help="Rocm是否使用AITER Attention",
@@ -98,6 +109,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--use_asm_pa",
         env_name="USE_ASM_PA",
+        bind_to=(hw_kernel_config, 'use_asm_pa'),
         type=str2bool,
         default=True,
         help="Rocm是否使用AITER ASM Attention",
@@ -106,6 +118,7 @@ def init_hw_kernel_group_args(parser):
     hw_kernel_group.add_argument(
         "--use_swizzleA",
         env_name="USE_SWIZZLEA",
+        bind_to=(hw_kernel_config, 'use_swizzleA'),
         type=str2bool,
         default=False,
         help="hipBLASLt GEMM 是否使用 swizzle",
diff --git a/rtp_llm/server/server_args/jit_group_args.py b/rtp_llm/server/server_args/jit_group_args.py
index 8cc557488..702db2548 100644
--- a/rtp_llm/server/server_args/jit_group_args.py
+++ b/rtp_llm/server/server_args/jit_group_args.py
@@ -1,4 +1,4 @@
-def init_jit_group_args(parser):
+def init_jit_group_args(parser, jit_config):
     ##############################################################################################################
     # JIT Configuration
     ##############################################################################################################
@@ -6,6 +6,7 @@ def init_jit_group_args(parser):
     jit_group.add_argument(
         "--remote_jit_dir",
         env_name="REMOTE_JIT_DIR",
+        bind_to=(jit_config, 'remote_jit_dir'),
         type=str,
         default="/mnt/nas1",
         help="JIT远程cache目录",
diff --git a/rtp_llm/server/server_args/kv_cache_group_args.py b/rtp_llm/server/server_args/kv_cache_group_args.py
index de812ee70..146fa54db 100644
--- a/rtp_llm/server/server_args/kv_cache_group_args.py
+++ b/rtp_llm/server/server_args/kv_cache_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_kv_cache_group_args(parser):
+def init_kv_cache_group_args(parser, kv_cache_config):
     ##############################################################################################################
     # KV Cache 相关配置
     ##############################################################################################################
@@ -9,6 +9,7 @@ def init_kv_cache_group_args(parser):
     kv_cache_group.add_argument(
         "--reuse_cache",
         env_name="REUSE_CACHE",
+        bind_to=(kv_cache_config, 'reuse_cache'),
         type=str2bool,
         default=False,
         help="控制是否激活KV Cache的重用机制。设置为 True 启用 , False 关闭",
@@ -16,6 +17,7 @@ def init_kv_cache_group_args(parser):
     kv_cache_group.add_argument(
         "--multi_task_prompt",
         env_name="MULTI_TASK_PROMPT",
+        bind_to=(kv_cache_config, 'multi_task_prompt'),
         type=str,
         default=None,
         help="指定一个多任务提示（multi-task prompt），为一个路径，系统会读取路径指定的多任务json文件。默认为空",
@@ -23,6 +25,7 @@ def init_kv_cache_group_args(parser):
     kv_cache_group.add_argument(
         "--multi_task_prompt_str",
         env_name="MULTI_TASK_PROMPT_STR",
+        bind_to=(kv_cache_config, 'multi_task_prompt_str'),
         type=str,
         default=None,
         help="指定一个多任务提示字符串（multi-task prompt string），为多任务纯json字符串，类似于系统提示词。默认为空 ",
@@ -30,6 +33,7 @@ def init_kv_cache_group_args(parser):
     kv_cache_group.add_argument(
         "--int8_kv_cache",
         env_name="INT8_KV_CACHE",
+        bind_to=(kv_cache_config, 'int8_kv_cache'),
         type=int,
         default=0,
         help="是否开启INT8的KV_CACHE",
@@ -37,6 +41,7 @@ def init_kv_cache_group_args(parser):
     kv_cache_group.add_argument(
         "--fp8_kv_cache",
         env_name="FP8_KV_CACHE",
+        bind_to=(kv_cache_config, 'fp8_kv_cache'),
         type=int,
         default=0,
         help="是否开启FP8的KV_CACHE",
@@ -44,6 +49,7 @@ def init_kv_cache_group_args(parser):
     kv_cache_group.add_argument(
         "--kv_cache_mem_mb",
         env_name="KV_CACHE_MEM_MB",
+        bind_to=(kv_cache_config, 'kv_cache_mem_mb'),
         type=int,
         default=-1,
         help="KV_CACHE的大小",
@@ -51,13 +57,15 @@ def init_kv_cache_group_args(parser):
     kv_cache_group.add_argument(
         "--seq_size_per_block",
         env_name="SEQ_SIZE_PER_BLOCK",
-        type=str,
-        default=None,
+        bind_to=(kv_cache_config, 'seq_size_per_block'),
+        type=int,
+        default=64,
         help="单独一个KV_CACHE的Block里面token的数量",
     )
     kv_cache_group.add_argument(
         "--test_block_num",
         env_name="TEST_BLOCK_NUM",
+        bind_to=(kv_cache_config, 'test_block_num'),
         type=int,
         default=0,
         help="在测试时强制指定BLOCK的数量",
@@ -65,6 +73,7 @@ def init_kv_cache_group_args(parser):
     kv_cache_group.add_argument(
         "--memory_block_cache_size_mb",
         env_name="MEMORY_BLOCK_CACHE_SIZE_MB",
+        bind_to=(kv_cache_config, 'memory_block_cache_size_mb'),
         type=int,
         default=0,
         help="单个RANK MemoryBlockCache 的大小, 单位为MB",
@@ -72,6 +81,7 @@ def init_kv_cache_group_args(parser):
     kv_cache_group.add_argument(
         "--memory_block_cache_sync_timeout_ms",
         env_name="MEMORY_BLOCK_CACHE_SYNC_TIMEOUT_MS",
+        bind_to=(kv_cache_config, 'memory_block_cache_sync_timeout_ms'),
         type=int,
         default=10000,
         help="MemoryBlockCache 多TP同步的超时时间, 单位为毫秒",
diff --git a/rtp_llm/server/server_args/load_group_args.py b/rtp_llm/server/server_args/load_group_args.py
index 3d785655d..204333e37 100644
--- a/rtp_llm/server/server_args/load_group_args.py
+++ b/rtp_llm/server/server_args/load_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_load_group_args(parser):
+def init_load_group_args(parser, load_config):
     ##############################################################################################################
     # Load Configuration
     ##############################################################################################################
@@ -9,6 +9,7 @@ def init_load_group_args(parser):
     load_group.add_argument(
         "--phy2log_path",
         env_name="PHY2LOG_PATH",
+        bind_to=(load_config, 'phy2log_path'),
         type=str,
         default="",
         help="python日志输出路径",
@@ -16,6 +17,7 @@ def init_load_group_args(parser):
     load_group.add_argument(
         "--converter_num_per_gpu",
         env_name="CONVERTER_NUM_PER_GPU",
+        bind_to=(load_config, 'converter_num_per_gpu'),
         type=int,
         default=4,
         help="每个GPU做多少个转化",
@@ -23,6 +25,7 @@ def init_load_group_args(parser):
     load_group.add_argument(
         "--tokenizers_parallelism",
         env_name="TOKENIZERS_PARALLELISM",
+        bind_to=(load_config, 'tokenizers_parallelism'),
         type=str2bool,
         default=False,
         help="分词器并行度",
@@ -30,6 +33,7 @@ def init_load_group_args(parser):
     load_group.add_argument(
         "--load_ckpt_num_process",
         env_name="LOAD_CKPT_NUM_PROCESS",
+        bind_to=(load_config, 'load_ckpt_num_process'),
         type=int,
         default=0,
         help="加载Checkpoint的进程数量",
diff --git a/rtp_llm/server/server_args/lora_group_args.py b/rtp_llm/server/server_args/lora_group_args.py
index a9120b8fa..ae2bab3b5 100644
--- a/rtp_llm/server/server_args/lora_group_args.py
+++ b/rtp_llm/server/server_args/lora_group_args.py
@@ -1,17 +1,23 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_lora_group_args(parser):
+def init_lora_group_args(parser, lora_config):
     ##############################################################################################################
     # Lora Configuration
     ##############################################################################################################
     lora_group = parser.add_argument_group("Lora Configuration")
     lora_group.add_argument(
-        "--lora_info", env_name="LORA_INFO", type=str, default="{}", help="Lora的信息"
+        "--lora_info",
+        env_name="LORA_INFO",
+        bind_to=(lora_config, 'lora_info'),
+        type=str,
+        default="{}",
+        help="Lora的信息"
     )
     lora_group.add_argument(
         "--merge_lora",
         env_name="MERGE_LORA",
+        bind_to=(lora_config, 'merge_lora'),
         type=str2bool,
         default=True,
         help="Lora合并",
diff --git a/rtp_llm/server/server_args/misc_group_args.py b/rtp_llm/server/server_args/misc_group_args.py
index 4acc54e77..1ea1962b9 100644
--- a/rtp_llm/server/server_args/misc_group_args.py
+++ b/rtp_llm/server/server_args/misc_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_misc_group_args(parser):
+def init_misc_group_args(parser, misc_config):
     ##############################################################################################################
     # Miscellaneous 配置
     ##############################################################################################################
@@ -10,6 +10,7 @@ def init_misc_group_args(parser):
     misc_group.add_argument(
         "--disable_pdl",
         env_name="DISABLE_PDL",
+        bind_to=(misc_config.misc_config, 'disable_pdl'),
         type=str2bool,
         default=True,
         help="是否禁用PDL",
@@ -18,7 +19,49 @@ def init_misc_group_args(parser):
     misc_group.add_argument(
         "--aux_string",
         env_name="AUX_STRING",
+        bind_to=(misc_config.misc_config, 'aux_string'),
         type=str,
         default="",
         help="管控环境变量字符串",
     )
+
+    misc_group.add_argument(
+        "--oss_endpoint",
+        env_name="OSS_ENDPOINT",
+        bind_to=(misc_config, 'oss_endpoint'),
+        type=str,
+        default=None,
+        help="OSS端点",
+    )
+    misc_group.add_argument(
+        "--dashscope_api_key",
+        env_name="DASHSCOPE_API_KEY",
+        bind_to=(misc_config, 'dashscope_api_key'),
+        type=str,
+        default="EMPTY",
+        help="Dashscope API Key",
+    )
+    misc_group.add_argument(
+        "--dashscope_http_url",
+        env_name="DASHSCOPE_HTTP_URL",
+        bind_to=(misc_config, 'dashscope_http_url'),
+        type=str,
+        default=None,
+        help="Dashscope HTTP URL",
+    )
+    misc_group.add_argument(
+        "--dashscope_websocket_url",
+        env_name="DASHSCOPE_WEBSOCKET_URL",
+        bind_to=(misc_config, 'dashscope_websocket_url'),
+        type=str,
+        default=None,
+        help="Dashscope WebSocket URL",
+    )
+    misc_group.add_argument(
+        "--openai_api_key",
+        env_name="OPENAI_API_KEY",
+        bind_to=(misc_config, 'openai_api_key'),
+        type=str,
+        default="EMPTY",
+        help="OpenAI API Key",
+    )
diff --git a/rtp_llm/server/server_args/model_group_args.py b/rtp_llm/server/server_args/model_group_args.py
index dffb85499..2e1060b39 100644
--- a/rtp_llm/server/server_args/model_group_args.py
+++ b/rtp_llm/server/server_args/model_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_model_group_args(parser):
+def init_model_group_args(parser, model_args):
     ##############################################################################################################
     # Model Configuration
     ##############################################################################################################
@@ -9,6 +9,7 @@ def init_model_group_args(parser):
     model_group.add_argument(
         "--extra_data_path",
         env_name="EXTRA_DATA_PATH",
+        bind_to=(model_args, 'extra_data_path'),
         type=str,
         default=None,
         help="额外的数据路径",
@@ -16,6 +17,7 @@ def init_model_group_args(parser):
     model_group.add_argument(
         "--local_extra_data_path",
         env_name="LOCAL_EXTRA_DATA_PATH",
+        bind_to=(model_args, 'local_extra_data_path'),
         type=str,
         default=None,
         help="本地额外数据路径",
@@ -23,6 +25,7 @@ def init_model_group_args(parser):
     model_group.add_argument(
         "--tokenizer_path",
         env_name="TOKENIZER_PATH",
+        bind_to=(model_args, 'tokenizer_path'),
         type=str,
         default=None,
         help="分词器的路径",
@@ -30,6 +33,7 @@ def init_model_group_args(parser):
     model_group.add_argument(
         "--act_type",
         env_name="ACT_TYPE",
+        bind_to=(model_args, 'act_type'),
         type=str,
         default="FP16",
         help="计算使用的数据类型",
@@ -37,6 +41,7 @@ def init_model_group_args(parser):
     model_group.add_argument(
         "--use_float32",
         env_name="USE_FLOAT32",
+        bind_to=(model_args, 'use_float32'),
         type=str2bool,
         default=False,
         help="是否使用FP32",
@@ -44,6 +49,7 @@ def init_model_group_args(parser):
     model_group.add_argument(
         "--original_checkpoint_path",
         env_name="ORIGINAL_CHECKPOINT_PATH",
+        bind_to=(model_args, 'original_checkpoint_path'),
         type=str,
         default=None,
         help="原始的checkpoint的路径",
@@ -51,83 +57,56 @@ def init_model_group_args(parser):
     model_group.add_argument(
         "--mla_ops_type",
         env_name="MLA_OPS_TYPE",
+        bind_to=(model_args, 'mla_ops_type'),
         type=str,
         default="AUTO",
-        help="Multi Latent Attention的操作类型",
+        help="Multi Latent Attention的操作类型（将自动转换为枚举）",
     )
     model_group.add_argument(
-        "--ft_plugin_path",
-        env_name="FT_PLUGIN_PATH",
+        "--task_type",
+        env_name="TASK_TYPE",
+        bind_to=(model_args, 'task_type'),
         type=str,
         default=None,
-        help="插件路径",
+        help="任务类型（将自动转换为枚举）"
     )
     model_group.add_argument(
-        "--weight_type",
-        env_name="WEIGHT_TYPE",
+        "--model_type",
+        env_name="MODEL_TYPE",
+        bind_to=(model_args, 'model_type'),
         type=str,
         default=None,
-        help="模型权重类型",
-    )
-    model_group.add_argument(
-        "--task_type", env_name="TASK_TYPE", type=str, default=None, help="任务类型"
-    )
-    model_group.add_argument(
-        "--model_type", env_name="MODEL_TYPE", type=str, default=None, help="模型类型"
+        help="模型类型"
     )
     model_group.add_argument(
         "--checkpoint_path",
         env_name="CHECKPOINT_PATH",
+        bind_to=(model_args, 'ckpt_path'),
         type=str,
         default=None,
         help="Checkpoint路径",
     )
-    model_group.add_argument(
-        "--oss_endpoint",
-        env_name="OSS_ENDPOINT",
-        type=str,
-        default=None,
-        help="OSS端点",
-    )
     model_group.add_argument(
         "--ptuning_path",
         env_name="PTUNING_PATH",
+        bind_to=(model_args, 'ptuning_path'),
         type=str,
         default=None,
         help="PTuning路径",
     )
-    model_group.add_argument(
-        "--dashscope_api_key",
-        env_name="DASHSCOPE_API_KEY",
-        type=str,
-        default="EMPTY",
-        help="Dashscope API Key",
-    )
-    model_group.add_argument(
-        "--dashscope_http_url",
-        env_name="DASHSCOPE_HTTP_URL",
-        type=str,
-        default=None,
-        help="Dashscope HTTP URL",
-    )
-    model_group.add_argument(
-        "--dashscope_websocket_url",
-        env_name="DASHSCOPE_WEBSOCKET_URL",
-        type=str,
-        default=None,
-        help="Dashscope WebSocket URL",
-    )
-    model_group.add_argument(
-        "--openai_api_key",
-        env_name="OPENAI_API_KEY",
-        type=str,
-        default="EMPTY",
-        help="OpenAI API Key",
-    )
     model_group.add_argument(
         "--json_model_override_args",
         env_name="JSON_MODEL_OVERRIDE_ARGS",
+        bind_to=(model_args, 'json_model_override_args'),
         type=str,
         default="{}",
         help="A dictionary in JSON string format used to override default model configurations.",
     )
+    model_group.add_argument(
+        "--max_seq_len",
+        env_name="MAX_SEQ_LEN",
+        bind_to=(model_args, 'max_seq_len'),
+        type=int,
+        default=8192,
+        help="最大序列长度",
+    )
diff --git a/rtp_llm/server/server_args/model_specific_group_args.py b/rtp_llm/server/server_args/model_specific_group_args.py
index d40a71464..d1fee17dd 100644
--- a/rtp_llm/server/server_args/model_specific_group_args.py
+++ b/rtp_llm/server/server_args/model_specific_group_args.py
@@ -1,4 +1,4 @@
-def init_model_specific_group_args(parser):
+def init_model_specific_group_args(parser, model_specific_config):
     ##############################################################################################################
     # 模型特定配置
     ##############################################################################################################
@@ -7,6 +7,7 @@ def init_model_specific_group_args(parser):
     model_specific_group.add_argument(
         "--max_lora_model_size",
         env_name="MAX_LORA_MODEL_SIZE",
+        bind_to=(model_specific_config, 'max_lora_model_size'),
         type=int,
         default=-1,
         help="指定 LoRA 模型的最大允许大小。",
diff --git a/rtp_llm/server/server_args/moe_group_args.py b/rtp_llm/server/server_args/moe_group_args.py
index 93856c826..0b0310d2a 100644
--- a/rtp_llm/server/server_args/moe_group_args.py
+++ b/rtp_llm/server/server_args/moe_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_moe_group_args(parser):
+def init_moe_group_args(parser, py_eplb_config):
     ##############################################################################################################
     # MOE 特性
     ##############################################################################################################
@@ -57,6 +57,7 @@ def init_moe_group_args(parser):
     moe_group.add_argument(
         "--eplb_control_step",
         env_name="EPLB_CONTROL_STEP",
+        bind_to=(py_eplb_config, 'eplb_control_step'),
         type=int,
         default=100,
         help="为 EPLB (Expert Placement Load Balancing) 控制器指定控制周期或步骤参数。这可能影响专家的负载均衡调整的频率或粒度。",
@@ -65,6 +66,7 @@ def init_moe_group_args(parser):
     moe_group.add_argument(
         "--eplb_test_mode",
         env_name="EPLB_TEST_MODE",
+        bind_to=(py_eplb_config, 'eplb_test_mode'),
         type=str2bool,
         default=False,
         help="设置为 `True` 时，为 ExpertBalancer 组件启用测试模式。用于调试或特定的测试场景。",
@@ -73,6 +75,7 @@ def init_moe_group_args(parser):
     moe_group.add_argument(
         "--eplb_balance_layer_per_step",
         env_name="EPLB_BALANCE_LAYER_PER_STEP",
+        bind_to=(py_eplb_config, 'eplb_balance_layer_per_step'),
         type=int,
         default=1,
         help="设置 eplb 每次更新的层数。",
@@ -99,13 +102,6 @@ def init_moe_group_args(parser):
         default=0,
         help="冗余专家个数",
     )
-    moe_group.add_argument(
-        "--hack_ep_single_entry",
-        env_name="HACK_EP_SINGLE_ENTRY",
-        type=int,
-        default=0,
-        help="HACK_EP_SINGLE_ENTRY",
-    )
     moe_group.add_argument(
         "--balance_method",
         env_name="BALANCE_METHOD",
@@ -134,3 +130,10 @@ def init_moe_group_args(parser):
         default=1024,
         help="moe normal使用masked的最大token数目",
     )
+    moe_group.add_argument(
+        "--use_all_gather",
+        env_name="USE_ALL_GATHER",
+        type=str2bool,
+        default=False,
+        help="是否使用 all_gather 进行通信。",
+    )
diff --git a/rtp_llm/server/server_args/parallel_group_args.py b/rtp_llm/server/server_args/parallel_group_args.py
index dc2a1b6b6..201666c51 100644
--- a/rtp_llm/server/server_args/parallel_group_args.py
+++ b/rtp_llm/server/server_args/parallel_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_parallel_group_args(parser):
+def init_parallel_group_args(parser, parallelism_config, ffn_disaggregate_config):
     ##############################################################################################################
     # Parallelism and Distributed Setup Configuration
     ##############################################################################################################
@@ -11,6 +11,7 @@ def init_parallel_group_args(parser):
     parallel_group.add_argument(
         "--tp_size",
         env_name="TP_SIZE",
+        bind_to=(parallelism_config, 'tp_size'),
         type=int,
         default=None,
         help="指定用于张量并行度。",
@@ -18,6 +19,7 @@ def init_parallel_group_args(parser):
     parallel_group.add_argument(
         "--ep_size",
         env_name="EP_SIZE",
+        bind_to=(parallelism_config, 'ep_size'),
         type=int,
         default=None,
         help="定义用于专家并行（Expert Parallelism）的模型（专家）实例数量。",
@@ -25,6 +27,7 @@ def init_parallel_group_args(parser):
     parallel_group.add_argument(
         "--dp_size",
         env_name="DP_SIZE",
+        bind_to=(parallelism_config, 'dp_size'),
         type=int,
         default=None,
         help="设置数据并行（Data Parallelism）的副本数量或组大小。",
@@ -32,6 +35,7 @@ def init_parallel_group_args(parser):
     parallel_group.add_argument(
         "--world_size",
         env_name="WORLD_SIZE",
+        bind_to=(parallelism_config, 'world_size'),
         type=int,
         default=None,
         help="分布式设置中使用的GPU总数。通常情况下，`WORLD_SIZE = TP_SIZE * DP_SIZE`",
@@ -39,6 +43,7 @@ def init_parallel_group_args(parser):
     parallel_group.add_argument(
         "--world_rank",
         env_name="WORLD_RANK",
+        bind_to=(parallelism_config, 'world_rank'),
         type=int,
         default=None,
         help="当前进程/GPU在分布式系统中的全局唯一编号（从0到 `WORLD_SIZE - 1`）。",
@@ -46,6 +51,7 @@ def init_parallel_group_args(parser):
     parallel_group.add_argument(
         "--local_world_size",
         env_name="LOCAL_WORLD_SIZE",
+        bind_to=(parallelism_config, 'local_world_size'),
         type=int,
         default=None,
         help="在多节点分布式设置中，当前节点（Node）上使用的GPU设备数量。",
@@ -53,6 +59,7 @@ def init_parallel_group_args(parser):
     parallel_group.add_argument(
         "--ffn_sp_size",
         env_name="FFN_SP_SIZE",
+        bind_to=(parallelism_config, 'ffn_sp_size'),
         type=int,
         default=1,
         help="FFN层序列并行大小。",
@@ -60,6 +67,7 @@ def init_parallel_group_args(parser):
     parallel_group.add_argument(
         "--enable_ffn_disaggregate",
         env_name="ENABLE_FFN_DISAGGREGATE",
+        bind_to=(ffn_disaggregate_config, 'enable_ffn_disaggregate'),
         type=str2bool,
         default=False,
         help="启用AF分离功能。",
diff --git a/rtp_llm/server/server_args/profile_debug_logging_group_args.py b/rtp_llm/server/server_args/profile_debug_logging_group_args.py
index fa6e2a79a..3c00364c7 100644
--- a/rtp_llm/server/server_args/profile_debug_logging_group_args.py
+++ b/rtp_llm/server/server_args/profile_debug_logging_group_args.py
@@ -64,18 +64,6 @@ def init_profile_debug_logging_group_args(parser):
         default="",
         help="指定开启Torch的Profile时对应的生成目录",
     )
-
-    profile_debug_logging_group.add_argument(
-        "--log_path", env_name="LOG_PATH", type=str, default="logs", help="日志路径"
-    )
-    profile_debug_logging_group.add_argument(
-        "--log_file_backup_count",
-        env_name="LOG_FILE_BACKUP_COUNT",
-        type=int,
-        default=16,
-        help="日志文件备份数量",
-    )
-
     profile_debug_logging_group.add_argument(
         "--nccl_debug_file",
         env_name="NCCL_DEBUG_FILE",
diff --git a/rtp_llm/server/server_args/quantization_group_args.py b/rtp_llm/server/server_args/quantization_group_args.py
index 50376a31a..015a53c0f 100644
--- a/rtp_llm/server/server_args/quantization_group_args.py
+++ b/rtp_llm/server/server_args/quantization_group_args.py
@@ -1,4 +1,4 @@
-def init_quantization_group_args(parser):
+def init_quantization_group_args(parser, quantization_config):
     ##############################################################################################################
     # Quantization Configuration
     ##############################################################################################################
@@ -6,10 +6,16 @@ def init_quantization_group_args(parser):
     quantization_group.add_argument(
         "--int8_mode",
         env_name="INT8_MODE",
+        bind_to=(quantization_config, 'int8_mode'),
         type=int,
         default=0,
         help="权重类型是否使用int8模式",
     )
     quantization_group.add_argument(
-        "--quantization", env_name="QUANTIZATION", type=str, default=None, help=""
+        "--quantization",
+        env_name="QUANTIZATION",
+        bind_to=(quantization_config, 'quantization'),
+        type=str,
+        default=None,
+        help=""
     )
diff --git a/rtp_llm/server/server_args/render_group_args.py b/rtp_llm/server/server_args/render_group_args.py
index 0bf1810a0..42bb5d094 100644
--- a/rtp_llm/server/server_args/render_group_args.py
+++ b/rtp_llm/server/server_args/render_group_args.py
@@ -1,4 +1,4 @@
-def init_render_group_args(parser):
+def init_render_group_args(parser, render_config):
     ##############################################################################################################
     # Render Configuration
     ##############################################################################################################
@@ -6,6 +6,7 @@ def init_render_group_args(parser):
     render_group.add_argument(
         "--model_template_type",
         env_name="MODEL_TEMPLATE_TYPE",
+        bind_to=(render_config, 'model_template_type'),
         type=str,
         default=None,
         help="模型的模版类型",
@@ -13,6 +14,7 @@ def init_render_group_args(parser):
     render_group.add_argument(
         "--default_chat_template_key",
         env_name="DEFAULT_CHAT_TEMPLATE_KEY",
+        bind_to=(render_config, 'default_chat_template_key'),
         type=str,
         default="default",
         help="OpenAI的chat模型键",
@@ -20,6 +22,7 @@ def init_render_group_args(parser):
     render_group.add_argument(
         "--default_tool_use_template_key",
         env_name="DEFAULT_TOOL_USE_TEMPLATE_KEY",
+        bind_to=(render_config, 'default_tool_use_template_key'),
         type=str,
         default="tool_use",
         help="默认工具使用的模板的key",
@@ -27,6 +30,7 @@ def init_render_group_args(parser):
     render_group.add_argument(
         "--llava_chat_template",
         env_name="LLAVA_CHAT_TEMPLATE",
+        bind_to=(render_config, 'llava_chat_template'),
         type=str,
         default="",
         help="LLava模型的会话模板",
diff --git a/rtp_llm/server/server_args/role_group_args.py b/rtp_llm/server/server_args/role_group_args.py
index d657813ee..4dddba4d6 100644
--- a/rtp_llm/server/server_args/role_group_args.py
+++ b/rtp_llm/server/server_args/role_group_args.py
@@ -1,4 +1,4 @@
-def init_role_group_args(parser):
+def init_role_group_args(parser, role_config):
     ##############################################################################################################
     #  Role配置
     ##############################################################################################################
@@ -6,6 +6,7 @@ def init_role_group_args(parser):
     role_group.add_argument(
         "--role_type",
         env_name="ROLE_TYPE",
+        bind_to=(role_config, 'role_type'),
         type=str,
         default="PDFUSION",
         help="role的类型: 包含PDFUSION / PREFILL / DECODE / VIT / FRONTEND 几种类型",
diff --git a/rtp_llm/server/server_args/scheduler_group_args.py b/rtp_llm/server/server_args/scheduler_group_args.py
index c8cdca3a0..5aec63b18 100644
--- a/rtp_llm/server/server_args/scheduler_group_args.py
+++ b/rtp_llm/server/server_args/scheduler_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_scheduler_group_args(parser):
+def init_scheduler_group_args(parser, runtime_config):
     ##############################################################################################################
     # 调度器配置
     ##############################################################################################################
@@ -9,7 +9,40 @@ def init_scheduler_group_args(parser):
     scheduler_group.add_argument(
         "--use_batch_decode_scheduler",
         env_name="USE_BATCH_DECODE_SCHEDULER",
+        bind_to=(runtime_config, 'use_batch_decode_scheduler'),
         type=str2bool,
         default=False,
         help="若为 True，则启用一个专门为decode阶段优化的特化调度器。此调度器在 decode 期间以固定大小的 batch 处理请求。若为 False，系统将使用一个 FIFO-based的默认调度器，默认调度器采用continuous batching。",
     )
+    scheduler_group.add_argument(
+        "--use_gather_batch_scheduler",
+        env_name="USE_GATHER_BATCH_SCHEDULER",
+        bind_to=(runtime_config, 'use_gather_batch_scheduler'),
+        type=str2bool,
+        default=False,
+        help="若为 True，则启用 gather batch scheduler。",
+    )
+    scheduler_group.add_argument(
+        "--enable_speculative_decoding",
+        env_name="ENABLE_SPECULATIVE_DECODING",
+        bind_to=(runtime_config, 'enable_speculative_decoding'),
+        type=str2bool,
+        default=False,
+        help="若为 True，则启用推测解码。",
+    )
+    scheduler_group.add_argument(
+        "--pre_allocate_op_mem",
+        env_name="PRE_ALLOCATE_OP_MEM",
+        bind_to=(runtime_config, 'pre_allocate_op_mem'),
+        type=str2bool,
+        default=True,
+        help="是否预分配操作内存。",
+    )
+    scheduler_group.add_argument(
+        "--max_block_size_per_item",
+        env_name="MAX_BLOCK_SIZE_PER_ITEM",
+        bind_to=(runtime_config, 'max_block_size_per_item'),
+        type=int,
+        default=16,
+        help="每个 item 的最大 block 大小。",
+    )
diff --git a/rtp_llm/server/server_args/server_args.py b/rtp_llm/server/server_args/server_args.py
index d68e17011..ee852cb61 100644
--- a/rtp_llm/server/server_args/server_args.py
+++ b/rtp_llm/server/server_args/server_args.py
@@ -2,9 +2,9 @@ import argparse
 import glob
 import logging
 import os
-from typing import Any, Dict, Optional, Sequence, TypeVar
+from typing import Any, Dict, List, Optional, Sequence, Tuple, TypeVar, Union
 
-from rtp_llm.config.py_config_modules import StaticConfig
+from rtp_llm.config.py_config_modules import PyEnvConfigs
 from rtp_llm.server.server_args.batch_decode_scheduler_group_args import (
     init_batch_decode_scheduler_group_args,
 )
@@ -51,25 +51,86 @@ from rtp_llm.server.server_args.rpc_discovery_group_args import (
 )
 from rtp_llm.server.server_args.scheduler_group_args import init_scheduler_group_args
 from rtp_llm.server.server_args.server_group_args import init_server_group_args
-from rtp_llm.server.server_args.sparse_group_args import init_sparse_group_args
 from rtp_llm.server.server_args.speculative_decoding_group_args import (
     init_speculative_decoding_group_args,
 )
 from rtp_llm.server.server_args.threefs_group_args import init_threefs_group_args
 from rtp_llm.server.server_args.vit_group_args import init_vit_group_args
-from rtp_llm.server.server_args.worker_group_args import init_worker_group_args
 
 _T = TypeVar("_T")
 
 
+class ConfigBinding:
+    """配置绑定描述符，用于将解析的参数值绑定到配置对象"""
+    
+    def __init__(self, action: argparse.Action, bind_to: Union[Tuple[Any, str], str, List[Union[Tuple[Any, str], str]]]):
+        """
+        Args:
+            action: argparse.Action 对象
+            bind_to: 绑定目标，可以是 (config_obj, 'attr_name') 元组、'path.to.attr' 字符串，或这些的列表
+        """
+        self.action = action
+        self.dest = action.dest
+        self.bind_to = bind_to
+        self._resolved_bind_to: Optional[List[Tuple[Any, str]]] = None
+    
+    def resolve_bind_to(self, root_config: Any) -> List[Tuple[Any, str]]:
+        """解析绑定目标，返回 (config_obj, attr_name) 元组列表"""
+        if self._resolved_bind_to is not None:
+            return self._resolved_bind_to
+        
+        resolved = []
+        
+        # Handle list of bindings
+        bind_to_list = self.bind_to if isinstance(self.bind_to, list) else [self.bind_to]
+        
+        for bind_target in bind_to_list:
+            if isinstance(bind_target, tuple) and len(bind_target) == 2:
+                # 直接是 (config_obj, 'attr_name') 形式
+                config_obj, attr_name = bind_target
+                resolved.append((config_obj, attr_name))
+            elif isinstance(bind_target, str):
+                # 字符串路径形式，如 'server_config.frontend_server_count'
+                parts = bind_target.split('.')
+                config_obj = root_config
+                for part in parts[:-1]:
+                    config_obj = getattr(config_obj, part)
+                attr_name = parts[-1]
+                resolved.append((config_obj, attr_name))
+            else:
+                raise ValueError(f"Invalid bind_to format: {bind_target}")
+        
+        self._resolved_bind_to = resolved
+        return resolved
+    
+    def apply(self, value: Any, root_config: Any) -> None:
+        """应用绑定：将值设置到配置对象"""
+        bindings = self.resolve_bind_to(root_config)
+        for config_obj, attr_name in bindings:
+            setattr(config_obj, attr_name, value)
+
+
 class EnvArgumentGroup:
     def __init__(self, group: argparse._ArgumentGroup, parser: "EnvArgumentParser"):
         self._group = group
         self._parser = parser
 
     def add_argument(
-        self, *args, env_name: Optional[str] = None, **kwargs
+        self, 
+        *args, 
+        env_name: Optional[str] = None,
+        bind_to: Optional[Union[Tuple[Any, str], str]] = None,
+        **kwargs
     ) -> argparse.Action:
+        """
+        Add an argument to the group.
+        
+        Args:
+            *args: 标准 argparse add_argument 参数
+            env_name: 环境变量名称（保留用于兼容，但不再自动更新到 os.environ）
+            bind_to: 配置绑定目标，可以是 (config_obj, 'attr_name') 或 'path.to.attr' 字符串
+            **kwargs: 其他 argparse add_argument 参数
+        """
         if "metavar" not in kwargs and "type" in kwargs:
             type_ = kwargs["type"]
             if isinstance(type_, type) and issubclass(type_, bool):
@@ -79,6 +140,12 @@ class EnvArgumentGroup:
             elif isinstance(type_, type) and issubclass(type_, str):
                 kwargs["metavar"] = "STR"
         action = self._group.add_argument(*args, **kwargs)
+        
+        # 注册配置绑定
+        if bind_to is not None:
+            self._parser._register_config_binding(action, bind_to)
+        
+        # 保留 env 映射（用于兼容和日志）
         self._parser._register_env_mapping(action, args, env_name)
         return action
 
@@ -91,11 +158,26 @@ class EnvArgumentParser(argparse.ArgumentParser):
         self.env_prefix = env_prefix.upper()
         self._env_mappings: Dict[str, str] = {}
         self._groups: Dict[str, EnvArgumentGroup] = {}
+        self._config_bindings: List[ConfigBinding] = []  # 配置绑定列表
+        self._root_config: Optional[Any] = None  # 根配置对象（PyEnvConfigs）
 
         super().__init__(*args, **kwargs)
 
         self._default_group = EnvArgumentGroup(self._positionals, self)
         self._optional_group = EnvArgumentGroup(self._optionals, self)
+    
+    def set_root_config(self, root_config: Any) -> None:
+        """设置根配置对象，用于解析字符串路径形式的 bind_to"""
+        self._root_config = root_config
+    
+    def _register_config_binding(
+        self, 
+        action: argparse.Action, 
+        bind_to: Union[Tuple[Any, str], str]
+    ) -> None:
+        """注册参数到配置对象的绑定关系"""
+        binding = ConfigBinding(action, bind_to)
+        self._config_bindings.append(binding)
 
     def add_argument_group(self, *args, **kwargs) -> EnvArgumentGroup:
         group = super().add_argument_group(*args, **kwargs)
@@ -152,29 +234,142 @@ class EnvArgumentParser(argparse.ArgumentParser):
         args: Optional[Sequence[str]] = None,
         namespace: Optional[argparse.Namespace] = None,
     ) -> argparse.Namespace:
-        logging.info("Parsing arguments and setting environment variables...")
+        logging.info("Parsing arguments and applying config bindings...")
+        
+        # If args is None, check if we should read from environment variables
+        # argparse will use sys.argv when args is None, so we need to check sys.argv first
+        import sys
+        has_cmd_args = args is not None or (len(sys.argv) > 1)
+        
+        if args is None:
+            # Check if there are command line arguments (more than just program name)
+            if not has_cmd_args:
+                # No command line arguments, read from environment variables and construct args list
+                args = []
+                # Read values from environment variables for all registered arguments
+                for dest, env_name in self._env_mappings.items():
+                    env_value = os.environ.get(env_name)
+                    if env_value is not None:
+                        # Find the action for this dest
+                        action = None
+                        for action_item in self._actions:
+                            if hasattr(action_item, 'dest') and action_item.dest == dest:
+                                action = action_item
+                                break
+                        
+                        if action is not None:
+                            # Get the option string (e.g., "--model_type")
+                            option_string = None
+                            for option in action.option_strings:
+                                if option.startswith("--"):
+                                    option_string = option
+                                    break
+                            
+                            if option_string:
+                                args.extend([option_string, env_value])
+            # If has_cmd_args is True, args remains None and argparse will use sys.argv
+        
         parsed_args = super().parse_args(args, namespace)
-
+        
+        # After parsing, if there were command line arguments, fill in missing values from environment variables
+        # This allows mixing command line arguments and environment variables
+        if has_cmd_args:
+            # Build a set of argument names that were provided via command line
+            provided_args = set()
+            if args is not None:
+                # If args was provided, check which arguments are in the args list
+                i = 0
+                while i < len(args):
+                    arg = args[i]
+                    if arg.startswith("--"):
+                        # Find the action for this option
+                        for action_item in self._actions:
+                            if arg in action_item.option_strings:
+                                provided_args.add(action_item.dest)
+                                # Check if this action requires a value
+                                if action_item.nargs in (None, '?', 1):
+                                    # Skip the value if present
+                                    if i + 1 < len(args) and not args[i + 1].startswith("-"):
+                                        i += 1
+                                break
+                    i += 1
+            else:
+                # If args is None, argparse used sys.argv, so check sys.argv
+                i = 1  # Skip program name
+                while i < len(sys.argv):
+                    arg = sys.argv[i]
+                    if arg.startswith("--"):
+                        # Find the action for this option
+                        for action_item in self._actions:
+                            if arg in action_item.option_strings:
+                                provided_args.add(action_item.dest)
+                                # Check if this action requires a value
+                                if action_item.nargs in (None, '?', 1):
+                                    # Skip the value if present
+                                    if i + 1 < len(sys.argv) and not sys.argv[i + 1].startswith("-"):
+                                        i += 1
+                                break
+                    i += 1
+            
+            # Now fill in missing values from environment variables
+            for dest, env_name in self._env_mappings.items():
+                # Only set from environment if the value wasn't provided via command line
+                if dest not in provided_args:
+                    env_value = os.environ.get(env_name)
+                    if env_value is not None:
+                        # Find the action to get the type converter
+                        action = None
+                        for action_item in self._actions:
+                            if hasattr(action_item, 'dest') and action_item.dest == dest:
+                                action = action_item
+                                break
+                        
+                        if action is not None:
+                            # Convert the value using the action's type
+                            if action.type is not None:
+                                try:
+                                    converted_value = action.type(env_value)
+                                    setattr(parsed_args, dest, converted_value)
+                                except (ValueError, TypeError):
+                                    # If conversion fails, skip this value
+                                    pass
+                            else:
+                                # No type converter, use as string
+                                setattr(parsed_args, dest, env_value)
+
+        # 应用所有配置绑定
+        if self._root_config is not None:
+            self._apply_config_bindings(parsed_args)
+        
+        # 不再自动更新 os.environ，但保留日志记录（用于调试）
         for dest, env_name in self._env_mappings.items():
             value = getattr(parsed_args, dest, None)
-
-            if value is None:
-                continue
-
-            if env_name in os.environ and value == self.get_default(dest):
-                continue
-
-            env_value: str
-            if isinstance(value, bool):
-                env_value = "1" if value else "0"
-            elif isinstance(value, list):
-                env_value = ",".join(map(str, value))
-            else:
-                env_value = str(value)
-            logging.info(f"{env_name} = {env_value}")
-            os.environ[env_name] = env_value
+            if value is not None:
+                env_value: str
+                if isinstance(value, bool):
+                    env_value = "1" if value else "0"
+                elif isinstance(value, list):
+                    env_value = ",".join(map(str, value))
+                else:
+                    env_value = str(value)
+                logging.debug(f"[EnvMapping] {env_name} = {env_value}")
 
         return parsed_args
+    
+    def _apply_config_bindings(self, parsed_args: argparse.Namespace) -> None:
+        """应用所有配置绑定，将解析的参数值设置到配置对象"""
+        for binding in self._config_bindings:
+            value = getattr(parsed_args, binding.dest, None)
+            if value is not None:
+                try:
+                    binding.apply(value, self._root_config)
+                    logging.debug(
+                        f"[ConfigBinding] {binding.dest} -> {binding.bind_to} = {value}"
+                    )
+                except Exception as e:
+                    logging.warning(
+                        f"Failed to apply config binding for {binding.dest}: {e}"
+                    )
 
     def print_env_mappings(self, group_name: Optional[str] = None) -> None:
         logging.info("Argument -> Environment Variable Mappings:")
@@ -208,57 +403,135 @@ class EnvArgumentParser(argparse.ArgumentParser):
             return self._env_mappings.copy()
 
 
-def init_all_group_args(parser: EnvArgumentParser) -> None:
+def init_all_group_args(parser: EnvArgumentParser, py_env_configs: PyEnvConfigs) -> None:
     """
-    初始化所有参数组到解析器中
+    初始化所有参数组到解析器中，并绑定到配置对象
 
     Args:
         parser: EnvArgumentParser实例
+        py_env_configs: PyEnvConfigs配置对象，用于绑定参数
     """
-    init_batch_decode_scheduler_group_args(parser)
-    init_cache_store_group_args(parser)
-    init_concurrent_group_args(parser)
-    init_device_resource_group_args(parser)
-    init_embedding_group_args(parser)
-    init_engine_group_args(parser)
-    init_fifo_scheduler_group_args(parser)
-    init_fmha_group_args(parser)
-    init_gang_group_args(parser)
-    init_generate_group_args(parser)
-    init_hw_kernel_group_args(parser)
-    init_kv_cache_group_args(parser)
-    init_threefs_group_args(parser)
-    init_load_group_args(parser)
-    init_lora_group_args(parser)
-    init_misc_group_args(parser)
-    init_model_group_args(parser)
-    init_model_specific_group_args(parser)
-    init_moe_group_args(parser)
-    init_parallel_group_args(parser)
+    init_batch_decode_scheduler_group_args(parser, py_env_configs.runtime_config.batch_decode_scheduler_config)
+    init_cache_store_group_args(parser, py_env_configs.cache_store_config)
+    init_concurrent_group_args(parser, py_env_configs.concurrency_config)
+    init_device_resource_group_args(parser, py_env_configs.device_resource_config, py_env_configs.runtime_config)
+    init_embedding_group_args(parser, py_env_configs.embedding_config)
+    init_engine_group_args(parser, py_env_configs.runtime_config)
+    init_fifo_scheduler_group_args(parser, py_env_configs.runtime_config.fifo_scheduler_config)
+    init_fmha_group_args(parser, py_env_configs.fmha_config)
+    init_gang_group_args(parser, py_env_configs.gang_config)
+    init_generate_group_args(parser, py_env_configs.generate_env_config)
+    init_hw_kernel_group_args(parser, py_env_configs.py_hw_kernel_config)
+    init_kv_cache_group_args(parser, py_env_configs.kv_cache_config)
+    init_threefs_group_args(parser, py_env_configs.kv_cache_config)
+    init_load_group_args(parser, py_env_configs.load_config)
+    init_lora_group_args(parser, py_env_configs.lora_config)
+    init_misc_group_args(parser, py_env_configs.misc_config)
+    init_model_group_args(parser, py_env_configs.model_args)
+    init_model_specific_group_args(parser, py_env_configs.model_specific_config)
+    init_moe_group_args(parser, py_env_configs.py_eplb_config)
+    init_parallel_group_args(parser, py_env_configs.parallelism_config, py_env_configs.ffn_disaggregate_config)
     init_profile_debug_logging_group_args(parser)
-    init_quantization_group_args(parser)
-    init_render_group_args(parser)
-    init_role_group_args(parser)
+    init_quantization_group_args(parser, py_env_configs.quantization_config)
+    init_render_group_args(parser, py_env_configs.render_config)
+    init_role_group_args(parser, py_env_configs.role_config)
     init_rpc_discovery_group_args(parser)
-    init_scheduler_group_args(parser)
-    init_server_group_args(parser)
-    init_sparse_group_args(parser)
+    init_scheduler_group_args(parser, py_env_configs.runtime_config)
+    init_server_group_args(parser, py_env_configs.server_config)
     init_speculative_decoding_group_args(parser)
-    init_vit_group_args(parser)
-    init_worker_group_args(parser)
-    init_jit_group_args(parser)
+    init_vit_group_args(parser, py_env_configs.vit_config)
+    init_jit_group_args(parser, py_env_configs.jit_config)
     init_pd_separation_group_args(parser)
 
 
 def setup_args():
     parser = EnvArgumentParser(description="RTP LLM")
 
-    # 使用统一的函数初始化所有参数组
-    init_all_group_args(parser)
-
-    parser.parse_args()
+    # 先创建配置对象
+    py_env_configs = PyEnvConfigs()
+    
+    # 设置根配置对象，用于解析字符串路径形式的 bind_to
+    parser.set_root_config(py_env_configs)
+
+    # 使用统一的函数初始化所有参数组，并绑定到配置对象
+    init_all_group_args(parser, py_env_configs)
+
+    # 解析参数（会自动应用所有配置绑定）
+    parsed_args = parser.parse_args()
+
+    # Set environment variables for C++ code to read
+    # This is needed because C++ code may read environment variables directly
+    # Track which arguments were explicitly provided (not just defaults)
+    import sys
+    import argparse
+    provided_dests = set()
+    if hasattr(parsed_args, '__dict__'):
+        # Check sys.argv to see which arguments were explicitly provided
+        i = 1  # Skip program name
+        while i < len(sys.argv):
+            arg = sys.argv[i]
+            if arg.startswith("--"):
+                # Find the action for this option
+                for action_item in parser._actions:
+                    if arg in action_item.option_strings:
+                        provided_dests.add(action_item.dest)
+                        # Check if this action requires a value
+                        if action_item.nargs in (None, '?', 1):
+                            # Skip the value if present
+                            if i + 1 < len(sys.argv) and not sys.argv[i + 1].startswith("-"):
+                                i += 1
+                        break
+            i += 1
+    
+    for dest, env_name in parser._env_mappings.items():
+        value = getattr(parsed_args, dest, None)
+        if value is not None:
+            env_value: str
+            if isinstance(value, bool):
+                env_value = "1" if value else "0"
+            elif isinstance(value, list):
+                env_value = ",".join(map(str, value))
+            else:
+                env_value = str(value)
+            # Find the action to check default value
+            action = None
+            for action_item in parser._actions:
+                if hasattr(action_item, 'dest') and action_item.dest == dest:
+                    action = action_item
+                    break
+            
+            # For empty strings, set if:
+            # 1. The argument was explicitly provided, OR
+            # 2. The parameter has no default value (default_value is None)
+            # For parameters with empty string defaults, don't set unless explicitly provided
+            should_set = True
+            if env_value == "":
+                # Check if this argument was explicitly provided
+                if dest not in provided_dests:
+                    # Not explicitly provided, check default value
+                    if action is not None:
+                        # Get default value, handling argparse.SUPPRESS and None
+                        default_value = getattr(action, 'default', None)
+                        # Check if default is argparse.SUPPRESS (means no default)
+                        if default_value == argparse.SUPPRESS:
+                            default_value = None
+                        
+                        # Check if default is empty string (using both == and isinstance for safety)
+                        if default_value == "" or (isinstance(default_value, str) and len(default_value) == 0):
+                            # Default is empty string, don't set unless explicitly provided
+                            should_set = False
+                        elif default_value is not None and default_value != "":
+                            # Non-empty default, don't set empty string
+                            should_set = False
+                        # If default_value is None, keep should_set = True (parameter has no default)
+                    else:
+                        # No action found, don't set empty string
+                        should_set = False
+            if should_set:
+                os.environ[env_name] = env_value
 
     # add rocm env config, if using default value, change it to optimize version
+    # 这些特殊处理仍然需要设置环境变量（因为可能被 C++ 代码读取）
     if os.path.exists("/dev/kfd") and os.getenv("FT_DISABLE_CUSTOM_AR") is None:
         os.environ["FT_DISABLE_CUSTOM_AR"] = "0"
         logging.info(
@@ -276,4 +549,5 @@ def setup_args():
         logging.info("[MI308X] disable ENABLE_COMM_OVERLAP by default.")
 
     parser.print_env_mappings()
-    StaticConfig.update_from_env()
+
+    return py_env_configs
diff --git a/rtp_llm/server/server_args/server_group_args.py b/rtp_llm/server/server_args/server_group_args.py
index 44b1bca19..a26965c6f 100644
--- a/rtp_llm/server/server_args/server_group_args.py
+++ b/rtp_llm/server/server_args/server_group_args.py
@@ -1,4 +1,4 @@
-def init_server_group_args(parser):
+def init_server_group_args(parser, server_config):
     ##############################################################################################################
     # Server Configuration
     ##############################################################################################################
@@ -6,6 +6,7 @@ def init_server_group_args(parser):
     server_group.add_argument(
         "--frontend_server_count",
         env_name="FRONTEND_SERVER_COUNT",
+        bind_to=(server_config, 'frontend_server_count'),
         type=int,
         default=4,
         help="前端服务器启动进程数量",
@@ -13,6 +14,7 @@ def init_server_group_args(parser):
     server_group.add_argument(
         "--start_port",
         env_name="START_PORT",
+        bind_to=(server_config, 'start_port'),
         type=int,
         default=8088,
         help="服务启动端口",
@@ -20,6 +22,7 @@ def init_server_group_args(parser):
     server_group.add_argument(
         "--timeout_keep_alive",
         env_name="TIMEOUT_KEEP_ALIVE",
+        bind_to=(server_config, 'timeout_keep_alive'),
         type=int,
         default=5,
         help="健康检查的超时时间",
@@ -27,7 +30,16 @@ def init_server_group_args(parser):
     server_group.add_argument(
         "--frontend_server_id",
         env_name="FRONTEND_SERVER_ID",
+        bind_to=(server_config, 'frontend_server_id'),
         type=int,
         default=0,
         help="前端服务器序号",
     )
+    server_group.add_argument(
+        "--worker_info_port_num",
+        env_name="WORKER_INFO_PORT_NUM",
+        bind_to=(server_config, 'worker_info_port_num'),
+        type=int,
+        default=7,
+        help="worker的总的端口的数量",
+    )
diff --git a/rtp_llm/server/server_args/sparse_group_args.py b/rtp_llm/server/server_args/sparse_group_args.py
deleted file mode 100644
index 597311ca0..000000000
--- a/rtp_llm/server/server_args/sparse_group_args.py
+++ /dev/null
@@ -1,12 +0,0 @@
-def init_sparse_group_args(parser):
-    ##############################################################################################################
-    # Sparse Configuration
-    ##############################################################################################################
-    sparse_group = parser.add_argument_group("Sparse Configuration")
-    sparse_group.add_argument(
-        "--sparse_config_file",
-        env_name="SPARSE_CONFIG_FILE",
-        type=str,
-        default=None,
-        help="稀疏模型的配置文件路径",
-    )
diff --git a/rtp_llm/server/server_args/test/server_args_test.py b/rtp_llm/server/server_args/test/server_args_test.py
index bb6282581..49292b3ac 100644
--- a/rtp_llm/server/server_args/test/server_args_test.py
+++ b/rtp_llm/server/server_args/test/server_args_test.py
@@ -71,9 +71,8 @@ class ServerArgsDefaultTest(TestCase):
         # self.assertIsNone(env.get("FT_ALOG_CONF_PATH"))
         self.assertEqual(env.get("LOG_LEVEL"), "INFO")
         self.assertEqual(env.get("GEN_TIMELINE_SYNC"), "0")
-        self.assertEqual(env.get("TORCH_CUDA_PROFILER_DIR"), "")
-        self.assertEqual(env.get("LOG_PATH"), "logs")
-        self.assertEqual(env.get("LOG_FILE_BACKUP_COUNT"), "16")
+        # TORCH_CUDA_PROFILER_DIR has default="", so it won't be set unless explicitly provided
+        self.assertIsNone(env.get("TORCH_CUDA_PROFILER_DIR"))
         # self.assertIsNone(env.get("NCCL_DEBUG_FILE"))
         self.assertEqual(env.get("DEBUG_LOAD_SERVER"), "0")
         self.assertEqual(env.get("HACK_LAYER_NUM"), "0")
@@ -107,7 +106,8 @@ class ServerArgsDefaultTest(TestCase):
         self.assertEqual(env.get("ENABLE_LAYER_MICRO_BATCH"), "0")
         self.assertEqual(env.get("NOT_USE_DEFAULT_STREAM"), "0")
         self.assertEqual(env.get("RESERVER_RUNTIME_MEM_MB"), "1024")
-        self.assertEqual(env.get("SPECIFY_GPU_ARCH"), "")
+        # SPECIFY_GPU_ARCH has default="", so it won't be set unless explicitly provided
+        self.assertIsNone(env.get("SPECIFY_GPU_ARCH"))
         self.assertIsNone(env.get("ACEXT_GEMM_CONFIG_DIR"))
         self.assertEqual(env.get("DEVICE_RESERVE_MEMORY_BYTES"), "0")
 
@@ -134,11 +134,13 @@ class ServerArgsDefaultTest(TestCase):
         self.assertEqual(env.get("MAX_LORA_MODEL_SIZE"), "-1")
 
         # 11. 投机采样配置
-        self.assertEqual(env.get("SP_MODEL_TYPE"), "")
-        self.assertEqual(env.get("SP_TYPE"), "")
+        # SP_MODEL_TYPE and SP_TYPE have default="", so they won't be set unless explicitly provided
+        self.assertIsNone(env.get("SP_MODEL_TYPE"))
+        self.assertIsNone(env.get("SP_TYPE"))
         self.assertEqual(env.get("SP_MIN_TOKEN_MATCH"), "2")
         self.assertEqual(env.get("SP_MAX_TOKEN_MATCH"), "2")
-        self.assertEqual(env.get("TREE_DECODE_CONFIG"), "")
+        # TREE_DECODE_CONFIG has default="", so it won't be set unless explicitly provided
+        self.assertIsNone(env.get("TREE_DECODE_CONFIG"))
         self.assertEqual(env.get("GEN_NUM_PER_CIRCLE"), "1")
         self.assertIsNone(env.get("SP_ACT_TYPE"))
         self.assertIsNone(env.get("SP_QUANTIZATION"))
@@ -175,7 +177,8 @@ class ServerArgsDefaultTest(TestCase):
         self.assertEqual(env.get("FAKE_GANG_ENV"), "0")
         self.assertEqual(env.get("GANG_ANNOCATION_PATH"), "/etc/podinfo/annotations")
         self.assertIsNone(env.get("GANG_CONFIG_STRING"))
-        self.assertEqual(env.get("ZONE_NAME"), "")
+        # ZONE_NAME has default="", so it won't be set unless explicitly provided
+        self.assertIsNone(env.get("ZONE_NAME"))
         self.assertIsNone(env.get("DISTRIBUTE_CONFIG_FILE"))
         self.assertEqual(env.get("DIST_BARRIER_TIMEOUT"), "45")
         self.assertEqual(env.get("GANG_SLEEP_TIME"), "10")
@@ -188,7 +191,8 @@ class ServerArgsDefaultTest(TestCase):
         self.assertEqual(
             env.get("TRT_CACHE_PATH"), os.path.join(os.getcwd(), "trt_cache")
         )
-        self.assertEqual(env.get("DOWNLOAD_HEADERS"), "")
+        # DOWNLOAD_HEADERS has default="", so it won't be set unless explicitly provided
+        self.assertIsNone(env.get("DOWNLOAD_HEADERS"))
         self.assertEqual(env.get("MM_CACHE_ITEM_NUM"), "10")
         self.assertEqual(env.get("URL_CACHE_ITEM_NUM"), "100")
 
@@ -212,13 +216,13 @@ class ServerArgsDefaultTest(TestCase):
         self.assertEqual(env.get("INT8_MODE"), "0")
         self.assertIsNone(env.get("QUANTIZATION"))
 
-        # 22. Sparse Configuration
-        self.assertIsNone(env.get("SPARSE_CONFIG_FILE"))
+        # 22. Sparse Configuration (deprecated)
 
         # 23. Engine Configuration
         self.assertEqual(env.get("WARM_UP"), "1")
         self.assertEqual(env.get("WARM_UP_WITH_LOSS"), "0")
-        self.assertEqual(env.get("MAX_SEQ_LEN"), "0")
+        # MAX_SEQ_LEN is in ModelConfig, not server_args, so it won't be set here
+        self.assertIsNone(env.get("MAX_SEQ_LEN"))
 
         # 24. Embedding Configuration
         self.assertEqual(env.get("EMBEDDING_MODEL"), "0")
@@ -235,7 +239,6 @@ class ServerArgsDefaultTest(TestCase):
         self.assertEqual(env.get("USE_FLOAT32"), "0")
         self.assertIsNone(env.get("ORIGINAL_CHECKPOINT_PATH"))
         self.assertEqual(env.get("MLA_OPS_TYPE"), "AUTO")
-        self.assertIsNone(env.get("FT_PLUGIN_PATH"))
         self.assertIsNone(env.get("WEIGHT_TYPE"))
         self.assertIsNone(env.get("TASK_TYPE"))
         self.assertIsNone(env.get("MODEL_TYPE"))
@@ -253,7 +256,8 @@ class ServerArgsDefaultTest(TestCase):
         self.assertEqual(env.get("MERGE_LORA"), "1")
 
         # 28. Load Configuration
-        self.assertEqual(env.get("PHY2LOG_PATH"), "")
+        # PHY2LOG_PATH has default="", so it won't be set unless explicitly provided
+        self.assertIsNone(env.get("PHY2LOG_PATH"))
         self.assertEqual(env.get("CONVERTER_NUM_PER_GPU"), "4")
         self.assertEqual(env.get("TOKENIZERS_PARALLELISM"), "0")
         self.assertEqual(env.get("LOAD_CKPT_NUM_PROCESS"), "0")
@@ -262,7 +266,8 @@ class ServerArgsDefaultTest(TestCase):
         self.assertIsNone(env.get("MODEL_TEMPLATE_TYPE"))
         self.assertEqual(env.get("DEFAULT_CHAT_TEMPLATE_KEY"), "default")
         self.assertEqual(env.get("DEFAULT_TOOL_USE_TEMPLATE_KEY"), "tool_use")
-        self.assertEqual(env.get("LLAVA_CHAT_TEMPLATE"), "")
+        # LLAVA_CHAT_TEMPLATE has default="", so it won't be set unless explicitly provided
+        self.assertIsNone(env.get("LLAVA_CHAT_TEMPLATE"))
 
         # 30. Miscellaneous Configuration
         self.assertEqual(env.get("DISABLE_PDL"), "1")
@@ -374,10 +379,8 @@ class ServerArgsSetTest(TestCase):
             "True",
             "--torch_cuda_profiler_dir",
             "/path/to/dir",
-            "--log_path",
-            "/tmp/logs",
-            "--log_file_backup_count",
-            "32",
+            # Note: log_path and log_file_backup_count are in ProfilingDebugLoggingConfig
+            # They are not command-line arguments, but are set via environment variables
             "--nccl_debug_file",
             "/tmp/nccl.log",
             "--debug_load_server",
@@ -467,8 +470,6 @@ class ServerArgsSetTest(TestCase):
             "9999",
             "--redundant_expert",
             "2",
-            "--hack_ep_single_entry",
-            "1",
             "--balance_method",
             "greedy",
             "--eplb_force_repack",
@@ -607,16 +608,14 @@ class ServerArgsSetTest(TestCase):
             "1",
             "--quantization",
             "w8a8",
-            # 22. Sparse Configuration
-            "--sparse_config_file",
-            "/path/to/sparse.conf",
+            # 22. Sparse Configuration (deprecated)
             # 23. Engine Configuration
             "--warm_up",
             "0",
             "--warm_up_with_loss",
             "1",
-            "--max_seq_len",
-            "8192",
+            # Note: max_seq_len is in ModelConfig, not ModelArgs
+            # It will be set when ModelConfig is created from model_args
             # 24. Embedding Configuration
             "--embedding_model",
             "1",
@@ -760,8 +759,6 @@ class ServerArgsSetTest(TestCase):
         self.assertEqual(env["LOG_LEVEL"], "ERROR")
         self.assertEqual(env["GEN_TIMELINE_SYNC"], "1")
         self.assertEqual(env["TORCH_CUDA_PROFILER_DIR"], "/path/to/dir")
-        self.assertEqual(env["LOG_PATH"], "/tmp/logs")
-        self.assertEqual(env["LOG_FILE_BACKUP_COUNT"], "32")
         self.assertEqual(env["NCCL_DEBUG_FILE"], "/tmp/nccl.log")
         self.assertEqual(env["DEBUG_LOAD_SERVER"], "1")
         self.assertEqual(env["HACK_LAYER_NUM"], "2")
@@ -899,13 +896,13 @@ class ServerArgsSetTest(TestCase):
         self.assertEqual(env["INT8_MODE"], "1")
         self.assertEqual(env["QUANTIZATION"], "w8a8")
 
-        # 22. Sparse Configuration
-        self.assertEqual(env["SPARSE_CONFIG_FILE"], "/path/to/sparse.conf")
+        # 22. Sparse Configuration (deprecated)
 
         # 23. Engine Configuration
         self.assertEqual(env["WARM_UP"], "0")
         self.assertEqual(env["WARM_UP_WITH_LOSS"], "1")
-        self.assertEqual(env["MAX_SEQ_LEN"], "8192")
+        # MAX_SEQ_LEN is in ModelConfig, not server_args, so it won't be set here
+        # Note: The test sets --max_seq_len 8192, but it's handled by ModelConfig, not server_args
 
         # 24. Embedding Configuration
         self.assertEqual(env["EMBEDDING_MODEL"], "1")
@@ -922,7 +919,6 @@ class ServerArgsSetTest(TestCase):
         self.assertEqual(env["USE_FLOAT32"], "1")
         self.assertEqual(env["ORIGINAL_CHECKPOINT_PATH"], "/path/to/original/ckpt")
         self.assertEqual(env["MLA_OPS_TYPE"], "CUSTOM")
-        self.assertEqual(env["FT_PLUGIN_PATH"], "/path/to/plugin")
         self.assertEqual(env["WEIGHT_TYPE"], "FP16")
         self.assertEqual(env["TASK_TYPE"], "generation")
         self.assertEqual(env["MODEL_TYPE"], "qwen")
@@ -954,9 +950,8 @@ class ServerArgsSetTest(TestCase):
 
         # 30. Miscellaneous Configuration
         self.assertEqual(env["DISABLE_PDL"], "1")
-        self.assertEqual(
-            env["AUX_STRING"], ""
-        )
+        # AUX_STRING has default="", so it won't be set unless explicitly provided
+        self.assertIsNone(env.get("AUX_STRING"))
 
         # 31. PD-Separation Configuration
         self.assertEqual(env["PREFILL_RETRY_TIMES"], "2")
@@ -966,5 +961,194 @@ class ServerArgsSetTest(TestCase):
         self.assertEqual(env["REMOTE_JIT_DIR"], "/home/admin/jit_dir")
 
 
+class ServerArgsPyEnvConfigsTest(TestCase):
+    """Test that environment variables and command line arguments are correctly set to py_env_configs structure."""
+    
+    def setUp(self):
+        self._environ_backup = os.environ.copy()
+        self._argv_backup = sys.argv.copy()
+        os.environ.clear()
+    
+    def tearDown(self):
+        os.environ.clear()
+        os.environ.update(self._environ_backup)
+        sys.argv = self._argv_backup
+    
+    def test_env_vars_set_to_py_env_configs(self):
+        """Test that environment variables are correctly set to py_env_configs."""
+        # Set environment variables
+        os.environ["MODEL_TYPE"] = "qwen"
+        os.environ["CHECKPOINT_PATH"] = "/path/to/checkpoint"
+        os.environ["ACT_TYPE"] = "BF16"
+        os.environ["TP_SIZE"] = "4"
+        os.environ["DP_SIZE"] = "2"
+        os.environ["WORLD_SIZE"] = "8"
+        os.environ["CONCURRENCY_LIMIT"] = "64"
+        os.environ["MAX_CONTEXT_BATCH_SIZE"] = "32"
+        os.environ["WARM_UP"] = "1"
+        os.environ["MAX_SEQ_LEN"] = "4096"
+        
+        sys.argv = ["prog"]
+        
+        # Import and setup args
+        import rtp_llm.server.server_args.server_args
+        importlib.reload(rtp_llm.server.server_args.server_args)
+        py_env_configs = rtp_llm.server.server_args.server_args.setup_args()
+        
+        # Verify model_args
+        self.assertEqual(py_env_configs.model_args.model_type, "qwen")
+        self.assertEqual(py_env_configs.model_args.ckpt_path, "/path/to/checkpoint")
+        self.assertEqual(py_env_configs.model_args.act_type, "BF16")
+        
+        # Verify parallelism_config
+        self.assertEqual(py_env_configs.parallelism_config.tp_size, 4)
+        self.assertEqual(py_env_configs.parallelism_config.dp_size, 2)
+        self.assertEqual(py_env_configs.parallelism_config.world_size, 8)
+        
+        # Verify concurrency_config
+        self.assertEqual(py_env_configs.concurrency_config.concurrency_limit, 64)
+        
+        # Verify fifo_scheduler_config
+        self.assertEqual(py_env_configs.runtime_config.fifo_scheduler_config.max_context_batch_size, 32)
+        
+        # Verify runtime_config (warm_up is now in RuntimeConfig)
+        self.assertEqual(py_env_configs.runtime_config.warm_up, True)  # bool in C++
+        # Note: max_seq_len is in ModelConfig, not RuntimeConfig or EngineConfig
+        # It will be set when ModelConfig is created from model_args
+    
+    def test_cmd_args_set_to_py_env_configs(self):
+        """Test that command line arguments are correctly set to py_env_configs."""
+        sys.argv = [
+            "prog",
+            "--model_type", "llama",
+            "--checkpoint_path", "/path/to/llama/checkpoint",
+            "--act_type", "FP16",
+            "--tp_size", "8",
+            "--dp_size", "4",
+            "--world_size", "32",
+            "--concurrency_limit", "128",
+            "--max_context_batch_size", "64",
+            "--warm_up", "0",
+            # Note: max_seq_len is in ModelConfig, not ModelArgs
+            # It will be set when ModelConfig is created from model_args
+        ]
+        
+        # Import and setup args
+        import rtp_llm.server.server_args.server_args
+        importlib.reload(rtp_llm.server.server_args.server_args)
+        py_env_configs = rtp_llm.server.server_args.server_args.setup_args()
+        
+        # Verify model_args
+        self.assertEqual(py_env_configs.model_args.model_type, "llama")
+        self.assertEqual(py_env_configs.model_args.ckpt_path, "/path/to/llama/checkpoint")
+        self.assertEqual(py_env_configs.model_args.act_type, "FP16")
+        
+        # Verify parallelism_config
+        self.assertEqual(py_env_configs.parallelism_config.tp_size, 8)
+        self.assertEqual(py_env_configs.parallelism_config.dp_size, 4)
+        self.assertEqual(py_env_configs.parallelism_config.world_size, 32)
+        
+        # Verify concurrency_config
+        self.assertEqual(py_env_configs.concurrency_config.concurrency_limit, 128)
+        
+        # Verify fifo_scheduler_config
+        self.assertEqual(py_env_configs.runtime_config.fifo_scheduler_config.max_context_batch_size, 64)
+        
+        # Verify runtime_config (warm_up is now in RuntimeConfig)
+        self.assertEqual(py_env_configs.runtime_config.warm_up, False)  # bool in C++
+        # Note: max_seq_len is in ModelConfig, not RuntimeConfig or EngineConfig
+        # It will be set when ModelConfig is created from model_args
+    
+    def test_cmd_args_override_env_vars(self):
+        """Test that command line arguments override environment variables."""
+        # Set environment variables
+        os.environ["MODEL_TYPE"] = "qwen"
+        os.environ["CHECKPOINT_PATH"] = "/path/to/qwen/checkpoint"
+        os.environ["ACT_TYPE"] = "BF16"
+        os.environ["TP_SIZE"] = "4"
+        os.environ["CONCURRENCY_LIMIT"] = "32"
+        
+        # Set command line arguments (should override env vars)
+        sys.argv = [
+            "prog",
+            "--model_type", "llama",
+            "--checkpoint_path", "/path/to/llama/checkpoint",
+            "--act_type", "FP16",
+            "--tp_size", "8",
+            "--concurrency_limit", "64",
+        ]
+        
+        # Import and setup args
+        import rtp_llm.server.server_args.server_args
+        importlib.reload(rtp_llm.server.server_args.server_args)
+        py_env_configs = rtp_llm.server.server_args.server_args.setup_args()
+        
+        # Verify that command line arguments override environment variables
+        self.assertEqual(py_env_configs.model_args.model_type, "llama")  # Overridden
+        self.assertEqual(py_env_configs.model_args.ckpt_path, "/path/to/llama/checkpoint")  # Overridden
+        self.assertEqual(py_env_configs.model_args.act_type, "FP16")  # Overridden
+        self.assertEqual(py_env_configs.parallelism_config.tp_size, 8)  # Overridden
+        self.assertEqual(py_env_configs.concurrency_config.concurrency_limit, 64)  # Overridden
+    
+    def test_mixed_env_and_cmd_args(self):
+        """Test mixed environment variables and command line arguments."""
+        # Set some environment variables
+        os.environ["MODEL_TYPE"] = "qwen"
+        os.environ["CHECKPOINT_PATH"] = "/path/to/qwen/checkpoint"
+        os.environ["ACT_TYPE"] = "BF16"
+        os.environ["DP_SIZE"] = "2"
+        os.environ["WORLD_SIZE"] = "8"
+        
+        # Set some command line arguments
+        sys.argv = [
+            "prog",
+            "--tp_size", "4",
+            "--concurrency_limit", "64",
+            "--max_context_batch_size", "32",
+        ]
+        
+        # Import and setup args
+        import rtp_llm.server.server_args.server_args
+        importlib.reload(rtp_llm.server.server_args.server_args)
+        py_env_configs = rtp_llm.server.server_args.server_args.setup_args()
+        
+        # Verify values from environment variables
+        self.assertEqual(py_env_configs.model_args.model_type, "qwen")
+        self.assertEqual(py_env_configs.model_args.ckpt_path, "/path/to/qwen/checkpoint")
+        self.assertEqual(py_env_configs.model_args.act_type, "BF16")
+        self.assertEqual(py_env_configs.parallelism_config.dp_size, 2)
+        self.assertEqual(py_env_configs.parallelism_config.world_size, 8)
+        
+        # Verify values from command line arguments
+        self.assertEqual(py_env_configs.parallelism_config.tp_size, 4)
+        self.assertEqual(py_env_configs.concurrency_config.concurrency_limit, 64)
+        self.assertEqual(py_env_configs.runtime_config.fifo_scheduler_config.max_context_batch_size, 32)
+    
+    def test_batch_decode_scheduler_config(self):
+        """Test that batch_decode_scheduler_config is correctly set."""
+        sys.argv = [
+            "prog",
+            "--use_batch_decode_scheduler", "1",
+            "--batch_decode_scheduler_batch_size", "16",
+            "--batch_decode_scheduler_warmup_type", "1",
+        ]
+        
+        # Import and setup args
+        import rtp_llm.server.server_args.server_args
+        importlib.reload(rtp_llm.server.server_args.server_args)
+        py_env_configs = rtp_llm.server.server_args.server_args.setup_args()
+        
+        # Verify batch_decode_scheduler_config
+        self.assertEqual(py_env_configs.runtime_config.use_batch_decode_scheduler, True)
+        self.assertEqual(py_env_configs.runtime_config.batch_decode_scheduler_config.batch_decode_scheduler_batch_size, 16)
+        self.assertEqual(py_env_configs.runtime_config.batch_decode_scheduler_config.batch_decode_scheduler_warmup_type, 1)
+        
+        # Verify it's also set in the C++ binding object
+        runtime_config = py_env_configs.runtime_config
+        self.assertEqual(runtime_config.use_batch_decode_scheduler, True)
+        self.assertEqual(runtime_config.batch_decode_scheduler_config.batch_decode_scheduler_batch_size, 16)
+        self.assertEqual(runtime_config.batch_decode_scheduler_config.batch_decode_scheduler_warmup_type, 1)
+
+
 if __name__ == "__main__":
     main()
diff --git a/rtp_llm/server/server_args/threefs_group_args.py b/rtp_llm/server/server_args/threefs_group_args.py
index 8a724b0a2..776f515e9 100644
--- a/rtp_llm/server/server_args/threefs_group_args.py
+++ b/rtp_llm/server/server_args/threefs_group_args.py
@@ -1,7 +1,7 @@
 from rtp_llm.server.server_args.util import str2bool
 
 
-def init_threefs_group_args(parser):
+def init_threefs_group_args(parser, kv_cache_config):
     ##############################################################################################################
     # 3FS 配置
     ##############################################################################################################
@@ -9,6 +9,7 @@ def init_threefs_group_args(parser):
     threefs_group.add_argument(
         "--enable_3fs",
         env_name="ENABLE_3FS",
+        bind_to=(kv_cache_config, 'enable_3fs'),
         type=str2bool,
         default=False,
         help="是否启用 3FS 存储 KVCache. 打开此开关需要先打开 REUSE_CACHE",
@@ -16,6 +17,7 @@ def init_threefs_group_args(parser):
     threefs_group.add_argument(
         "--threefs_match_timeout_ms",
         env_name="THREEFS_MATCH_TIMEOUT_MS",
+        bind_to=(kv_cache_config, 'match_timeout_ms'),
         type=int,
         default=1000,
         help="所有 RANK 从远端匹配 KVCache 的超时时间, 单位为毫秒",
@@ -23,6 +25,7 @@ def init_threefs_group_args(parser):
     threefs_group.add_argument(
         "--threefs_rpc_get_cache_timeout_ms",
         env_name="THREEFS_RPC_GET_CACHE_TIMEOUT_MS",
+        bind_to=(kv_cache_config, 'rpc_get_cache_timeout_ms'),
         type=int,
         default=3000,
         help="所有 RANK 从远端拉取 KVCache 的超时时间, 单位为毫秒",
@@ -30,6 +33,7 @@ def init_threefs_group_args(parser):
     threefs_group.add_argument(
         "--threefs_rpc_put_cache_timeout_ms",
         env_name="THREEFS_RPC_PUT_CACHE_TIMEOUT_MS",
+        bind_to=(kv_cache_config, 'rpc_put_cache_timeout_ms'),
         type=int,
         default=3000,
         help="所有 RANK 向远端存储 KVCache 的超时时间, 单位为毫秒",
@@ -37,6 +41,7 @@ def init_threefs_group_args(parser):
     threefs_group.add_argument(
         "--threefs_read_timeout_ms",
         env_name="THREEFS_READ_TIMEOUT_MS",
+        bind_to=(kv_cache_config, 'threefs_read_timeout_ms'),
         type=int,
         default=1000,
         help="3FS 读 KVCache 的超时时间, 单位为毫秒",
@@ -44,6 +49,7 @@ def init_threefs_group_args(parser):
     threefs_group.add_argument(
         "--threefs_write_timeout_ms",
         env_name="THREEFS_WRITE_TIMEOUT_MS",
+        bind_to=(kv_cache_config, 'threefs_write_timeout_ms'),
         type=int,
         default=2000,
         help="3FS 写 KVCache 的超时时间, 单位为毫秒",
@@ -51,6 +57,7 @@ def init_threefs_group_args(parser):
     threefs_group.add_argument(
         "--threefs_read_iov_size",
         env_name="THREEFS_READ_IOV_SIZE",
+        bind_to=(kv_cache_config, 'threefs_read_iov_size'),
         type=int,
         default=1 << 32,
         help="3FS 读 IOV 大小, 单位为字节",
@@ -58,14 +65,8 @@ def init_threefs_group_args(parser):
     threefs_group.add_argument(
         "--threefs_write_iov_size",
         env_name="THREEFS_WRITE_IOV_SIZE",
+        bind_to=(kv_cache_config, 'threefs_write_iov_size'),
         type=int,
         default=1 << 32,
         help="3FS 写 IOV 大小, 单位为字节",
     )
-    threefs_group.add_argument(
-        "--max_block_size_per_item",
-        env_name="MAX_BLOCK_SIZE_PER_ITEM",
-        type=int,
-        default=16,
-        help="KVCache 分块存储每个 item 最大容纳 block 的数量",
-    )
diff --git a/rtp_llm/server/server_args/vit_group_args.py b/rtp_llm/server/server_args/vit_group_args.py
index d305dd843..0580e3776 100644
--- a/rtp_llm/server/server_args/vit_group_args.py
+++ b/rtp_llm/server/server_args/vit_group_args.py
@@ -1,7 +1,22 @@
 import os
+from rtp_llm.ops import VitSeparation
 
+def _convert_vit_separation(value):
+    """Convert int to VitSeparation enum."""
+    
+    if isinstance(value, int):
+        if value == 0:
+            return VitSeparation.VIT_SEPARATION_LOCAL
+        elif value == 1:
+            return VitSeparation.VIT_SEPARATION_ROLE
+        elif value == 2:
+            return VitSeparation.VIT_SEPARATION_REMOTE
+        else:
+            raise ValueError(f"Invalid vit_separation value: {value}")
+    return value
 
-def init_vit_group_args(parser):
+
+def init_vit_group_args(parser, vit_config):
     ##############################################################################################################
     # Vit Configuration
     ##############################################################################################################
@@ -9,16 +24,23 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--vit_separation",
         env_name="VIT_SEPARATION",
-        type=int,
-        default=0,
+        bind_to=(vit_config, 'vit_separation'),
+        type=_convert_vit_separation,
+        default=VitSeparation.VIT_SEPARATION_LOCAL,  # Convert default to enum
         help="VIT是否和主进程进行分离",
     )
     vit_group.add_argument(
-        "--vit_trt", env_name="VIT_TRT", type=int, default=0, help="VIT是否使用TRT库"
+        "--vit_trt",
+        env_name="VIT_TRT",
+        bind_to=(vit_config, 'vit_trt'),
+        type=int,
+        default=0,
+        help="VIT是否使用TRT库"
     )
     vit_group.add_argument(
         "--trt_cache_enabled",
         env_name="TRT_CACHE_ENABLED",
+        bind_to=(vit_config, 'trt_cache_enabled'),
         type=int,
         default=0,
         help="是否使用TRT_CACHE",
@@ -26,6 +48,7 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--trt_cache_path",
         env_name="TRT_CACHE_PATH",
+        bind_to=(vit_config, 'trt_cache_path'),
         type=str,
         default=os.path.join(os.getcwd(), "trt_cache"),
         help="TRT_CACHE路径",
@@ -33,6 +56,7 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--download_headers",
         env_name="DOWNLOAD_HEADERS",
+        bind_to=(vit_config, 'download_headers'),
         type=str,
         default="",
         help="是否需要下载headers",
@@ -40,6 +64,7 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--mm_cache_item_num",
         env_name="MM_CACHE_ITEM_NUM",
+        bind_to=(vit_config, 'mm_cache_item_num'),
         type=int,
         default=10,
         help="多模态开启的Cache的大小",
@@ -47,6 +72,7 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--url_cache_item_num",
         env_name="URL_CACHE_ITEM_NUM",
+        bind_to=(vit_config, 'url_cache_item_num'),
         type=int,
         default=100,
         help="多模态开启的用于URL的Cache的大小",
@@ -54,6 +80,7 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--use_igraph_cache",
         env_name="USE_IGRAPH_CACHE",
+        bind_to=(vit_config, 'use_igraph_cache'),
         type=bool,
         default=True,
         help="访问igraph是否开启cache",
@@ -61,6 +88,7 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--igraph_search_dom",
         env_name="IGRAPH_SEARCH_DOM",
+        bind_to=(vit_config, 'igraph_search_dom'),
         type=str,
         default="com.taobao.search.igraph.common",
         help="访问igraph使用的vipserver地址",
@@ -68,6 +96,7 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--igraph_vipserver",
         env_name="IGRAPH_VIPSERVER",
+        bind_to=(vit_config, 'igraph_vipserver'),
         type=int,
         default=0,
         help="是否使用vipserver访问igraph",
@@ -75,6 +104,7 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--igraph_table_name",
         env_name="IGRAPH_TABLE_NAME",
+        bind_to=(vit_config, 'igraph_table_name'),
         type=str,
         default="",
         help="igraph的表名",
@@ -82,6 +112,7 @@ def init_vit_group_args(parser):
     vit_group.add_argument(
         "--igraph_default_key",
         env_name="IGRAPH_DEFAULT_KEY",
+        bind_to=(vit_config, 'default_key'),
         type=str,
         default=None,
         help="访问igraph失败时默认使用的key",
diff --git a/rtp_llm/server/server_args/worker_group_args.py b/rtp_llm/server/server_args/worker_group_args.py
deleted file mode 100644
index bdf6954df..000000000
--- a/rtp_llm/server/server_args/worker_group_args.py
+++ /dev/null
@@ -1,12 +0,0 @@
-def init_worker_group_args(parser):
-    ##############################################################################################################
-    # Worker Configuration
-    ##############################################################################################################
-    worker_group = parser.add_argument_group("Worker Configuration")
-    worker_group.add_argument(
-        "--worker_info_port_num",
-        env_name="WORKER_INFO_PORT_NUM",
-        type=int,
-        default=7,
-        help="worker的总的端口的数量",
-    )
diff --git a/rtp_llm/server/vit_rpc_server.py b/rtp_llm/server/vit_rpc_server.py
index 052783ca5..2f81030df 100644
--- a/rtp_llm/server/vit_rpc_server.py
+++ b/rtp_llm/server/vit_rpc_server.py
@@ -12,8 +12,13 @@ from rtp_llm.cpp.model_rpc.proto.model_rpc_service_pb2_grpc import (
     MultimodalRpcServiceServicer,
     add_MultimodalRpcServiceServicer_to_server,
 )
+from rtp_llm.config.engine_config import EngineConfig
+from rtp_llm.config.py_config_modules import PyEnvConfigs
+from rtp_llm.distribute.gang_server import GangServer
 from rtp_llm.distribute.worker_info import g_worker_info
 from rtp_llm.model_factory import ModelFactory
+from rtp_llm.ops import MMModelConfig
+from rtp_llm.server.server_args.server_args import setup_args
 from rtp_llm.utils.grpc_util import trans_from_tensor, trans_tensor
 from rtp_llm.utils.mm_process_engine import MMEmbeddingRes, MMProcessEngine
 from rtp_llm.utils.multimodal_util import MMUrlType
@@ -69,7 +74,45 @@ class MultimodalRpcServer(MultimodalRpcServiceServicer):
 
 
 def vit_start_server():
-    model = ModelFactory.create_from_env()
+    py_env_configs = setup_args()
+    
+    # Create GangServer and get gang_info
+    gang_server = GangServer(py_env_configs.gang_config, py_env_configs.server_config)
+    gang_server.start()
+    gang_info = gang_server._gang_info
+    
+    # Create and fully initialize engine config (global singleton)
+    engine_config = EngineConfig.create(py_env_configs, gang_info=gang_info)
+    
+    # Create model configs (ModelConfig construction is handled in ModelFactory)
+    # All model metadata (lora_infos, multi_task_prompt, model_name, template_type)
+    # is set in py_model_config by create_model_configs()
+    py_model_config, propose_py_model_config = ModelFactory.create_model_configs(
+        engine_config=engine_config,
+        model_args=py_env_configs.model_args,
+        lora_config=py_env_configs.lora_config,
+        generate_env_config=py_env_configs.generate_env_config,
+        embedding_config=py_env_configs.embedding_config,
+    )
+    
+    mm_model_config = MMModelConfig()
+    
+    # Create model using new API
+    # All metadata is already in py_model_config
+    # vit_config is needed for multimodal models
+    model = ModelFactory.from_model_configs(
+        model_config=py_model_config,
+        mm_model_config=mm_model_config,
+        engine_config=engine_config,
+        gang_info=gang_info,
+        vit_config=py_env_configs.vit_config,
+        propose_model_config=propose_py_model_config,
+    )
+    
+    # Load default generate config if needed
+    if py_env_configs.generate_env_config:
+        ModelFactory.load_default_generate_config(model, py_env_configs.generate_env_config)
+    
     server = grpc.server(
         futures.ThreadPoolExecutor(max_workers=200),
         options=[
@@ -78,7 +121,7 @@ def vit_start_server():
         ],
     )
     add_MultimodalRpcServiceServicer_to_server(
-        MultimodalRpcServer(MMProcessEngine(model)), server
+        MultimodalRpcServer(MMProcessEngine(model, model.vit_config)), server
     )
     server.add_insecure_port(f"0.0.0.0:{g_worker_info.rpc_server_port}")
     server.start()
diff --git a/rtp_llm/start_backend_server.py b/rtp_llm/start_backend_server.py
index 1e50cf700..2619f9dbb 100644
--- a/rtp_llm/start_backend_server.py
+++ b/rtp_llm/start_backend_server.py
@@ -14,12 +14,11 @@ from typing import List
 import torch
 from setproctitle import setproctitle
 
-from rtp_llm.config.py_config_modules import GangConfig, PyEnvConfigs, VitConfig
-
 CUR_PATH = os.path.dirname(os.path.abspath(__file__))
 sys.path.append(os.path.join(str(CUR_PATH), ".."))
 
-from rtp_llm.distribute.worker_info import g_parallel_info, g_worker_info
+from rtp_llm.config.py_config_modules import PyEnvConfigs
+from rtp_llm.distribute.worker_info import g_parallel_info, g_worker_info, update_worker_info
 from rtp_llm.server.backend_app import BackendApp
 from rtp_llm.server.vit_rpc_server import vit_start_server
 from rtp_llm.utils.concurrency_controller import (
@@ -29,17 +28,12 @@ from rtp_llm.utils.concurrency_controller import (
 from rtp_llm.utils.util import copy_gemm_config
 
 
-def local_rank_start(global_controller: ConcurrencyController):
+def local_rank_start(global_controller: ConcurrencyController, py_env_configs: PyEnvConfigs):
     copy_gemm_config()
     app = None
-    ## collect all args and envs.
-    py_env_configs = PyEnvConfigs()
-    py_env_configs.update_from_env()
     try:
         # avoid multiprocessing load failed
-        # reload for multiprocessing.start_method == fork
-        g_parallel_info.reload()
-        g_worker_info.reload()
+        update_worker_info(py_env_configs.server_config.start_port, py_env_configs.server_config.worker_info_port_num)
         if g_parallel_info.world_size > 1:
             setproctitle(f"rtp_llm_rank-{g_parallel_info.local_rank}")
         logging.info(f"start local {g_worker_info}, {g_parallel_info}")
@@ -54,11 +48,11 @@ def local_rank_start(global_controller: ConcurrencyController):
         logging.info("GPU not found: using CPU")
 
 
-def multi_rank_start(global_controller: ConcurrencyController):
+def multi_rank_start(global_controller: ConcurrencyController, py_env_configs: PyEnvConfigs):
     try:
         multiprocessing.set_start_method("spawn")
     except RuntimeError as e:
-        logging.warn(str(e))
+        logging.warning(str(e))
 
     local_world_size = min(torch.cuda.device_count(), g_parallel_info.world_size)
     if "LOCAL_WORLD_SIZE" in os.environ:
@@ -88,15 +82,13 @@ def multi_rank_start(global_controller: ConcurrencyController):
         os.environ["WORLD_RANK"] = str(world_rank)
         proc = Process(
             target=local_rank_start,
-            args=(global_controller,),
+            args=(global_controller, py_env_configs),
             name=f"rank-{world_rank}",
         )
         proc.start()
         procs.append(proc)
 
-    gang_config = GangConfig()
-    gang_config.update_from_env()
-    if gang_config.fake_gang_env:
+    if py_env_configs.gang_config.fake_gang_env:
         return procs
 
     first_dead_time = 0
@@ -178,22 +170,22 @@ def clear_jit_filelock():
             os.remove(file)
 
 
-def start_backend_server(global_controller: ConcurrencyController):
+def start_backend_server(global_controller: ConcurrencyController, py_env_configs: PyEnvConfigs):
     setproctitle("rtp_llm_backend_server")
     os.makedirs("logs", exist_ok=True)
     load_gpu_nic_affinity()
 
     clear_jit_filelock()
 
-    ## collect all args and envs.
-    vit_config = VitConfig()
-    vit_config.update_from_env()
+    update_worker_info(py_env_configs.server_config.start_port, py_env_configs.server_config.worker_info_port_num)
+
     # TODO(xinfei.sxf) fix this
-    if vit_config.vit_separation == 1:
+    from rtp_llm.ops import VitSeparation
+    if py_env_configs.vit_config.vit_separation == VitSeparation.VIT_SEPARATION_ROLE:
         return vit_start_server()
 
     if not torch.cuda.is_available():
-        return local_rank_start(global_controller)
+        return local_rank_start(global_controller, py_env_configs)
 
     if (
         g_parallel_info.world_size % torch.cuda.device_count() != 0
@@ -205,9 +197,9 @@ def start_backend_server(global_controller: ConcurrencyController):
         )
 
     if torch.cuda.device_count() > 1 and g_parallel_info.world_size > 1:
-        return multi_rank_start(global_controller)
+        return multi_rank_start(global_controller, py_env_configs)
     else:
-        return local_rank_start(global_controller)
+        return local_rank_start(global_controller, py_env_configs)
 
 
 def main():
diff --git a/rtp_llm/start_frontend_server.py b/rtp_llm/start_frontend_server.py
index b6ee8d866..5f6c7da6a 100644
--- a/rtp_llm/start_frontend_server.py
+++ b/rtp_llm/start_frontend_server.py
@@ -1,5 +1,4 @@
 import logging
-import logging.config
 import os
 import sys
 import traceback
@@ -11,18 +10,15 @@ from rtp_llm.config.py_config_modules import PyEnvConfigs
 CUR_PATH = os.path.dirname(os.path.abspath(__file__))
 sys.path.append(os.path.join(str(CUR_PATH), ".."))
 
-from rtp_llm.distribute.worker_info import FrontendServerInfo
+from rtp_llm.distribute.worker_info import FrontendServerInfo, update_worker_info
 from rtp_llm.frontend.frontend_app import FrontendApp
 from rtp_llm.utils.concurrency_controller import (
     ConcurrencyController,
     set_global_controller,
 )
 
-
-def start_frontend_server(rank_id: int, server_id: int, global_controller: ConcurrencyController):
-    ## collect all args and envs.
-    py_env_configs = PyEnvConfigs()
-    py_env_configs.update_from_env()
+def start_frontend_server(rank_id: int, server_id: int, global_controller: ConcurrencyController, py_env_configs: PyEnvConfigs):
+    # Set rank_id and server_id on the passed config
     py_env_configs.server_config.frontend_server_id = server_id
     py_env_configs.server_config.rank_id = rank_id
     setproctitle(f"rtp_llm_frontend_server_rank_{rank_id}_server_{server_id}")
@@ -30,6 +26,7 @@ def start_frontend_server(rank_id: int, server_id: int, global_controller: Concu
     g_frontend_server_info = FrontendServerInfo(
         py_env_configs.server_config.frontend_server_id
     )
+    update_worker_info(py_env_configs.server_config.start_port, py_env_configs.server_config.worker_info_port_num)
     try:
         logging.info(f"g_frontend_server_info = {g_frontend_server_info}")
         set_global_controller(global_controller)
diff --git a/rtp_llm/start_server.py b/rtp_llm/start_server.py
index 3423f9db2..7f48ea260 100644
--- a/rtp_llm/start_server.py
+++ b/rtp_llm/start_server.py
@@ -6,16 +6,72 @@ import time
 
 import requests
 
-from rtp_llm.config.py_config_modules import ServerConfig
-from rtp_llm.metrics import kmonitor
-from rtp_llm.ops import ProfilingDebugLoggingConfig
-
 CUR_PATH = os.path.dirname(os.path.abspath(__file__))
 sys.path.append(os.path.join(str(CUR_PATH), ".."))
 
-from rtp_llm.distribute.worker_info import WorkerInfo, g_parallel_info
+import json
+
+from rtp_llm.config.py_config_modules import PyEnvConfigs
+from rtp_llm.distribute.worker_info import WorkerInfo, g_parallel_info, update_worker_info
 from rtp_llm.server.server_args.server_args import setup_args
 from rtp_llm.utils.concurrency_controller import init_controller
+from rtp_llm.utils.fuser import fetch_remote_file_to_local
+
+
+def fetch_model_files_to_local(py_env_configs: PyEnvConfigs):
+    """Fetch remote model files to local and update py_env_configs in place."""
+    # Fetch checkpoint_path from model_args
+    model_args = py_env_configs.model_args
+    if model_args.ckpt_path:
+        model_args.ckpt_path = fetch_remote_file_to_local(
+            model_args.ckpt_path
+        )
+    
+    # Fetch tokenizer_path from model_args
+    tokenizer_path = model_args.tokenizer_path
+    if not tokenizer_path:
+        tokenizer_path = model_args.ckpt_path
+    if tokenizer_path:
+        model_args.tokenizer_path = fetch_remote_file_to_local(tokenizer_path)
+    
+    # Fetch extra_data_path from model_args
+    if model_args.extra_data_path:
+        local_extra_data_path = fetch_remote_file_to_local(
+            model_args.extra_data_path
+        )
+        model_args.local_extra_data_path = local_extra_data_path
+    
+    # Fetch ptuning_path from model_args
+    if model_args.ptuning_path:
+        model_args.ptuning_path = fetch_remote_file_to_local(
+            model_args.ptuning_path
+        )
+    
+    # Fetch lora paths
+    lora_config = py_env_configs.lora_config
+    if lora_config.lora_info:
+        try:
+            lora_infos = json.loads(lora_config.lora_info)
+            for lora_name, lora_path in lora_infos.items():
+                lora_infos[lora_name] = fetch_remote_file_to_local(lora_path)
+            # Update lora_info back to string format
+            lora_config.lora_info = json.dumps(lora_infos)
+        except (json.JSONDecodeError, TypeError) as e:
+            logging.warning(f"Failed to parse lora_info: {e}, skipping lora path fetching")
+    
+    # Fetch sp_checkpoint_path if exists
+    sp_config = py_env_configs.sp_config
+    if sp_config.sp_checkpoint_path:
+        sp_config.sp_checkpoint_path = fetch_remote_file_to_local(
+            sp_config.sp_checkpoint_path
+        )
+    
+    logging.info(
+        f"Fetched model files - checkpoint_path: {model_args.ckpt_path}, "
+        f"tokenizer_path: {model_args.tokenizer_path}, "
+        f"ptuning_path: {model_args.ptuning_path}, "
+        f"extra_data_path: {model_args.local_extra_data_path}"
+    )
 
 
 def check_server_health(server_port):
@@ -34,24 +90,20 @@ def check_server_health(server_port):
         return False
 
 
-def start_backend_server_impl(global_controller):
+def start_backend_server_impl(global_controller, py_env_configs):
     from rtp_llm.start_backend_server import start_backend_server
 
-    profiling_debug_config = ProfilingDebugLoggingConfig()
-    profiling_debug_config.update_from_env()
     # only for debug
-    if profiling_debug_config.debug_load_server:
-        start_backend_server(global_controller)
+    if py_env_configs.profiling_debug_config.debug_load_server:
+        start_backend_server(global_controller, py_env_configs)
         os._exit(-1)
     backend_process = multiprocessing.Process(
-        target=start_backend_server, args=(global_controller,), name="backend_server"
+        target=start_backend_server, args=(global_controller, py_env_configs), name="backend_server"
     )
     backend_process.start()
 
     retry_interval_seconds = 5
-    server_config = ServerConfig()
-    server_config.update_from_env()
-    start_port = server_config.start_port
+    start_port = py_env_configs.server_config.start_port
     backend_server_port = WorkerInfo.backend_server_port_offset(0, start_port)
     while True:
         if not backend_process.is_alive():
@@ -71,12 +123,10 @@ def start_backend_server_impl(global_controller):
     return backend_process
 
 
-def start_frontend_server_impl(global_controller, backend_process):
+def start_frontend_server_impl(global_controller, backend_process, py_env_configs):
     from rtp_llm.start_frontend_server import start_frontend_server
 
-    server_config = ServerConfig()
-    server_config.update_from_env()
-    frontend_server_count = server_config.frontend_server_count
+    frontend_server_count = py_env_configs.server_config.frontend_server_count
     if frontend_server_count < 1:
         logging.info(
             "frontend server's count is {frontend_server_count}, this may be a mistake"
@@ -100,14 +150,14 @@ def start_frontend_server_impl(global_controller, backend_process):
         for i in range(frontend_server_count):
             process = multiprocessing.Process(
                 target=start_frontend_server,
-                args=(rank, i, global_controller),
+                args=(rank, i, global_controller, py_env_configs),
                 name=f"frontend_server_{i}",
             )
             frontend_processes.append(process)
             process.start()
 
     retry_interval_seconds = 5
-    start_port = server_config.start_port
+    start_port = py_env_configs.server_config.start_port
 
     while True:
         if not all(proc.is_alive() for proc in frontend_processes):
@@ -148,34 +198,39 @@ def monitor_and_release_process(backend_process, frontend_process):
 
 
 def main():
-    setup_args()
+    py_env_configs: PyEnvConfigs = setup_args()
+    fetch_model_files_to_local(py_env_configs)
+    update_worker_info(py_env_configs.server_config.start_port, py_env_configs.server_config.worker_info_port_num)
 
-    start_server()
+    start_server(py_env_configs)
 
 
-def start_server():
+def start_server(py_env_configs):
     try:
         multiprocessing.set_start_method("spawn")
     except RuntimeError as e:
-        logging.warn(str(e))
-    global_controller = init_controller()
+        logging.warning(str(e))
+    global_controller = init_controller(py_env_configs.concurrency_config,
+                                        dp_size=g_parallel_info.dp_size)
     backend_process = None
     frontend_process = None
     try:
         if os.environ.get("ROLE_TYPE", "") != "FRONTEND":
             logging.info("start backend server")
-            backend_process = start_backend_server_impl(global_controller)
+            backend_process = start_backend_server_impl(global_controller, py_env_configs)
             logging.info(f"backend server process = {backend_process}")
 
         logging.info("start frontend server")
         frontend_process = start_frontend_server_impl(
-            global_controller, backend_process
+            global_controller, backend_process, py_env_configs
         )
         logging.info(f"frontend server process = {frontend_process}")
 
         logging.info(f"后端RPC 服务监听的ip为 0.0.0.0，ip/ip段可自定义为所需范围")
     except Exception as e:
+        import traceback
         logging.error(f"start failed, {str(e)}")
+        logging.error(f"Traceback:\n{traceback.format_exc()}")
     finally:
         monitor_and_release_process(backend_process, frontend_process)
 
diff --git a/rtp_llm/test/concurrency_limit_test.py b/rtp_llm/test/concurrency_limit_test.py
index 1f714e677..e9d4a15f3 100644
--- a/rtp_llm/test/concurrency_limit_test.py
+++ b/rtp_llm/test/concurrency_limit_test.py
@@ -79,7 +79,7 @@ class ConcurrencyLimitTest(TestCase):
         self.port = random.randint(20000, 30000)
         os.environ["CONCURRENCY_LIMIT"] = "16"
         os.environ["START_PORT"] = str(self.port)
-        g_worker_info.reload()
+        g_worker_info.reload(self.port)
         self.frontend_app = FrontendApp()
         self.backend_app = BackendApp()
 
diff --git a/rtp_llm/test/generate_config_test.py b/rtp_llm/test/generate_config_test.py
index bfb8f3a59..bc9707850 100644
--- a/rtp_llm/test/generate_config_test.py
+++ b/rtp_llm/test/generate_config_test.py
@@ -4,14 +4,21 @@ from unittest import TestCase, main
 
 from transformers import AutoTokenizer
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.config.py_config_modules import StaticConfig
+from rtp_llm.config.engine_config import SpecialTokens
 from rtp_llm.frontend.tokenizer_factory.tokenizers.tokenization_qwen import (
     QWenTokenizer,
 )
 from rtp_llm.pipeline.pipeline import Pipeline
 
 
+class MockGenerateEnvConfig:
+    """Mock GenerateEnvConfig for testing."""
+    def __init__(self, think_mode=0, think_end_token_id=-1, think_end_tag="</think>\n\n"):
+        self.think_mode = think_mode
+        self.think_end_token_id = think_end_token_id
+        self.think_end_tag = think_end_tag
+
+
 class GenerateConfigTest(TestCase):
     def __init__(self, *args: Any, **kwargs: Any):
         super().__init__(*args, **kwargs)
@@ -40,12 +47,13 @@ class GenerateConfigTest(TestCase):
         }
 
     def test_simple(self):
-        parameter = GptInitModelParameters(0, 0, 0, 0, 0)
+        special_tokens = SpecialTokens()
         generate_config = Pipeline.create_generate_config(
-            tokenizer=None,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=self._create_generate_config(),
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=None,
+            generate_env_config=None,
         )
         self.assertEqual(generate_config.stop_words_list, [[8848]])
         self.assertEqual(generate_config.stop_words_str, ["hello", "what's your name"])
@@ -54,10 +62,11 @@ class GenerateConfigTest(TestCase):
         self.assertEqual(generate_config.max_new_tokens, 100)
 
         generate_config = Pipeline.create_generate_config(
-            tokenizer=None,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config={},
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=None,
+            generate_env_config=None,
             **self._create_generate_config(),
         )
         self.assertEqual(generate_config.stop_words_list, [[8848]])
@@ -67,12 +76,13 @@ class GenerateConfigTest(TestCase):
         self.assertEqual(generate_config.max_new_tokens, 100)
 
     def test_kwargs_overwrite(self):
-        parameter = GptInitModelParameters(0, 0, 0, 0, 0)
+        special_tokens = SpecialTokens()
         generate_config = Pipeline.create_generate_config(
-            tokenizer=None,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=self._create_generate_config(),
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=None,
+            generate_env_config=None,
             **self._create_kwargs(),
         )
         self.assertEqual(generate_config.stop_words_list, [[1551]])
@@ -82,14 +92,15 @@ class GenerateConfigTest(TestCase):
         self.assertEqual(generate_config.max_new_tokens, 20)
 
     def test_stop_words_merge(self):
-        parameter = GptInitModelParameters(0, 0, 0, 0, 0)
-        parameter.special_tokens.stop_words_id_list = [[1233, 19912]]
-        parameter.special_tokens.stop_words_str_list = ["gg"]
+        special_tokens = SpecialTokens()
+        special_tokens.stop_words_id_list = [[1233, 19912]]
+        special_tokens.stop_words_str_list = ["gg"]
         generate_config = Pipeline.create_generate_config(
-            tokenizer=None,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=self._create_generate_config(),
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=None,
+            generate_env_config=None,
         )
         self.assertEqual(generate_config.stop_words_list, [[8848], [1233, 19912]])
         self.assertEqual(
@@ -97,17 +108,18 @@ class GenerateConfigTest(TestCase):
         )
 
     def test_stop_words_merge_with_toeknizer(self):
-        parameter = GptInitModelParameters(0, 0, 0, 0, 0)
-        parameter.special_tokens.stop_words_id_list = [[1233, 19912]]
-        parameter.special_tokens.stop_words_str_list = ["gg"]
+        special_tokens = SpecialTokens()
+        special_tokens.stop_words_id_list = [[1233, 19912]]
+        special_tokens.stop_words_str_list = ["gg"]
         tokenizer = QWenTokenizer(
             f"{self.test_data_path}/model_test/fake_test/testdata/qwen_7b/tokenizer/qwen.tiktoken"
         )
         generate_config = Pipeline.create_generate_config(
-            tokenizer=tokenizer,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=self._create_generate_config(),
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=tokenizer,
+            generate_env_config=None,
         )
         self.assertEqual(
             generate_config.stop_words_list,
@@ -118,97 +130,107 @@ class GenerateConfigTest(TestCase):
         )
 
     def test_select_tokens_id(self):
-        parameter = GptInitModelParameters(0, 0, 0, 0, 0)
+        special_tokens = SpecialTokens()
         generate_config = Pipeline.create_generate_config(
-            tokenizer=None,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=self._create_generate_config_for_select_tokens_id(),
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=None,
+            generate_env_config=None,
         )
         self.assertEqual(generate_config.select_tokens_id, [0, 3])
         self.assertEqual(generate_config.select_tokens_str, [])
 
         with self.assertRaisesRegex(Exception, "should be less than vocab_size"):
             generate_config = Pipeline.create_generate_config(
-                tokenizer=None,
-                vocab_size=2,
-                special_tokens=parameter.special_tokens,
                 generate_config=self._create_generate_config_for_select_tokens_id(),
+                vocab_size=2,
+                special_tokens=special_tokens,
+                tokenizer=None,
+                generate_env_config=None,
             )
 
     def test_same(self):
-        parameter = GptInitModelParameters(0, 0, 0, 0, 0)
-        parameter.special_tokens.stop_words_id_list = [[1233, 19912]]
-        parameter.special_tokens.stop_words_str_list = ["gg"]
+        special_tokens = SpecialTokens()
+        special_tokens.stop_words_id_list = [[1233, 19912]]
+        special_tokens.stop_words_str_list = ["gg"]
 
         a = Pipeline.create_generate_config(
-            tokenizer=None,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=self._create_generate_config(),
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=None,
+            generate_env_config=None,
         )
         b = Pipeline.create_generate_config(
-            tokenizer=None,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=self._create_generate_config(),
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=None,
+            generate_env_config=None,
         )
         a.gen_hash_value()
         b.gen_hash_value()
         self.assertTrue(a.is_same(b))
 
     def test_add_thinking_params(self):
-        StaticConfig.generate_env_config.think_mode = 1
-        StaticConfig.generate_env_config.think_end_token_id = 102
-        parameter = GptInitModelParameters(0, 0, 0, 0, 0)
+        generate_env_config = MockGenerateEnvConfig(think_mode=1, think_end_token_id=102)
+        special_tokens = SpecialTokens()
         tokenizer = QWenTokenizer(
             f"{self.test_data_path}/model_test/fake_test/testdata/qwen_7b/tokenizer/qwen.tiktoken"
         )
         generate_config_dict = self._create_generate_config()
         generate_config_dict.update({"max_thinking_tokens": 109})
         generate_config = Pipeline.create_generate_config(
-            tokenizer=tokenizer,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=generate_config_dict,
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=tokenizer,
+            generate_env_config=generate_env_config,
         )
         self.assertEqual(generate_config.max_thinking_tokens, 109)
         self.assertEqual(generate_config.in_think_mode, True)
         self.assertEqual(generate_config.end_think_token_ids, [102])
 
     def test_add_thinking_params_with_think_token(self):
-        StaticConfig.generate_env_config.think_mode = 1
-        StaticConfig.generate_env_config.think_end_token_id = -1
-        StaticConfig.generate_env_config.think_end_tag = "</think>"
-        parameter = GptInitModelParameters(0, 0, 0, 0, 0)
+        generate_env_config = MockGenerateEnvConfig(
+            think_mode=1, 
+            think_end_token_id=-1, 
+            think_end_tag="</think>"
+        )
+        special_tokens = SpecialTokens()
         tokenizer_path = f"{self.test_data_path}/model_test/fake_test/testdata/deepseek_r1_qwen_14b_tokenizer"
         tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
         generate_config_dict = self._create_generate_config()
         generate_config_dict.update({"max_thinking_tokens": 20})
         generate_config = Pipeline.create_generate_config(
-            tokenizer=tokenizer,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=generate_config_dict,
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=tokenizer,
+            generate_env_config=generate_env_config,
         )
         self.assertEqual(generate_config.max_thinking_tokens, 20)
         self.assertEqual(generate_config.in_think_mode, True)
         self.assertEqual(generate_config.end_think_token_ids, [151649])
 
     def test_add_thinking_params_with_think_token_2(self):
-        StaticConfig.generate_env_config.think_mode = 1
-        StaticConfig.generate_env_config.think_end_token_id = -1
-        StaticConfig.generate_env_config.think_end_tag = "</think>\n\n"
-        parameter = GptInitModelParameters(0, 0, 0, 0, 0)
+        generate_env_config = MockGenerateEnvConfig(
+            think_mode=1, 
+            think_end_token_id=-1, 
+            think_end_tag="</think>\n\n"
+        )
+        special_tokens = SpecialTokens()
         tokenizer_path = f"{self.test_data_path}/model_test/fake_test/testdata/deepseek_r1_qwen_14b_tokenizer"
         tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
         generate_config_dict = self._create_generate_config()
         generate_config_dict.update({"max_thinking_tokens": 20})
         generate_config = Pipeline.create_generate_config(
-            tokenizer=tokenizer,
-            vocab_size=100,
-            special_tokens=parameter.special_tokens,
             generate_config=generate_config_dict,
+            vocab_size=100,
+            special_tokens=special_tokens,
+            tokenizer=tokenizer,
+            generate_env_config=generate_env_config,
         )
         self.assertEqual(generate_config.max_thinking_tokens, 20)
         self.assertEqual(generate_config.in_think_mode, True)
diff --git a/rtp_llm/test/model_test/test_util/fake_model_loader.py b/rtp_llm/test/model_test/test_util/fake_model_loader.py
index 9e2cbd423..b51e14de4 100644
--- a/rtp_llm/test/model_test/test_util/fake_model_loader.py
+++ b/rtp_llm/test/model_test/test_util/fake_model_loader.py
@@ -3,7 +3,7 @@ import logging
 import os
 
 from rtp_llm.async_decoder_engine.async_model import AsyncModel
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.model_factory import ModelConfig, ModelFactory
 from rtp_llm.utils.weight_type import WEIGHT_TYPE
 
@@ -61,48 +61,86 @@ class FakeModelLoader(object):
             quantization=self.quantization,
         )
 
-        raw_config: GptInitModelParameters = model_cls.create_config(model_config)
-        raw_config.model_specific_config.load_python_model = self.load_py_model
-        raw_config.device_resource_config.device_reserve_memory_bytes = (
-            self.device_reserve_memory_bytes
+        raw_config: ModelConfig = model_cls._create_config(self.ckpt_path)
+        # Apply model_config settings
+        raw_config.ckpt_path = self.ckpt_path
+        raw_config.tokenizer_path = self.tokenizer_path
+        raw_config.max_seq_len = self.max_seq_len if self.max_seq_len > 0 else raw_config.max_seq_len
+        raw_config.quantization = model_config.quantization
+        raw_config.act_type = self.act_type
+        raw_config.model_type = self.model_type
+        
+        # Update from config.json
+        raw_config.head_num_ = config_json.get(
+            "num_attention_heads", raw_config.head_num_
         )
-        raw_config.warm_up = self.warm_up
-        raw_config.head_num = config_json.get(
-            "num_attention_heads", raw_config.head_num
-        )
-        raw_config.head_num_kv = config_json.get(
-            "num_key_value_heads", raw_config.head_num_kv
+        raw_config.head_num_kv_ = config_json.get(
+            "num_key_value_heads", raw_config.head_num_kv_
         )
         if config_json.get("multi_query_attention", False):
-            raw_config.head_num_kv = config_json["multi_query_group_num"]
-        raw_config.size_per_head = config_json.get(
-            "kv_channels", raw_config.size_per_head
+            raw_config.head_num_kv_ = config_json["multi_query_group_num"]
+        raw_config.size_per_head_ = config_json.get(
+            "kv_channels", raw_config.size_per_head_
         )
         raw_config.inter_size = config_json.get(
             "ffn_hidden_size", raw_config.inter_size
         )
-        raw_config.inter_padding_size = config_json.get(
-            "ffn_inter_padding_size", raw_config.inter_padding_size
+        raw_config.inter_padding_size_ = config_json.get(
+            "ffn_inter_padding_size", raw_config.inter_padding_size_
         )
-        raw_config.layer_num = config_json.get("num_layers", raw_config.layer_num)
+        raw_config.num_layers = config_json.get("num_layers", raw_config.num_layers)
         raw_config.vocab_size = config_json.get(
             "padded_vocab_size", raw_config.vocab_size
         )
         raw_config.pre_seq_len = config_json.get("pre_seq_len", raw_config.pre_seq_len)
-
-        raw_config.update_common(
-            ckpt_path=self.ckpt_path,
-            lora_infos=None,
-            tokenizer_path=self.tokenizer_path,
-            quantization=model_config.quantization,
-            data_type=self.data_type,
-            kv_cache_type=self.kv_cache_type,
-            max_seq_len=self.max_seq_len,
-            seq_size_per_block=64,
-            gen_num_per_circle=1,
-            ptuning_path=None,
+        raw_config.is_causal_ = self.is_causal
+        
+        # Create ModelArgs for build_py_model_config
+        from rtp_llm.config.model_args import ModelArgs
+        from rtp_llm.config.engine_config import EngineConfig
+        from rtp_llm.config.mm_model_config import MMModelConfig
+        from rtp_llm.config.model_config import build_py_model_config
+        from rtp_llm.config.kv_cache_config import KVCacheConfig
+        from rtp_llm.ops import HWKernelConfig, ProfilingDebugLoggingConfig, ParallelismConfig
+        
+        # Create ModelArgs from raw_config
+        model_args = ModelArgs()
+        model_args.ckpt_path = self.ckpt_path
+        model_args.tokenizer_path = self.tokenizer_path
+        model_args.model_type = self.model_type
+        model_args.act_type = self.act_type
+        
+        # Create minimal configs for testing
+        engine_config = EngineConfig()
+        kv_cache_config = KVCacheConfig()
+        py_hw_kernel_config = HWKernelConfig()
+        profiling_debug_logging_config = ProfilingDebugLoggingConfig()
+        parallelism_config = ParallelismConfig()
+        
+        # Build engine config (minimal setup for testing)
+        # Note: EngineConfig.create() requires full py_env_configs, so we'll skip it for fake loader
+        # and just set minimal values
+        engine_config.kv_cache_config = kv_cache_config
+        engine_config.parallelism_config = parallelism_config
+        
+        # Build model config
+        py_model_config = raw_config
+        build_py_model_config(
+            py_model_config=py_model_config,
+            model_args=model_args,
+            kv_cache_config=kv_cache_config,
+            py_hw_kernel_config=py_hw_kernel_config,
+            profiling_debug_logging_config=profiling_debug_logging_config,
+            parallelism_config=parallelism_config,
+        )
+        
+        # Create MMModelConfig (minimal for testing)
+        mm_model_config = MMModelConfig()
+        
+        model = model_cls(
+            py_model_config=py_model_config,
+            mm_model_config=mm_model_config,
+            engine_config=engine_config,
         )
-        raw_config.is_causal = self.is_causal
-        model = model_cls.from_config(raw_config)
         model = AsyncModel(model, None)
         return model
diff --git a/rtp_llm/test/model_test/test_util/model_test_base.py b/rtp_llm/test/model_test/test_util/model_test_base.py
index a77569ac6..9de432365 100644
--- a/rtp_llm/test/model_test/test_util/model_test_base.py
+++ b/rtp_llm/test/model_test/test_util/model_test_base.py
@@ -10,8 +10,6 @@ import torch
 from rtp_llm.async_decoder_engine.async_model import AsyncModel
 from rtp_llm.model_factory import ModelConfig, ModelFactory
 from rtp_llm.pipeline.pipeline import Pipeline
-from rtp_llm.utils.ft_plugin import plguin_loader
-
 
 class ModelTestBase(TestCase):
     """
@@ -37,11 +35,6 @@ class ModelTestBase(TestCase):
         self.test_loss = test_loss
         self.fake_name = fake_name
 
-        os.environ["FT_PLUGIN_PATH"] = os.path.join(
-            os.getcwd(), "rtp_llm/plugins/ret_hidden_states.py"
-        )
-        plguin_loader.reload()
-
         logging.info(f"model_type: {self.model_type}")
         logging.info(f"tokenizer path: {self.tokenizer_path}")
         logging.info(f"check point path: {self.ckpt_path}")
@@ -248,8 +241,20 @@ class ModelTestBase(TestCase):
     def simple_test(self, is_fake: bool):
         model = self._load_model()
         try:
-            pipeline = Pipeline(model.config, model.tokenizer)
-            if model.config.pre_seq_len > 0:
+            from rtp_llm.config.py_config_modules import GangConfig
+            gang_config = GangConfig()
+            pipeline = Pipeline(
+                special_tokens=model.model.py_model_config.special_tokens,
+                pd_sep_config=model.model.engine_config.pd_sep_config,
+                runtime_config=model.model.engine_config.runtime_config,
+                ffn_disaggregate_config=model.model.engine_config.parallelism_config.ffn_disaggregate_config,
+                max_seq_len=model.model.py_model_config.max_seq_len,
+                seq_size_per_block=model.model.engine_config.kv_cache_config.seq_size_per_block,
+                tokenizer=model.tokenizer,
+                gang_config=gang_config,
+                sp_config=model.model.engine_config.sp_config,
+            )
+            if model.model.py_model_config.pre_seq_len > 0:
                 model_str = "/ptuning"
             else:
                 model_str = ""
diff --git a/rtp_llm/test/openai_response_test.py b/rtp_llm/test/openai_response_test.py
index 9c64e7928..6b5c343c4 100644
--- a/rtp_llm/test/openai_response_test.py
+++ b/rtp_llm/test/openai_response_test.py
@@ -13,7 +13,8 @@ from transformers import AutoTokenizer
 from typing_extensions import override
 
 from rtp_llm.config.generate_config import GenerateConfig
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
+from rtp_llm.config.engine_config import SpecialTokens
 from rtp_llm.frontend.tokenizer_factory.tokenizer_factory import TokenizerFactory
 from rtp_llm.frontend.tokenizer_factory.tokenizers import BaseTokenizer
 from rtp_llm.frontend.tokenizer_factory.tokenizers.tokenization_qwen import (
@@ -254,8 +255,42 @@ class BaseToolCallTestSuite:
         tokenizer = self._create_tokenizer(tokenizer_path)
 
         self.parent.model.tokenizer = tokenizer
+        # Create minimal configs for test
+        from rtp_llm.config.py_config_modules import ModelArgs, GenerateEnvConfig, RenderConfig, PyMiscellaneousConfig, VitConfig
+        from rtp_llm.server.backend_rpc_server_visitor import BackendRPCServerVisitor
+        from rtp_llm.ops import SpecialTokens as OpsSpecialTokens
+        model_args = ModelArgs()
+        model_args.model_type = self._get_model_type()
+        generate_env_config = GenerateEnvConfig()
+        render_config = RenderConfig()
+        misc_config = PyMiscellaneousConfig()
+        vit_config = VitConfig()
+        special_tokens = OpsSpecialTokens()
+        if hasattr(self.parent.model.config, 'special_tokens') and self.parent.model.config.special_tokens:
+            special_tokens = self.parent.model.config.special_tokens
+        from rtp_llm.config.py_config_modules import GangConfig
+        gang_config = GangConfig()
+        backend_rpc_server_visitor = BackendRPCServerVisitor(
+            max_seq_len=self.parent.model.config.max_seq_len,
+            seq_size_per_block=64,
+            pd_sep_config=None,
+            runtime_config=None,
+            ffn_disaggregate_config=None,
+            gang_config=gang_config,
+        )
         self.parent.endpoint = OpenaiEndpoint(
-            self.parent.model.config, self.parent.model.tokenizer, None
+            model_args=model_args,
+            generate_env_config=generate_env_config,
+            render_config=render_config,
+            misc_config=misc_config,
+            vit_config=vit_config,
+            special_tokens=special_tokens,
+            max_seq_len=self.parent.model.config.max_seq_len,
+            template_type=None,
+            model_name="",
+            ckpt_path="",
+            tokenizer=self.parent.model.tokenizer,
+            backend_rpc_server_visitor=backend_rpc_server_visitor,
         )
 
         return tokenizer
@@ -365,13 +400,14 @@ class OpenaiResponseTest(IsolatedAsyncioTestCase):
         self.test_data_path = os.path.join(
             os.getcwd(), "rtp_llm/test/model_test/fake_test/testdata"
         )
-        model_params = GptInitModelParameters(
-            head_num=1024,
-            size_per_head=1024,
-            layer_num=1024,
-            max_seq_len=1024,
-            vocab_size=1024,
-        )
+        from rtp_llm.config.py_config_modules import PyEnvConfigs
+        model_params = ModelConfig()
+        model_params.head_num_ = 1024
+        model_params.size_per_head_ = 1024
+        model_params.num_layers = 1024
+        model_params.max_seq_len = 1024
+        model_params.vocab_size = 1024
+        model_params.special_tokens = SpecialTokens()
         self.model = FakeModel(None)
         self.model.config = model_params
 
@@ -380,7 +416,34 @@ class OpenaiResponseTest(IsolatedAsyncioTestCase):
             f"{self.test_data_path}/qwen_7b/tokenizer/qwen.tiktoken"
         )
         self.model.tokenizer = tokenizer
-        self.endpoint = OpenaiEndpoint(self.model.config, self.model.tokenizer, None)
+        from rtp_llm.config.py_config_modules import PyEnvConfigs
+        from rtp_llm.server.backend_rpc_server_visitor import BackendRPCServerVisitor
+        from rtp_llm.ops import SpecialTokens as OpsSpecialTokens
+        py_env_configs = PyEnvConfigs()
+        backend_rpc_server_visitor = BackendRPCServerVisitor(
+            max_seq_len=self.model.config.max_seq_len,
+            seq_size_per_block=64,
+            pd_sep_config=None,
+            runtime_config=None,
+            ffn_disaggregate_config=None,
+        )
+        special_tokens = OpsSpecialTokens()
+        if hasattr(self.model.config, 'special_tokens_') and self.model.config.special_tokens:
+            special_tokens = self.model.config.special_tokens
+        self.endpoint = OpenaiEndpoint(
+            model_args=py_env_configs.model_args,
+            generate_env_config=py_env_configs.generate_env_config,
+            render_config=py_env_configs.render_config,
+            misc_config=py_env_configs.misc_config,
+            vit_config=py_env_configs.vit_config,
+            special_tokens=special_tokens,
+            max_seq_len=self.model.config.max_seq_len,
+            template_type=None,
+            model_name="",
+            ckpt_path=py_env_configs.model_args.ckpt_path or "",
+            tokenizer=self.model.tokenizer,
+            backend_rpc_server_visitor=backend_rpc_server_visitor,
+        )
         test_ids = [
             198,
             84169,
@@ -474,7 +537,34 @@ class OpenaiResponseTest(IsolatedAsyncioTestCase):
             f"{self.test_data_path}/qwen_7b/tokenizer/qwen.tiktoken"
         )
         self.model.tokenizer = tokenizer
-        self.endpoint = OpenaiEndpoint(self.model.config, self.model.tokenizer, None)
+        from rtp_llm.config.py_config_modules import PyEnvConfigs
+        from rtp_llm.server.backend_rpc_server_visitor import BackendRPCServerVisitor
+        from rtp_llm.ops import SpecialTokens as OpsSpecialTokens
+        py_env_configs = PyEnvConfigs()
+        backend_rpc_server_visitor = BackendRPCServerVisitor(
+            max_seq_len=self.model.config.max_seq_len,
+            seq_size_per_block=64,
+            pd_sep_config=None,
+            runtime_config=None,
+            ffn_disaggregate_config=None,
+        )
+        special_tokens = OpsSpecialTokens()
+        if hasattr(self.model.config, 'special_tokens_') and self.model.config.special_tokens:
+            special_tokens = self.model.config.special_tokens
+        self.endpoint = OpenaiEndpoint(
+            model_args=py_env_configs.model_args,
+            generate_env_config=py_env_configs.generate_env_config,
+            render_config=py_env_configs.render_config,
+            misc_config=py_env_configs.misc_config,
+            vit_config=py_env_configs.vit_config,
+            special_tokens=special_tokens,
+            max_seq_len=self.model.config.max_seq_len,
+            template_type=None,
+            model_name="",
+            ckpt_path=py_env_configs.model_args.ckpt_path or "",
+            tokenizer=self.model.tokenizer,
+            backend_rpc_server_visitor=backend_rpc_server_visitor,
+        )
         test_ids = [198, 84169, 25, 49434, 239, 73670, 37029]
         render_params = RendererParams(
             model_type="qwen",
@@ -505,7 +595,34 @@ class OpenaiResponseTest(IsolatedAsyncioTestCase):
             f"{self.test_data_path}/qwen_7b/tokenizer/qwen.tiktoken"
         )
         self.model.tokenizer = tokenizer
-        self.endpoint = OpenaiEndpoint(self.model.config, self.model.tokenizer, None)
+        from rtp_llm.config.py_config_modules import PyEnvConfigs
+        from rtp_llm.server.backend_rpc_server_visitor import BackendRPCServerVisitor
+        from rtp_llm.ops import SpecialTokens as OpsSpecialTokens
+        py_env_configs = PyEnvConfigs()
+        backend_rpc_server_visitor = BackendRPCServerVisitor(
+            max_seq_len=self.model.config.max_seq_len,
+            seq_size_per_block=64,
+            pd_sep_config=None,
+            runtime_config=None,
+            ffn_disaggregate_config=None,
+        )
+        special_tokens = OpsSpecialTokens()
+        if hasattr(self.model.config, 'special_tokens_') and self.model.config.special_tokens:
+            special_tokens = self.model.config.special_tokens
+        self.endpoint = OpenaiEndpoint(
+            model_args=py_env_configs.model_args,
+            generate_env_config=py_env_configs.generate_env_config,
+            render_config=py_env_configs.render_config,
+            misc_config=py_env_configs.misc_config,
+            vit_config=py_env_configs.vit_config,
+            special_tokens=special_tokens,
+            max_seq_len=self.model.config.max_seq_len,
+            template_type=None,
+            model_name="",
+            ckpt_path=py_env_configs.model_args.ckpt_path or "",
+            tokenizer=self.model.tokenizer,
+            backend_rpc_server_visitor=backend_rpc_server_visitor,
+        )
         test_ids = [
             25,
             220,
@@ -654,7 +771,34 @@ class OpenaiResponseTest(IsolatedAsyncioTestCase):
             f"{self.test_data_path}/qwen_7b/tokenizer/qwen.tiktoken"
         )
         self.model.tokenizer = tokenizer
-        self.endpoint = OpenaiEndpoint(self.model.config, self.model.tokenizer, None)
+        from rtp_llm.config.py_config_modules import PyEnvConfigs
+        from rtp_llm.server.backend_rpc_server_visitor import BackendRPCServerVisitor
+        from rtp_llm.ops import SpecialTokens as OpsSpecialTokens
+        py_env_configs = PyEnvConfigs()
+        backend_rpc_server_visitor = BackendRPCServerVisitor(
+            max_seq_len=self.model.config.max_seq_len,
+            seq_size_per_block=64,
+            pd_sep_config=None,
+            runtime_config=None,
+            ffn_disaggregate_config=None,
+        )
+        special_tokens = OpsSpecialTokens()
+        if hasattr(self.model.config, 'special_tokens_') and self.model.config.special_tokens:
+            special_tokens = self.model.config.special_tokens
+        self.endpoint = OpenaiEndpoint(
+            model_args=py_env_configs.model_args,
+            generate_env_config=py_env_configs.generate_env_config,
+            render_config=py_env_configs.render_config,
+            misc_config=py_env_configs.misc_config,
+            vit_config=py_env_configs.vit_config,
+            special_tokens=special_tokens,
+            max_seq_len=self.model.config.max_seq_len,
+            template_type=None,
+            model_name="",
+            ckpt_path=py_env_configs.model_args.ckpt_path or "",
+            tokenizer=self.model.tokenizer,
+            backend_rpc_server_visitor=backend_rpc_server_visitor,
+        )
         test_ids = [
             25,
             220,
@@ -2113,9 +2257,36 @@ class OpenaiResponseTest(IsolatedAsyncioTestCase):
         tokenizer = TokenizerFactory.create(
             "", "rtp_llm/test/tokenizer_test/testdata/chatglm3_tokenizer", "chatglm3"
         )
-        self.model.config.py_env_configs.model_config.model_type = "chatglm3"
+        self.model.config.py_env_configs.model_args.model_type = "chatglm3"
         self.model.tokenizer = tokenizer
-        self.endpoint = OpenaiEndpoint(self.model.config, self.model.tokenizer, None)
+        from rtp_llm.config.py_config_modules import PyEnvConfigs
+        from rtp_llm.server.backend_rpc_server_visitor import BackendRPCServerVisitor
+        from rtp_llm.ops import SpecialTokens as OpsSpecialTokens
+        py_env_configs = PyEnvConfigs()
+        backend_rpc_server_visitor = BackendRPCServerVisitor(
+            max_seq_len=self.model.config.max_seq_len,
+            seq_size_per_block=64,
+            pd_sep_config=None,
+            runtime_config=None,
+            ffn_disaggregate_config=None,
+        )
+        special_tokens = OpsSpecialTokens()
+        if hasattr(self.model.config, 'special_tokens_') and self.model.config.special_tokens:
+            special_tokens = self.model.config.special_tokens
+        self.endpoint = OpenaiEndpoint(
+            model_args=py_env_configs.model_args,
+            generate_env_config=py_env_configs.generate_env_config,
+            render_config=py_env_configs.render_config,
+            misc_config=py_env_configs.misc_config,
+            vit_config=py_env_configs.vit_config,
+            special_tokens=special_tokens,
+            max_seq_len=self.model.config.max_seq_len,
+            template_type=None,
+            model_name="",
+            ckpt_path=py_env_configs.model_args.ckpt_path or "",
+            tokenizer=self.model.tokenizer,
+            backend_rpc_server_visitor=backend_rpc_server_visitor,
+        )
         self.assertEqual(self.endpoint.stop_words_id_list, [[64795], [64797], [2]])
         self.assertEqual(
             self.endpoint.stop_words_str_list, ["<|user|>", "<|observation|>"]
@@ -2129,7 +2300,34 @@ class OpenaiResponseTest(IsolatedAsyncioTestCase):
             "", f"{self.test_data_path}/deepseek_r1_qwen_14b_tokenizer", "qwen_2"
         )
         self.model.tokenizer = tokenizer
-        self.endpoint = OpenaiEndpoint(self.model.config, self.model.tokenizer, None)
+        from rtp_llm.config.py_config_modules import PyEnvConfigs
+        from rtp_llm.server.backend_rpc_server_visitor import BackendRPCServerVisitor
+        from rtp_llm.ops import SpecialTokens as OpsSpecialTokens
+        py_env_configs = PyEnvConfigs()
+        backend_rpc_server_visitor = BackendRPCServerVisitor(
+            max_seq_len=self.model.config.max_seq_len,
+            seq_size_per_block=64,
+            pd_sep_config=None,
+            runtime_config=None,
+            ffn_disaggregate_config=None,
+        )
+        special_tokens = OpsSpecialTokens()
+        if hasattr(self.model.config, 'special_tokens_') and self.model.config.special_tokens:
+            special_tokens = self.model.config.special_tokens
+        self.endpoint = OpenaiEndpoint(
+            model_args=py_env_configs.model_args,
+            generate_env_config=py_env_configs.generate_env_config,
+            render_config=py_env_configs.render_config,
+            misc_config=py_env_configs.misc_config,
+            vit_config=py_env_configs.vit_config,
+            special_tokens=special_tokens,
+            max_seq_len=self.model.config.max_seq_len,
+            template_type=None,
+            model_name="",
+            ckpt_path=py_env_configs.model_args.ckpt_path or "",
+            tokenizer=self.model.tokenizer,
+            backend_rpc_server_visitor=backend_rpc_server_visitor,
+        )
 
         test_ids = [151648, 198, 73670, 73670, 73670, 151649, 271, 37029, 37029, 37029]
         render_params = RendererParams(
diff --git a/rtp_llm/test/perf_test/multi_node/local_server_runner.py b/rtp_llm/test/perf_test/multi_node/local_server_runner.py
index be0df3ba6..25019f6e6 100644
--- a/rtp_llm/test/perf_test/multi_node/local_server_runner.py
+++ b/rtp_llm/test/perf_test/multi_node/local_server_runner.py
@@ -23,7 +23,7 @@ try:
 except ImportError:
     pass
 
-from rtp_llm.config.py_config_modules import PyEnvConfigs, StaticConfig
+from rtp_llm.config.py_config_modules import PyEnvConfigs
 from rtp_llm.distribute.gang_info import members_from_test_env
 from rtp_llm.test.perf_test.batch_decode_test import run_single
 from rtp_llm.test.perf_test.test_util import create_query
@@ -34,9 +34,10 @@ from rtp_llm.utils.fuser import fetch_remote_file_to_local
 # uvloop_setup()
 
 
-def wait_master_done(env_dict: Dict[str, str] = {}) -> None:
+def wait_master_done(env_dict: Dict[str, str] = {}, world_rank: int = 0) -> None:
+    # Get gang_config_string from environment variable or env_dict
     dist_config_str = env_dict.get(
-        "GANG_CONFIG_STRING", StaticConfig.gang_config.gang_config_string
+        "GANG_CONFIG_STRING", os.environ.get("GANG_CONFIG_STRING", None)
     )
     if not dist_config_str:
         raise RuntimeError("no gang config string, unexpected!")
@@ -44,7 +45,6 @@ def wait_master_done(env_dict: Dict[str, str] = {}) -> None:
     master_member = dist_members[0]
     master_host = master_member.ip
     master_port = master_member.server_port
-    world_rank = StaticConfig.parallelism_distributed_config.world_rank
     while True:
         logging.info(
             f"rank [{world_rank}] waiting for master {master_host}:{master_port} done"
@@ -150,11 +150,10 @@ if __name__ == "__main__":
     os.environ["MAX_SEQ_LEN"] = str(max_seq_len + 20)
 
     py_env_configs = PyEnvConfigs()
-    py_env_configs.update_from_env()
     port = py_env_configs.server_config.start_port
     world_rank = py_env_configs.parallelism_distributed_config.world_rank
     log_dir_name = (
-        f"test_output_{py_env_configs.model_config.model_type}_{py_env_configs.parallelism_distributed_config.dp_size}"
+        f"test_output_{py_env_configs.model_args.model_type}_{py_env_configs.parallelism_distributed_config.dp_size}"
         f"_{py_env_configs.parallelism_distributed_config.tp_size}_{py_env_configs.parallelism_distributed_config.world_rank}"
         f"_{time.strftime('%Y%m%d_%H%M%S')}"
     ).upper()
@@ -173,15 +172,15 @@ if __name__ == "__main__":
         logging.info(f"setpgrp error: {e}")
 
     tokenizer_path = fetch_remote_file_to_local(
-        py_env_configs.model_config.tokenizer_path
+        py_env_configs.model_args.tokenizer_path
     )
     if tokenizer_path is None:
         raise RuntimeError(
-            f"fetch tokenizer path failed, tokenizer_path: {py_env_configs.model_config.tokenizer_path}"
+            f"fetch tokenizer path failed, tokenizer_path: {py_env_configs.model_args.tokenizer_path}"
         )
 
     input_query_dict = create_query(
-        py_env_configs.model_config.model_type,
+        py_env_configs.model_args.model_type,
         tokenizer_path,
         input_len_list,
     )
@@ -193,7 +192,7 @@ if __name__ == "__main__":
             raise Exception("server start failed")
         if world_rank:
             logging.info(f"world rank non-zero: {world_rank}, wait for main.")
-            wait_master_done()
+            wait_master_done(world_rank=world_rank)
         else:
             test_main(
                 port,
diff --git a/rtp_llm/test/slice_stop_word_list_test.py b/rtp_llm/test/slice_stop_word_list_test.py
index f4f037871..fb7e70726 100644
--- a/rtp_llm/test/slice_stop_word_list_test.py
+++ b/rtp_llm/test/slice_stop_word_list_test.py
@@ -22,7 +22,16 @@ class SliceStopWordListTest(TestCase):
         model = FakeModelLoader(
             "llama", ckpt_path, ckpt_path, max_seq_len=1024
         ).load_model()
-        self.pipeline = Pipeline(model.config, model.tokenizer)
+        self.pipeline = Pipeline(
+            special_tokens=model.model.py_model_config.special_tokens,
+            pd_sep_config=model.model.engine_config.pd_sep_config,
+            runtime_config=model.model.engine_config.runtime_config,
+            ffn_disaggregate_config=model.model.engine_config.parallelism_config.ffn_disaggregate_config,
+            max_seq_len=model.model.py_model_config.max_seq_len,
+            seq_size_per_block=model.engine_config.kv_cache_config.seq_size_per_block,
+            tokenizer=model.tokenizer,
+            py_env_configs=model.model.py_env_configs,
+        )
 
     async def mock_generate(self):
         yield GenerateOutputs(
diff --git a/rtp_llm/test/stop_words_pipeline_test.py b/rtp_llm/test/stop_words_pipeline_test.py
index 0c28656f3..dd2293521 100644
--- a/rtp_llm/test/stop_words_pipeline_test.py
+++ b/rtp_llm/test/stop_words_pipeline_test.py
@@ -2,8 +2,14 @@ from typing import List
 from unittest import TestCase, main
 
 from rtp_llm.pipeline import Pipeline
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig as PyModelConfig
 from rtp_llm.config.generate_config import GenerateConfig
+from rtp_llm.ops import (
+    FfnDisAggregateConfig,
+    ModelConfig,
+    PDSepConfig,
+    RuntimeConfig,
+)
 from rtp_llm.utils.base_model_datatypes import GenerateOutput
 from rtp_llm.utils.word_util import get_stop_word_slices
 
@@ -11,14 +17,27 @@ from rtp_llm.utils.word_util import get_stop_word_slices
 class StopWordTest(TestCase):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
+        # Create C++ config objects
+        model_config = ModelConfig()
+        model_config.num_heads_ = 8
+        model_config.size_per_head_ = 128
+        model_config.num_layers = 1
+        model_config.max_seq_len = 32
+        model_config.vocab_size = 1024
+        
+        pd_sep_config = PDSepConfig()
+        runtime_config = RuntimeConfig()
+        ffn_disaggregate_config = FfnDisAggregateConfig()
+        
+        # Create Python ModelConfig
+        py_model_config = PyModelConfig()
+        
         self.pipeline = Pipeline(
-            GptInitModelParameters(
-                head_num=8,
-                size_per_head=128,
-                layer_num=1,
-                max_seq_len=32,
-                vocab_size=1024,
-            ),
+            model_config,
+            pd_sep_config,
+            runtime_config,
+            ffn_disaggregate_config,
+            py_model_config,
             None,
         )
 
diff --git a/rtp_llm/test/utils/maga_server_manager.py b/rtp_llm/test/utils/maga_server_manager.py
index 6f2d24906..5df9a46d6 100644
--- a/rtp_llm/test/utils/maga_server_manager.py
+++ b/rtp_llm/test/utils/maga_server_manager.py
@@ -206,7 +206,7 @@ class MagaServerManager(object):
 
         for _ in range(retry_times):
             try:
-                logging.info(f"{url} {query}")
+                logging.info(f"curl {url} -d '{json.dumps(query)}'")
                 response = requests.post(url, json=query)
                 if response.status_code == 200:
                     logging.debug("%s", response.text)
diff --git a/rtp_llm/tools/api/model_basic_info_analyzer.py b/rtp_llm/tools/api/model_basic_info_analyzer.py
index 21cdf812d..ba03f1b4d 100644
--- a/rtp_llm/tools/api/model_basic_info_analyzer.py
+++ b/rtp_llm/tools/api/model_basic_info_analyzer.py
@@ -164,7 +164,14 @@ def _load_as_ft_style(
         tokenizer_path=None,
         quantization=quantization,
     )
-    config: GptInitModelParameters = model_cls.create_config(model_config)
+    config: ModelConfig = model_cls._create_config(model_path)
+    # Apply model_config settings to config
+    config.ckpt_path = model_config.ckpt_path
+    config.tokenizer_path = model_config.tokenizer_path or model_config.ckpt_path
+    config.max_seq_len = model_config.max_seq_len or 0
+    config.quantization = model_config.quantization
+    config.act_type = model_config.act_type
+    config.model_type = model_type
     is_quant_weight = config.quant_algo.isQuant()
     quant_config = None
     if is_quant_weight:
@@ -194,14 +201,14 @@ def _load_as_ft_style(
     try:
         param_count = model_cls.eval_model_param_count(config)
     except Exception as e:
-        param_count = BaseModel.eval_model_param_count(config)
+        param_count = config.model_param_count()
         logging.error(f"eval model param count failed: {str(e)}")
     total_size = None
     try:
         total_size = model_cls.eval_model_size(config)
     except Exception as e:
-        total_size = BaseModel.eval_model_size(config)
-        logging.error(f"eval model param count failed: {str(e)}")
+        total_size = config.eval_model_size()
+        logging.error(f"eval model size failed: {str(e)}")
 
     raw_config_dict = _get_raw_config(model_path)
 
@@ -209,7 +216,7 @@ def _load_as_ft_style(
         ft_model_type=ft_model_type,
         param_count=param_count,
         model_size=total_size,
-        hidden_size=config.gpt_init_params.hidden_size,
+        hidden_size=config.py_model_config.hidden_size,
         architectures=raw_config_dict.get("architectures", None),
         llm_architectures=None,
         is_quant_weight=is_quant_weight,
diff --git a/rtp_llm/tools/api/model_size_evaluator_api.py b/rtp_llm/tools/api/model_size_evaluator_api.py
index 3e9cea0a4..bfa38e393 100755
--- a/rtp_llm/tools/api/model_size_evaluator_api.py
+++ b/rtp_llm/tools/api/model_size_evaluator_api.py
@@ -3,10 +3,9 @@ import logging
 import logging.config
 from typing import Any, Dict, Union
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.config.quant_config import init_quant_config
 from rtp_llm.model_factory import ModelFactory
-from rtp_llm.models.base_model import ModelConfig
 from rtp_llm.tools.api.hf_model_helper import HfStyleModelInfo, get_hf_model_info
 from rtp_llm.tools.api.utils import handler_error
 from rtp_llm.utils.fuser import fetch_remote_file_to_local, umount_file
@@ -25,7 +24,14 @@ def eval_model_size(env_params, model_type, model_path, ptuning_path):
         tokenizer_path=None,
         quantization=quantization,
     )
-    config: GptInitModelParameters = model_cls.create_config(model_config)
+    config: ModelConfig = model_cls._create_config(model_path)
+    # Apply model_config settings to config
+    config.ckpt_path = model_config.ckpt_path
+    config.tokenizer_path = model_config.tokenizer_path or model_config.ckpt_path
+    config.max_seq_len = model_config.max_seq_len or 0
+    config.quantization = model_config.quantization
+    config.act_type = model_config.act_type
+    config.model_type = model_type
     return model_cls.eval_model_size(config), model_cls.eval_model_param_count(config)
 
 
diff --git a/rtp_llm/tools/convert/weights_convert.py b/rtp_llm/tools/convert/weights_convert.py
index c622ff68a..34c54a021 100644
--- a/rtp_llm/tools/convert/weights_convert.py
+++ b/rtp_llm/tools/convert/weights_convert.py
@@ -11,11 +11,9 @@ from typing import Dict, Optional
 import torch
 from safetensors import safe_open
 
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.config.py_config_modules import StaticConfig
+from rtp_llm.config.model_config import ModelConfig
 from rtp_llm.distribute.worker_info import ParallelInfo
 from rtp_llm.model_factory import ModelFactory
-from rtp_llm.models.base_model import ModelConfig
 from rtp_llm.tools.api.model_basic_info_analyzer import (
     parse_ft_model_type,
     parse_model_basic_info,
@@ -102,7 +100,8 @@ class WeightConverter:
         return self.world_size if max_pool_size > self.world_size else max_pool_size
 
     def _estimate_max_convert_parallel_num(self):
-        converter_num_per_gpu = StaticConfig.load_config.converter_num_per_gpu
+        # Get converter_num_per_gpu from environment variable, default to 4
+        converter_num_per_gpu = int(os.environ.get("CONVERTER_NUM_PER_GPU", "4"))
         try:
             cuda_count = torch.cuda.device_count()
             assert cuda_count >= 1
@@ -126,11 +125,9 @@ class WeightConverter:
                     quantization=quantization,
                 )
                 paralle_info = ParallelInfo.from_params(env_params)
-                config: GptInitModelParameters = self.model_cls.create_config(
-                    model_config, paralle_info
-                )
+                config: ModelConfig = self.model_cls._create_config(self.model_path)
 
-                one_layer_model_size_mb = model_size_mb / config.layer_num
+                one_layer_model_size_mb = model_size_mb / config.num_layers
                 if model_size_mb < dump_buffer_size_mb:
                     need_size_mb = model_size_mb
                 else:
@@ -161,14 +158,15 @@ class WeightConverter:
         env_params.update({"WORLD_RANK": world_rank})
         env_params.update({"DP_RANK": dp_rank})
         env_params.update({"TP_RANK": tp_rank})
-        StaticConfig.update_from_env()
 
         quantization = ModelConfig.get_quantization_from_params(env_params)
+        # Get kv_cache_dtype from environment variable, default to empty string (auto)
+        kv_cache_dtype = env_params.get("KV_CACHE_DTYPE", os.environ.get("KV_CACHE_DTYPE", ""))
         model_config = ModelConfig(
             model_type=self.model_type,
             ckpt_path=self.model_path,
             act_type=env_params.get(ModelConfig.ACT_TYPE),
-            kv_cache_type=StaticConfig.py_kv_cache_config.kv_cache_dtype,
+            kv_cache_type=kv_cache_dtype,
             ptuning_path=None,
             max_seq_len=0,
             tokenizer_path=self.model_path,
@@ -176,10 +174,44 @@ class WeightConverter:
         )
         paralle_info = ParallelInfo.from_params(env_params)
         logging.info(f"begin convert model rank:{paralle_info}")
-        config: GptInitModelParameters = self.model_cls.create_config(
-            model_config, paralle_info
+        # Create config using _create_config
+        py_model_config: ModelConfig = self.model_cls._create_config(self.model_path)
+        # Apply model_config settings
+        py_model_config.ckpt_path = model_config.ckpt_path
+        py_model_config.tokenizer_path = model_config.tokenizer_path or model_config.ckpt_path
+        py_model_config.max_seq_len = model_config.max_seq_len or 0
+        py_model_config.quantization = model_config.quantization
+        py_model_config.act_type = model_config.act_type
+        py_model_config.model_type = self.model_type
+        
+        # Initialize precision config
+        py_model_config.init_precision_config(
+            py_model_config.act_type,
+            model_config.kv_cache_type if hasattr(model_config, 'kv_cache_type') else "",
+            None
+        )
+        
+        # Setup paths
+        py_model_config.setup_paths(
+            py_model_config.ckpt_path,
+            py_model_config.tokenizer_path,
+            model_config.ptuning_path,
+            py_model_config.max_seq_len,
+        )
+        
+        # Create minimal configs for model instantiation
+        from rtp_llm.config.engine_config import EngineConfig
+        from rtp_llm.config.mm_model_config import MMModelConfig
+        
+        mm_model_config = MMModelConfig()
+        engine_config = EngineConfig()
+        
+        model = self.model_cls.from_config(
+            py_model_config=py_model_config,
+            mm_model_config=mm_model_config,
+            engine_config=engine_config,
+            parallel_info=paralle_info,
         )
-        model = self.model_cls(config)
         loader = model.create_model_loader(paralle_info)
         max_retry_times = 3
         for i in range(max_retry_times):
diff --git a/rtp_llm/tools/quant/BUILD b/rtp_llm/tools/quant/BUILD
deleted file mode 100644
index f608688b7..000000000
--- a/rtp_llm/tools/quant/BUILD
+++ /dev/null
@@ -1,18 +0,0 @@
-package(default_visibility = ["//:__subpackages__"])
-
-py_library(
-    name = "quant",
-    srcs = glob([
-        "*.py",
-    ]),
-    deps = [
-        "//rtp_llm:utils",
-        "//rtp_llm:config",
-        "//rtp_llm:structure",
-        "//rtp_llm:models",
-        "//rtp_llm:openai_api",
-        "//rtp_llm/server:server",
-        "//rtp_llm/tools:model_assistant_api"
-    ]
-)
-
diff --git a/rtp_llm/tools/quant/README.md b/rtp_llm/tools/quant/README.md
deleted file mode 100644
index 318d8a87f..000000000
--- a/rtp_llm/tools/quant/README.md
+++ /dev/null
@@ -1,4 +0,0 @@
-examples:
-```bash
-LD_LIBRARY_PATH=/usr/local/cuda/compat/:/usr/local/nvidia/lib64/:/usr/lib64/:/usr/local/cuda/lib64/:${LD_LIBRARY_PATH} CUDA_VISIBLE_DEVICES="1,2,3,4" PYTHONPATH=/home/luoli.hn/work/FasterTransformer:$PYTHONPATH  /opt/conda310/bin/python rtp_llm/tools/quant/weights_quant.py --pretrained_model_dir="xxx" --output_dir_base="/home/luoli.hn/try/qwen-4bit" --dataset="/home/luoli.hn/work/try/taowise_sft_ec_agent_20240305_chatml_string.jsonl" --quant_type="GPTQ" --dataset_format="json" --quant_config='{"bits":4,"group_size":128, "desc_act": false}' --sample_strategy='{"distinct_attrs":["task"]}'
-```
\ No newline at end of file
diff --git a/rtp_llm/tools/quant/__init__.py b/rtp_llm/tools/quant/__init__.py
deleted file mode 100644
index 16a727ce6..000000000
--- a/rtp_llm/tools/quant/__init__.py
+++ /dev/null
@@ -1,22 +0,0 @@
-import importlib
-import pkgutil
-
-from .base_quanter import BaseQuanter
-
-package_name = __name__
-
-# 遍历当前包下的所有模块
-for _, module_name, _ in pkgutil.iter_modules(__path__, package_name + "."):
-    # 动态导入模块
-    module = importlib.import_module(module_name)
-    # 遍历模块中的所有属性
-    for attribute_name in dir(module):
-        attribute = getattr(module, attribute_name)
-        # 检查属性是否是 BaseQuanter 的子类
-        if (
-            isinstance(attribute, type)
-            and issubclass(attribute, BaseQuanter)
-            and attribute is not BaseQuanter
-        ):
-            # 在这里执行注册逻辑，例如调用 register 方法
-            attribute.register()
diff --git a/rtp_llm/tools/quant/awq_quanter.py b/rtp_llm/tools/quant/awq_quanter.py
deleted file mode 100644
index 7634c0171..000000000
--- a/rtp_llm/tools/quant/awq_quanter.py
+++ /dev/null
@@ -1,50 +0,0 @@
-import logging
-import os
-from typing import Dict, List
-
-import torch
-from awq import AutoAWQForCausalLM
-
-from rtp_llm.tools.quant.base_quanter import QUANT_TYPE, BaseQuanter
-
-
-class AwqQuanter(BaseQuanter):
-    def __init__(
-        self, quantize_config: Dict[str, str], model_path: str, offload_folder: str
-    ):
-        super().__init__()
-        self.quantize_config = quantize_config
-        max_memory = {}
-        per_gpu_max_memory = int(
-            torch.cuda.get_device_properties(torch.device("cuda:0")).total_memory
-            * 0.95
-            / 1024
-            / 1024
-            / 1024
-        )
-        cuda_devices = os.environ.get("CUDA_VISIBLE_DEVICES", None)
-        cuda_device_list = (
-            cuda_devices.split(",")
-            if cuda_devices is not None
-            else [str(i) for i in range(torch.cuda.device_count())]
-        )
-        max_memory.update(
-            {int(i): f"{per_gpu_max_memory}GIB" for i in range(len(cuda_device_list))}
-        )
-        logging.info(f"max_memory: {max_memory}")
-
-        model = AutoAWQForCausalLM.from_pretrained(model_path, True)
-        self.model = model.eval().half()
-
-    @classmethod
-    def quant_type(cls):
-        return QUANT_TYPE.AWQ
-
-    def _quant(self, examples: List[Dict[str, torch.Tensor]]):
-        examples = [_.get("input_ids").tolist() for _ in examples]
-        self.model.quantize(
-            tokenizer=None, quant_config=self.quantize_config, calib_data=examples
-        )
-
-    def _save_quantized(self, output_path: str):
-        self.model.save_quantized(output_path)
diff --git a/rtp_llm/tools/quant/base_quanter.py b/rtp_llm/tools/quant/base_quanter.py
deleted file mode 100644
index 3d8575423..000000000
--- a/rtp_llm/tools/quant/base_quanter.py
+++ /dev/null
@@ -1,90 +0,0 @@
-import logging
-from enum import Enum
-from typing import Any, Dict, List, Type
-
-import torch
-
-from rtp_llm.utils.time_util import Timer
-
-
-class QUANT_TYPE(Enum):
-    GPTQ = "gptq"
-    AWQ = "awq"
-    SMQ = "smq"
-    FP8 = "fp8"  # fix later
-
-    @classmethod
-    def from_str(cls, value: str) -> "QUANT_TYPE":
-        lower_value = value.lower()
-        for _, member in cls.__members__.items():
-            if lower_value == member.value:
-                return member
-        raise ValueError("No enum member with value %s" % value)
-
-    def to_str(self) -> str:
-        return self.value
-
-
-_quanter_factory: Dict[QUANT_TYPE, Type[Any]] = {}
-
-
-def register_quanter(quant_type: QUANT_TYPE, quanter_type: Any):
-    global _quanter_factory
-    if quant_type in _quanter_factory and _quanter_factory[quant_type] != quanter_type:
-        raise Exception(
-            f"try register quanter failed, quant_type: {quant_type} quanter:{quanter_type} conflict with {_quanter_factory[quant_type]}"
-        )
-
-    _quanter_factory[quant_type] = quanter_type
-
-
-class BaseQuanter:
-    @classmethod
-    def register(cls):
-        register_quanter(cls.quant_type(), cls)
-
-    @classmethod
-    def quant_type(cls):
-        raise NotImplementedError("quant_type method is not implement")
-
-    def quant(self, examples: List[Dict[str, torch.Tensor]]):
-        with Timer() as t:
-            self._quant(examples)
-        logging.info(f"quantize model use:{t.cost_ms()/1000:.0f}s")
-
-    def _quant(self, examples: List[Dict[str, torch.Tensor]]):
-        raise NotImplementedError("quant method is not implement")
-
-    def save_quantized_model(self, output_path: str):
-        save_ret = False
-        # output_path: str = fetch_remote_file_to_local(output_path, MountRwMode.RWMODE_RW)
-        for _ in range(3):
-            try:
-                self._save_quantized(output_path)
-                save_ret = True
-                break
-            except BaseException as e:
-                logging.warn(f"save to {output_path} failed, e: {str(e)}")
-
-        if not save_ret:
-            raise Exception(f"save to {output_path} failed")
-
-    def _save_quantized(self, output_path: str):
-        raise NotImplementedError("save method is not implement")
-
-
-class QuanterFactory:
-    @staticmethod
-    def get_quant_cls(quant_type: str):
-        global _quanter_factory
-        return _quanter_factory[quant_type]
-
-    @staticmethod
-    def create_quanter(
-        quant_type: QUANT_TYPE,
-        quantize_config: Dict[str, str],
-        model_path: str,
-        offload_folder: str,
-    ):
-        quanter_cls = QuanterFactory.get_quant_cls(quant_type)
-        return quanter_cls(quantize_config, model_path, offload_folder)
diff --git a/rtp_llm/tools/quant/datasets_adapter.py b/rtp_llm/tools/quant/datasets_adapter.py
deleted file mode 100644
index 8c60d4525..000000000
--- a/rtp_llm/tools/quant/datasets_adapter.py
+++ /dev/null
@@ -1,114 +0,0 @@
-import os
-from enum import Enum
-from typing import Any, Dict, NamedTuple, Optional
-from urllib.parse import urlparse
-
-import datasets
-
-from rtp_llm.utils.fuser import fetch_remote_file_to_local
-from rtp_llm.utils.time_util import timer_wrapper
-
-
-class DatasetParams(NamedTuple):
-    source: str
-    data_format: Optional[str] = None
-    load_args: Dict[str, Any] = {}
-
-
-class DatasetType(Enum):
-    RTP_LLM_ACCESS_LOG = 1
-    RTP_LLM_ACCESS_LOG_JSON_STR = 2
-    TEXT = 3
-    CHAT_PROMPT = 4
-
-    @classmethod
-    def from_str(cls, value: Optional[str]) -> "DatasetType":
-        lower_value = value.lower() if value else None
-        for name, member in cls.__members__.items():
-            if lower_value == name.lower():
-                return member
-        raise ValueError("No enum member with value %s" % value)
-
-
-class DatasetsAdapter:
-    @staticmethod
-    @timer_wrapper("load dataset")
-    def load_dataset(dataset_params: DatasetParams) -> datasets.Dataset:
-        # 如果是文件从文件load
-        source = dataset_params.source
-        data_format = dataset_params.data_format
-        parse_result = urlparse(source)
-        if parse_result.scheme:
-            # from remote, fetch remote to local
-            local_path = fetch_remote_file_to_local(source)
-            return DatasetsAdapter._load_dataset_from_local(
-                local_path, data_format, **dataset_params.load_args
-            )
-        elif os.path.exists(source):
-            return DatasetsAdapter._load_dataset_from_local(
-                source, data_format, **dataset_params.load_args
-            )
-        # 否则从已经存在的dataset中获取
-        return datasets.load_dataset(source, **dataset_params.load_args)
-
-    @staticmethod
-    def _load_dataset_from_local(
-        path: str, data_format: Optional[str] = None, **load_args
-    ):
-        if os.path.isdir(path):
-            assert len(os.listdir(path)) == 1
-            path = os.path.join(path, os.listdir(path)[0])
-
-        if not data_format:
-            if path.endswith((".json", ".jsonl")):
-                data_format = "json"
-            elif path.endswith(".csv"):
-                data_format = "csv"
-            elif path.endswith(".text"):
-                data_format = "text"
-            elif path.endswith(".pkl"):
-                data_format = "pandas"
-            elif path.endswith("access.log") or path.split("/")[-1].startswith(
-                "access.log-"
-            ):
-                data_format = "text"
-            else:
-                data_format = "text"
-        return datasets.load_dataset(data_format.lower(), data_files=path, **load_args)
-
-    @staticmethod
-    def parse_dataset_type(
-        dataset: datasets.Dataset, dataset_params: DatasetParams
-    ) -> Optional[DatasetType]:
-        if set(["log_time", "request.request_json", "response.responses"]).issubset(
-            set(dataset.column_names)
-        ):
-            return DatasetType.RTP_LLM_ACCESS_LOG
-        elif "prompt" in dataset.column_names:
-            return DatasetType.CHAT_PROMPT
-        elif "text" in dataset.column_names:
-            dataset_type = DatasetsAdapter.parse_dataset_type_from_dataset_params(
-                dataset_params
-            )
-            return dataset_type if dataset_type else DatasetType.TEXT
-        return DatasetType.from_str(dataset_params.data_format)
-
-    @staticmethod
-    def parse_dataset_type_from_dataset_params(dataset_params: DatasetParams):
-        source = dataset_params.source
-        parse_result = urlparse(source)
-        local_path: Optional[str] = None
-        if parse_result.scheme:
-            # from remote, fetch remote to local
-            local_path = fetch_remote_file_to_local(source)
-        elif os.path.exists(source):
-            local_path = source
-        else:
-            return None
-        if os.path.isdir(local_path):
-            assert len(os.listdir(local_path)) == 1
-            local_path = os.listdir(local_path)[0]
-        if local_path.endswith("access.log") or local_path.split("/")[-1].startswith(
-            "access.log-"
-        ):
-            return DatasetType.RTP_LLM_ACCESS_LOG_JSON_STR
diff --git a/rtp_llm/tools/quant/fp8_quanter.py b/rtp_llm/tools/quant/fp8_quanter.py
deleted file mode 100644
index e35d61633..000000000
--- a/rtp_llm/tools/quant/fp8_quanter.py
+++ /dev/null
@@ -1,254 +0,0 @@
-import copy
-import json
-import logging
-import os
-import time
-from typing import Dict, List
-
-import safetensors
-import torch
-from transformers import AutoConfig, AutoModelForCausalLM
-
-from rtp_llm.tools.quant.base_quanter import QUANT_TYPE, BaseQuanter
-
-"""
-FP8_DEFAULT_CFG = {
-    "quant_cfg": {
-        "*weight_quantizer": {"num_bits": (4, 3), "axis": None},
-        "*input_quantizer": {"num_bits": (4, 3), "axis": None},
-        "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-        "default": {"num_bits": (4, 3), "axis": None},
-    },
-    "algorithm": "max",
-}
-"""
-"""
-KV_CACHE_CFG = {
-    "*.query_key_value.output_quantizer": {
-        "num_bits": 8,
-        "axis": None,
-        "enable": True
-    },
-    "*.Wqkv.output_quantizer": {
-        "num_bits": 8,
-        "axis": None,
-        "enable": True
-    },
-    "*.W_pack.output_quantizer": {
-        "num_bits": 8,
-        "axis": None,
-        "enable": True
-    },
-    "*.c_attn.output_quantizer": {
-        "num_bits": 8,
-        "axis": None,
-        "enable": True
-    },
-    "*.k_proj.output_quantizer": {
-        "num_bits": 8,
-        "axis": None,
-        "enable": True
-    },
-    "*.v_proj.output_quantizer": {
-        "num_bits": 8,
-        "axis": None,
-        "enable": True
-    },
-}
-
-"""
-
-
-class Fp8Quanter(BaseQuanter):
-    FP8_DEFAULT_CFG = {
-        "quant_cfg": {
-            "*weight_quantizer": {"num_bits": (4, 3), "axis": None},
-            "*input_quantizer": {"num_bits": (4, 3), "axis": None},
-            "*block_sparse_moe.gate*": {"enable": False},  # Skip the MOE router
-            "default": {"num_bits": (4, 3), "axis": None},
-        },
-        "algorithm": "max",
-    }
-    KV_CACHE_CFG = {
-        "*.query_key_value.output_quantizer": {
-            "num_bits": 8,
-            "axis": None,
-            "enable": True,
-        },
-        "*.Wqkv.output_quantizer": {"num_bits": 8, "axis": None, "enable": True},
-        "*.W_pack.output_quantizer": {"num_bits": 8, "axis": None, "enable": True},
-        "*.c_attn.output_quantizer": {"num_bits": 8, "axis": None, "enable": True},
-        "*.k_proj.output_quantizer": {"num_bits": 8, "axis": None, "enable": True},
-        "*.v_proj.output_quantizer": {"num_bits": 8, "axis": None, "enable": True},
-    }
-
-    def __init__(
-        self, quantize_config: Dict[str, str], model_path: str, offload_folder: str
-    ):
-        super().__init__()
-        self.quantize_config = quantize_config
-
-        max_memory = {}
-        per_gpu_max_memory = int(
-            torch.cuda.get_device_properties(torch.device("cuda:0")).total_memory
-            * 0.95
-            / 1024
-            / 1024
-            / 1024
-        )
-        cuda_devices = os.environ.get("CUDA_VISIBLE_DEVICES", None)
-        cuda_device_list = (
-            cuda_devices.split(",")
-            if cuda_devices is not None
-            else [str(i) for i in range(torch.cuda.device_count())]
-        )
-        max_memory.update(
-            {int(i): f"{per_gpu_max_memory}GIB" for i in range(len(cuda_device_list))}
-        )
-        logging.info(f"max_memory: {max_memory}")
-
-        model = AutoModelForCausalLM.from_pretrained(
-            model_path,
-            torch_dtype="auto",
-            low_cpu_mem_usage=True,
-            trust_remote_code=True,
-            offload_folder=offload_folder,
-            max_memory=max_memory,
-        )
-        self.model = model.eval().half()
-
-        self.quant_cfg = copy.deepcopy(Fp8Quanter.FP8_DEFAULT_CFG)
-        kv_cache_dtype = quantize_config.get("kv_cache_dtype", None)
-        if kv_cache_dtype is not None:
-            if kv_cache_dtype == "fp8":
-                for value in Fp8Quanter.KV_CACHE_CFG.values():
-                    value.update({"num_bits": (4, 3)})  # type: ignore
-            self.quant_cfg["quant_cfg"].update(Fp8Quanter.KV_CACHE_CFG)  # type: ignore
-
-    def _quant(self, examples: List[Dict[str, torch.Tensor]]):
-        examples = [_.get("input_ids").tolist() for _ in examples]
-        import modelopt.torch.quantization as atq
-
-        def calibrate_loop():
-            if examples is None:
-                return
-            """Adjusts weights and scaling factors based on selected algorithms."""
-            for idx, example in enumerate(examples):
-                print(f"Calibrating batch {idx}")
-                example = torch.cat(example, dim=0)
-                # model might be mapped to different device because the device_map is auto
-                self.model(example.to(next(self.model.parameters()).device))
-
-        print("Starting quantization...")
-        start_time = time.time()
-        atq.quantize(self.model, self.quant_cfg, forward_loop=calibrate_loop)
-        end_time = time.time()
-        print(
-            "Quantization done. Total time used: {:.2f} s.".format(
-                end_time - start_time
-            )
-        )
-
-    @classmethod
-    def quant_type(cls):
-        return QUANT_TYPE.FP8
-
-    def _save_quantized(self, output_path: str):
-        with torch.inference_mode():
-            if model_type is None:
-                print(
-                    f"Unknown model type {type(model).__name__}. Continue exporting..."
-                )
-                model_type = f"unknown:{type(model).__name__}"
-
-            export_path = output_path
-            start_time = time.time()
-            from modelopt.torch.export import export_tensorrt_llm_checkpoint
-
-            export_tensorrt_llm_checkpoint(
-                self.model,
-                model_type,
-                getattr(torch, dtype),
-                export_dir=export_path,
-                inference_tensor_parallel=1,
-                inference_pipeline_parallel=1,
-            )
-
-            with open(f"{export_path}/config.json", "r") as f:
-                tensorrt_llm_config = json.load(f)
-
-            # Workaround for MOE router quantization
-            if "moe_num_experts" in tensorrt_llm_config and qformat != "full_prec":
-                if "exclude_modules" not in tensorrt_llm_config["quantization"]:
-                    # Append router and lm_head because we need both excluded
-                    tensorrt_llm_config["quantization"]["exclude_modules"] = [
-                        "router",
-                        "lm_head",
-                    ]
-                else:
-                    tensorrt_llm_config["quantization"]["exclude_modules"].append(
-                        "router"
-                    )
-
-            with open(f"{export_path}/config.json", "w") as f:
-                json.dump(tensorrt_llm_config, f, indent=4)
-
-            # Workaround for Modelopt 0.9.x fp8_kv_cache knob issue
-            if qformat == "fp8" and kv_cache_dtype is None:
-                with open(f"{export_path}/config.json", "r") as f:
-                    tensorrt_llm_config = json.load(f)
-                tensorrt_llm_config["quantization"]["kv_cache_quant_algo"] = None
-                with open(f"{export_path}/config.json", "w") as f:
-                    json.dump(tensorrt_llm_config, f, indent=4)
-
-            # Workaround for share_embedding_table
-            if pp_size == 1:
-                with safetensors.safe_open(
-                    f"{export_path}/rank0.safetensors", framework="pt", device="cpu"
-                ) as f:
-                    share_embedding_table = "lm_head.weight" not in f.keys()
-                if share_embedding_table:
-                    with open(f"{export_path}/config.json", "r") as f:
-                        tensorrt_llm_config = json.load(f)
-                    tensorrt_llm_config["share_embedding_table"] = True
-                    with open(f"{export_path}/config.json", "w") as f:
-                        json.dump(tensorrt_llm_config, f, indent=4)
-
-            # Workaround for gpt2 position embedding
-            if model_type == "gpt2":
-                for rank in range(tp_size):
-                    weights = {}
-                    with safetensors.safe_open(
-                        f"{export_path}/rank{rank}.safetensors",
-                        framework="pt",
-                        device="cpu",
-                    ) as f:
-                        for key in f.keys():
-                            weights[key] = f.get_tensor(key)
-                    if "transformer.positional_embedding.weight" in weights:
-                        weights["transformer.position_embedding.weight"] = weights.pop(
-                            "transformer.positional_embedding.weight"
-                        )
-                    safetensors.torch.save_file(
-                        weights, f"{export_path}/rank{rank}.safetensors"
-                    )
-
-            # Workaround for qwen version
-            if model_type == "qwen":
-                with open(f"{export_path}/config.json", "r") as f:
-                    tensorrt_llm_config = json.load(f)
-                qwen_config = AutoConfig.from_pretrained(
-                    model_dir, trust_remote_code=True
-                )
-                tensorrt_llm_config["qwen_type"] = qwen_config.model_type
-                tensorrt_llm_config["intermediate_size"] = qwen_config.intermediate_size
-                with open(f"{export_path}/config.json", "w") as f:
-                    json.dump(tensorrt_llm_config, f, indent=4)
-
-            torch.cuda.empty_cache()  # otherwise torch is keeping using GPU, other routine like build engine has less free GPU to use
-            end_time = time.time()
-            print(
-                "Quantized model exported to {} \nTotal time used {:.2f} s.".format(
-                    export_path, end_time - start_time
-                )
-            )
diff --git a/rtp_llm/tools/quant/gptq_quanter.py b/rtp_llm/tools/quant/gptq_quanter.py
deleted file mode 100644
index cc75942fc..000000000
--- a/rtp_llm/tools/quant/gptq_quanter.py
+++ /dev/null
@@ -1,54 +0,0 @@
-import logging
-import os
-from typing import Dict, List
-
-import torch
-from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
-
-from rtp_llm.tools.quant.base_quanter import QUANT_TYPE, BaseQuanter
-
-
-class GptqQuanter(BaseQuanter):
-    def __init__(
-        self, quantize_config: Dict[str, str], model_path: str, offload_folder: str
-    ):
-        super().__init__()
-        self.quantize_config = quantize_config
-        quant_config = BaseQuantizeConfig(**quantize_config)
-        max_memory = {}
-        per_gpu_max_memory = int(
-            torch.cuda.get_device_properties(torch.device("cuda:0")).total_memory
-            * 0.95
-            / 1024
-            / 1024
-            / 1024
-        )
-        cuda_devices = os.environ.get("CUDA_VISIBLE_DEVICES", None)
-        cuda_device_list = (
-            cuda_devices.split(",")
-            if cuda_devices is not None
-            else [str(i) for i in range(torch.cuda.device_count())]
-        )
-        max_memory.update(
-            {int(i): f"{per_gpu_max_memory}GIB" for i in range(len(cuda_device_list))}
-        )
-        logging.info(f"max_memory: {max_memory}")
-        model = AutoGPTQForCausalLM.from_pretrained(
-            model_path,
-            quantize_config=quant_config,
-            low_cpu_mem_usage=True,
-            trust_remote_code=True,
-            offload_folder=offload_folder,
-            max_memory=max_memory,
-        )
-        self.model = model.eval().half()
-
-    def _quant(self, examples: List[Dict[str, torch.Tensor]]):
-        self.model.quantize(examples)
-
-    @classmethod
-    def quant_type(cls):
-        return QUANT_TYPE.GPTQ
-
-    def _save_quantized(self, output_path: str):
-        self.model.save_quantized(output_path)
diff --git a/rtp_llm/tools/quant/weights_quant.py b/rtp_llm/tools/quant/weights_quant.py
deleted file mode 100644
index 6b6a0ba81..000000000
--- a/rtp_llm/tools/quant/weights_quant.py
+++ /dev/null
@@ -1,508 +0,0 @@
-import argparse
-import hashlib
-import json
-import logging
-import logging.config
-import math
-import os
-import random
-import shutil
-import time
-from collections import Counter
-from typing import Any, Dict, List, NamedTuple, Optional, Union
-
-import datasets
-import numpy as np
-import torch
-from transformers import PreTrainedTokenizer, PreTrainedTokenizerBase
-
-from rtp_llm.config.exceptions import ExceptionType, FtRuntimeException
-from rtp_llm.config.generate_config import GenerateConfig, RequestFormat
-from rtp_llm.config.gpt_init_model_parameters import GptInitModelParameters
-from rtp_llm.frontend.tokenizer_factory.tokenizer_factory import TokenizerFactory
-from rtp_llm.model_factory import ModelFactory
-from rtp_llm.models.base_model import ModelConfig
-from rtp_llm.openai.api_datatype import ChatCompletionRequest
-from rtp_llm.openai.renderer_factory import ChatRendererFactory
-from rtp_llm.openai.renderers.custom_renderer import RendererParams
-from rtp_llm.pipeline.default_plugin import DefaultPlugin
-from rtp_llm.structure.request_extractor import RequestExtractor
-from rtp_llm.tools.api.model_basic_info_analyzer import parse_ft_model_type
-from rtp_llm.tools.quant.base_quanter import QUANT_TYPE, BaseQuanter, QuanterFactory
-from rtp_llm.tools.quant.datasets_adapter import (
-    DatasetParams,
-    DatasetsAdapter,
-    DatasetType,
-)
-from rtp_llm.utils.fuser import MountRwMode, fetch_remote_file_to_local
-from rtp_llm.utils.time_util import Timer, timer_wrapper
-from rtp_llm.utils.weight_type import WEIGHT_TYPE
-
-CUR_PATH: str = os.path.dirname(os.path.abspath(__file__))
-
-
-from auto_gptq.modeling._base import BaseGPTQForCausalLM
-
-
-def register_gptq_models(hf_model_type: str, model_gptq_cls: Type[BaseGPTQForCausalLM]):
-    from auto_gptq.modeling.auto import GPTQ_CAUSAL_LM_MODEL_MAP
-
-    registed_model_gptq_cls = GPTQ_CAUSAL_LM_MODEL_MAP.get(hf_model_type)
-    if registed_model_gptq_cls and registed_model_gptq_cls != model_gptq_cls:
-        raise Exception(
-            f"try register {hf_model_type}'s gtpq_cls {model_gptq_cls} confict with registed_cls: {registed_model_gptq_cls}"
-        )
-    GPTQ_CAUSAL_LM_MODEL_MAP.update({hf_model_type: model_gptq_cls})
-    logging.info(f"register {hf_model_type}'s gtpq_cls {model_gptq_cls}")
-
-    from auto_gptq.modeling._const import SUPPORTED_MODELS
-
-    if hf_model_type not in SUPPORTED_MODELS:
-        SUPPORTED_MODELS.append(hf_model_type)
-        logging.info("append {hf_model_type} to gptq supported_models")
-    else:
-        logging.info(f"hf_model_type:[{hf_model_type}] have been registed")
-
-
-from awq.models.base import BaseAWQForCausalLM
-
-
-def register_awq_models(hf_model_type: str, model_gptq_cls: Type[BaseAWQForCausalLM]):
-    from awq.models.auto import AWQ_CAUSAL_LM_MODEL_MAP
-
-    registed_model_awq_cls = AWQ_CAUSAL_LM_MODEL_MAP.get(hf_model_type)
-    if registed_model_awq_cls and registed_model_awq_cls != model_gptq_cls:
-        raise Exception(
-            f"try register {hf_model_type}'s awq_cls {model_gptq_cls} confict with registed_cls: {registed_model_awq_cls}"
-        )
-    AWQ_CAUSAL_LM_MODEL_MAP.update({hf_model_type: model_gptq_cls})
-    logging.info(f"register {hf_model_type}'s awq_cls {model_gptq_cls}")
-
-    from awq.models.base import TRANSFORMERS_AUTO_MAPPING_DICT
-
-    if hf_model_type not in TRANSFORMERS_AUTO_MAPPING_DICT:
-        TRANSFORMERS_AUTO_MAPPING_DICT[hf_model_type] = "AutoModelForCausalLM"
-        logging.info("append {hf_model_type} to gptq supported_models")
-    else:
-        logging.info(f"hf_model_type:[{hf_model_type}] have been registed")
-
-
-class SampleStrategy(NamedTuple):
-    distinct_attrs: List[str] = []
-    sample_size: int = 128
-
-
-class WeightsQuantizer:
-    def __init__(
-        self,
-        model_path: str,
-        model_type: Optional[str] = None,
-        offload_folder: Optional[str] = None,
-    ):
-        self.model_path = fetch_remote_file_to_local(model_path)
-        if not offload_folder:
-            temp_offload_folder = f'{hashlib.md5(self.model_path.encode("utf-8")).hexdigest()}_{time.time()}'
-            self.offload_folder = os.path.join(CUR_PATH, temp_offload_folder)
-            os.makedirs(self.offload_folder, exist_ok=True)
-        else:
-            self.offload_folder = offload_folder
-
-        assert self.model_path
-        if not model_type:
-            model_type = parse_ft_model_type(self.model_path).get("ft_model_type", None)
-            assert model_type
-        self.model_type = model_type
-
-        self.model_cls = ModelFactory.get_model_cls(self.model_type)
-        model_config = ModelConfig(
-            model_type=self.model_type,
-            ckpt_path=self.model_path,
-            act_type=WEIGHT_TYPE.FP16.to_str(),
-            ptuning_path=None,
-            max_seq_len=0,
-            tokenizer_path=self.model_path,
-        )
-        self.config: GptInitModelParameters = self.model_cls.create_config(model_config)
-        self.tokenizer = TokenizerFactory.create_from_env()
-        self.special_tokens = self.config.special_tokens
-        logging.info(f"max_seq_len:{self.tokenizer.model_max_length}")
-
-        self.eos_token_id = self.tokenizer.eos_token_id
-        if self.eos_token_id == None:
-            self.eos_token_id = self.config.special_tokens.eos_token_id
-
-        self.stop_words_id_list = self.config.special_tokens.stop_words_id_list
-
-        render_params = RendererParams(
-            model_type=model_type,
-            max_seq_len=self.config.max_seq_len,
-            eos_token_id=self.eos_token_id,
-            stop_word_ids_list=self.stop_words_id_list,
-        )
-
-        self._open_ai_request_render = self.chat_renderer = (
-            ChatRendererFactory.get_renderer(self.tokenizer, render_params)
-        )
-
-    def quantize(
-        self,
-        quant_type_str: str,
-        quantize_config: Dict[str, str],
-        dataset_params: DatasetParams,
-        sample_strategy: SampleStrategy,
-        output_path: str,
-    ):
-        ret_code = 0
-        try:
-            dataset = DatasetsAdapter.load_dataset(dataset_params)
-            if isinstance(dataset, datasets.dataset_dict.DatasetDict):
-                dataset = dataset["train"]
-
-            dataset = self.stratified_sampling_by_attributes(dataset, sample_strategy)
-
-            quant_type = QUANT_TYPE.from_str(quant_type_str)
-            quanter = self.create_quanter(quant_type, quantize_config)
-
-            examples = self.create_tokenized_samples(dataset, dataset_params)
-
-            with Timer() as t:
-                # quanter.quantize(examples)
-                examples_for_quant: List[Dict[str, torch.Tensor]] = [
-                    {
-                        "input_ids": input_ids,
-                        "attention_mask": torch.ones_like(input_ids),
-                    }
-                    for input_ids in examples
-                ]
-                quanter.quant(examples_for_quant)
-            logging.info(f"quantize model use:{t.cost_ms()/1000:.0f}s")
-            save_ret = False
-            try:
-                self._save_quantized_model(quanter, quantize_config, output_path)
-                save_ret = True
-            except BaseException as e:
-                logging.warn(f"save to {output_path} failed, e: {str(e)}")
-
-            if not save_ret:
-                raise Exception(f"save to {output_path} failed")
-
-        except BaseException as e:
-            logging.info(f"run failed : {e}")
-            logging.exception(e)
-            ret_code = -1
-        finally:
-            self.release_temp_resource()
-            return ret_code
-
-    @staticmethod
-    @timer_wrapper("dataset sample")
-    def stratified_sampling_by_attributes(dataset, sample_strategy: SampleStrategy):
-        attributes = sample_strategy.distinct_attrs
-        if sample_strategy.sample_size > dataset.num_rows:
-            return dataset
-
-        sample_size = sample_strategy.sample_size
-        if not attributes:
-            indices = random.sample(range(dataset.num_rows), sample_size)
-            return dataset.select(indices)
-
-        # 1. 创建一个组合键，它将属性列表中的所有属性结合起来
-        def group_key(example):
-            return {"group_key": "+".join([str(example[attr]) for attr in attributes])}
-
-        # 2. 为每个样本添加组合键
-        dataset = dataset.map(group_key)
-
-        # 3. 获取每个组合键的计数
-        group_counts = Counter(dataset["group_key"])
-        total_count = dataset.num_rows
-
-        # 4. 计算每个组合键的采样比例，并确定应该采样的数量
-        proportional_sample_sizes = {
-            group_key: math.ceil(sample_size * count / total_count)
-            for group_key, count in group_counts.items()
-        }
-
-        # 5. 补偿四舍五入造成的样本数量差异
-        while sum(proportional_sample_sizes.values()) > sample_size:
-            # 从计数最多的组开始减少
-            group_key_to_reduce = max(
-                proportional_sample_sizes,
-                key=lambda k: (proportional_sample_sizes[k], group_counts[k]),
-            )
-            proportional_sample_sizes[group_key_to_reduce] -= 1
-
-        # 6. 从每个分组采样
-        sampled_indices = []
-        for group_key, size in proportional_sample_sizes.items():
-            group_indices = [
-                i
-                for i, example in enumerate(dataset["group_key"])
-                if example == group_key
-            ]
-            if len(group_indices) <= size:
-                sampled_indices.extend(group_indices)
-            else:
-                sampled_indices.extend(
-                    np.random.choice(group_indices, size=size, replace=False)
-                )
-
-        # 7. 从数据集中选择采样的样本
-        sampled_dataset = dataset.select(sampled_indices)
-        return sampled_dataset
-
-    @timer_wrapper("pretrain load model")
-    def create_quanter(
-        self, quant_type: QUANT_TYPE, quantize_config: Dict[str, str]
-    ) -> BaseQuanter:
-        return QuanterFactory.create_quanter(
-            quant_type, quantize_config, self.model_path, self.offload_folder
-        )
-
-    @timer_wrapper("encode samples")
-    def create_tokenized_samples(
-        self, dataset: datasets.Dataset, dataset_params: DatasetParams
-    ) -> List[torch.Tensor]:
-        # 根据dataset 的类型获取
-        dataset_type: Optional[DatasetType] = DatasetsAdapter.parse_dataset_type(
-            dataset, dataset_params
-        )
-        if not dataset_type:
-            raise TypeError(f"not support this dataset:{dataset}]")
-
-        if dataset_type == DatasetType.RTP_LLM_ACCESS_LOG:
-            return self._encode_ft_access_log(dataset)
-        elif dataset_type == DatasetType.RTP_LLM_ACCESS_LOG_JSON_STR:
-            return self._encode_ft_acccess_log_json_str(dataset)
-        elif dataset_type == DatasetType.TEXT:
-            return self._encode_text(dataset)
-
-    @timer_wrapper("release resource")
-    def release_temp_resource(self):
-        def rm_path(directory):
-            if os.path.exists(directory):
-                shutil.rmtree(directory)
-                logging.info(f"The directory {directory} has been removed successfully")
-            else:
-                logging.info(
-                    f"The directory {directory} does not exist, nothing to remove"
-                )
-
-        rm_path(self.offload_folder)
-
-    @timer_wrapper("save quantized model")
-    def _save_quantized_model(
-        self, quanter: BaseQuanter, quantize_config: Dict[str, str], output_path: str
-    ):
-        output_path = fetch_remote_file_to_local(output_path, MountRwMode.RWMODE_RW)
-        quanter.save_quantized_model(output_path)
-        self.tokenizer.save_pretrained(output_path)
-        # rewrite config.json
-        quantize_config.update({"quant_method": quanter.quant_type().to_str()})
-        # load config.json
-        config_file = os.path.join(output_path, "config.json")
-        logging.info(f"rewrite config_file:{config_file}")
-        with open(config_file) as f:
-            config = json.load(f)
-        quantize_config.update(config.get("quantization_config", {}))
-        config.update({"quantization_config": quantize_config})
-        with open(config_file, "w") as f:
-            json.dump(config, f, indent=4, sort_keys=True, ensure_ascii=False)
-        # cp tokenizer and model
-
-        # touch done
-        done_file = os.path.join(output_path, "done")
-        with open(done_file, "w") as f:
-            pass
-
-    def _encode_ft_access_log(self, dataset: datasets.Dataset) -> List[torch.Tensor]:
-        samples = []
-        for data in dataset:
-            request_json_str: str = data["request.request_json"]
-            try:
-                request = json.loads(request_json_str)
-            except Exception:
-                continue
-
-            if not request:
-                continue
-
-            response_json_str: str = data["response.responses"]
-            if not response_json_str:
-                continue
-            try:
-                responses = json.loads(response_json_str)
-            except Exception:
-                continue
-            if not responses:
-                continue
-
-            response = responses[0]
-
-            if request.get("messages"):
-                token_ids = self._encode_openai_request(request, response)
-            else:
-                token_ids = self._encode_pipeline_request(request, response)
-            if token_ids is not None:
-                samples.append(token_ids)
-
-        return samples
-
-    def _encode_ft_acccess_log_json_str(
-        self, dataset: datasets.Dataset
-    ) -> List[torch.Tensor]:
-        # DOTO(luoli.hn) 某些任务需要根据指定字段进行采样
-        samples = []
-        for data in dataset:
-            raw_data_str = data["text"]
-            try:
-                raw_data = json.loads(raw_data_str)
-            except Exception:
-                continue
-            request = raw_data.get("request", {}).get("request_json", None)
-            response = raw_data.get("response", {}).get("responses", None)
-            if not request or not response:
-                continue
-
-            if "messages" in request:
-                token_ids = self._encode_openai_request(request, response)
-            else:
-                token_ids = self._encode_pipeline_request(response, request)
-            if token_ids is not None:
-                samples.append(token_ids)
-        return samples
-
-    def _encode_text(self, dataset: datasets.Dataset) -> List[torch.Tensor]:
-        samples = []
-        for data in dataset:
-            text_data = data["text"]
-            text_data = json.loads(text_data)
-            assert isinstance(text_data, str)
-            token_ids = self.tokenizer.encode(text_data)
-            if token_ids and len(token_ids) < self.config.max_seq_len:
-                samples.append(torch.tensor(token_ids, dtype=torch.int))
-                logging.info(f"sample length: {len(samples[-1])}")
-        return samples
-
-    def _encode_openai_request(
-        self, request: Dict[str, Any], response: Dict[str, Any]
-    ) -> torch.Tensor:
-        # concat request & response
-        response_choices = response[0].get("choices")
-        request["messages"].extend([choice["message"] for choice in response_choices])
-        chat_request = ChatCompletionRequest.model_validate(request)
-
-        rendered_input = self.chat_renderer.render_chat(chat_request)
-        input_ids = torch.tensor(rendered_input.input_ids, dtype=torch.int)
-        return input_ids
-
-    def _encode_pipeline_request(
-        self, request: Dict[str, Any], response: Dict[str, Any]
-    ) -> None:
-        inference_request, remain_args = RequestExtractor(
-            GenerateConfig()
-        ).extract_request(request)
-        if inference_request.batch_infer or inference_request.num_return_sequences > 1:
-            return None
-
-        if (
-            inference_request.generate_configs[0].request_format
-            == RequestFormat.CHAT_API
-        ):
-            response_str = response.get("response")[0]
-            if isinstance(inference_request.input_texts[0], str):
-                inference_request.input_texts[0] = json.loads(
-                    inference_request.input_texts[0]
-                )
-            inference_request.input_texts[0].append(
-                {"role": "assistant", "content": response_str}
-            )
-        else:
-            inference_request.input_texts[0] = (
-                inference_request.input_texts[0] + response.get("response")[0]
-            )
-        prompt = inference_request.input_texts[0]
-        if len(prompt) == 0:
-            raise FtRuntimeException(
-                ExceptionType.EMPTY_PROMPT_ERROR,
-                "prompt should have at least one token!",
-            )
-        if type(prompt) is not str:
-            raise FtRuntimeException(
-                ExceptionType.ERROR_INPUT_FORMAT_ERROR,
-                "expect string prompt, actual: " + str(prompt),
-            )
-        token_ids = self.tokenizer.encode(prompt)
-        return torch.tensor(token_ids, dtype=torch.int)
-
-
-def main():
-    # 创建 ArgumentParser 对象
-    parser = argparse.ArgumentParser(description="Quantize model weights.")
-
-    # 添加参数
-    parser.add_argument(
-        "--pretrained_model_dir", type=str, help="Pretrained model path"
-    )
-    parser.add_argument("--output_dir_base", type=str, help="Output base folder")
-    parser.add_argument("--dataset", type=str, help="Output base folder")
-    parser.add_argument("--quant_type", type=str, help="Quant type.")
-    parser.add_argument(
-        "--dataset_format",
-        type=str,
-        default="",
-        help="[Optinal] dataset_format for load_dataset, when dataset is file.",
-    )
-    parser.add_argument(
-        "--dataset_load_args",
-        type=str,
-        default="{}",
-        help="[Optinal] Json desc the Args : used for load dataset",
-    )
-    parser.add_argument(
-        "--model_type",
-        type=str,
-        default="",
-        help="[Optinal] the model_type to be quantized.",
-    )
-    parser.add_argument(
-        "--quant_config", type=str, help="Json desc the quantization config"
-    )
-    parser.add_argument(
-        "--workdir_path",
-        type=str,
-        default=None,
-        help="Json desc the quantization config",
-    )
-    parser.add_argument(
-        "--sample_strategy",
-        type=str,
-        default="{}",
-        help="Strategy for sample dateset will be used to quantize model",
-    )
-
-    # 解析参数
-    args = parser.parse_args()
-    weights_quantizer = WeightsQuantizer(
-        args.pretrained_model_dir, args.model_type, args.workdir_path
-    )
-
-    dataset_params = DatasetParams(
-        source=args.dataset,
-        data_format=args.dataset_format,
-        load_args=json.loads(args.dataset_load_args),
-    )
-    sample_strategy = SampleStrategy(**json.loads(args.sample_strategy))
-    ret_code = weights_quantizer.quantize(
-        args.quant_type,
-        json.loads(args.quant_config),
-        dataset_params,
-        sample_strategy,
-        args.output_dir_base,
-    )
-    exit(ret_code)
-
-
-if __name__ == "__main__":
-    # logging.config.dictConfig(LOGGING_CONFIG)
-    main()
diff --git a/rtp_llm/utils/base_model_datatypes.py b/rtp_llm/utils/base_model_datatypes.py
index 41d6af6d4..66e1b18d4 100644
--- a/rtp_llm/utils/base_model_datatypes.py
+++ b/rtp_llm/utils/base_model_datatypes.py
@@ -3,10 +3,8 @@ from typing import Any, Dict, List, NamedTuple, Optional
 import torch
 from pydantic import BaseModel as PyBaseModel
 
-from rtp_llm.config.generate_config import GenerateConfig, RoleAddr, RoleType
+from rtp_llm.config.generate_config import GenerateConfig, RoleAddr
 from rtp_llm.utils.multimodal_util import MultimodalInput
-from rtp_llm.utils.weight_type import WEIGHT_TYPE
-
 
 class EmbeddingOutput:
     text_embedding: torch.Tensor
@@ -127,82 +125,3 @@ class GenerateContext(NamedTuple):
     all_start_time: Any
     cache_indirection: Any
     output_token_ids: Any
-
-
-class ModelConfig:
-    KV_CACHE_DTYPE = "KV_CACHE_DTYPE"
-    QUANTIZATION_KEY = "QUANTIZATION"
-    ACT_TYPE = "ACT_TYPE"
-    WEIGHT_TYPE = "WEIGHT_TYPE"  # Compatible for old config
-    INT8_MODE = "INT8_MODE"  # Compatible for old config
-
-    SP_KV_CACHE_DTYPE = "SP_KV_CACHE_DTYPE"
-    SP_QUANTIZATION_KEY = "SP_QUANTIZATION"
-    SP_ACT_TYPE = "SP_ACT_TYPE"
-    SP_WEIGHT_TYPE = "SP_WEIGHT_TYPE"  # Compatible for old config
-
-    def __init__(
-        self,
-        model_type: str = "",
-        ckpt_path: str = "",
-        tokenizer_path: str = "",
-        act_type: str = None,
-        kv_cache_type: str = None,
-        max_seq_len: int = 0,
-        seq_size_per_block: int = 8,
-        gen_num_per_circle: int = 1,
-        ptuning_path: Optional[str] = None,
-        lora_infos: Optional[Dict[str, str]] = None,
-        ref_module: Optional[torch.nn.Module] = None,
-        ref_dict: Dict[str, torch.Tensor] = {},
-        sp_type: str = "",
-        quantization: str = "",
-    ):
-        self.model_type: str = model_type
-        self.ckpt_path: str = ckpt_path
-        self.tokenizer_path: str = tokenizer_path
-        self.act_type: str = act_type
-        self.kv_cache_type: str = kv_cache_type
-        self.quantization = quantization
-        self.max_seq_len: int = max_seq_len
-        self.seq_size_per_block: int = seq_size_per_block
-        self.gen_num_per_circle: int = gen_num_per_circle
-        self.ptuning_path: Optional[str] = ptuning_path
-        self.lora_infos: Optional[Dict[str, str]] = lora_infos
-        self.ref_module: Optional[torch.nn.Module] = ref_module
-        self.ref_dict: Dict[str, torch.Tensor] = ref_dict
-        self.sp_type: str = sp_type
-
-    def add_ref_module(self, ref_module: Optional[torch.nn.Module]):
-        self.ref_module = ref_module
-
-    def add_ref_dict(self, ref_dict: Dict[str, torch.Tensor]):
-        self.ref_dict = ref_dict
-
-    def _replace(self, **kwargs: Any):
-        for k, v in kwargs.items():
-            if k in self.__dict__:
-                self.__dict__[k] = v
-        return self
-
-    @staticmethod
-    def get_quantization_from_params(env_params: Dict[str, str]):
-        if (not env_params.get(ModelConfig.QUANTIZATION_KEY)) and (
-            env_params.get(ModelConfig.WEIGHT_TYPE, "").upper() == "INT8"
-            or int(env_params.get(ModelConfig.INT8_MODE, "0")) == 1
-        ):
-            quantization = "INT8"
-        else:
-            quantization = env_params.get(ModelConfig.QUANTIZATION_KEY)
-        return quantization
-
-    @staticmethod
-    def get_sp_quantization_from_params(env_params: Dict[str, str]):
-        if not env_params.get(ModelConfig.SP_QUANTIZATION_KEY) and (
-            env_params.get(ModelConfig.SP_WEIGHT_TYPE, "").upper() == "INT8"
-            or int(env_params.get(ModelConfig.INT8_MODE, "0")) == 1
-        ):
-            quantization = "INT8"
-        else:
-            quantization = env_params.get(ModelConfig.SP_QUANTIZATION_KEY)
-        return quantization
diff --git a/rtp_llm/utils/concurrency_controller.py b/rtp_llm/utils/concurrency_controller.py
index fba1117fc..bf7a359e8 100644
--- a/rtp_llm/utils/concurrency_controller.py
+++ b/rtp_llm/utils/concurrency_controller.py
@@ -3,7 +3,6 @@ import os
 from multiprocessing import Lock, Value
 from typing import Optional
 
-from rtp_llm.config.py_config_modules import StaticConfig
 from rtp_llm.distribute.worker_info import g_parallel_info
 
 
@@ -52,9 +51,19 @@ class ConcurrencyController:
 global_controller: Optional[ConcurrencyController] = None
 
 
-def init_controller():
-    concurrency_limit = StaticConfig.concurrency_config.concurrency_limit
-    global_concurrency_limit = concurrency_limit * g_parallel_info.dp_size
+def init_controller(concurrency_config, dp_size=None):
+    """Initialize concurrency controller.
+    
+    Args:
+        concurrency_config: ConcurrencyConfig object.
+        dp_size: Data parallel size. If None, uses g_parallel_info.dp_size.
+    """
+    
+    if dp_size is None:
+        dp_size = g_parallel_info.dp_size
+    
+    concurrency_limit = concurrency_config.concurrency_limit
+    global_concurrency_limit = concurrency_limit * dp_size
     logging.info(
         f"concurrency_limit : {concurrency_limit}, global_concurrency_limit : {global_concurrency_limit}"
     )
@@ -72,6 +81,9 @@ def get_global_controller() -> ConcurrencyController:
         return global_controller
 
     if int(os.environ.get("FT_SERVER_TEST", 0)) == 1:
-        set_global_controller(init_controller())
+        # For test mode, create a default controller
+        from rtp_llm.config.py_config_modules import ConcurrencyConfig
+        default_config = ConcurrencyConfig()
+        set_global_controller(init_controller(default_config))
 
     return global_controller  # type: ignore
diff --git a/rtp_llm/utils/dump_config_utils.py b/rtp_llm/utils/dump_config_utils.py
index 2cc387bbd..15246fe40 100644
--- a/rtp_llm/utils/dump_config_utils.py
+++ b/rtp_llm/utils/dump_config_utils.py
@@ -3,25 +3,6 @@ from typing import Any, Dict, List, NamedTuple
 
 import prettytable as pt
 
-
-def dump_model_to_table(config_map: Dict[str, Any]):
-    return dump_config_to_table("MODEL CONFIG", config_map)
-
-
-def dump_engine_to_table(config_map: Dict[str, Any]):
-    return dump_config_to_table("ENGINE CONFIG", config_map)
-
-
-def dump_config_to_table(title: str, config_map: Dict[str, Any]):
-    table = pt.PrettyTable()
-    table.title = title
-    table.align = "l"
-    table.field_names = ["Options", "Values"]
-    for option, value in config_map.items():
-        table.add_row([option, value])
-    logging.info(table)
-
-
 def dump_lora_infos_to_table(title: str, lora_infos: List[NamedTuple]):
     if len(lora_infos) == 0:
         logging.info("There is no lora_info")
diff --git a/rtp_llm/utils/mm_process_engine.py b/rtp_llm/utils/mm_process_engine.py
index ce489ac7f..863e4e356 100644
--- a/rtp_llm/utils/mm_process_engine.py
+++ b/rtp_llm/utils/mm_process_engine.py
@@ -20,10 +20,14 @@ class MMEmbeddingRes:
 
 
 class MMProcessEngine:
-    def __init__(self, model):
+    def __init__(self, model, vit_config):
         self.model = model
+        self.vit_config = vit_config
         self.contains_pos: bool = self.model.config.mm_position_ids_style != 0
-        self.run_batch: bool = self.model.config.vit_run_batch
+        self.run_batch: bool = self.vit_config.vit_run_batch
+        self.download_headers = self.vit_config.download_headers
+        self.url_cache_size = self.vit_config.url_cache_item_num
+        self.mm_cache_size = self.vit_config.mm_cache_item_num
 
     def _maybe_tensor_to_list(self, tensor: torch.Tensor):
         if len(tensor.shape) > 2:
@@ -56,7 +60,12 @@ class MMProcessEngine:
             pos: Optional[List[torch.Tensor]] = [] if self.contains_pos else None
             for index in range(len(urls)):
                 embedding, pos_ids = self.model.mm_part.mm_embedding(
-                    urls[index], types[index], configs=configs[index]
+                    urls[index], 
+                    types[index], 
+                    download_headers=self.download_headers,
+                    url_cache_size=self.url_cache_size,
+                    mm_cache_size=self.mm_cache_size,
+                    configs=configs[index]
                 )
                 res.extend(self._maybe_tensor_to_list(embedding))
                 if self.contains_pos:
diff --git a/rtp_llm/utils/multimodal_util.py b/rtp_llm/utils/multimodal_util.py
index a429b6212..b06289b47 100644
--- a/rtp_llm/utils/multimodal_util.py
+++ b/rtp_llm/utils/multimodal_util.py
@@ -11,7 +11,6 @@ from typing import Optional
 import requests
 import torch
 
-from rtp_llm.config.py_config_modules import StaticConfig
 from rtp_llm.utils.lru_dict import LruDict
 from rtp_llm.utils.oss_util import get_bytes_io_from_oss_path
 
@@ -34,17 +33,21 @@ def request_get(url, headers):
     return REQUEST_GET(url, headers)
 
 
-if StaticConfig.vit_config.download_headers != "":
-    HTTP_HEADS = json.loads(StaticConfig.vit_config.download_headers)
-else:
-    HTTP_HEADS = {
-        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0",
-        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
-    }
+def _get_http_heads(download_headers: str = ""):
+    """Get HTTP headers from download_headers string.
+    
+    Args:
+        download_headers: JSON string containing HTTP headers. If empty, returns default headers.
+    """
+    if download_headers != "":
+        return json.loads(download_headers)
+    else:
+        return {
+            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0",
+            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
+        }
+
 
-BASE64_PREFIX = "data:image/jpeg;base64,"
-URL_CACHE_SIZE = StaticConfig.vit_config.url_cache_item_num
-MM_CACHE_SIZE = StaticConfig.vit_config.mm_cache_item_num
 
 
 def get_base64_prefix(s):
@@ -130,10 +133,17 @@ def retry_on_assertion_error(retries: int = 3):
     return decorator
 
 
-def get_json_result_from_url(url: str):
+def get_json_result_from_url(url: str, download_headers: str = ""):
+    """Get JSON result from URL.
+    
+    Args:
+        url: URL to fetch from.
+        download_headers: JSON string containing HTTP headers. If empty, uses default headers.
+    """
+    headers = _get_http_heads(download_headers)
     try:
         if url.startswith("http") or url.startswith("https"):
-            response = requests.get(url, stream=True, headers=HTTP_HEADS, timeout=10)
+            response = requests.get(url, stream=True, headers=headers, timeout=10)
             if response.status_code == 200:
                 res = response.content.decode("utf-8")
             else:
@@ -151,12 +161,24 @@ def get_json_result_from_url(url: str):
     return res
 
 
-def get_bytes_io_from_url(url: str):
-    cached_res = url_data_cache_.check_cache(url)
+def get_bytes_io_from_url(url: str, download_headers: str = "", url_cache_size: int = 10, url_data_cache=None):
+    """Get BytesIO from URL.
+    
+    Args:
+        url: URL to fetch from.
+        download_headers: JSON string containing HTTP headers. If empty, uses default headers.
+        url_cache_size: Size of URL data cache. Default is 10.
+        url_data_cache: Optional MMDataCache object. If None, creates a new one with url_cache_size.
+    """
+    if url_data_cache is None:
+        url_data_cache = MMDataCache(url_cache_size)
+    
+    cached_res = url_data_cache.check_cache(url)
     if cached_res is None:
+        headers = _get_http_heads(download_headers)
         try:
             if url.startswith("http") or url.startswith("https"):
-                response = request_get(url, HTTP_HEADS)
+                response = request_get(url, headers)
                 if response.status_code == 200:
                     res = BytesIO(response.content)
                 else:
@@ -174,7 +196,7 @@ def get_bytes_io_from_url(url: str):
                 res = buf
         except Exception as e:
             raise Exception(f"download and load {url} error, exception {e}")
-        url_data_cache_.insert_cache(url, res)
+        url_data_cache.insert_cache(url, res)
         return res
     else:
         cached_res.seek(0)
@@ -204,5 +226,5 @@ class MMDataCache(object):
             self.mm_data_cache[url] = data
 
 
-vit_emb_cache_ = MMDataCache(MM_CACHE_SIZE)
-url_data_cache_ = MMDataCache(URL_CACHE_SIZE)
+# Global cache instance for VIT embeddings
+vit_emb_cache_ = MMDataCache(cache_size=10)
diff --git a/rtp_llm/utils/oss_util.py b/rtp_llm/utils/oss_util.py
index 46b52aaa7..4c2e638bd 100644
--- a/rtp_llm/utils/oss_util.py
+++ b/rtp_llm/utils/oss_util.py
@@ -3,20 +3,24 @@ from io import BytesIO
 import oss2
 from oss2.credentials import EnvironmentVariableCredentialsProvider
 
-from rtp_llm.config.py_config_modules import StaticConfig
 
 
-def get_bytes_io_from_oss_path(url: str):
+def get_bytes_io_from_oss_path(url: str, oss_endpoint: str):
+    """Get BytesIO from OSS path.
+    
+    Args:
+        url: OSS URL path (e.g., "oss://bucket-name/path/to/file")
+        oss_endpoint: OSS endpoint.
+    """
     if not url.startswith("oss://"):
         raise Exception(f"oss path format expect start with oss://, actual: {url}")
     url = url.replace("oss://", "")
     bucket_name, path = url.split("/", 1)
-    endpoint = StaticConfig.model_config.oss_endpoint
     # The AccessKey pair of an Alibaba Cloud account has permissions on all API operations. Using these credentials to perform operations in OSS is a high-risk operation. We recommend that you use a RAM user to call API operations or perform routine O&M. To create a RAM user, log on to the RAM console.
     auth = oss2.ProviderAuth(EnvironmentVariableCredentialsProvider())
     # In this example, the endpoint of the China (Hangzhou) region is used. Specify your actual endpoint.
     # Specify the name of the bucket. Example: examplebucket.
-    bucket = oss2.Bucket(auth, endpoint, bucket_name, connect_timeout=100)
+    bucket = oss2.Bucket(auth, oss_endpoint, bucket_name, connect_timeout=100)
     object = bucket.get_object(path)
     io = BytesIO(object.read())
     return io
diff --git a/rtp_llm/utils/util.py b/rtp_llm/utils/util.py
index 4ef4d9b87..9b7c7a6d7 100644
--- a/rtp_llm/utils/util.py
+++ b/rtp_llm/utils/util.py
@@ -55,6 +55,18 @@ def to_torch_dtype(maybe_str_dtype: Union[str, torch.dtype]) -> torch.dtype:
     if isinstance(maybe_str_dtype, torch.dtype):
         dtype = maybe_str_dtype
     else:
+        # Handle enum types (pybind11 enums have 'name' attribute)
+        if hasattr(maybe_str_dtype, 'name'):
+            # Convert enum to string using name attribute
+            maybe_str_dtype = maybe_str_dtype.name
+        elif not isinstance(maybe_str_dtype, str):
+            # Convert other types to string
+            maybe_str_dtype = str(maybe_str_dtype)
+        
+        # Remove TYPE_ prefix if present (e.g., TYPE_BF16 -> BF16)
+        if maybe_str_dtype.startswith('TYPE_'):
+            maybe_str_dtype = maybe_str_dtype[5:]  # Remove 'TYPE_' prefix
+        
         try:
             dtype = {
                 "bf16": torch.bfloat16,
diff --git a/stub_source b/stub_source
index 2799cfe46..330a53118 120000
--- a/stub_source
+++ b/stub_source
@@ -1 +1 @@
-open_source
\ No newline at end of file
+../internal_source
\ No newline at end of file
diff --git a/tests/mla/mla_attention_layer.cc b/tests/mla/mla_attention_layer.cc
index f170a38b3..e8a460f47 100644
--- a/tests/mla/mla_attention_layer.cc
+++ b/tests/mla/mla_attention_layer.cc
@@ -40,7 +40,6 @@ MlaAttnLayerOp::MlaAttnLayerOp(int64_t head_num,
                                double  softmax_extra_scale) {
     rtp_llm::initLogger();
     GptInitParameter gpt_init_parameter;
-    gpt_init_parameter.update_from_env_for_test();
     rtp_llm::DeviceFactory::initDevices(gpt_init_parameter);
     device_      = rtp_llm::DeviceFactory::getDefaultDevice();
     attn_configs = AttentionConfigs({
@@ -50,7 +49,7 @@ MlaAttnLayerOp::MlaAttnLayerOp(int64_t head_num,
         static_cast<size_t>(hidden_size),
         RopeConfig(),
         64,
-        AttentionMaskType::causalMask,
+        true,
         1.0f,
         true,
         false,
diff --git a/tests/mla/mla_context_attention.cc b/tests/mla/mla_context_attention.cc
index a31a4164b..e664f9fc2 100644
--- a/tests/mla/mla_context_attention.cc
+++ b/tests/mla/mla_context_attention.cc
@@ -48,7 +48,6 @@ MlaContextAttnOp::MlaContextAttnOp(int64_t mla_type,
 
     auto gpt_params          = GptInitParameter();
     gpt_params.mla_ops_type_ = MlaOpsType(mla_type);
-    gpt_params.update_from_env_for_test();
     rtp_llm::DeviceFactory::initDevices(gpt_params);
     device_ = rtp_llm::DeviceFactory::getDefaultDevice();
     ;
@@ -59,7 +58,7 @@ MlaContextAttnOp::MlaContextAttnOp(int64_t mla_type,
         static_cast<size_t>(hidden_size),
         RopeConfig(),
         64,
-        AttentionMaskType::causalMask,
+        true,
         1.0f,
         true,
         false,
diff --git a/tests/mla/mla_decoder_attention.cc b/tests/mla/mla_decoder_attention.cc
index 67cbf9742..822c94241 100644
--- a/tests/mla/mla_decoder_attention.cc
+++ b/tests/mla/mla_decoder_attention.cc
@@ -119,7 +119,6 @@ MlaDecoderAttnOp::MlaDecoderAttnOp(int64_t mla_ops_type,
     rtp_llm::initLogger();
     auto gpt_params          = GptInitParameter();
     gpt_params.mla_ops_type_ = MlaOpsType(mla_ops_type);
-    gpt_params.update_from_env_for_test();
     rtp_llm::DeviceFactory::initDevices(gpt_params);
     device_      = rtp_llm::DeviceFactory::getDefaultDevice();
     attn_configs = AttentionConfigs({
@@ -129,7 +128,7 @@ MlaDecoderAttnOp::MlaDecoderAttnOp(int64_t mla_ops_type,
         static_cast<size_t>(hidden_size),
         RopeConfig(),
         64,
-        AttentionMaskType::causalMask,
+        true,
         1.0f,
         true,
         false,
diff --git a/tests/mla/mla_rotary_kvcache_test.cc b/tests/mla/mla_rotary_kvcache_test.cc
index bbd616379..da935a0e3 100644
--- a/tests/mla/mla_rotary_kvcache_test.cc
+++ b/tests/mla/mla_rotary_kvcache_test.cc
@@ -66,7 +66,7 @@ MlaRotaryKVCacheOp::MlaRotaryKVCacheOp(int64_t mla_type,
         static_cast<size_t>(hidden_size),
         RopeConfig(),
         64,
-        AttentionMaskType::causalMask,
+        true,
         1.0f,
         true,
         false,
