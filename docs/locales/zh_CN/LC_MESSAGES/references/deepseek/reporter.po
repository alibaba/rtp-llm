# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, RTP-LLM
# This file is distributed under the same license as the RTP-LLM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: RTP-LLM 0.2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-12 17:38+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../references/deepseek/reporter.md:1
msgid "DeepSeek Replay Tech Report"
msgstr "DeepSeek Replay技术报告"

#: ../../references/deepseek/reporter.md:3
msgid "Overview"
msgstr "概述"

#: ../../references/deepseek/reporter.md:5
msgid ""
"DeepSeek-V3 has demonstrated strong performance in multiple evaluations, "
"becoming one of the most attention-grabbing open-source large models. Due"
" to its large-scale MoE architecture, optimizing inference performance is"
" a key challenge for engineering deployment. In February, the DeepSeek "
"team successively open-sourced key components including DeepEP, DeepGEMM,"
" FlashMLA, and EPLB. Based on the open-source community's work, we "
"completed optimization work on RTP-LLM, aligning with the performance of "
"the DeepSeek inference system."
msgstr "DeepSeek-V3在多项评估中表现出色，成为最受关注的开源大模型之一。由于其大规模MoE架构，优化推理性能是工程部署的关键挑战。2月，DeepSeek团队相继开源了DeepEP、DeepGEMM、FlashMLA和EPLB等关键组件。基于开源社区的工作，我们完成了RTP-LLM的优化工作，与DeepSeek推理系统性能对齐。"

#: ../../references/deepseek/reporter.md:7
msgid ""
"RTP-LLM is an LLM inference acceleration engine developed by Alibaba "
"Aicheng Technology, primarily serving Alibaba Group's internal business. "
"This article will share some key technical points, shortcomings, and "
"reflections from the implementation process, as a way to thank the open-"
"source community for their help. The relevant code is being organized and"
" refactored, and complete code and reproduction methods will be updated "
"soon."
msgstr "RTP-LLM是阿里巴巴爱程技术开发的LLM推理加速引擎，主要服务于阿里巴巴集团内部业务。本文将分享实现过程中的一些关键技术点、不足和思考，以此感谢开源社区的帮助。相关代码正在整理和重构中，完整代码和复现方法将尽快更新。"

#: ../../references/deepseek/reporter.md:9
msgid ""
"According to the introduction in [DeepSeek Inference System "
"Overview](https://github.com/deepseek-ai/open-infra-"
"index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md)"
msgstr "根据[DeepSeek推理系统概览](https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md)中的介绍"

#: ../../references/deepseek/reporter.md:12
msgid ""
"Total input tokens: 608B, of which 342B tokens (56.3%) hit the on-disk KV"
" cache."
msgstr "总输入token：608B，其中342B token（56.3%）命中磁盘KV缓存。"

#: ../../references/deepseek/reporter.md:14
msgid ""
"Total output tokens: 168B. The average output speed was 20–22 tokens per "
"second, and the average kvcache length per output token was 4,989 tokens."
msgstr "总输出token：168B。平均输出速度为每秒20-22个token，每个输出token的平均kvcache长度为4,989个token。"

#: ../../references/deepseek/reporter.md:16
msgid ""
"Each H800 node delivers an average throughput of ~73.7k tokens/s input "
"(including cache hits) during prefilling or ~14.8k tokens/s output during"
" decoding."
msgstr "每个H800节点在预填充期间提供约73.7k token/s的输入吞吐量（包括缓存命中）或在解码期间提供约14.8k token/s的输出吞吐量。"

#: ../../references/deepseek/reporter.md:19
msgid ""
"**In actual production services, the DeepSeek inference system achieves a"
" prefill throughput of 32.2K per H800 node** and **a decode throughput of"
" 14.8K TPS per H800 node**. In RTP-LLM testing, using 4K input/2K output,"
" **under 1.6s TTFT and 50ms ITL constraints, we achieved prefill "
"performance of 42.6K TPS per H800 node and decode performance of 14.7K "
"TPS per H800 node**."
msgstr "**在实际生产服务中，DeepSeek推理系统实现了每个H800节点32.2K的预填充吞吐量**和**每个H800节点14.8K TPS的解码吞吐量**。在RTP-LLM测试中，使用4K输入/2K输出，**在1.6s TTFT和50ms ITL约束下，我们实现了每个H800节点42.6K TPS的预填充性能和每个H800节点14.7K TPS的解码性能**。"

#: ../../references/deepseek/reporter.md:21
msgid "Test Results"
msgstr "测试结果"

#: ../../references/deepseek/reporter.md:23
msgid "Settings"
msgstr "设置"

#: ../../references/deepseek/reporter.md:25
msgid ""
"We deployed in **Alibaba Cloud Lingjun H800 RoCE environment using PD "
"separation and distributed EP architecture**, setting TP=1, DP=EP=number "
"of GPUs. The prefill single-instance specification is 4 nodes with 32 "
"GPUs, and the decode single-instance specification is 18 nodes with 144 "
"GPUs. During testing, we used 4 prefill instances and 1 decode instance, "
"totaling 272 H800 GPUs."
msgstr "我们在**阿里云灵骏H800 RoCE环境**中部署，采用PD分离和分布式EP架构，设置TP=1，DP=EP=GPU数量。Prefill单实例规格为4个节点32个GPU，Decode单实例规格为18个节点144个GPU。测试期间，我们使用了4个Prefill实例和1个Decode实例，共计272个H800 GPU。"

#: ../../references/deepseek/reporter.md:27
msgid ""
"The test adopted a 4:1 PD instance ratio, which is not the perfect PD "
"ratio. In actual production loads, more complex input/output length "
"fluctuations will be faced, requiring integration with scheduling systems"
" to dynamically and elastically adjust the number of PD instances."
msgstr "测试采用4:1的PD实例比例，这不是完美的PD比例。在实际生产负载中，将面临更复杂的输入/输出长度波动，需要与调度系统集成，动态弹性调整PD实例数量。"

#: ../../references/deepseek/reporter.md:29
msgid "Prefill"
msgstr "预填充"

#: ../../references/deepseek/reporter.md:31
msgid "![image.png](../../pics/prefill.png)"
msgstr "![image.png](../../pics/prefill.png)"

#: ../../references/deepseek/reporter.md:31
#: ../../references/deepseek/reporter.md:41
#: ../../references/deepseek/reporter.md:45
#: ../../references/deepseek/reporter.md:57
#: ../../references/deepseek/reporter.md:101
msgid "image.png"
msgstr ""

#: ../../references/deepseek/reporter.md:33
msgid ""
"The prefill instance uses 32 EP deployment. Under extreme pressure, a "
"single GPU executing 2 4K requests takes 1.5s, with a throughput of 5333 "
"TPS."
msgstr "预填充实例使用32 EP部署。在极端压力下，单个GPU执行2个4K请求需要1.5秒，吞吐量为5333 TPS。"

#: ../../references/deepseek/reporter.md:35
msgid ""
"The test did not simulate the impact of cache, which is one of the "
"subsequent areas for improvement."
msgstr "测试未模拟缓存的影响，这是后续需要改进的领域之一。"

#: ../../references/deepseek/reporter.md:37
msgid ""
"**RTP-LLM also supports hybrid TP/DP/EP deployment. It is recommended to "
"use TP=1 on high-compute H800 GPUs; on compute-constrained cards like "
"H20, choose TP=2/4 based on latency constraints.**"
msgstr "**RTP-LLM也支持混合TP/DP/EP部署。建议在高算力H800 GPU上使用TP=1；在算力受限的卡如H20上，根据延迟约束选择TP=2/4。**"

#: ../../references/deepseek/reporter.md:39
msgid "Decode"
msgstr "解码"

#: ../../references/deepseek/reporter.md:41
msgid "![image.png](../../pics/decode.png)"
msgstr "![image.png](../../pics/decode.png)"

#: ../../references/deepseek/reporter.md:43
msgid ""
"The decode instance uses 144 EP deployment (128 + 16 redundant). Due to "
"implementation differences, the host takes 2ms less time, but the device "
"is slightly slower. The analysis indicates that the reasons are RoCE vs. "
"IB network differences, lack of CUDA Graph optimization, and some slow "
"kernel implementations. This is also a direction for future optimization."
msgstr "解码实例使用144 EP部署（128 + 16冗余）。由于实现差异，主机端耗时减少2ms，但设备端稍慢。分析表明原因是RoCE与IB网络差异、缺少CUDA Graph优化以及一些内核实现较慢。这也是未来优化的方向。"

#: ../../references/deepseek/reporter.md:45
msgid "![image.png](../../pics/throughput_latency.png)"
msgstr "![image.png](../../pics/throughput_latency.png)"

#: ../../references/deepseek/reporter.md:47
msgid ""
"The figure above shows the decode phase pressure test curve. At lower "
"concurrency, a single user can reach 42 TPS. At 13200 concurrency, the "
"SLA limit of 20 TPS per user is reached, with a single GPU throughput of "
"1850 TPS."
msgstr "上图显示了解码阶段的压力测试曲线。在较低并发情况下，单个用户可达到42 TPS。在13200并发时，达到每个用户20 TPS的SLA限制，单GPU吞吐量为1850 TPS。"

#: ../../references/deepseek/reporter.md:49
msgid ""
"Before DeepEP was open-sourced, we implemented distributed EP through "
"All2All, achieving excellent throughput improvements compared to single-"
"node setups, but with excessive latency. Besides high network latency, "
"All2All brought severe host synchronization overhead, which was also "
"detrimental to overlapping network and computation time. **It is "
"recommended that GPUs not supporting the DeepEP mechanism can "
"equivalently implement Pure Device All2All to achieve similar "
"performance; ASIC accelerator cards can go further and directly perform "
"MoE/Dispatch/Combine overlap.**"
msgstr "在DeepEP开源之前，我们通过All2All实现了分布式EP，在吞吐量方面相比单节点设置有显著提升，但延迟过高。除了高网络延迟外，All2All还带来了严重的主机同步开销，这对网络和计算时间的重叠也是不利的。**建议不支持DeepEP机制的GPU可以等效实现纯设备All2All以达到相似性能；ASIC加速卡可以更进一步，直接执行MoE/Dispatch/Combine重叠。**"

#: ../../references/deepseek/reporter.md:51
msgid "Implementation and Tricks"
msgstr "实现与技巧"

#: ../../references/deepseek/reporter.md:53
msgid "EPLB"
msgstr "EPLB"

#: ../../references/deepseek/reporter.md:55
msgid ""
"The figure below shows the EPLB latency impact test. We found that the EP"
" balancing state is significantly affected by test data, **and test data "
"cannot completely simulate real application load states**. EP load "
"balancing strategies remain an area for in-depth exploration in the "
"future."
msgstr "下图显示了EPLB延迟影响测试。我们发现EP平衡状态受测试数据显著影响，**测试数据无法完全模拟真实应用负载状态**。EP负载均衡策略仍是未来需要深入探索的领域。"

#: ../../references/deepseek/reporter.md:57
msgid "![image.png](../../pics/eplb.png)"
msgstr "![image.png](../../pics/eplb.png)"

#: ../../references/deepseek/reporter.md:59
msgid "MicroBatch & Overlapping"
msgstr "微批次与重叠"

#: ../../references/deepseek/reporter.md:61
msgid ""
"To enable GPU computation and network communication to overlap, we fully "
"implemented the Prefill/Decode Micro Batching solution and integrated it "
"with DeepEP's overlap mechanism. During the process, we made the "
"following observations:"
msgstr "为了使GPU计算和网络通信能够重叠，我们完全实现了Prefill/Decode微批处理解决方案，并将其与DeepEP的重叠机制集成。在此过程中，我们做出了以下观察："

#: ../../references/deepseek/reporter.md:63
msgid ""
"Whether for Prefill or Decode, since the Dispatch phase transfers FP8 "
"tensors while the Combine phase transfers FP16 tensors, the communication"
" time for the Combine phase is significantly higher than Dispatch. "
"Therefore, **when designing overlap solutions, larger time blocks need to"
" be considered to cover the Combine phase communication. Introducing "
"quantized communication in the inference phase is a potential improvement"
" direction for the future.**"
msgstr "无论是在Prefill还是Decode阶段，由于Dispatch阶段传输FP8张量而Combine阶段传输FP16张量，Combine阶段的通信时间显著高于Dispatch阶段。因此，**在设计重叠解决方案时，需要考虑更大的时间块来覆盖Combine阶段通信。在推理阶段引入量化通信是未来的潜在改进方向。**"

#: ../../references/deepseek/reporter.md:65
msgid ""
"For Prefill, the time spent on Attention accounts for a relatively small "
"proportion. The final Attention+MoE gate portion and the MoE MLP portion "
"spend similar amounts of time, both of which can cover the relatively "
"long communication time of the Combine phase. Only request segmentation "
"is needed, and the computation/communication of two MicroBatches can "
"interleave. An important detail is that **Shared Expert computation is "
"always overlaid in the Combine portion to ensure that the computation "
"time covering Combine is more than that covering Dispatch**."
msgstr "对于Prefill阶段，Attention所花费的时间占比相对较小。最终的Attention+MoE门控部分和MoE MLP部分花费的时间相近，都能覆盖Combine阶段相对较长的通信时间。只需要请求分段，两个MicroBatch的计算/通信就可以交错进行。一个重要细节是**共享专家计算总是与Combine部分重叠，以确保覆盖Combine的计算时间多于覆盖Dispatch的时间**。"

#: ../../references/deepseek/reporter.md:67
msgid ""
"Considering the Qwen3 model, although it has no Shared Expert, the same "
"overlap scheme can still be adopted in the Decode phase, using the "
"Attention operator as a boundary to insert MLP computation, covering "
"Dispatch and Combine communication times before and after respectively. "
"At the framework level, to be compatible with both DeepEP and Vanilla "
"All2All communication overlap functions and considering extension to "
"various hardware, we developed **a unified communication callback "
"interface, enabling MicroBatch capabilities to be easily extended to "
"other accelerator cards**."
msgstr "考虑到Qwen3模型，虽然它没有共享专家，但在Decode阶段仍可采用相同的重叠方案，使用Attention算子作为边界插入MLP计算，分别覆盖前后Dispatch和Combine通信时间。在框架层面，为了兼容DeepEP和Vanilla All2All通信重叠功能并考虑扩展到各种硬件，我们开发了**统一的通信回调接口，使MicroBatch功能能够轻松扩展到其他加速卡**。"

#: ../../references/deepseek/reporter.md:69
msgid "MTP"
msgstr "MTP"

#: ../../references/deepseek/reporter.md:71
msgid ""
"We added MTP speculative sampling support to our previously implemented "
"[general speculative sampling "
"framework](https://mp.weixin.qq.com/s/EiSRF2ORy22I1pimCCvcPQ). MTP is the"
" most critical link in DeepSeek ITL optimization. The only way to "
"increase computational intensity in the decode phase is to increase GEMM "
"BS. **KV Cache capacity limits Global BS, and MTP only requires BS/2 to "
"achieve the same computational intensity as the original**. Enabling "
"MicroBatch for computation-communication overlap has the side effect of "
"increased latency. **MTP can reduce average ITL and compensate for the "
"latency caused by MicroBatch**. A win-win situation."
msgstr "我们在先前实现的[通用投机采样框架](https://mp.weixin.qq.com/s/EiSRF2ORy22I1pimCCvcPQ)中增加了MTP投机采样支持。MTP是DeepSeek ITL优化中最关键的环节。在解码阶段增加计算强度的唯一方法是增加GEMM BS。**KV缓存容量限制了全局BS，而MTP只需要BS/2就能达到与原来相同的计算强度**。启用MicroBatch进行计算-通信重叠会产生延迟增加的副作用。**MTP可以减少平均ITL并补偿MicroBatch引起的延迟**。双赢的局面。"

#: ../../references/deepseek/reporter.md:73
msgid "PD Disaggregation"
msgstr "PD分离"

#: ../../references/deepseek/reporter.md:75
msgid ""
"In the DeepSeek-V3 model, due to significant differences in Prefill and "
"Decode computational requirements and different EP strategies, PD "
"separation deployment is a necessary choice. We extended **support for "
"Prefill-Decode deployment with different TP specifications, which is "
"particularly important for low-compute cards**. We implemented two PD "
"load balancing strategies: KV Cache-based balancing and BS-based "
"balancing. The test data has small BS variance, and under high pressure "
"and **high EP traffic, BS balancing is more important for "
"Dispatch/Combine latency**. In production environments, BS variance and "
"Seq variance factors need to be comprehensively considered, or Decode "
"instances can be further split according to traffic characteristics."
msgstr "在DeepSeek-V3模型中，由于Prefill和Decode计算需求存在显著差异以及不同的EP策略，PD分离部署是必要的选择。我们扩展了**对不同TP规格的Prefill-Decode部署的支持，这对低算力卡尤其重要**。我们实现了两种PD负载均衡策略：基于KV缓存的均衡和基于BS的均衡。测试数据的BS方差较小，在高压和**高EP流量下，BS均衡对Dispatch/Combine延迟更重要**。在生产环境中，需要综合考虑BS方差和Seq方差因素，或者可以根据流量特征进一步拆分Decode实例。"

#: ../../references/deepseek/reporter.md:77
msgid "DeepEP / Network"
msgstr "DeepEP / 网络"

#: ../../references/deepseek/reporter.md:79
msgid ""
"DeepEP is primarily optimized for IB environments. When facing the "
"diverse underlying environments and technology stacks in actual "
"production, to achieve engineering deployment and optimal performance, we"
" made the following optimizations and improvements:"
msgstr "DeepEP主要针对IB环境进行优化。面对实际生产中多样化的底层环境和技术栈，为了实现工程部署和最佳性能，我们进行了以下优化和改进："

#: ../../references/deepseek/reporter.md:81
msgid ""
"Dual uplink performance fix: Through in-depth analysis of Normal kernel "
"(few QP, large messages) and Low latency kernel (many QP, small messages)"
" characteristics, we provided a pure IAAS layer fix function without "
"introducing performance overhead. Specifically, we provided message-level"
" and queue-level load balancing solutions for Normal kernel and Low "
"Latency kernel at the NVSHMEM layer. The optimized version maintains the "
"stability advantages of dual uplinks while achieving communication "
"performance that can match or even slightly surpass single uplink IB "
"network solutions."
msgstr "双上联性能修复：通过深入分析Normal内核（少数QP，大消息）和低延迟内核（多数QP，小消息）的特性，我们提供了一个纯IAAS层修复功能，不会引入性能开销。具体而言，我们在NVSHMEM层为Normal内核和低延迟内核提供了消息级和队列级负载均衡解决方案。优化版本在保持双上联稳定性优势的同时，实现了可匹配甚至略微超越单上联IB网络解决方案的通信性能。"

#: ../../references/deepseek/reporter.md:83
msgid ""
"Communication mode optimization: By jointly considering intra-node and "
"inter-node network architectures, we optimized intra-node and inter-node "
"traffic patterns, fully utilized available links in the system, achieved "
"traffic balance between network tracks and planes, avoided network "
"traffic conflicts and collisions, and maximized overall system "
"communication efficiency. In Low Latency communication mode, "
"communication latency can be reduced by 60%+."
msgstr "通信模式优化：通过综合考虑节点内和节点间网络架构，我们优化了节点内和节点间流量模式，充分利用系统中的可用链路，实现了网络轨道和层面之间的流量平衡，避免了网络流量冲突和碰撞，最大化整体系统通信效率。在低延迟通信模式下，通信延迟可减少60%以上。"

#: ../../references/deepseek/reporter.md:85
msgid ""
"Intra-node topology self-repair capability: Abnormal intra-node topology "
"reporting affects communication links between network cards and GPUs, "
"leading to network performance degradation. To solve this problem, we "
"implemented intra-node topology self-repair functionality, shielding "
"upper layers from underlying server hardware and software differences, "
"ensuring affinity relationships between GPUs and network cards across "
"different machine types."
msgstr "节点内拓扑自修复能力：异常的节点内拓扑报告会影响网卡和GPU之间的通信链路，导致网络性能下降。为解决此问题，我们实现了节点内拓扑自修复功能，屏蔽上层与底层服务器硬件和软件差异，确保不同类型机器间GPU和网卡的亲和关系。"

#: ../../references/deepseek/reporter.md:87
msgid ""
"Virtualization environment adaptation: To flexibly support complex and "
"variable business scenarios, we supported a high-performance network "
"solution based on commercial card hardware SRIOV virtualization, solved "
"the adaptation problem between SRIOV and DeepEP, and completed large-"
"scale deployment through optimization to make VF and PF performance "
"consistent."
msgstr "虚拟化环境适配：为了灵活支持复杂多变的业务场景，我们支持基于商业卡硬件SRIOV虚拟化的高性能网络解决方案，解决了SRIOV与DeepEP的适配问题，并通过优化完成大规模部署，使VF和PF性能保持一致。"

#: ../../references/deepseek/reporter.md:89
msgid "CUDA Kernel Fusion"
msgstr "CUDA内核融合"

#: ../../references/deepseek/reporter.md:91
msgid ""
"We conducted detailed analysis of the CUDA kernel execution flow and "
"optimized based on model characteristics:"
msgstr "我们对CUDA内核执行流程进行了详细分析，并根据模型特点进行了优化："

#: ../../references/deepseek/reporter.md:93
msgid ""
"**Moved some matrix multiplication to BF16 format computation**. FP8 "
"matrix multiplication incurs greater overhead due to the need for "
"quantization operations when the scale is insufficient."
msgstr "**将部分矩阵乘法移至BF16格式计算**。FP8矩阵乘法由于在规模不足时需要量化操作而产生更大的开销。"

#: ../../references/deepseek/reporter.md:95
msgid ""
"Advanced the transpose in Rotary Embedding to the weight loading phase to"
" avoid introducing Elementwise operators."
msgstr "将旋转Embedding中的转置操作提前到权重加载阶段，以避免引入元素级运算符。"

#: ../../references/deepseek/reporter.md:97
msgid "Fuse Quantization and Transpose before GEMM computation."
msgstr "在GEMM计算之前融合量化和转置操作。"

#: ../../references/deepseek/reporter.md:99
msgid "Future plans include fusing Activation and Quantization."
msgstr "未来计划包括融合激活函数和量化操作。"

#: ../../references/deepseek/reporter.md:101
msgid "![image.png](../../pics/fusion.png)"
msgstr "![image.png](../../pics/fusion.png)"

#: ../../references/deepseek/reporter.md:103
msgid "PDL"
msgstr "PDL"

#: ../../references/deepseek/reporter.md:105
msgid ""
"The Hopper architecture introduced Programmatic Dependent Launch (PDL), "
"allowing two adjacent kernels on the same CUDA stream to execute "
"overlapped, enabling the latter kernel to complete initialization and "
"other work in advance while the former kernel is executing. By "
"introducing PDL into GEMM kernels, **we can execute GEMM initialization "
"operations in advance during the computation of other kernels like "
"Quantization, improving overall system performance**."
msgstr "Hopper架构引入了可编程依赖启动（PDL），允许同一CUDA流上的两个相邻内核重叠执行，使后者内核能够在前者内核执行期间提前完成初始化和其他工作。通过在GEMM内核中引入PDL，**我们可以在其他内核（如量化内核）计算期间提前执行GEMM初始化操作，从而提高整体系统性能**。"

#: ../../references/deepseek/reporter.md:107
msgid ""
"The introduction of PDL also brings more possibilities for kernel-level "
"optimization, such as GEMM **Weight Prefetch**. After overlapping "
"Quantization operations with GEMM through PDL, prefetch operations for "
"weights can be added to the overlapping portion of the GEMM kernel, so "
"that when the MMA is actually executed, the required weight tensor is "
"already in the L2 cache, achieving the purpose of accelerating the GEMM "
"kernel."
msgstr "PDL的引入也为内核级优化带来了更多可能性，例如GEMM**权重预取**。通过PDL将量化操作与GEMM重叠后，可以在GEMM内核的重叠部分添加权重预取操作，这样当MMA实际执行时，所需的权重张量已在L2缓存中，从而达到加速GEMM内核的目的。"

#: ../../references/deepseek/reporter.md:109
msgid "Framework Overhead"
msgstr "框架开销"

#: ../../references/deepseek/reporter.md:111
msgid ""
"Overall framework overhead mainly concentrates on two parts: **one part "
"is host overhead between adjacent Forward Steps, around 1.5ms; the other "
"part is kernel launch overhead, around 2ms**."
msgstr "整体框架开销主要集中在两个部分：**一部分是相邻前向步骤之间的主机开销，约1.5ms；另一部分是内核启动开销，约2ms**。"

#: ../../references/deepseek/reporter.md:113
msgid ""
"The main issue with host overhead between Forward Steps is our bulky "
"Dynamic Batch implementation, whose performance overhead is linearly "
"related to BS. We lightweighted the Dynamic Batch implementation, "
"processing operations not dependent on the next step asynchronously or "
"with multithreading. Ideally, under 128 BS conditions, we can achieve "
"under 200us. The current excessive host overhead is largely due to an "
"additional Dynamic Batch in MTP situations, which can be further "
"optimized away."
msgstr "前向步骤之间主机开销的主要问题是我们的庞大动态批处理实现，其性能开销与BS呈线性关系。我们对动态批处理实现进行了轻量化改造，对不依赖于下一步的操作进行异步或多线程处理。理想情况下，在128 BS条件下，我们可以实现低于200微秒的性能。当前过多的主机开销主要源于MTP情况下的额外动态批处理，这可以进一步优化掉。"

#: ../../references/deepseek/reporter.md:115
msgid ""
"**Kernel Launch Overhead is primarily due to too many GPU kernels**. With"
" MicroBatch enabled, the number of GPU kernels doubles, making the "
"problem more severe. A better solution is CUDA Graph. Here, performance "
"and architecture complexity need to be balanced. The RTP-LLM framework "
"has already avoided host-side launch overhead issues through C++ "
"implementation. However, we observed that **even when launch speed far "
"exceeds kernel execution speed, there is still some launch overhead at "
"the GPU device level**, which CUDA Graph can mitigate to some extent. "
"**We look forward to NVIDIA being able to thoroughly solve this problem "
"in future drivers or hardware versions**."
msgstr "**内核启动开销主要是由于GPU内核过多**。启用MicroBatch后，GPU内核数量翻倍，使问题更加严重。更好的解决方案是CUDA Graph。在这里，性能和架构复杂性需要平衡。RTP-LLM框架已经通过C++实现避免了主机端启动开销问题。然而，我们观察到**即使启动速度远超内核执行速度，GPU设备级别仍存在一些启动开销**，CUDA Graph可以在一定程度上缓解这个问题。**我们期待NVIDIA能在未来的驱动或硬件版本中彻底解决这个问题**。"

#: ../../references/deepseek/reporter.md:117
msgid "Weights Loading"
msgstr "权重加载"

#: ../../references/deepseek/reporter.md:119
msgid ""
"Model weight loading speed directly affects R&D and deployment "
"efficiency. For 671B weights, we achieved **minute-level loading** "
"through optimization with the following specific plan:"
msgstr "模型权重加载速度直接影响研发和部署效率。对于671B权重，我们通过优化实现了**分钟级加载**，具体方案如下："

#: ../../references/deepseek/reporter.md:121
msgid ""
"**Weight preprocessing and format pre-conversion**. RTP-LLM needs to "
"perform Split, Transpose, and other operations on weights during loading."
" We designed a preprocessing system to convert raw weights into the "
"weight format required by the framework in advance. After preprocessing, "
"computational overhead during loading is eliminated."
msgstr "**权重预处理和格式预转换**。RTP-LLM在加载过程中需要对权重执行分割、转置等操作。我们设计了一个预处理系统，提前将原始权重转换为框架所需的权重格式。预处理后，加载过程中的计算开销被消除。"

#: ../../references/deepseek/reporter.md:123
msgid ""
"**Direct IO + Pinned Memory acceleration for large file reading**. To "
"address the I/O bottleneck of individual weight files exceeding 100GB, we"
" used Direct IO to bypass the system Page Cache mechanism, established a "
"fixed memory pool through CUDA Pinned Memory, and eliminated multiple "
"memory copies between kernel space and user space."
msgstr "**直接IO + 固定内存加速大文件读取**。为了解决单个权重文件超过100GB的I/O瓶颈，我们使用直接IO绕过系统页面缓存机制，通过CUDA固定内存建立固定内存池，并消除了内核空间和用户空间之间的多次内存拷贝。"

#: ../../references/deepseek/reporter.md:125
msgid "Limitations and Future Work"
msgstr "局限性和未来工作"

#: ../../references/deepseek/reporter.md:127
msgid ""
"In terms of operator performance, we have not yet fully aligned with "
"DeepSeek. Core operators such as Prefill Attention and Decode "
"Quantization have certain performance gaps and require further "
"optimization. Additionally, CUDA Graph is also a key improvement "
"direction."
msgstr "在算子性能方面，我们尚未完全与DeepSeek对齐。预填充注意力和解码量化的核⼼算子存在一定的性能差距，需要进一步优化。此外，CUDA Graph也是一个关键的改进方向。"

#: ../../references/deepseek/reporter.md:129
msgid ""
"EPLB essentially requires deep collaboration between algorithm design and"
" system engineering. Currently, there is no universal and efficient "
"solution. For dynamic load distribution characteristics under specific "
"application scenarios, more adaptive and robust load balancing strategies"
" need to be explored."
msgstr "EPLB本质上需要算法设计和系统工程之间的深度协作。目前还没有通用且高效的解决方案。对于特定应用场景下的动态负载分布特征，需要探索更多自适应和鲁棒的负载均衡策略。"

#: ../../references/deepseek/reporter.md:131
msgid ""
"MicroBatch is not the only solution for computation-communication "
"overlap. Combining excellent work such as FLUX and Triton-distributed, "
"multiple parallel mode fusion is a direction worth exploring in the "
"future."
msgstr "MicroBatch并非计算-通信重叠的唯一解决方案。结合FLUX和Triton-distributed等优秀工作，多种并行模式融合是未来值得探索的方向。"

#: ../../references/deepseek/reporter.md:133
msgid ""
"On DeepSeek-V3, the Pure EP solution matches well with 6K length short "
"sequence tasks. For longer sequence scenarios, constrained by KV Cache "
"capacity, more sophisticated parallel modes need to be designed to "
"improve MoE computational efficiency."
msgstr "在DeepSeek-V3上，纯EP解决方案与6K长度的短序列任务匹配良好。对于更长序列场景，受限于KV缓存容量，需要设计更复杂的并行模式来提高MoE计算效率。"

#: ../../references/deepseek/reporter.md:135
msgid ""
"In large-scale testing and deployment practices, we observed multiple "
"instances where single GPU failures caused the entire 144 GPU decode "
"instance to fail. To address this, we introduced ACCL combined with "
"service discovery mechanisms in the PD separation architecture, building "
"a serverless PD service with elasticity and high availability. We plan to"
" further combine task schedulers and communication library capabilities "
"in the future to build a Serverless CCL (Collective Communication "
"Library) framework with high fault tolerance and elastic scaling "
"capabilities."
msgstr "在大规模测试和部署实践中，我们观察到多个单个GPU故障导致整个144 GPU解码实例失败的情况。为了解决这个问题，我们在PD分离架构中引入了ACCL结合服务发现机制，构建了具有弹性和高可用性的无服务器PD服务。我们计划未来进一步结合任务调度器和通信库功能，构建具有高容错性和弹性扩展能力的无服务器CCL（集体通信库）框架。"

#: ../../references/deepseek/reporter.md:137
msgid ""
"Unlike H800, the heterogeneous computing cards that can be scaled in our "
"production environment generally have lower compute power. Optimizing "
"throughput under TTFT and ITL constraints in this context is a highly "
"challenging problem. At the same time, how to optimize performance well "
"across various card types and generations is also a problem we need to "
"work to solve."
msgstr "与H800不同，我们生产环境中可扩展的异构计算卡通常具有较低的算力。在这种情况下，在TTFT和ITL约束下优化吞吐量是一个极具挑战性的问题。同时，如何在各种卡类型和代际间良好地优化性能也是我们需要努力解决的问题。"

#: ../../references/deepseek/reporter.md:139
msgid "Thanks"
msgstr "感谢"

#: ../../references/deepseek/reporter.md:141
msgid ""
"Through two months of continuous effort, we have aligned with the "
"performance of the DeepSeek inference engine. We thank the open-source "
"community for sharing excellent open-source models such as DeepSeek, "
"Qwen, and Llama, as well as excellent engineering engines and "
"optimizations such as FasterTransformer, TensoRT-LLM, FlashAttention, "
"FlashInfer, Transformers, vLLM, and SGLang. We believe that open-source, "
"openness, and communication are the inevitable path to achieving AGI. We "
"hope to jointly promote AI technology innovation and ecosystem prosperity"
" through in-depth discussion and exchange with the community."
msgstr ""
"经过两个月的持续努力，我们已与DeepSeek推理引擎的性能对齐。我们感谢开源社区分享优秀的开源模型，如DeepSeek、Qwen和Llama，以及优秀的工程引擎和优化工具，如FasterTransformer、TensoRT-LLM、FlashAttention、FlashInfer、Transformers、vLLM和SGLang。我们相信开源、开放和交流是实现AGI的必由之路。我们希望通过与社区的深入讨论和交流，共同推动AI技术创新和生态繁荣。"
