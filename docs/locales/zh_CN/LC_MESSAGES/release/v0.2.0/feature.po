# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, RTP-LLM
# This file is distributed under the same license as the RTP-LLM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: RTP-LLM 0.2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-26 10:19+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../release/v0.2.0/feature.md:2
msgid "Overview"
msgstr "概述"

#: ../../release/v0.2.0/feature.md:3
msgid "RTP-LLM First Release Version:0.2.0(2025.09)"
msgstr "RTP-LLM 首个发布版本：0.2.0(2025.09)"

#: ../../release/v0.2.0/feature.md:4
msgid "Features"
msgstr "功能特性"

#: ../../release/v0.2.0/feature.md:5
msgid "Framkework  Advanced Feature"
msgstr "框架高级特性"

#: ../../release/v0.2.0/feature.md:6
msgid ""
"[PD Disaggregation](../../backend/pd_disaggregation.ipynb) && [PD "
"Entrance Transpose](../../backend/pd_entrance_transpose.md)"
msgstr ""
"[PD 分离架构](../../backend/pd_disaggregation.ipynb) && [PD "
"入口转置](../../backend/pd_entrance_transpose.md)"

#: ../../release/v0.2.0/feature.md:7
msgid ""
"[Attention Support more Backend](../../backend/attention_backend.md): "
"XQA, FlashInfer"
msgstr "[注意力机制支持更多后端](../../backend/attention_backend.md): XQA, FlashInfer"

#: ../../release/v0.2.0/feature.md:8
msgid "[Speculative Decoding](../../backend/speculative_decoding.md)"
msgstr "[推测解码](../../backend/speculative_decoding.md)"

#: ../../release/v0.2.0/feature.md:9
msgid "[EPLB](../../references/deepseek/reporter.md#eplb)"
msgstr "[EPLB](../../references/deepseek/reporter.md#eplb)"

#: ../../release/v0.2.0/feature.md:10
msgid ""
"[MicroBatch & Overlapping](../../references/deepseek/reporter.md"
"#microbatch-overlapping)"
msgstr "[微批处理与重叠](../../references/deepseek/reporter.md#microbatch-overlapping)"

#: ../../release/v0.2.0/feature.md:11
msgid "[MTP](../../references/deepseek/reporter.md#mtp)"
msgstr "[MTP](../../references/deepseek/reporter.md#mtp)"

#: ../../release/v0.2.0/feature.md:12
msgid "[DeepEP](../../references/deepseek/reporter.md#deepep-network)"
msgstr "[DeepEP](../../references/deepseek/reporter.md#deepep-network)"

#: ../../release/v0.2.0/feature.md:13
msgid "[LoadBalance](../../backend/flexlb.md)"
msgstr "[负载均衡](../../backend/flexlb.md)"

#: ../../release/v0.2.0/feature.md:14
msgid "[3FS](../../backend/3fs.md)"
msgstr "[3FS](../../backend/3fs.md)"

#: ../../release/v0.2.0/feature.md:15
msgid "[FP8 KVCache](../../backend/KvCache.md)"
msgstr "[FP8 KV缓存](../../backend/KvCache.md)"

#: ../../release/v0.2.0/feature.md:16
msgid "[REUSE KV CACHE](../../backend/reuse_kv_cache.md)"
msgstr "[KV缓存重用](../../backend/reuse_kv_cache.md)"

#: ../../release/v0.2.0/feature.md:17
msgid "[Quantization](../../backend/quantization.md)"
msgstr "[量化](../../backend/quantization.md)"

#: ../../release/v0.2.0/feature.md:18
msgid "[MultiLoRA](../../backend/lora.ipynb)"
msgstr "[多LoRA](../../backend/lora.ipynb)"

#: ../../release/v0.2.0/feature.md:19
msgid "[Attention FFN Disaggregation](../../backend/af_disaggregation.md)"
msgstr "[注意力FFN分离架构](../../backend/af_disaggregation.md)"

#: ../../release/v0.2.0/feature.md:20
msgid "[Frontend/Backend Disaggregation](../../backend/Frontend.md)"
msgstr "[前端/后端分离架构](../../backend/Frontend.md)"

#: ../../release/v0.2.0/feature.md:23
msgid "New Models"
msgstr "新增模型"

#: ../../release/v0.2.0/feature.md
msgid "**Model Family (Variants)**"
msgstr "**模型系列（变体）**"

#: ../../release/v0.2.0/feature.md
msgid "**Example HuggingFace Identifier**"
msgstr "**示例HuggingFace标识符**"

#: ../../release/v0.2.0/feature.md
msgid "**Description**"
msgstr "**描述**"

#: ../../release/v0.2.0/feature.md
msgid "**Support CardType**"
msgstr "**支持的显卡类型**"

#: ../../release/v0.2.0/feature.md
msgid "**DeepSeek** (v1, v2, v3/R1)"
msgstr "**DeepSeek** (v1, v2, v3/R1)"

#: ../../release/v0.2.0/feature.md
msgid "`deepseek-ai/DeepSeek-R1`"
msgstr "`deepseek-ai/DeepSeek-R1`"

#: ../../release/v0.2.0/feature.md
msgid ""
"Series of advanced reasoning-optimized models (including a 671B MoE) "
"trained with reinforcement learning; <br>top performance on complex "
"reasoning, math, and code tasks.<br> [RTP-LLM provides Deepseek v3/R1 "
"model-specific optimizations](../../references/deepseek/reporter.md)"
msgstr ""
"通过强化学习训练的高级推理优化模型系列（包括671B MoE）；<br>在复杂推理、数学和代码任务上表现卓越。<br> [RTP-"
"LLM为Deepseek v3/R1模型提供特定优化](../../references/deepseek/reporter.md)"

#: ../../release/v0.2.0/feature.md
msgid "NV ✅<br> AMD ✅"
msgstr "英伟达 ✅<br> AMD ✅"

#: ../../release/v0.2.0/feature.md
msgid "**Kimi** (Kimi-K2)"
msgstr "**Kimi** (Kimi-K2)"

#: ../../release/v0.2.0/feature.md
msgid "`moonshotai/Kimi-K2-Instruct`"
msgstr "`moonshotai/Kimi-K2-Instruct`"

#: ../../release/v0.2.0/feature.md
msgid ""
"Moonshot's MoE LLMs with 1 trillion parameters, exceptional on agentic "
"intellegence"
msgstr "月之暗面拥有1万亿参数的MoE大语言模型，在智能代理方面表现卓越"

#: ../../release/v0.2.0/feature.md
msgid "**Qwen** (v1, v1.5, v2, v2.5, v3, QWQ, Qwen3-Coder)"
msgstr "**Qwen** (v1, v1.5, v2, v2.5, v3, QWQ, Qwen3-Coder)"

#: ../../release/v0.2.0/feature.md
msgid "`Qwen/Qwen3-235B-A22B`"
msgstr "`Qwen/Qwen3-235B-A22B`"

#: ../../release/v0.2.0/feature.md
#, fuzzy
msgid ""
"Series of advanced reasoning-optimized models, <br>Significantly improved"
" performance on reasoning tasks,<br> including logical reasoning, "
"mathematics, science, coding, and academic benchmarks that typically "
"require human expertise — achieving state-of-the-art results among open-"
"source thinking models.<br>Markedly better general capabilities, such as "
"instruction following, tool usage, text generation, and alignment with "
"human preferences.<br>Enhanced 256K long-context understanding "
"capabilities."
msgstr "高级推理优化模型系列，<br>在推理任务上性能显著提升，<br>包括逻辑推理、数学、科学、编码和通常需要人类专业知识的学术基准测试——在开源思考模型中达到最先进的结果。<br>明显更好的通用能力，如指令跟随、工具使用、文本生成和与人类偏好的对齐。<br>增强的256K长上下文理解能力。"

#: ../../release/v0.2.0/feature.md
msgid "**QwenVL** (VL2, VL2.5, VL3)"
msgstr "**QwenVL** (VL2, VL2.5, VL3)"

#: ../../release/v0.2.0/feature.md
msgid "`Qwen/Qwen2-VL-2B`"
msgstr "`Qwen/Qwen2-VL-2B`"

#: ../../release/v0.2.0/feature.md
msgid "Series of advanced  Vision-language model series based on Qwen2.5/Qwen3"
msgstr "基于Qwen2.5/Qwen3的高级视觉语言模型系列"

#: ../../release/v0.2.0/feature.md
msgid "NV ✅<br> AMD ❌"
msgstr "英伟达 ✅<br> AMD ❌"

#: ../../release/v0.2.0/feature.md
msgid "**Llama**"
msgstr "**Llama**"

#: ../../release/v0.2.0/feature.md
msgid "`meta-llama/Llama-4-Scout-17B-16E-Instruct`"
msgstr "`meta-llama/Llama-4-Scout-17B-16E-Instruct`"

#: ../../release/v0.2.0/feature.md
msgid ""
"Meta’s open LLM series, spanning 7B to 400B parameters (Llama 2, 3, and "
"new Llama 4) with well-recognized performance."
msgstr "Meta的开放大语言模型系列，参数规模从7B到400B（Llama 2、3和新Llama 4），具有广受认可的性能。"

#: ../../release/v0.2.0/feature.md:32
msgid "Bug Fixs"
msgstr "错误修复"

#: ../../release/v0.2.0/feature.md:33
msgid ""
"P/D Disaggregation dead lock casuse by request cancel/failed before "
"remote running"
msgstr "由远程运行前请求取消/失败导致的P/D分离架构死锁"

#: ../../release/v0.2.0/feature.md:34
msgid "Raw Request stream stop_words cause fake hang"
msgstr "原始请求流stop_words导致假挂起"

#: ../../release/v0.2.0/feature.md:35
msgid "some speculative decoding bugs"
msgstr ""

#: ../../release/v0.2.0/feature.md:36
msgid "Warmup produce nan maybe influence kvcache"
msgstr ""

#: ../../release/v0.2.0/feature.md:37
msgid "Not success query make bad kvcache case wrong answer"
msgstr ""

#: ../../release/v0.2.0/feature.md:38
msgid "UseAllGather takes effect automatically according to the DP/TP"
msgstr ""

#: ../../release/v0.2.0/feature.md:39
msgid "UseAllGather with deepgemm coredump cause by topk type is bad."
msgstr ""

#: ../../release/v0.2.0/feature.md:40
msgid "FlexLb too many log cause bad performance"
msgstr ""

#: ../../release/v0.2.0/feature.md:41
msgid "Flexlb support PD_FUSION"
msgstr ""

#: ../../release/v0.2.0/feature.md:43
msgid "Question of omission"
msgstr "遗漏问题"

#: ../../release/v0.2.0/feature.md:44
msgid ""
"In 3fs Case need more MEM or set FRONTEND_SERVER_COUNT=1 to reduce "
"frontend_server mem usage in P/D when Use Frontend Disaggregation."
msgstr "在3FS情况下，使用前端分离架构时需要更多内存或设置FRONTEND_SERVER_COUNT=1来减少P/D中frontend_server的内存使用。"

#: ../../release/v0.2.0/feature.md:45
msgid "too many dynamic lora need more reserver_runtime_mem_mb"
msgstr "过多的动态LoRA需要更多的reserver_runtime_mem_mb"

#: ../../release/v0.2.0/feature.md:46
msgid "AMD not support MoE models"
msgstr "AMD不支持MoE模型"

#: ../../release/v0.2.0/feature.md:47
msgid "MoE model without shared_experter cannot use enable-layer-micro-batch"
msgstr "没有shared_experter的MoE模型无法使用enable-layer-micro-batch"

#: ../../release/v0.2.0/feature.md:48
msgid "P/D Disaggregation with EPLB and MTP step > 1 may cause Prefill Hang"
msgstr "带有EPLB和MTP step > 1的P/D分离架构可能导致Prefill挂起"

#: ../../release/v0.2.0/feature.md:49
#, fuzzy
msgid "Embedding of VL Model is not ok cause by position id is wrong"
msgstr "因为position id 不对导致VL de Embedding 结果有问题"

#: ../../release/v0.2.0/feature.md:50
msgid ""
"FlexLb: Frequent switching of a large number of machines results in the "
"performance degradation of flexlb"
msgstr ""

#: ../../release/v0.2.0/feature.md:53
msgid "Performance"
msgstr "性能"

#: ../../release/v0.2.0/feature.md:55
msgid "Compatibility"
msgstr "兼容性"

