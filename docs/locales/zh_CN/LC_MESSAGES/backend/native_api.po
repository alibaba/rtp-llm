# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, RTP-LLM
# This file is distributed under the same license as the RTP-LLM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: RTP-LLM 0.2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-09 17:27+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../backend/native_api.ipynb:9
msgid "RTP-LLM Native APIs"
msgstr ""

#: ../../backend/native_api.ipynb:11
msgid ""
"Apart from the OpenAI compatible APIs, the RTP-LLM Runtime also provides "
"its native server APIs. We introduce these following APIs:"
msgstr ""

#: ../../backend/native_api.ipynb:14
msgid "method_name"
msgstr ""

#: ../../backend/native_api.ipynb:14
msgid "example_request"
msgstr ""

#: ../../backend/native_api.ipynb:14
msgid "is_post"
msgstr ""

#: ../../backend/native_api.ipynb:14
msgid "is_get"
msgstr ""

#: ../../backend/native_api.ipynb:14
msgid "desc"
msgstr ""

#: ../../backend/native_api.ipynb:16
msgid "``/``"
msgstr ""

#: ../../backend/native_api.ipynb:16
#, python-brace-format
msgid ""
"``{\"prompt\": \"Hello\", \"generate_config\": {\"max_new_tokens\": 10, "
"\"top_k\": 1, \"top_p\": 0}}``"
msgstr ""

#: ../../backend/native_api.ipynb:16 ../../backend/native_api.ipynb:20
#: ../../backend/native_api.ipynb:24 ../../backend/native_api.ipynb:27
#: ../../backend/native_api.ipynb:31 ../../backend/native_api.ipynb:35
#: ../../backend/native_api.ipynb:38 ../../backend/native_api.ipynb:41
#: ../../backend/native_api.ipynb:44 ../../backend/native_api.ipynb:48
#: ../../backend/native_api.ipynb:52 ../../backend/native_api.ipynb:55
#: ../../backend/native_api.ipynb:58 ../../backend/native_api.ipynb:61
#: ../../backend/native_api.ipynb:64 ../../backend/native_api.ipynb:67
#: ../../backend/native_api.ipynb:70 ../../backend/native_api.ipynb:73
#: ../../backend/native_api.ipynb:79 ../../backend/native_api.ipynb:85
#: ../../backend/native_api.ipynb:90
msgid "✅"
msgstr ""

#: ../../backend/native_api.ipynb:16 ../../backend/native_api.ipynb:24
#: ../../backend/native_api.ipynb:27 ../../backend/native_api.ipynb:31
#: ../../backend/native_api.ipynb:52 ../../backend/native_api.ipynb:55
#: ../../backend/native_api.ipynb:58 ../../backend/native_api.ipynb:61
#: ../../backend/native_api.ipynb:64 ../../backend/native_api.ipynb:67
#: ../../backend/native_api.ipynb:70 ../../backend/native_api.ipynb:73
#: ../../backend/native_api.ipynb:79 ../../backend/native_api.ipynb:85
#: ../../backend/native_api.ipynb:90
msgid "❌"
msgstr ""

#: ../../backend/native_api.ipynb:16
msgid "Basic text-generation endpoint (backward-compatible with early versions)."
msgstr ""

#: ../../backend/native_api.ipynb:20
msgid "``/chat/render``"
msgstr ""

#: ../../backend/native_api.ipynb:20 ../../backend/native_api.ipynb:24
#, python-brace-format
msgid "``{\"messages\": [{\"role\": \"user\",\"content\": \"hello？\"}]}``"
msgstr ""

#: ../../backend/native_api.ipynb:20
msgid ""
"Render the chat template into the final prompt that will be sent to the "
"model."
msgstr ""

#: ../../backend/native_api.ipynb:24
msgid "``/v1/chat/render``"
msgstr ""

#: ../../backend/native_api.ipynb:24
msgid "v1 path for ``/chat/render`` (POST only)."
msgstr ""

#: ../../backend/native_api.ipynb:27
msgid "``/tokenizer/encode``"
msgstr ""

#: ../../backend/native_api.ipynb:27 ../../backend/native_api.ipynb:31
#, python-brace-format
msgid "``{\"prompt\": \"hello\"}``"
msgstr ""

#: ../../backend/native_api.ipynb:27
msgid "Encode text into a list of token IDs using the internal tokenizer."
msgstr ""

#: ../../backend/native_api.ipynb:31
msgid "``/tokenize``"
msgstr ""

#: ../../backend/native_api.ipynb:31
msgid "Lightweight tokenization endpoint that returns an array of tokens."
msgstr ""

#: ../../backend/native_api.ipynb:35
msgid "``/rtp_llm/worker_status``"
msgstr ""

#: ../../backend/native_api.ipynb:35 ../../backend/native_api.ipynb:38
#, python-brace-format
msgid "``{ \"latest_cache_version\": -1}``"
msgstr ""

#: ../../backend/native_api.ipynb:35
msgid "Detailed status of a worker in the RTP-LLM framework."
msgstr ""

#: ../../backend/native_api.ipynb:38
msgid "``/worker_status``"
msgstr ""

#: ../../backend/native_api.ipynb:38
msgid "Query runtime status of the inference worker."
msgstr ""

#: ../../backend/native_api.ipynb:41
msgid "``/health``"
msgstr ""

#: ../../backend/native_api.ipynb:41 ../../backend/native_api.ipynb:44
#, python-brace-format
msgid "``{}``"
msgstr ""

#: ../../backend/native_api.ipynb:41
msgid "Generic health check; returns whether the service is alive."
msgstr ""

#: ../../backend/native_api.ipynb:44
msgid "``/status``"
msgstr ""

#: ../../backend/native_api.ipynb:44
msgid ""
"Retrieve comprehensive status information for the current service "
"instance."
msgstr ""

#: ../../backend/native_api.ipynb:48
msgid "``/health_check``"
msgstr ""

#: ../../backend/native_api.ipynb:48
#, python-brace-format
msgid "``{\"latest_cache_version\": -1}``"
msgstr ""

#: ../../backend/native_api.ipynb:48
msgid "Deep health check that includes a cache version number."
msgstr ""

#: ../../backend/native_api.ipynb:52
msgid "``/update``"
msgstr ""

#: ../../backend/native_api.ipynb:52
#, python-brace-format
msgid ""
"``{\"peft_info\": {\"lora_info\": {\"lora_0\": \"/lora/llama-lora-"
"test/\"}}}``"
msgstr ""

#: ../../backend/native_api.ipynb:52
msgid "Hot-reload LoRA info into the running service."
msgstr ""

#: ../../backend/native_api.ipynb:55 ../../backend/native_api.ipynb:301
msgid "``/v1/models``"
msgstr ""

#: ../../backend/native_api.ipynb:55
msgid "List currently deployed models (OpenAI-compatible)."
msgstr ""

#: ../../backend/native_api.ipynb:58 ../../backend/native_api.ipynb:327
msgid "``/set_log_level``"
msgstr ""

#: ../../backend/native_api.ipynb:58
#, python-brace-format
msgid "``{ \"log_level\": \"INFO\"}``"
msgstr ""

#: ../../backend/native_api.ipynb:58
msgid "Dynamically adjust the service log level."
msgstr ""

#: ../../backend/native_api.ipynb:61 ../../backend/native_api.ipynb:355
msgid "``/update_eplb_config``"
msgstr ""

#: ../../backend/native_api.ipynb:61
#, python-brace-format
msgid "``{\"model\": \"EPLB\", \"update_time\":1000}``"
msgstr ""

#: ../../backend/native_api.ipynb:61
msgid "Update the EPLB (Elastic Load Balancer) configuration."
msgstr ""

#: ../../backend/native_api.ipynb:64
msgid "``/v1/embeddings``"
msgstr ""

#: ../../backend/native_api.ipynb:64
#, python-brace-format
msgid "``{\"input\": \"who are u\", \"model\": \"text-embedding-ada-002\"}``"
msgstr ""

#: ../../backend/native_api.ipynb:64
msgid "OpenAI-compatible dense-vector embedding endpoint."
msgstr ""

#: ../../backend/native_api.ipynb:67
msgid "``/v1/embeddings/dense``"
msgstr ""

#: ../../backend/native_api.ipynb:67 ../../backend/native_api.ipynb:70
#, python-brace-format
msgid "``{\"input\": \"who are u\"}``"
msgstr ""

#: ../../backend/native_api.ipynb:67
msgid "Return **dense** embeddings only."
msgstr ""

#: ../../backend/native_api.ipynb:70
msgid "``/v1/embeddings/sparse``"
msgstr ""

#: ../../backend/native_api.ipynb:70
msgid "Return **sparse** embeddings only (e.g., BM25/TF-IDF)."
msgstr ""

#: ../../backend/native_api.ipynb:73
msgid "``/v1/embeddings/colbert``"
msgstr ""

#: ../../backend/native_api.ipynb:73
#, python-brace-format
msgid "``{\"input\":[\"hello, what is your name?\",\"hello\"],\"model\":\"xx\"}``"
msgstr ""

#: ../../backend/native_api.ipynb:73
msgid ""
"Return **ColBERT** late-interaction multi-vector representations for "
"high-accuracy semantic retrieval."
msgstr ""

#: ../../backend/native_api.ipynb:79
msgid "``/v1/embeddings/similarity``"
msgstr ""

#: ../../backend/native_api.ipynb:79
#, python-brace-format
msgid ""
"``{\"left\":[\"hello, what is your name?\"],\"right\":[\"hello\",\"what "
"is your "
"name\"],\"embedding_config\":{\"type\":\"sparse\"},\"model\":\"xx\"}``"
msgstr ""

#: ../../backend/native_api.ipynb:79
msgid ""
"Accept query–doc pairs and return pairwise similarities (cosine/dot) "
"directly, skipping the separate embedding step."
msgstr ""

#: ../../backend/native_api.ipynb:85
msgid "``/v1/classifier``"
msgstr ""

#: ../../backend/native_api.ipynb:85
#, python-brace-format
msgid ""
"``{\"input\":[[\"what is panda?\",\"hi\"],[\"what is panda?\",\"The giant"
" panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply "
"panda, is a bear species endemic to China.\"]],\"model\":\"xx\"}``"
msgstr ""

#: ../../backend/native_api.ipynb:85
msgid ""
"Generic text-classification endpoint supporting tasks such as sentiment "
"or topic classification."
msgstr ""

#: ../../backend/native_api.ipynb:90
msgid "``/v1/reranker``"
msgstr ""

#: ../../backend/native_api.ipynb:90
#, python-brace-format
msgid ""
"``{\"query\":\"what is panda? \",\"documents\":[\"hi\",\"The giant panda "
"(Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, "
"is a bear species endemic to China.\",\"gg\"]}``"
msgstr ""

#: ../../backend/native_api.ipynb:90
msgid ""
"Rerank a list of retrieved documents by relevance and return the "
"reordered results."
msgstr ""

#: ../../backend/native_api.ipynb:95
msgid ""
"We mainly use **requests** to test these APIs in the following examples. "
"You can also use **curl**."
msgstr ""

#: ../../backend/native_api.ipynb:107
msgid "Launch A Server"
msgstr ""

#: ../../backend/native_api.ipynb:144
msgid "Generate (text generation model)"
msgstr ""

#: ../../backend/native_api.ipynb:146
msgid ""
"Generate completions. This is similar to the ``/v1/completions`` in "
"OpenAI API. Detailed parameters can be found in the `sampling parameters "
"<./sampling_params.md>`__."
msgstr ""

#: ../../backend/native_api.ipynb:174
msgid "Chat Render / Tokenizer"
msgstr ""

#: ../../backend/native_api.ipynb:176
msgid "``/chat/render``\\ 、\\ ``/v1/chat/render``: Chat Template Render"
msgstr ""

#: ../../backend/native_api.ipynb:177
msgid "``/tokenizer/encode``, ``/tokenize``: Raw prompt tokenize"
msgstr ""

#: ../../backend/native_api.ipynb:217
msgid "Worker Status"
msgstr ""

#: ../../backend/native_api.ipynb:219
msgid ""
"``/rtp_llm/worker_status``\\ 、\\ ``/worker_status``: Server for "
"processing snapshot, includes RunningTask, FinishedTask, CacheStatus."
msgstr ""

#: ../../backend/native_api.ipynb:245
msgid "Health Check"
msgstr ""

#: ../../backend/native_api.ipynb:247
msgid "``/health``\\ 、\\ ``/status``: Check the health of the server."
msgstr ""

#: ../../backend/native_api.ipynb:271
msgid "Update Lora Info"
msgstr ""

#: ../../backend/native_api.ipynb:273
msgid "``/update``: Update full LoRA Info"
msgstr ""

#: ../../backend/native_api.ipynb:299
msgid "Get Model Info"
msgstr ""

#: ../../backend/native_api.ipynb:325
msgid "Update Log Level"
msgstr ""

#: ../../backend/native_api.ipynb:353
msgid "Update EPLB Config for MoE"
msgstr ""

#: ../../backend/native_api.ipynb:381
msgid "Encode (embedding model)"
msgstr ""

#: ../../backend/native_api.ipynb:383
msgid ""
"Encode text into embeddings. Note that this API is only available for "
"`embedding models <openai_api_embeddings.html#openai-apis-embedding>`__ "
"and will raise an error for generation models. Therefore, we launch a new"
" server to server an embedding model."
msgstr ""

#: ../../backend/native_api.ipynb:483
msgid "v1/rerank (cross encoder rerank model)"
msgstr ""

#: ../../backend/native_api.ipynb:485
msgid ""
"Rerank a list of documents given a query using a cross-encoder model. "
"Note that this API is only available for cross encoder model like `BAAI"
"/bge-reranker-v2-m3 <https://huggingface.co/BAAI/bge-reranker-v2-m3>`__ "
"with ``attention-backend`` ``triton`` and ``torch_native``."
msgstr ""

#: ../../backend/native_api.ipynb:556
msgid "Classify"
msgstr ""

#: ../../backend/native_api.ipynb:558
msgid ""
"RTP-LL Runtime also supports classify models. Here we use a classify "
"model to classify the quality of pairwise generations."
msgstr ""

