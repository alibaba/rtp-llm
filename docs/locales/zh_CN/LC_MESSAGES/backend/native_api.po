# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, RTP-LLM
# This file is distributed under the same license as the RTP-LLM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: RTP-LLM 0.2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-19 11:21+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../backend/native_api.ipynb:9
msgid "RTP-LLM Native APIs"
msgstr "RTP-LLM Native API"

#: ../../backend/native_api.ipynb:11
msgid ""
"Apart from the OpenAI compatible APIs, the RTP-LLM Runtime also provides "
"its native server APIs. We introduce these following APIs:"
msgstr "除了OpenAI兼容的API之外，RTP-LLM运行时还提供其Native服务器API。我们介绍以下API："

#: ../../backend/native_api.ipynb:14
msgid "method_name"
msgstr "方法名"

#: ../../backend/native_api.ipynb:14
msgid "example_request"
msgstr "请求示例"

#: ../../backend/native_api.ipynb:14
msgid "is_post"
msgstr "是否POST"

#: ../../backend/native_api.ipynb:14
msgid "is_get"
msgstr "是否GET"

#: ../../backend/native_api.ipynb:14
msgid "desc"
msgstr "描述"

#: ../../backend/native_api.ipynb:16
msgid "``/``"
msgstr "``/``"

#: ../../backend/native_api.ipynb:16
#, python-brace-format
msgid ""
"``{\"prompt\": \"Hello\", \"generate_config\": {\"max_new_tokens\": 10, "
"\"top_k\": 1, \"top_p\": 0}}``"
msgstr "``{\"prompt\": \"Hello\", \"generate_config\": {\"max_new_tokens\": 10, \"top_k\": 1, \"top_p\": 0}}``"

#: ../../backend/native_api.ipynb:16 ../../backend/native_api.ipynb:20
#: ../../backend/native_api.ipynb:24 ../../backend/native_api.ipynb:27
#: ../../backend/native_api.ipynb:31 ../../backend/native_api.ipynb:35
#: ../../backend/native_api.ipynb:38 ../../backend/native_api.ipynb:41
#: ../../backend/native_api.ipynb:44 ../../backend/native_api.ipynb:48
#: ../../backend/native_api.ipynb:52 ../../backend/native_api.ipynb:55
#: ../../backend/native_api.ipynb:58 ../../backend/native_api.ipynb:61
#: ../../backend/native_api.ipynb:64 ../../backend/native_api.ipynb:67
#: ../../backend/native_api.ipynb:70 ../../backend/native_api.ipynb:73
#: ../../backend/native_api.ipynb:79 ../../backend/native_api.ipynb:85
#: ../../backend/native_api.ipynb:90
msgid "✅"
msgstr ""

#: ../../backend/native_api.ipynb:16 ../../backend/native_api.ipynb:24
#: ../../backend/native_api.ipynb:27 ../../backend/native_api.ipynb:31
#: ../../backend/native_api.ipynb:52 ../../backend/native_api.ipynb:55
#: ../../backend/native_api.ipynb:58 ../../backend/native_api.ipynb:61
#: ../../backend/native_api.ipynb:64 ../../backend/native_api.ipynb:67
#: ../../backend/native_api.ipynb:70 ../../backend/native_api.ipynb:73
#: ../../backend/native_api.ipynb:79 ../../backend/native_api.ipynb:85
#: ../../backend/native_api.ipynb:90
msgid "❌"
msgstr ""

#: ../../backend/native_api.ipynb:16
msgid "Basic text-generation endpoint (backward-compatible with early versions)."
msgstr "基本文本生成端点（向后兼容早期版本）。"

#: ../../backend/native_api.ipynb:20
msgid "``/chat/render``"
msgstr "``/chat/render``"

#: ../../backend/native_api.ipynb:20 ../../backend/native_api.ipynb:24
#, python-brace-format
msgid "``{\"messages\": [{\"role\": \"user\",\"content\": \"hello？\"}]}``"
msgstr "``{\"messages\": [{\"role\": \"user\",\"content\": \"hello？\"}]}``"

#: ../../backend/native_api.ipynb:20
msgid ""
"Render the chat template into the final prompt that will be sent to the "
"model."
msgstr "将聊天模板渲染成将发送给模型的最终提示。"

#: ../../backend/native_api.ipynb:24
msgid "``/v1/chat/render``"
msgstr "``/v1/chat/render``"

#: ../../backend/native_api.ipynb:24
msgid "v1 path for ``/chat/render`` (POST only)."
msgstr "``/chat/render``的v1路径（仅POST）。"

#: ../../backend/native_api.ipynb:27
msgid "``/tokenizer/encode``"
msgstr "``/tokenizer/encode``"

#: ../../backend/native_api.ipynb:27 ../../backend/native_api.ipynb:31
#, python-brace-format
msgid "``{\"prompt\": \"hello\"}``"
msgstr "``{\"prompt\": \"hello\"}``"

#: ../../backend/native_api.ipynb:27
msgid "Encode text into a list of token IDs using the internal tokenizer."
msgstr "使用内部tokenizer将文本编码成token ID列表。"

#: ../../backend/native_api.ipynb:31
msgid "``/tokenize``"
msgstr "``/tokenize``"

#: ../../backend/native_api.ipynb:31
msgid "Lightweight tokenization endpoint that returns an array of tokens."
msgstr "轻量级tokenization端点，返回token数组。"

#: ../../backend/native_api.ipynb:35
msgid "``/rtp_llm/worker_status``"
msgstr "``/rtp_llm/worker_status``"

#: ../../backend/native_api.ipynb:35 ../../backend/native_api.ipynb:38
#, python-brace-format
msgid "``{ \"latest_cache_version\": -1}``"
msgstr "``{ \"latest_cache_version\": -1}``"

#: ../../backend/native_api.ipynb:35
msgid "Detailed status of a worker in the RTP-LLM framework."
msgstr "RTP-LLM框架中worker的详细状态。"

#: ../../backend/native_api.ipynb:38
msgid "``/worker_status``"
msgstr "``/worker_status``"

#: ../../backend/native_api.ipynb:38
msgid "Query runtime status of the inference worker."
msgstr "查询推理worker的运行时状态。"

#: ../../backend/native_api.ipynb:41
msgid "``/health``"
msgstr "``/health``"

#: ../../backend/native_api.ipynb:41 ../../backend/native_api.ipynb:44
#, python-brace-format
msgid "``{}``"
msgstr "``{}``"

#: ../../backend/native_api.ipynb:41
msgid "Generic health check; returns whether the service is alive."
msgstr "通用健康检查；返回服务是否存活。"

#: ../../backend/native_api.ipynb:44
msgid "``/status``"
msgstr "``/status``"

#: ../../backend/native_api.ipynb:44
msgid ""
"Retrieve comprehensive status information for the current service "
"instance."
msgstr "获取当前服务实例的全面状态信息。"

#: ../../backend/native_api.ipynb:48
msgid "``/health_check``"
msgstr "``/health_check``"

#: ../../backend/native_api.ipynb:48
#, python-brace-format
msgid "``{\"latest_cache_version\": -1}``"
msgstr "``{\"latest_cache_version\": -1}``"

#: ../../backend/native_api.ipynb:48
msgid "Deep health check that includes a cache version number."
msgstr "深度健康检查，包含缓存版本号。"

#: ../../backend/native_api.ipynb:52
msgid "``/update``"
msgstr "``/update``"

#: ../../backend/native_api.ipynb:52
#, python-brace-format
msgid ""
"``{\"peft_info\": {\"lora_info\": {\"lora_0\": \"/lora/llama-lora-"
"test/\"}}}``"
msgstr "``{\"peft_info\": {\"lora_info\": {\"lora_0\": \"/lora/llama-lora-test/\"}}}``"

#: ../../backend/native_api.ipynb:52
msgid "Hot-reload LoRA info into the running service."
msgstr "热加载LoRA信息到运行中的服务。"

#: ../../backend/native_api.ipynb:55 ../../backend/native_api.ipynb:301
msgid "``/v1/models``"
msgstr "``/v1/models``"

#: ../../backend/native_api.ipynb:55
msgid "List currently deployed models (OpenAI-compatible)."
msgstr "列出当前部署的模型（OpenAI兼容）。"

#: ../../backend/native_api.ipynb:58 ../../backend/native_api.ipynb:327
msgid "``/set_log_level``"
msgstr "``/set_log_level``"

#: ../../backend/native_api.ipynb:58
#, python-brace-format
msgid "``{ \"log_level\": \"INFO\"}``"
msgstr "``{ \"log_level\": \"INFO\"}``"

#: ../../backend/native_api.ipynb:58
msgid "Dynamically adjust the service log level."
msgstr "动态调整服务日志级别。"

#: ../../backend/native_api.ipynb:61 ../../backend/native_api.ipynb:355
msgid "``/update_eplb_config``"
msgstr "``/update_eplb_config``"

#: ../../backend/native_api.ipynb:61
#, python-brace-format
msgid "``{\"model\": \"EPLB\", \"update_time\":1000}``"
msgstr "``{\"model\": \"EPLB\", \"update_time\":1000}``"

#: ../../backend/native_api.ipynb:61
msgid "Update the EPLB (Elastic Load Balancer) configuration."
msgstr "更新EPLB（弹性负载均衡器）配置。"

#: ../../backend/native_api.ipynb:64
msgid "``/v1/embeddings``"
msgstr "``/v1/embeddings``"

#: ../../backend/native_api.ipynb:64
#, python-brace-format
msgid "``{\"input\": \"who are u\", \"model\": \"text-embedding-ada-002\"}``"
msgstr "``{\"input\": \"who are u\", \"model\": \"text-embedding-ada-002\"}``"

#: ../../backend/native_api.ipynb:64
msgid "OpenAI-compatible dense-vector embedding endpoint."
msgstr "OpenAI兼容的稠密向量嵌入端点。"

#: ../../backend/native_api.ipynb:67
msgid "``/v1/embeddings/dense``"
msgstr "``/v1/embeddings/dense``"

#: ../../backend/native_api.ipynb:67 ../../backend/native_api.ipynb:70
#, python-brace-format
msgid "``{\"input\": \"who are u\"}``"
msgstr "``{\"input\": \"who are u\"}``"

#: ../../backend/native_api.ipynb:67
msgid "Return **dense** embeddings only."
msgstr "仅返回**稠密**嵌入。"

#: ../../backend/native_api.ipynb:70
msgid "``/v1/embeddings/sparse``"
msgstr "``/v1/embeddings/sparse``"

#: ../../backend/native_api.ipynb:70
msgid "Return **sparse** embeddings only (e.g., BM25/TF-IDF)."
msgstr "仅返回**稀疏**嵌入（例如，BM25/TF-IDF）。"

#: ../../backend/native_api.ipynb:73
msgid "``/v1/embeddings/colbert``"
msgstr "``/v1/embeddings/colbert``"

#: ../../backend/native_api.ipynb:73
#, python-brace-format
msgid "``{\"input\":[\"hello, what is your name?\",\"hello\"],\"model\":\"xx\"}``"
msgstr "``{\"input\":[\"hello, what is your name?\",\"hello\"],\"model\":\"xx\"}``"

#: ../../backend/native_api.ipynb:73
msgid ""
"Return **ColBERT** late-interaction multi-vector representations for "
"high-accuracy semantic retrieval."
msgstr "返回用于高精度语义检索的**ColBERT**后期交互多向量表示。"

#: ../../backend/native_api.ipynb:79
msgid "``/v1/embeddings/similarity``"
msgstr "``/v1/embeddings/similarity``"

#: ../../backend/native_api.ipynb:79
#, python-brace-format
msgid ""
"``{\"left\":[\"hello, what is your name?\"],\"right\":[\"hello\",\"what "
"is your "
"name\"],\"embedding_config\":{\"type\":\"sparse\"},\"model\":\"xx\"}``"
msgstr "``{\"left\":[\"hello, what is your name?\"],\"right\":[\"hello\",\"what is your name\"],\"embedding_config\":{\"type\":\"sparse\"},\"model\":\"xx\"}``"

#: ../../backend/native_api.ipynb:79
msgid ""
"Accept query–doc pairs and return pairwise similarities (cosine/dot) "
"directly, skipping the separate embedding step."
msgstr "接受查询-文档对，直接返回成对相似度（余弦/点积），跳过单独的嵌入步骤。"

#: ../../backend/native_api.ipynb:85
msgid "``/v1/classifier``"
msgstr "``/v1/classifier``"

#: ../../backend/native_api.ipynb:85
#, python-brace-format
msgid ""
"``{\"input\":[[\"what is panda?\",\"hi\"],[\"what is panda?\",\"The giant"
" panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply "
"panda, is a bear species endemic to China.\"]],\"model\":\"xx\"}``"
msgstr "``{\"input\":[[\"what is panda?\",\"hi\"],[\"what is panda?\",\"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"]],\"model\":\"xx\"}``"

#: ../../backend/native_api.ipynb:85
msgid ""
"Generic text-classification endpoint supporting tasks such as sentiment "
"or topic classification."
msgstr "通用文本分类端点，支持情感分类或主题分类等任务。"

#: ../../backend/native_api.ipynb:90
msgid "``/v1/reranker``"
msgstr "``/v1/reranker``"

#: ../../backend/native_api.ipynb:90
#, python-brace-format
msgid ""
"``{\"query\":\"what is panda? \",\"documents\":[\"hi\",\"The giant panda "
"(Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, "
"is a bear species endemic to China.\",\"gg\"]}``"
msgstr "``{\"query\":\"what is panda? \",\"documents\":[\"hi\",\"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\"gg\"]}``"

#: ../../backend/native_api.ipynb:90
msgid ""
"Rerank a list of retrieved documents by relevance and return the "
"reordered results."
msgstr "根据相关性重新排序检索到的文档列表并返回重新排序的结果。"

#: ../../backend/native_api.ipynb:95
msgid ""
"We mainly use **requests** to test these APIs in the following examples. "
"You can also use **curl**."
msgstr "我们主要使用**requests**在以下示例中测试这些API。您也可以使用**curl**。"

#: ../../backend/native_api.ipynb:107
msgid "Launch A Server"
msgstr "启动服务器"

#: ../../backend/native_api.ipynb:144
msgid "Generate (text generation model)"
msgstr "生成（文本生成模型）"

#: ../../backend/native_api.ipynb:146
msgid ""
"Generate completions. This is similar to the ``/v1/completions`` in "
"OpenAI API. Detailed parameters can be found in the `sampling parameters "
"<./sampling_params.md>`__."
msgstr ""
"生成补全。这类似于OpenAI API中的``/v1/completions``。详细参数可以在`sampling parameters "
"<./sampling_params.md>`__中找到。"

#: ../../backend/native_api.ipynb:174
msgid "Chat Render / Tokenizer"
msgstr "聊天渲染/分词器"

#: ../../backend/native_api.ipynb:176
msgid "``/chat/render``\\ 、\\ ``/v1/chat/render``: Chat Template Render"
msgstr "``/chat/render``\\ 、\\ ``/v1/chat/render``: 聊天模板渲染"

#: ../../backend/native_api.ipynb:177
msgid "``/tokenizer/encode``, ``/tokenize``: Raw prompt tokenize"
msgstr "``/tokenizer/encode``、``/tokenize``: 原始提示分词"

#: ../../backend/native_api.ipynb:217
msgid "Worker Status"
msgstr "Worker状态"

#: ../../backend/native_api.ipynb:219
msgid ""
"``/rtp_llm/worker_status``\\ 、\\ ``/worker_status``: Server for "
"processing snapshot, includes RunningTask, FinishedTask, CacheStatus."
msgstr ""
"``/rtp_llm/worker_status``\\ 、\\ ``/worker_status``: "
"处理快照的服务器，包括RunningTask、FinishedTask、CacheStatus。"

#: ../../backend/native_api.ipynb:245
msgid "Health Check"
msgstr "健康检查"

#: ../../backend/native_api.ipynb:247
msgid "``/health``\\ 、\\ ``/status``: Check the health of the server."
msgstr "``/health``\\ 、\\ ``/status``: 检查服务器健康状况。"

#: ../../backend/native_api.ipynb:271
msgid "Update Lora Info"
msgstr "更新Lora信息"

#: ../../backend/native_api.ipynb:273
msgid "``/update``: Update full LoRA Info"
msgstr "``/update``: 更新完整LoRA信息"

#: ../../backend/native_api.ipynb:299
msgid "Get Model Info"
msgstr "获取模型信息"

#: ../../backend/native_api.ipynb:325
msgid "Update Log Level"
msgstr "更新日志级别"

#: ../../backend/native_api.ipynb:353
msgid "Update EPLB Config for MoE"
msgstr "为MoE更新EPLB配置"

#: ../../backend/native_api.ipynb:381
msgid "Encode (embedding model)"
msgstr "编码（嵌入模型）"

#: ../../backend/native_api.ipynb:383
msgid ""
"Encode text into embeddings. Note that this API is only available for "
"`embedding models <openai_api_embeddings.html#openai-apis-embedding>`__ "
"and will raise an error for generation models. Therefore, we launch a new"
" server to server an embedding model."
msgstr ""
"将文本编码为嵌入。请注意，此API仅适用于`嵌入模型<openai_api_embeddings.html#openai-apis-"
"embedding>`__，对于生成模型将引发错误。因此，我们启动一个新的服务器来服务嵌入模型。"

#: ../../backend/native_api.ipynb:483
msgid "v1/rerank (cross encoder rerank model)"
msgstr "v1/rerank（交叉编码器重排序模型）"

#: ../../backend/native_api.ipynb:485
msgid ""
"Rerank a list of documents given a query using a cross-encoder model. "
"Note that this API is only available for cross encoder model like `BAAI"
"/bge-reranker-v2-m3 <https://huggingface.co/BAAI/bge-reranker-v2-m3>`__ "
"with ``attention-backend`` ``triton`` and ``torch_native``."
msgstr ""
"使用交叉编码器模型对给定查询的文档列表进行重排序。请注意，此API仅适用于像`BAAI/bge-reranker-v2-m3 "
"<https://huggingface.co/BAAI/bge-reranker-v2-m3>`__这样的交叉编码器模型，并且需要"
"``attention-backend``为``triton``和``torch_native``。"

#: ../../backend/native_api.ipynb:556
msgid "Classify"
msgstr "分类"

#: ../../backend/native_api.ipynb:558
msgid ""
"RTP-LL Runtime also supports classify models. Here we use a classify "
"model to classify the quality of pairwise generations."
msgstr "RTP-LL运行时还支持分类模型。这里我们使用分类模型对成对生成的质量进行分类。"

#~ msgid "method_name"
#~ msgstr "方法名"

#~ msgid "example_request"
#~ msgstr "请求示例"

#~ msgid "is_post"
#~ msgstr "是否POST"

#~ msgid "is_get"
#~ msgstr "是否GET"

#~ msgid "desc"
#~ msgstr "描述"

#~ msgid "``/``"
#~ msgstr "``/``"

#~ msgid ""
#~ "``{\"prompt\": \"Hello\", \"generate_config\": "
#~ "{\"max_new_tokens\": 10, \"top_k\": 1, "
#~ "\"top_p\": 0}}``"
#~ msgstr ""
#~ "``{\"prompt\": \"Hello\", \"generate_config\": "
#~ "{\"max_new_tokens\": 10, \"top_k\": 1, "
#~ "\"top_p\": 0}}``"

#~ msgid "✅"
#~ msgstr "✅"

#~ msgid "❌"
#~ msgstr "❌"

#~ msgid ""
#~ "Basic text-generation endpoint (backward-"
#~ "compatible with early versions)."
#~ msgstr "基本文本生成端点（向后兼容早期版本）。"

#~ msgid "``/chat/render``"
#~ msgstr "``/chat/render``"

#~ msgid "``{\"messages\": [{\"role\": \"user\",\"content\": \"hello？\"}]}``"
#~ msgstr "``{\"messages\": [{\"role\": \"user\",\"content\": \"hello？\"}]}``"

#~ msgid ""
#~ "Render the chat template into the "
#~ "final prompt that will be sent to"
#~ " the model."
#~ msgstr "将聊天模板渲染成将发送给模型的最终提示。"

#~ msgid "``/v1/chat/render``"
#~ msgstr "``/v1/chat/render``"

#~ msgid "v1 path for ``/chat/render`` (POST only)."
#~ msgstr "``/chat/render``的v1路径（仅POST）。"

#~ msgid "``/tokenizer/encode``"
#~ msgstr "``/tokenizer/encode``"

#~ msgid "``{\"prompt\": \"hello\"}``"
#~ msgstr ""

#~ msgid "Encode text into a list of token IDs using the internal tokenizer."
#~ msgstr "使用内部tokenizer将文本编码成token ID列表。"

#~ msgid "``/tokenize``"
#~ msgstr "``/tokenize``"

#~ msgid "Lightweight tokenization endpoint that returns an array of tokens."
#~ msgstr "轻量级tokenization端点，返回token数组。"

#~ msgid "``/rtp_llm/worker_status``"
#~ msgstr "``/rtp_llm/worker_status``"

#~ msgid "``{ \"latest_cache_version\": -1}``"
#~ msgstr ""

#~ msgid "Detailed status of a worker in the RTP-LLM framework."
#~ msgstr "RTP-LLM框架中worker的详细状态。"

#~ msgid "``/worker_status``"
#~ msgstr "``/worker_status``"

#~ msgid "Query runtime status of the inference worker."
#~ msgstr "查询推理worker的运行时状态。"

#~ msgid "``/health``"
#~ msgstr "``/health``"

#~ msgid "``{}``"
#~ msgstr ""

#~ msgid "Generic health check; returns whether the service is alive."
#~ msgstr "通用健康检查；返回服务是否存活。"

#~ msgid "``/status``"
#~ msgstr "``/status``"

#~ msgid ""
#~ "Retrieve comprehensive status information for"
#~ " the current service instance."
#~ msgstr "获取当前服务实例的全面状态信息。"

#~ msgid "``/health_check``"
#~ msgstr "``/health_check``"

#~ msgid "``{\"latest_cache_version\": -1}``"
#~ msgstr ""

#~ msgid "Deep health check that includes a cache version number."
#~ msgstr "深度健康检查，包含缓存版本号。"

#~ msgid "``/update``"
#~ msgstr "``/update``"

#~ msgid ""
#~ "``{\"peft_info\": {\"lora_info\": {\"lora_0\": "
#~ "\"/lora/llama-lora-test/\"}}}``"
#~ msgstr ""

#~ msgid "Hot-reload LoRA info into the running service."
#~ msgstr "热加载LoRA信息到运行中的服务。"

#~ msgid "List currently deployed models (OpenAI-compatible)."
#~ msgstr "列出当前部署的模型（OpenAI兼容）。"

#~ msgid "``{ \"log_level\": \"INFO\"}``"
#~ msgstr ""

#~ msgid "Dynamically adjust the service log level."
#~ msgstr "动态调整服务日志级别。"

#~ msgid "``{\"model\": \"EPLB\", \"update_time\":1000}``"
#~ msgstr ""

#~ msgid "Update the EPLB (Elastic Load Balancer) configuration."
#~ msgstr "更新EPLB（弹性负载均衡器）配置。"

#~ msgid "``/v1/embeddings``"
#~ msgstr "``/v1/embeddings``"

#~ msgid "``{\"input\": \"who are u\", \"model\": \"text-embedding-ada-002\"}``"
#~ msgstr ""

#~ msgid "OpenAI-compatible dense-vector embedding endpoint."
#~ msgstr "OpenAI兼容的稠密向量嵌入端点。"

#~ msgid "``/v1/embeddings/dense``"
#~ msgstr "``/v1/embeddings/dense``"

#~ msgid "``{\"input\": \"who are u\"}``"
#~ msgstr ""

#~ msgid "Return **dense** embeddings only."
#~ msgstr "仅返回**稠密**嵌入。"

#~ msgid "``/v1/embeddings/sparse``"
#~ msgstr "``/v1/embeddings/sparse``"

#~ msgid "Return **sparse** embeddings only (e.g., BM25/TF-IDF)."
#~ msgstr "仅返回**稀疏**嵌入（例如，BM25/TF-IDF）。"

#~ msgid "``/v1/embeddings/colbert``"
#~ msgstr "``/v1/embeddings/colbert``"

#~ msgid ""
#~ "``{\"input\":[\"hello, what is your "
#~ "name?\",\"hello\"],\"model\":\"xx\"}``"
#~ msgstr ""

#~ msgid ""
#~ "Return **ColBERT** late-interaction multi-"
#~ "vector representations for high-accuracy "
#~ "semantic retrieval."
#~ msgstr "返回用于高精度语义检索的**ColBERT**后期交互多向量表示。"

#~ msgid "``/v1/embeddings/similarity``"
#~ msgstr "``/v1/embeddings/similarity``"

#~ msgid ""
#~ "Accept query–doc pairs and return "
#~ "pairwise similarities (cosine/dot) directly, "
#~ "skipping the separate embedding step."
#~ msgstr "接受查询-文档对，直接返回成对相似度（余弦/点积），跳过单独的嵌入步骤。"

#~ msgid "``/v1/classifier``"
#~ msgstr "``/v1/classifier``"

#~ msgid ""
#~ "``{\"input\":[[\"what is panda?\",\"hi\"],[\"what is"
#~ " panda?\",\"The giant panda (Ailuropoda "
#~ "melanoleuca), sometimes called a panda "
#~ "bear or simply panda, is a bear"
#~ " species endemic to "
#~ "China.\"]],\"model\":\"xx\"}``"
#~ msgstr ""

#~ msgid ""
#~ "Generic text-classification endpoint "
#~ "supporting tasks such as sentiment or"
#~ " topic classification."
#~ msgstr "通用文本分类端点，支持情感分类或主题分类等任务。"

#~ msgid "``/v1/reranker``"
#~ msgstr "``/v1/reranker``"

#~ msgid ""
#~ "``{\"query\":\"what is panda? "
#~ "\",\"documents\":[\"hi\",\"The giant panda "
#~ "(Ailuropoda melanoleuca), sometimes called a"
#~ " panda bear or simply panda, is "
#~ "a bear species endemic to "
#~ "China.\",\"gg\"]}``"
#~ msgstr ""

#~ msgid ""
#~ "Rerank a list of retrieved documents "
#~ "by relevance and return the reordered"
#~ " results."
#~ msgstr "根据相关性重新排序检索到的文档列表并返回重新排序的结果。"

#~ msgid ""
#~ "``{\"left\":[\"hello, what is your "
#~ "name?\"],\"right\":[\"hello\",\"what is your "
#~ "name\"],\"embedding_config\":{\"type\":\"sparse\"},\"model\":\"xx\"}``"
#~ msgstr ""

