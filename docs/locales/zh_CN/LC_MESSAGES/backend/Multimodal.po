# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, RTP-LLM
# This file is distributed under the same license as the RTP-LLM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: RTP-LLM 0.2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-20 10:08+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../backend/Multimodal.md:1
msgid "Multimodal Part Debug and Separate Deployment"
msgstr "多模态部分调试和分离部署"

#: ../../backend/Multimodal.md:3
msgid "Multimodal Development"
msgstr "多模态开发"

#: ../../backend/Multimodal.md:5
msgid ""
"A multimodal model refers to an LLM that incorporates multimodal inputs. "
"Currently, the primary input format is URLs, which are distinguished by "
"specific placeholders using OpenAI formatting. Each multimodal model has "
"its unique preprocessing pipeline but must implement the following "
"interfaces:"
msgstr ""
"多模态模型是指包含多模态输入的大语言模型。目前，主要的输入格式是URL，"
"通过使用OpenAI格式的特定占位符来区分。每个多模态模型都有其独特的预处理"
"管道，但必须实现以下接口："

#: ../../backend/Multimodal.md:7
msgid ""
"The multimodal model must inherit from `MultimodalMixin` in "
"`rtp_llm/models/multimodal/multimodal_mixin.py` and instantiate `mm_part`"
" as the processing class for multimodal inputs."
msgstr ""
"多模态模型必须继承`rtp_llm/models/multimodal/multimodal_mixin.py`中的"
"`MultimodalMixin`，并实例化`mm_part`作为多模态输入的处理类。"

#: ../../backend/Multimodal.md:9
msgid ""
"`mm_part` has various interface implementations based on input types, "
"such as images, videos, or audio. The logic must be self-consistent, with"
" the most critical interfaces being `mm_embedding`, `_mm_preprocess`, and"
" `mm_process`:"
msgstr ""
"`mm_part`根据输入类型（如图像、视频或音频）有各种接口实现。逻辑必须自洽，"
"最关键的接口是`mm_embedding`、`_mm_preprocess`和`mm_process`："

#: ../../backend/Multimodal.md:11
msgid ""
"`mm_embedding` has a default implementation that calls `_mm_preprocess` "
"and `mm_process`, converting the multimodal input URL into an embedding "
"tensor and other information (e.g., position IDs)."
msgstr ""
"`mm_embedding`有默认实现，调用`_mm_preprocess`和`mm_process`，将多模态输入URL"
"转换为嵌入张量和其他信息（例如位置ID）。"

#: ../../backend/Multimodal.md:13
msgid ""
"`_mm_preprocess` also has default implementations for specific "
"modalities, preprocessing byte data from input url and preparing inputs "
"for mm_process. This separation is necessary because preprocessing is "
"CPU-bound, while subsequent processing is GPU-bound."
msgstr ""
"`_mm_preprocess`也有针对特定模态的默认实现，预处理来自输入URL的字节数据"
"并为mm_process准备输入。这种分离是必要的，因为预处理受CPU限制，而后续处理受GPU限制。"

#: ../../backend/Multimodal.md:15
msgid ""
"`mm_process` handles GPU-based transformation of preprocessed inputs into"
" outputs."
msgstr ""
"`mm_process`处理基于GPU的预处理输入到输出的转换。"

#: ../../backend/Multimodal.md:17
msgid ""
"For model weights, the required weights must be registered in "
"`GptInitModelParameters` under `mm_related_params.vit_weights`. Refer to "
"`BaseVitWeights` for specific implementation logic."
msgstr ""
"对于模型权重，所需的权重必须在`mm_related_params.vit_weights`下的"
"`GptInitModelParameters`中注册。具体实现逻辑请参考`BaseVitWeights`。"

#: ../../backend/Multimodal.md:19
msgid "Debug"
msgstr "调试"

#: ../../backend/Multimodal.md:21
msgid "Start multimodal part."
msgstr "启动多模态部分。"

#: ../../backend/Multimodal.md:32
msgid "Start a grpc client."
msgstr "启动grpc客户端。"

#: ../../backend/Multimodal.md:80
msgid "hints: Grpc port is START_PORT + 1."
msgstr "提示：Grpc端口是START_PORT + 1。"

