# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, RTP-LLM
# This file is distributed under the same license as the RTP-LLM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: RTP-LLM 0.2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-20 10:08+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../backend/speculative_decoding.md:1
msgid "Speculative Decoding"
msgstr ""

#: ../../backend/speculative_decoding.md:5
msgid "1. What is Speculative Sampling"
msgstr ""

#: ../../backend/speculative_decoding.md:6
msgid ""
"Speculative sampling is a **zero-precision-loss** universal inference "
"acceleration technique:"
msgstr ""

#: ../../backend/speculative_decoding.md:7
msgid ""
"A lightweight **Propose Model** generates several candidate tokens at "
"once;"
msgstr ""

#: ../../backend/speculative_decoding.md:8
msgid ""
"The original **Score Model (large model)** then validates these tokens in"
" parallel;"
msgstr ""

#: ../../backend/speculative_decoding.md:9
msgid ""
"Turning \"verification\" into a Prefill operation, thereby improving GPU "
"compute-to-memory ratio and reducing Decode latency."
msgstr ""

#: ../../backend/speculative_decoding.md:13
msgid "2. Speculative Sampling Algorithms Supported by RTP-LLM"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Name"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Introduction"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Source"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "**vanilla**"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "The most classic speculative sampling implementation"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid ""
"[Leviathan et al., "
"ICML'23](https://proceedings.mlr.press/v202/leviathan23a/leviathan23a.pdf)"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "**deterministic**"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Prompt-Lookup + Speculative Edit"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid ""
"[Prompt Lookup](https://github.com/apoorvumang/prompt-lookup-decoding), "
"[Cursor Blog](https://fireworks.ai/blog/cursor)"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "**mtp**"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Speculative sampling framework based on DeepSeek-V3"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "[DeepSeek-V3 Tech Report](https://arxiv.org/pdf/2412.19437)"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "**eagle3**"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "EAGLE-3"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "[EAGLE-3 Paper](https://arxiv.org/pdf/2503.01840)"
msgstr ""

#: ../../backend/speculative_decoding.md:24
msgid "3. Using Speculative Sampling in RTP-LLM"
msgstr ""

#: ../../backend/speculative_decoding.md:26
msgid ""
"Based on the original basic startup parameters, add the following "
"environment variables:"
msgstr ""

#: ../../backend/speculative_decoding.md
#: ../../backend/speculative_decoding.md:28
#: ../../backend/speculative_decoding.md:96
msgid "vanilla"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Arguments"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Value"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Description"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "--sp_type"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Speculative sampling strategy"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "--sp_checkpoint_path"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "<small model ckpt>"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Small model weight path"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "--sp_model_type"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "qwen"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Small model architecture, same as the main model"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "--sp_quantization"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "FP8_PER_BLOCK/FP8"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Small model quantization method: FP8, FP8_PER_BLOCK, etc."
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "--gen_num_per_cycle"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "5"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "How many tokens the small model proposes per cycle"
msgstr ""

#: ../../backend/speculative_decoding.md
#: ../../backend/speculative_decoding.md:38
#: ../../backend/speculative_decoding.md:103
msgid "deterministic"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "128"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "--sp_min_token_match"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "2"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Minimum length of n-gram token matching"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "--sp_max_token_match"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Maximum length of n-gram token matching"
msgstr ""

#: ../../backend/speculative_decoding.md
#: ../../backend/speculative_decoding.md:56
msgid "mtp"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "qwen_2_mtp"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "MTP small model type"
msgstr ""

#: ../../backend/speculative_decoding.md
#: ../../backend/speculative_decoding.md:66
msgid "eagle3"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "qwen_3_moe_eagle3"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "EAGLE3 small model type"
msgstr ""

#: ../../backend/speculative_decoding.md:78
msgid "4. Performance Observation & Tuning"
msgstr ""

#: ../../backend/speculative_decoding.md:81
msgid "4.1 Performance Observation"
msgstr ""

#: ../../backend/speculative_decoding.md:82
msgid "Add the following to the request body:"
msgstr ""

#: ../../backend/speculative_decoding.md:86
msgid "Example response fields:"
msgstr ""

#: ../../backend/speculative_decoding.md:93
msgid ""
"The most important metric for speculative sampling is "
"avg_tokens_per_iter, higher is better."
msgstr ""

#: ../../backend/speculative_decoding.md:95
msgid "4.2 Tuning"
msgstr ""

#: ../../backend/speculative_decoding.md:97
msgid "**Model Selection**:"
msgstr ""

#: ../../backend/speculative_decoding.md:98
msgid "Choose a smaller size from the same series (e.g., Qwen2.5-0.5B)."
msgstr ""

#: ../../backend/speculative_decoding.md:99
msgid "Apply INT4 quantization to the small model whenever possible."
msgstr ""

#: ../../backend/speculative_decoding.md:100
#: ../../backend/speculative_decoding.md:116
msgid "**gen_num_per_cycle**:"
msgstr ""

#: ../../backend/speculative_decoding.md:101
msgid "Default is 5; can be increased if acceptance rate >40%."
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Parameter"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Recommendation"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Notes"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "sp_min/max_token_match"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "n-gram length range"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "gen_num_per_cycle"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "128 (batch=1)"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Can be increased for long sequence editing scenarios"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "sp_edit"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Set to 1 for code/text editing, 0 otherwise"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Controls matching start point"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "sp_advice_prompt"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Only retain suffixes that may actually appear"
msgstr ""

#: ../../backend/speculative_decoding.md
msgid "Reduce invalid matches"
msgstr ""

#: ../../backend/speculative_decoding.md:112
msgid "mtp / eagle3"
msgstr ""

#: ../../backend/speculative_decoding.md:113
msgid "**Model Training**:"
msgstr ""

#: ../../backend/speculative_decoding.md:114
msgid ""
"Use https://github.com/SafeAILab/EAGLE to train small models for specific"
" business scenarios"
msgstr ""

#: ../../backend/speculative_decoding.md:115
msgid ""
"Need to ensure 1st token acceptance rate >80%, 2nd token acceptance rate "
">60%, 3rd token acceptance rate >40%"
msgstr ""

#: ../../backend/speculative_decoding.md:117
msgid ""
"Execution time of MTP small model can be assumed to be about 1ms. Based "
"on the main model's execution time and acceptance rate, the optimal "
"GEN_NUM_PER_CIRCLE can be calculated"
msgstr ""

#: ../../backend/speculative_decoding.md:118
msgid "**sp_quantization**"
msgstr ""

#: ../../backend/speculative_decoding.md:119
msgid ""
"On Hopper series, it is recommended to enable "
"sp_quantization=FP8_PER_BLOCK"
msgstr ""

