# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, RTP-LLM
# This file is distributed under the same license as the RTP-LLM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: RTP-LLM 0.2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-10-09 17:27+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../backend/MultiGPU.md:1
msgid "Multi-GPU Parallelism"
msgstr ""

#: ../../backend/MultiGPU.md:2
msgid "rtp-llm supports single-node multi-GPU, multi-node single-GPU, and multi-node multi-GPU parallel strategies. Multi-node parallelism requires rtp-llm version >= `0.1.11`."
msgstr ""

#: ../../backend/MultiGPU.md:5
msgid "Single-Node Multi-GPU"
msgstr ""

#: ../../backend/MultiGPU.md:6
msgid "To use single-node multi-GPU parallelism, you need to add environment variables `TP_SIZE` and `WORLD_SIZE` when starting the service. The request logic is consistent with single-GPU, refer to the following command:"
msgstr ""

#: ../../backend/MultiGPU.md:11
msgid "Multi-Node Single/Multi-GPU"
msgstr ""

#: ../../backend/MultiGPU.md:12
msgid "When starting the service, you need to configure the environment variable `DISTRIBUTE_CONFIG_FILE=/path/to/file`. The configuration content is JSON with the following format. The port is optional; if not filled, it is considered the same as the master port:"
msgstr ""

#: ../../backend/MultiGPU.md:28
msgid "The key in JSON and the name in the value should be consistent. The service will establish collective communication with the machine suffixed with `_part0` as rank0. At the same time, you need to configure `WORLD_SIZE`, `TP_SIZE`, and `TP_RANK`."
msgstr ""

#: ../../backend/MultiGPU.md:30
msgid "Multi-Node Single-GPU Startup Command"
msgstr ""

#: ../../backend/MultiGPU.md:31
msgid "rank0:"
msgstr ""

#: ../../backend/MultiGPU.md:35
#: ../../backend/MultiGPU.md:47
msgid "rank1:"
msgstr ""

#: ../../backend/MultiGPU.md:40
msgid "Multi-Node Multi-GPU Startup Command"
msgstr ""

#: ../../backend/MultiGPU.md:41
msgid "When `LOCAL_WORLD_SIZE` > 1, `WORLD_SIZE` % `LOCAL_WORLD_SIZE` == 0 is required. At this time, `LOCAL_WORLD_SIZE` GPUs will be used for inference on each machine. When setting `WORLD_RANK` for machines, it needs to be multiplied by `LOCAL_WORLD_SIZE`."
msgstr ""

#: ../../backend/MultiGPU.md:43
msgid "rank0"
msgstr ""

#: ../../backend/MultiGPU.md:52
msgid "Accessing the Service"
msgstr ""

#: ../../backend/MultiGPU.md:53
msgid "The service endpoint is the Uvicorn Server address of rank0, i.e., http://0.0.0.0:10000"
msgstr ""

#: ../../backend/MultiGPU.md:55
msgid "Common Issues and Solutions"
msgstr ""

#: ../../backend/MultiGPU.md:56
msgid "Startup error `Caught signal 7 (Bus error: nonexistent physical address)` Single-node multi-GPU communication uses shared memory, and this error is due to insufficient shared memory."
msgstr ""

#: ../../backend/MultiGPU.md:58
msgid "If started via Docker, add the `--shm-size=2g` parameter"
msgstr ""

#: ../../backend/MultiGPU.md:59
msgid "If deployed via K8s, shared memory can be set by adding a Memory-type volume to the container."
msgstr ""
